<PAPER>
  <S sid="0">Discriminative Syntactic Language Modeling For Speech Recognition</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We describe a method for discriminative training of a language model that makes use of syntactic features.</S>
    <S sid="2" ssid="2">We follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second &#8220;reranking&#8221; model is then used to choose an utterance from these 1000-best lists.</S>
    <S sid="3" ssid="3">The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.</S>
    <S sid="4" ssid="4">We describe experiments on the Switchboard speech recognition task.</S>
    <S sid="5" ssid="5">The syntactic features provide an additional 0.3% reduction in test&#8211;set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifiat &lt; which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">The predominant approach within language modeling for speech recognition has been to use an ngram language model, within the &#8220;source-channel&#8221; or &#8220;noisy-channel&#8221; paradigm.</S>
    <S sid="7" ssid="2">The language model assigns a probability Pl(w) to each string w in the language; the acoustic model assigns a conditional probability Pa(a|w) to each pair (a, w) where a is a sequence of acoustic vectors, and w is a string.</S>
    <S sid="8" ssid="3">For a given acoustic input a, the highest scoring string under the model is where Q &gt; 0 is some value that reflects the relative importance of the language model; Q is typically chosen by optimization on held-out data.</S>
    <S sid="9" ssid="4">In an n-gram language model, a Markov assumption is made, namely that each word depends only on the previous (n &#8722; 1) words.</S>
    <S sid="10" ssid="5">The parameters of the language model are usually estimated from a large quantity of text data.</S>
    <S sid="11" ssid="6">See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models.</S>
    <S sid="12" ssid="7">This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques.</S>
    <S sid="13" ssid="8">We build on the work in Roark et al. (2004a; 2004b), which was summarized and extended in Roark et al.</S>
    <S sid="14" ssid="9">(2005).</S>
    <S sid="15" ssid="10">These papers used discriminative methods for n-gram language models.</S>
    <S sid="16" ssid="11">Our approach reranks the 1000-best output from the Switchboard recognizer of Ljolje et al. (2003).1 Each candidate string w is parsed using the statistical parser of Collins (1999) to give a parse tree T (w).</S>
    <S sid="17" ssid="12">Information from the parse tree is incorporated in the model using a feature-vector approach: we define 4b(a, w) to be a d-dimensional feature vector which in principle could track arbitrary features of the string w together with the acoustic input a.</S>
    <S sid="18" ssid="13">In this paper we restrict 4b(a, w) to only consider the string w and/or the parse tree T (w) for w. For example, 4b(a, w) might track counts of context-free rule productions in T (w), or bigram lexical dependencies within T (w).</S>
    <S sid="19" ssid="14">The optimal string under our new model is defined as where the arg max is taken over all strings in the 1000-best list, and where a E Rd is a parameter vector specifying the &#8220;weight&#8221; for each feature in -b (note that we define (x, y) to be the inner, or dot 'Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists.</S>
    <S sid="20" ssid="15">The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. product, between vectors x and y).</S>
    <S sid="21" ssid="16">For this paper, we train the parameter vector a using the perceptron algorithm (Collins, 2004; Collins, 2002).</S>
    <S sid="22" ssid="17">The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets.</S>
    <S sid="23" ssid="18">A number of researchers have described work that incorporates syntactic language models into a speech recognizer.</S>
    <S sid="24" ssid="19">These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models.</S>
    <S sid="25" ssid="20">The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words.</S>
    <S sid="26" ssid="21">Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models.</S>
    <S sid="27" ssid="22">In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies.</S>
    <S sid="28" ssid="23">Our approach differs from previous work in a couple of important respects.</S>
    <S sid="29" ssid="24">First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model.</S>
    <S sid="30" ssid="25">We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modification of the underlying generative parsing or tagging model, and modifying a generative model is a rather indirect way of changing the features used by a model.</S>
    <S sid="31" ssid="26">In this respect, our approach is similar to that advocated in Rosenfeld et al. (2001), which used Maximum Entropy modeling to allow for the use of shallow syntactic features for language modeling.</S>
    <S sid="32" ssid="27">A second contrast between our work and previous work, including that of Rosenfeld et al. (2001), is in the use of discriminative parameter estimation techniques.</S>
    <S sid="33" ssid="28">The criterion we use to optimize the parameter vector a is closely related to the end goal in speech recognition, i.e., word error rate.</S>
    <S sid="34" ssid="29">Previous work (Roark et al., 2004a; Roark et al., 2004b) has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features being of the same type as the original language model.</S>
    <S sid="35" ssid="30">In this paper we extend this approach, by including syntactic features that were not in the baseline speech recognizer.</S>
    <S sid="36" ssid="31">This paper describe experiments using a variety of syntactic features within this approach.</S>
    <S sid="37" ssid="32">We tested the model on the Switchboard (SWB) domain, using the recognizer of Ljolje et al. (2003).</S>
    <S sid="38" ssid="33">The discriminative approach for n-gram modeling gave a 0.9% reduction in WER on this domain; the syntactic features we describe give a further 0.3% reduction.</S>
    <S sid="39" ssid="34">In the remainder of this paper, section 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments.</S>
    <S sid="40" ssid="35">Section 4 describes experiments using the approach.</S>
  </SECTION>
  <SECTION title="2 Background" number="2">
    <S sid="41" ssid="1">Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade.</S>
    <S sid="42" ssid="2">Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995).</S>
    <S sid="43" ssid="3">The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right.</S>
    <S sid="44" ssid="4">Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse.</S>
    <S sid="45" ssid="5">In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the particular parse.</S>
    <S sid="46" ssid="6">Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al., 2002; Xu et al., 2003).</S>
    <S sid="47" ssid="7">Roark (2001a; 2001b) pursued a different derivation strategy from Chelba and Jelinek, and used the parse probabilities directly to calculate the string probabilities.</S>
    <S sid="48" ssid="8">This work made use of a left-to-right, top-down, beam-search parser, which exploits rich lexico-syntactic features from the left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution.</S>
    <S sid="49" ssid="9">Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilities of all derivations for that string in the beam.</S>
    <S sid="50" ssid="10">Other work on syntactic language modeling includes that of Charniak (2001), which made use of a non-incremental, head-driven statistical parser to produce string probabilities.</S>
    <S sid="51" ssid="11">In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar, were used to exploit syntactic dependencies.</S>
    <S sid="52" ssid="12">The processing advantages of the finite-state encoding of the model has allowed for the use of probabilities calculated off-line from this model to be used in the first pass of decoding, which has provided additional benefits.</S>
    <S sid="53" ssid="13">Finally, Och et al. (2004) use a reranking approach with syntactic information within a machine translation system.</S>
    <S sid="54" ssid="14">Rosenfeld et al. (2001) investigated the use of syntactic features in a Maximum Entropy approach.</S>
    <S sid="55" ssid="15">In their paper, they used a shallow parser to annotate base constituents, and derived features from sequences of base constituents.</S>
    <S sid="56" ssid="16">The features were indicator features that were either (1) exact matches between a set or sequence of base constituents with those annotated on the hypothesis transcription; or (2) tri-tag features from the constituent sequence.</S>
    <S sid="57" ssid="17">The generative model that resulted from their feature set resulted in only a very small improvement in either perplexity or word-error-rate.</S>
    <S sid="58" ssid="18">We follow the framework of Collins (2002; 2004), recently applied to language modeling in Roark et al. (2004a; 2004b).</S>
    <S sid="59" ssid="19">The model we propose consists of the following components: In principle, the feature vector 4b(a, w) could take into account any features of the acoustic input a together with the utterance w. In this paper we make a couple of restrictions.</S>
    <S sid="60" ssid="20">First, we define the first feature to be -b1(a, w) = Q log Pl(w) + log P,,,(a|w) where Pl(w) and P,,,(a|w) are language and acoustic model scores from the baseline speech recognizer.</S>
    <S sid="61" ssid="21">In our experiments we kept Q fixed at the value used in the baseline recogniser.</S>
    <S sid="62" ssid="22">It can then be seen that our model is equivalent to the model in Eq.</S>
    <S sid="63" ssid="23">2.</S>
    <S sid="64" ssid="24">Second, we restrict the remaining features 'b2(a, w) ...'bd(a, w) to be sensitive to the string w alone.</S>
    <S sid="65" ssid="25">In this sense, the scope of this paper is limited to the language modeling problem.</S>
    <S sid="66" ssid="26">As one example, the language modeling features might take into account n-grams, for example through definitions such as 4b2(a, w) = Count of the the in w Previous work (Roark et al., 2004a; Roark et al., 2004b) considered features of this type.</S>
    <S sid="67" ssid="27">In this paper, we introduce syntactic features, which may be sensitive to the parse tree for w, for example 4g(a, w) = Count of S &#8212;* NP VP in T (w) where S &#8212;* NP VP is a context-free rule production.</S>
    <S sid="68" ssid="28">Section 3 describes the full set of features used in the empirical results presented in this paper.</S>
    <S sid="69" ssid="29">We now describe how the parameter vector &#945;&#65533; is estimated from a set of training utterances.</S>
    <S sid="70" ssid="30">The training set consists of examples (ai, wi) for i = 1... m, where ai is the i&#8217;th acoustic input, and wi is the transcription of this input.</S>
    <S sid="71" ssid="31">We briefly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs).</S>
    <S sid="72" ssid="32">Figure 1 shows the perceptron algorithm.</S>
    <S sid="73" ssid="33">It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example.</S>
    <S sid="74" ssid="34">For a full description of the algorithm, see Collins (2004; 2002).</S>
    <S sid="75" ssid="35">A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model.</S>
    <S sid="76" ssid="36">Similar approaches have been described in Johnson et al. (1999) and Lafferty et al.</S>
    <S sid="77" ssid="37">(2001).</S>
    <S sid="78" ssid="38">The objective function used in optimizing the parameters is Here, each si is the member of GEN(ai) which has lowest WER with respect to the target transcription wi.</S>
    <S sid="79" ssid="39">The first term in L(a) is the log-likelihood of the training data under a conditional log-linear model.</S>
    <S sid="80" ssid="40">The second term is a regularization term which penalizes large parameter values.</S>
    <S sid="81" ssid="41">C is a constant that dictates the relative weighting given to the two terms.</S>
    <S sid="82" ssid="42">The optimal parameters are defined as We refer to these models as global conditional loglinear models (GCLMs).</S>
    <S sid="83" ssid="43">Each of these algorithms has advantages.</S>
    <S sid="84" ssid="44">A number of results&#8212;e.g., in Sha and Pereira (2003) and Roark et al. (2004b)&#8212;suggest that the GCLM approach leads to slightly higher accuracy than the perceptron training method.</S>
    <S sid="85" ssid="45">However the perceptron converges very quickly, often in just a few passes over the training set&#8212;in comparison GCLM&#8217;s can take tens or hundreds of gradient calculations before convergence.</S>
    <S sid="86" ssid="46">In addition, the perceptron can be used as an effective feature selection technique, in that Input: A parameter specifying the number of iterations over the training set, T. A value for the first parameter, &#945;.</S>
    <S sid="87" ssid="47">A feature-vector representation 4)(a, w) E Rd.</S>
    <S sid="88" ssid="48">Training examples (ai, wi) for i = 1 ... m. An n-best list GEN(ai) for each training utterance.</S>
    <S sid="89" ssid="49">We take si to be the member of GEN(ai) which has the lowest WER when compared to wi. at each training example it only increments features seen on si or yi, effectively ignoring all other features seen on members of GEN(ai).</S>
    <S sid="90" ssid="50">For example, in the experiments in Roark et al. (2004a), the perceptron converged in around 3 passes over the training set, while picking non-zero values for around 1.4 million n-gram features out of a possible 41 million n-gram features seen in the training set.</S>
    <S sid="91" ssid="51">For the present paper, to get a sense of the relative effectiveness of various kinds of syntactic features that can be derived from the output of a parser, we are reporting results using just the perceptron algorithm.</S>
    <S sid="92" ssid="52">This has allowed us to explore more of the potential feature space than we would have been able to do using the more costly GCLM estimation techniques.</S>
    <S sid="93" ssid="53">In future we plan to apply GLCM parameter estimation methods to the task.</S>
  </SECTION>
  <SECTION title="3 Parse Tree Features" number="3">
    <S sid="94" ssid="1">We tagged each candidate transcription with (1) part-of-speech tags, using the tagger documented in Collins (2002); and (2) a full parse tree, using the parser documented in Collins (1999).</S>
    <S sid="95" ssid="2">The models for both of these were trained on the Switchboard treebank, and applied to candidate transcriptions in both the training and test sets.</S>
    <S sid="96" ssid="3">Each transcription received one POS-tag annotation and one parse tree annotation, from which features were extracted.</S>
    <S sid="97" ssid="4">Figure 2 shows a Penn Treebank style parse tree that is of the sort produced by the parser.</S>
    <S sid="98" ssid="5">Given such a structure, there is a tremendous amount of flexibility in selecting features.</S>
    <S sid="99" ssid="6">The first approach that we follow is to map each parse tree to sequences encoding part-of-speech (POS) decisions, and &#8220;shallow&#8221; parsing decisions.</S>
    <S sid="100" ssid="7">Similar representations have been used by (Rosenfeld et al., 2001; Wang and Harper, 2002).</S>
    <S sid="101" ssid="8">Figure 3 shows the sequential representations that we used.</S>
    <S sid="102" ssid="9">The first simply makes use of the POS tags for each word.</S>
    <S sid="103" ssid="10">The latter representations make use of sequences of non-terminals associated with lexical items.</S>
    <S sid="104" ssid="11">In 3(b), each word in the string is associated with the beginning or continuation of a shallow phrase or &#8220;chunk&#8221; in the tree.</S>
    <S sid="105" ssid="12">We include any non-terminals above the level of POS tags as potential chunks: a new &#8220;chunk&#8221; (VP, NP, PP etc.) begins whenever we see the initial word of the phrase dominated by the non-terminal.</S>
    <S sid="106" ssid="13">In 3(c), we show how POS tags can be added to these sequences.</S>
    <S sid="107" ssid="14">The final type of sequence mapping, shown in 3(d), makes a similar use of chunks, but preserves only the headword seen with each chunk.3 From these sequences of categories, various features can be extracted, to go along with the n-gram features used in the baseline.</S>
    <S sid="108" ssid="15">These include n-tag features, e.g. ti&#8722;2ti&#8722;1ti (where ti represents the we/PRP helped/VBD her/PRP paint/VB the/DT house/NN we/NPb helped/VPb her/NPb paint/VPb the/NPb house/NPc we/PRP-NPb helped/VBD-VPb her/PRP-NPb paint/VB-VPb the/DT-NPb house/NN-NPc we/NP helped/VP her/NP paint/VP house/NP tag in position i); and composite tag/word features, e.g. tiwi (where wi represents the word in position i) or, more complicated configurations, such as ti&#8722;2ti&#8722;1wi&#8722;1tiwi.</S>
    <S sid="109" ssid="16">These features can be extracted from whatever sort of tag/word sequence we provide for feature extraction, e.g.</S>
    <S sid="110" ssid="17">POS-tag sequences or shallow parse tag sequences.</S>
    <S sid="111" ssid="18">One variant that we performed in feature extraction had to do with how speech repairs (identified as EDITED constituents in the Switchboard style parse trees) and filled pauses or interjections (labeled with the INTJ label) were dealt with.</S>
    <S sid="112" ssid="19">In the simplest version, these are simply treated like other constituents in the parse tree.</S>
    <S sid="113" ssid="20">However, these can disrupt what may be termed the intended sequence of syntactic categories in the utterance, so we also tried skipping these constituents when mapping from the parse tree to shallow parse sequences.</S>
    <S sid="114" ssid="21">The second set of features we employed made use of the full parse tree when extracting features.</S>
    <S sid="115" ssid="22">For this paper, we examined several features templates of this type.</S>
    <S sid="116" ssid="23">First, we considered context-free rule instances, extracted from each local node in the tree.</S>
    <S sid="117" ssid="24">Second, we considered features based on lexical heads within the tree.</S>
    <S sid="118" ssid="25">Let us first distinguish between POS-tags and non-POS non-terminal categories by calling these latter constituents NTs.</S>
    <S sid="119" ssid="26">For each constituent NT in the tree, there is an associated lexical head (HNT) and the POS-tag of that lexical head (HPNT).</S>
    <S sid="120" ssid="27">Two simple features are NT/HNT and NT/HPNT for every NT constituent in the tree.</S>
    <S sid="121" ssid="28">Using the heads as identified in the parser, example features from the tree in figure 2 would be S/VBD, S/helped, NP/NN, and NP/house.</S>
    <S sid="122" ssid="29">Beyond these constituent/head features, we can look at the head-to-head dependencies of the sort used by the parser.</S>
    <S sid="123" ssid="30">Consider each local tree, consisting of a parent node (P), a head child (HCP), and k non-head children (C1 ... CO. For each non-head child CZ, it is either to the left or right of HCP, and is either adjacent or non-adjacent to HCP.</S>
    <S sid="124" ssid="31">We denote these positional features as an integer, positive if to the right, negative if to the left, 1 if adjacent, and 2 if non-adjacent.</S>
    <S sid="125" ssid="32">Table 1 shows four head-to-head features that can be extracted for each non-head child CZ.</S>
    <S sid="126" ssid="33">These features include dependencies between pairs of lexical items, between a single lexical item and the part-of-speech of another item, and between pairs of part-of-speech tags in the parse.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="127" ssid="1">The experimental set-up we use is very similar to that of Roark et al. (2004a; 2004b), and the extensions to that work in Roark et al.</S>
    <S sid="128" ssid="2">(2005).</S>
    <S sid="129" ssid="3">We make use of the Rich Transcription 2002 evaluation test set (rt02) as our development set, and use the Rich Transcription 2003 Spring evaluation CTS test set (rt03) as test set.</S>
    <S sid="130" ssid="4">The rt02 set consists of 6081 sentences (63804 words) and has three subsets: Switchboard 1, Switchboard 2, Switchboard Cellular.</S>
    <S sid="131" ssid="5">The rt03 set consists of 9050 sentences (76083 words) and has two subsets: Switchboard and Fisher.</S>
    <S sid="132" ssid="6">The training set consists of 297580 transcribed utterances (3297579 words)4.</S>
    <S sid="133" ssid="7">For each utterance, a weighted word-lattice was produced, representing alternative transcriptions, from the ASR system.</S>
    <S sid="134" ssid="8">The baseline ASR system that we are comparing against then performed a rescoring pass on these first pass lattices, allowing for better silence modeling, and replaces the trigram language model score with a 6-gram model.</S>
    <S sid="135" ssid="9">1000-best lists were then extracted from these lattices.</S>
    <S sid="136" ssid="10">For each candidate in the 1000best lists, we identified the number of edits (insertions, deletions or substitutions) for that candidate, relative to the &#8220;target&#8221; transcribed utterance.</S>
    <S sid="137" ssid="11">The oracle score for the 1000-best lists was 16.7%.</S>
    <S sid="138" ssid="12">To produce the word-lattices, each training utterance was processed by the baseline ASR system.</S>
    <S sid="139" ssid="13">In a naive approach, we would simply train the baseline system (i.e., an acoustic model and language model) on the entire training set, and then decode the training utterances with this system to produce lattices.</S>
    <S sid="140" ssid="14">We would then use these lattices with the perceptron algorithm.</S>
    <S sid="141" ssid="15">Unfortunately, this approach is likely to produce a set of training lattices that are very different from test lattices, in that they will have very low word-error rates, given that the lattice for each utterance was produced by a model that was trained on that utterance.</S>
    <S sid="142" ssid="16">To somewhat control for this, the training set was partitioned into 28 sets, and baseline Katz backoff trigram models were built for each set by including only transcripts from the other 27 sets.</S>
    <S sid="143" ssid="17">Lattices for each utterance were produced with an acoustic model that had been trained on the entire training set, but with a language model that was trained on the 27 data portions that did not include the current utterance.</S>
    <S sid="144" ssid="18">Since language models are generally far more prone to overtraining than standard acoustic models, this goes a long way toward making the training conditions similar to testing conditions.</S>
    <S sid="145" ssid="19">Similar procedures were used to train the parsing and tagging models for the training set, since the Switchboard treebank overlaps extensively with the ASR training utterances.</S>
    <S sid="146" ssid="20">Table 2 presents the word-error rates on rt02 and rt03 of the baseline ASR system, 1000-best perceptron and GCLM results from Roark et al. (2005) under this condition, and our 1000-best perceptron results.</S>
    <S sid="147" ssid="21">Note that our n-best result, using just ngram features, improves upon the perceptron result of (Roark et al., 2005) by 0.2 percent, putting us within 0.1 percent of their GCLM result for that condition.</S>
    <S sid="148" ssid="22">(Note that the perceptron&#8211;trained n-gram features were trigrams (i.e., n = 3).)</S>
    <S sid="149" ssid="23">This is due to a larger training set being used in our experiments; we have added data that was used as held-out data in (Roark et al., 2005) to the training set that we use.</S>
    <S sid="150" ssid="24">The first additional features that we experimented with were POS-tag sequence derived features.</S>
    <S sid="151" ssid="25">Let ti and wi be the POS tag and word at position i, respectively.</S>
    <S sid="152" ssid="26">We experimented with the following three feature definitions: Table 3 summarizes the results of these trials on the held out set.</S>
    <S sid="153" ssid="27">Using the simple features (number 1 above) yielded an improvement beyond just n-grams, but additional, more complicated features failed to yield additional improvements.</S>
    <S sid="154" ssid="28">Next, we considered features derived from shallow parsing sequences.</S>
    <S sid="155" ssid="29">Given the results from the POS-tag sequence derived features, for any given sequence, we simply use n-tag and tag/word features (number 1 above).</S>
    <S sid="156" ssid="30">The first sequence type from which we extracted features was the shallow parse tag sequence (S1), as shown in figure 3(b).</S>
    <S sid="157" ssid="31">Next, we tried the composite shallow/POS tag sequence (S2), as in figure 3(c).</S>
    <S sid="158" ssid="32">Finally, we tried extracting features from the shallow constituent sequence (S3), as shown in figure 3(d).</S>
    <S sid="159" ssid="33">When EDITED and INTJ nodes are ignored, we refer to this condition as S3-E. For full-parse feature extraction, we tried context-free rule features (CF) and head-to-head features (H2H), of the kind shown in table 1.</S>
    <S sid="160" ssid="34">Table 4 shows the results of these trials on rt02.</S>
    <S sid="161" ssid="35">Although the single digit precision in the table does not show it, the H2H trial, using features extracted from the full parses along with n-grams and POS-tag sequence features, was the best performing model on the held out data, so we selected it for application to the rt03 test data.</S>
    <S sid="162" ssid="36">This yielded 35.2% WER, a reduction of 0.3% absolute over what was achieved with just n-grams, which is significant at p &lt; 0.001,5 reaching a total reduction of 1.2% over the baseline recognizer.</S>
  </SECTION>
</PAPER>
