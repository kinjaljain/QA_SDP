<PAPER>
  <S sid="0">Chinese Named Entity Recognition Combining Statistical Model Wih Human Knowledge</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on.</S>
    <S sid="2" ssid="2">Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation.</S>
    <S sid="3" ssid="3">In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well.</S>
    <S sid="4" ssid="4">In order to avoid data sparseness problem, we employ a back-off model and YI CI CI LIN a Chinese thesaurus, to smooth the parameters in the model.</S>
    <S sid="5" ssid="5">The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">The NER task was first introduced as Message Understanding Conference (MUC) subtask in 1995 (MUC-6).</S>
    <S sid="7" ssid="2">Named Entities were defined as entity names (organizations, persons and locations), temporal expressions (dates and times) and number expressions (monetary values and percentages).</S>
    <S sid="8" ssid="3">Compared with the entity name recognition, the recognition of temporal and number expressions is simpler.</S>
    <S sid="9" ssid="4">So, our research focuses on the recognition of person, location and organization names.</S>
    <S sid="10" ssid="5">The Multilingual NE task first started in 1995(MET-1), including Chinese, Japanese, and Spanish in that year, and continued for Chinese, Japanese in 1998(MET-2).</S>
    <S sid="11" ssid="6">Compared with English NER, Chinese NER is more difficult.</S>
    <S sid="12" ssid="7">We think the main differences between Chinese NER and English NER lie in: First, unlike English, Chinese lacks the capitalization information that plays an important role in signaling named entities.</S>
    <S sid="13" ssid="8">Second, there is no space between words in Chinese, and we have to segment the text before NER.</S>
    <S sid="14" ssid="9">However, the errors in word segmentation will affect the result of NER.</S>
    <S sid="15" ssid="10">Third, Different types of named entities have different structures, especially for abbreviative entities.</S>
    <S sid="16" ssid="11">Therefore, a single unified model can&#8217;t capture all the types of entities.</S>
    <S sid="17" ssid="12">Typical structures of Chinese person name (CN), location name (LN) and organization name (ON) are as follows: CN--&gt;&lt;surname&gt; &lt;given name&gt; LN--&gt;&lt;name part&gt;* &lt;a salient word&gt; ON--&gt;{[person name] [organization name] [place name] [kernel name] }* [organization type] &lt;a salient word&gt; Here &lt;&gt;* means repeating one or several times.</S>
    <S sid="18" ssid="13">{}* means selecting at least one of items.</S>
    <S sid="19" ssid="14">Fourth, there are few openly available resources for Chinese NER.</S>
    <S sid="20" ssid="15">Thus we have to resort to the algorithm that doesn&#8217;t rely on large NER-tagged text corpus.</S>
    <S sid="21" ssid="16">Based on the above analysis, we present a hybrid algorithm that incorporating various types of human knowledge into a statistical model.</S>
    <S sid="22" ssid="17">The innovative points of our paper are as follows.</S>
    <S sid="23" ssid="18">First, the hybrid algorithm can make the best use of existing limited resources to develop an effective NER system.</S>
    <S sid="24" ssid="19">These resources include one-month&#8217;s Chinese People&#8217;s Daily tagged with NER tags by Peking University (which contains about two-million Chinese characters) and various types of human knowledge.</S>
    <S sid="25" ssid="20">Second, in order to compensate for the lack of labeled corpus, we use several types of human knowledge, such as /TONG YI CI CI LIN [Mei.J.J, et al. 1983], a general location names list, the list of the salient words in location name, the list of the salient words in organization names, a Chinese surnames list, the list of Chinese characters that could be included in transliterated person names, and so on.</S>
    <S sid="26" ssid="21">Third, we emphasize that human knowledge and statistical information should be combined very well.</S>
    <S sid="27" ssid="22">For example, a general LN list and a general famous ON list are used in our system.</S>
    <S sid="28" ssid="23">However, we only accept words in the lists as entity candidates with a probability.</S>
    <S sid="29" ssid="24">Whether it is a LN or ON depends on the context.</S>
    <S sid="30" ssid="25">This is different from other systems which accept them as a LN or ON once the system meets them.</S>
    <S sid="31" ssid="26">More details refer to section 4.</S>
    <S sid="32" ssid="27">This paper will be organized as follows.</S>
    <S sid="33" ssid="28">Section 2 is the background of NER.</S>
    <S sid="34" ssid="29">Section 3 describes the class-based statistical baseline Chinese NER model.</S>
    <S sid="35" ssid="30">Section 4 describes different types of human knowledge for different named entities recognitions and how to combine them with a statistical model organically in details.</S>
    <S sid="36" ssid="31">Section 5 is the evaluation and section 6 is the conclusion.</S>
  </SECTION>
  <SECTION title="2 Backgroud" number="2">
    <S sid="37" ssid="1">The researches on English NER have made impressive achievement.</S>
    <S sid="38" ssid="2">The best NER system [Mikheev, et al. 1999] in MUC7 achieved 95% precision and 92% recall.</S>
    <S sid="39" ssid="3">Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al, 1999] and so on.</S>
    <S sid="40" ssid="4">However, Chinese NER is still at its immature phase.</S>
    <S sid="41" ssid="5">Typical Chinese NER systems are as follows.</S>
    <S sid="42" ssid="6">NTU system [Hsin-His Chen, et al. 1997] relied on a statistical model when recognizing person names, but rules when recognizing location and organization names.</S>
    <S sid="43" ssid="7">In the formal run of MET-2, the total F-measure is 79.61%.</S>
    <S sid="44" ssid="8">As a result, they may miss the person names whose probability is lower than the threshold, the location and organization names may also be missed for those which don&#8217;t accord with the rules.</S>
    <S sid="45" ssid="9">[Yu et al. 1998] uses both a contextual model and a morphological model.</S>
    <S sid="46" ssid="10">However, their system requires information of POS tags, semantic tags and NE lists.</S>
    <S sid="47" ssid="11">The system obtains 86.38% Fmeasure.</S>
    <S sid="48" ssid="12">[CHUA et al. 2000] employs a combination of template-based rules supplemented by the defaultexception trees and decision tree that obtains over 91% F-measure on MET-2 test data.</S>
    <S sid="49" ssid="13">It also uses HowNet [Dong &amp; Dong 2000] to cluster semantically related words.</S>
    <S sid="50" ssid="14">[Jian Sun, 2002] presents a class-based language model for Chinese NER which achieves 81.79% F-measure on MET-2 test set and 78.75% F-measure on IEER test data.</S>
    <S sid="51" ssid="15">However, the model heavily depends on statistical information, and must be trained on large labeled corpus.</S>
    <S sid="52" ssid="16">For Chinese NER, we can&#8217;t achieve satisfactory performance if we use only a statistical model or handcrafted heuristic rules.</S>
    <S sid="53" ssid="17">Therefore, we have to resort to the algorithm that can incorporate human knowledge into a statistical model.</S>
    <S sid="54" ssid="18">In the following sections, we will introduce a statistical Chinese NER model first, and then incorporate various types of human knowledge into the statistical model in order to show the power of human knowledge for Chinese NER.</S>
  </SECTION>
  <SECTION title="3 The Baseline Class-based Statistical Model" number="3">
    <S sid="55" ssid="1">We regard NER as a tagging problem.</S>
    <S sid="56" ssid="2">Given a sequence of Chinese string W = w1w2 L wn, the task of NER is to find the most likely sequence of class sequence C* = c1c2 L cm (m &lt;= n) that maximizes the probability P(C  |W).</S>
    <S sid="57" ssid="3">We use Bayes&#8217; Rule to rewrite P(C  |W) as equation (3.1): So, the class-based baseline model can be expressed as equation (3.2).</S>
    <S sid="58" ssid="4">We call P(C) as the contextual model and P(W  |C) as the morphological model.</S>
    <S sid="59" ssid="5">Formally, we can regard such a class-based statistical model as ITMM.</S>
    <S sid="60" ssid="6">The classes used in our model are shown in Due to our small-sized labeled corpus, we use a statistical bi-gram language model as the contextual model.</S>
    <S sid="61" ssid="7">This model can be described as equation (3.3).</S>
    <S sid="62" ssid="8">= 1 Theoretically, trigram is more powerful for NER than bi-gram, however when training corpus is small, trigram can&#8217;t work effectively.</S>
    <S sid="63" ssid="9">Using bigram model, we still need ( )2 probabilities, some of which can&#8217;t be observed in our small-sized labeled corpus and some of which are unauthentic.</S>
    <S sid="64" ssid="10">That is, data sparseness is still serious.</S>
    <S sid="65" ssid="11">We will explain how to resolve data sparseness problem in details in section 3 and 4.</S>
    <S sid="66" ssid="12">TN) is a character-based tri -states unigram model.</S>
    <S sid="67" ssid="13">In principle, Chinese person name is composed of a surname (including single-character surname like &amp;quot; /wu&amp;quot; and double-character surname like&amp;quot; P( /wu The model for three-character-CN recognition is described as equation (3.5).</S>
    <S sid="68" ssid="14">The model for two-character-CN recognition is described as equation (3.6).</S>
    <S sid="69" ssid="15">For location names recognition, we use a wordbased bi-state unigram model, and divide words used in the location name into two parts: locationend-words (LE) and non-location-end words (NLE).</S>
    <S sid="70" ssid="16">That means the probability of the word used in the end position of location name is different fr om that of in other position.</S>
    <S sid="71" ssid="17">The model for location name recognition is shown in equation (3.8). om labeled training corpus. estimated fr i /Ouyang&amp;quot;) and a given name (one or two characters like &amp;quot; /peng&amp;quot; or &amp;quot; /youzheng&amp;quot;).</S>
    <S sid="72" ssid="18">So we divide Chinese name words into three parts as the surname (surCN), the middle name (midCN) and the end name (endCN), which means the probability of a specific character used in different position in person names The model of person names recognition (including Chinese person names abbreviated to CN and Transliterated person names abbreviated to (3.6) where cj = CN)means the probability of emitting the candidate person name under the state of CN.</S>
    <S sid="73" ssid="19">For TN, we divide transliterated name words into several different parts.</S>
    <S sid="74" ssid="20">That is, the probability of a word used in different position in TN is same.</S>
    <S sid="75" ssid="21">The model is as follows.</S>
    <S sid="76" ssid="22">For the model of organization names recognition, we use bi-state unigram that is similar to the location morphological model shown as equation (3.9): where OE means the word used in the end position of organization name, while NOE is not.</S>
    <S sid="77" ssid="23">The parameters in equation (3.9) are also estimated from the labeled training corpus.</S>
    <S sid="78" ssid="24">Data sparseness problem still exists.</S>
    <S sid="79" ssid="25">As some parameters were never observed in trained corpus, the model will back off to a less-powerful model.</S>
    <S sid="80" ssid="26">We employ escape probability to smooth the statistical model [Teahan, et al. 1999].</S>
    <S sid="81" ssid="27">An escape probability is the probability that a previously unseen character will occur.</S>
    <S sid="82" ssid="28">There is no theoretical basis for choosing the escape probability optimally.</S>
    <S sid="83" ssid="29">Here we estimate the escape probability in a particular context as: The probability of a word ci that has occurred c times in that context ci-1 is: While the probability of a word that has never occurred in that context is: where n is the number of times that context has appeared and d is the number of different symbols that have directly followed it.</S>
    <S sid="84" ssid="30">As a example, if we observe the bi-gram &amp;quot;A B&amp;quot; once in training corpus and &#8220;A C&amp;quot; three times, and nowhere else did we see the word &amp;quot;A&amp;quot;, then The Evaluation for the Baseline The baseline model was evaluated in terms of precision (P), recall (R) and F-measure (F) metrics. number of responses number of correct responses number of all NE where &#946; is a weighted constant often set to 1.</S>
    <S sid="85" ssid="31">We test the baseline system on the newswire test data for the 1999 IEER evaluation in Mandarin (http://www.nist.gov/speech/tests/ie-r/er_99/er_ 99. htm).</S>
    <S sid="86" ssid="32">Table 2 in section 4 summarizes the result of baseline model.</S>
  </SECTION>
  <SECTION title="4 The Hybrid Model Incorporating Human Knowledge into the Baseline" number="4">
    <S sid="87" ssid="1">From table 1, we find that the performance of the above statistical baseline model isn&#8217;t satisfactory.</S>
    <S sid="88" ssid="2">The problems mainly lie in: Data sparseness is still serious though we only use bi-gram contextual model, unigram morphological model and smooth the parameters with a back-off model.</S>
    <S sid="89" ssid="3">In order to recognize the named entities, we have to estimate the probability of every word in text as named entities.</S>
    <S sid="90" ssid="4">Thus redundant candidates not only enlarge search space but also result in many unpredictable errors.</S>
    <S sid="91" ssid="5">Abbreviative named entities especially organization abbreviation can&#8217;t be resolved by the baseline model.</S>
    <S sid="92" ssid="6">Because abbreviations have weak statistical regularities, so can&#8217;t be captured by such a baseline model.</S>
    <S sid="93" ssid="7">We try to resolve these problems by incorporating human knowledge.</S>
    <S sid="94" ssid="8">In fact, human being usually uses prior knowledge when recognizing named entities.</S>
    <S sid="95" ssid="9">In this section, we introduce the human knowledge that is used for NER and the method of how to incorporate them into the baseline model.</S>
    <S sid="96" ssid="10">Given a sequence of Chinese characters, the recognition process after combined with human knowledge consists of the five steps shown in Figure1.</S>
    <S sid="97" ssid="11">Chinese person names are composed of a surname and a given name.</S>
    <S sid="98" ssid="12">Usually the characters used for Chinese person names are limited.</S>
    <S sid="99" ssid="13">[Maosong Sun, Changning Huang, 1994] presents 365 most high frequently used surnames cover 99% Chinese surnames.</S>
    <S sid="100" ssid="14">1141 most high frequently used characters cover 99% Chinese given names.</S>
    <S sid="101" ssid="15">Similarly the characters used for transliterated names are also limited.</S>
    <S sid="102" ssid="16">We extract about 476 transliterated characters from the training corpus.</S>
    <S sid="103" ssid="17">The following is the human knowledge used for person name recognition and the method of how to incorporate them into the baseline.</S>
    <S sid="104" ssid="18">A Chinese single and plural surname list: Only those characters in the surname list can trigger person name recognition.</S>
    <S sid="105" ssid="19">A list of person title list: Only when the current character belongs to the surname list and the next word is in the title list, candidates are accepted.</S>
    <S sid="106" ssid="20">A transliterated character list: Only those consecutive characters in the transliterated character list form a candidate transliterated name.</S>
    <S sid="107" ssid="21">Person name can&#8217;t span any punctuation and the length of CN can&#8217;t exceed 8 characters while the length of TN is unrestrained.</S>
    <S sid="108" ssid="22">All these knowledge are used for restricting search space.</S>
    <S sid="109" ssid="23">A complete location name is composed of the name part and a salient word.</S>
    <S sid="110" ssid="24">For the location name &amp;quot; /Beijing City&amp;quot;, the name part is &amp;quot; /Beijing&amp;quot; and the salient word is &amp;quot; /city&amp;quot;.</S>
    <S sid="111" ssid="25">Unfortunately, the salient word is omitted in many occasions.</S>
    <S sid="112" ssid="26">So it is unfeasible to trigger LN recognition only depending on the salient words in location name.</S>
    <S sid="113" ssid="27">In order to improve the precision and recall of LN recognition, we use the following human knowledge.</S>
    <S sid="114" ssid="28">The method of incorporating them is also explained.</S>
    <S sid="115" ssid="29">A general location name list: The list includes the names of Chinese provinces and counties, foreign country and its capitals, some famous geographical names and foreign cities.</S>
    <S sid="116" ssid="30">If the current word is in the list, we accept it as a candidate LN.</S>
    <S sid="117" ssid="31">A location salient word list: If the word w; belongs to the list, 2~6 words before the salient word are accepted as candidate LNs.</S>
    <S sid="118" ssid="32">A general word list (such as verbs and prepositions) which usually is followed by a location name, such as &amp;quot; /at&amp;quot;, &amp;quot; /go&amp;quot;.</S>
    <S sid="119" ssid="33">If the word w; is in the list, 2~6 words following it are accepted as candidate LNs.</S>
    <S sid="120" ssid="34">An abbreviative location name list: If the current word is in the list, we accept it as a candidate LN such as &amp;quot; /China&amp;quot;, &amp;quot; /America&amp;quot;.</S>
    <S sid="121" ssid="35">Coordinate LN recognition: If w;_2 is a candidate LN and w;_1 is &amp;quot; &amp;quot;(a punctuation denoting coordinate relation), LN recognition is triggered at the position of word w;.</S>
    <S sid="122" ssid="36">Location name can&#8217;t span punctuations and its length couldn&#8217;t exceed 6 words.</S>
    <S sid="123" ssid="37">Knowledge , , , , can restrict search space while knowledge deals with abbreviative location name.</S>
    <S sid="124" ssid="38">The organization names recognition is the most difficult task.</S>
    <S sid="125" ssid="39">The reasons lie in nested ONs and abbreviative ONs especially.</S>
    <S sid="126" ssid="40">Nested ON means there are one or more location names, person names and/or organization names embedded in organization name.</S>
    <S sid="127" ssid="41">Typical structure of ON has been given in section 1.</S>
    <S sid="128" ssid="42">We can capture most of the nested organization names by several ON templates mentioned in the following section.</S>
    <S sid="129" ssid="43">Abbreviative ONs include continuous and discrete abbreviation which omits some words in the full name.</S>
    <S sid="130" ssid="44">Take &amp;quot; &amp;quot; as example, abbreviative ON of it may omit LN &amp;quot; /Shanghai&amp;quot;, organization types like&amp;quot; /supermarket&amp;quot;, &amp;quot; /stock&amp;quot;, &amp;quot; /limited&amp;quot;, and salient word like &amp;quot; /company&amp;quot; from full names but usually remains organization kernel &amp;quot; /Hualian&amp;quot;.</S>
    <S sid="131" ssid="45">Table 3 lists some examples of abbreviative ONs.</S>
    <S sid="132" ssid="46">So it is important to extract organization kernel from the full name in order to recognize abbreviative ON like &amp;quot; &amp;quot;.</S>
    <S sid="133" ssid="47">Moreover, an organization's abbreviative names usually occur after its' full name, unless it is a well-known organization.</S>
    <S sid="134" ssid="48">So this strategy for abbreviation organization name recognition is effective.</S>
    <S sid="135" ssid="49">The following is the human knowledge used for ON recognition and the method of how to incorporate them.</S>
    <S sid="136" ssid="50">An organization salient word (OrgSws) list: If the current word w; is in OrgSws list, 2~6 words before OrgSw are accepted as the candidate ONs.</S>
    <S sid="137" ssid="51">A general famous organization name list: If the current word is in the list, we accept it as a candidate ON such as &amp;quot; / State Department&amp;quot;, &amp;quot; / U.N. &amp;quot;.</S>
    <S sid="138" ssid="52">An organization names template list: We mainly use organization name templates to recognize the nested ONs.</S>
    <S sid="139" ssid="53">Some of these templates are as follows:</S>
  </SECTION>
  <SECTION title="ON--&gt;LN D* OrgSw ON--&gt;PN D* OrgSw ON--&gt;ON OrgSw" number="5">
    <S sid="140" ssid="1">D means words used in the middle of organization names.</S>
    <S sid="141" ssid="2">D* means repeating zero or more times.</S>
    <S sid="142" ssid="3">This component runs in the end stage of recognition process shown in Figure 1.</S>
    <S sid="143" ssid="4">An organization type list: The list is used to extract organization kernels from recognized ONs.</S>
    <S sid="144" ssid="5">We have a pool which memorizes ONs recognized in current paragraph and its kernel.</S>
    <S sid="145" ssid="6">If the current word belongs to organization kernel in pool, we accept it as a candidate ON.</S>
    <S sid="146" ssid="7">The idea is effective especially in financial domain which contains many stocks such as&amp;quot; /Shanghai Hualian&amp;quot;, &amp;quot; /Changjiang Technology&amp;quot;.</S>
    <S sid="147" ssid="8">Knowledge , , restrict search space while knowledge deals with abbreviative organization name.</S>
    <S sid="148" ssid="9">/TONG YI CI CI LIN classifies the words in terms of semantic similarity.</S>
    <S sid="149" ssid="10">Here we use it to resolve data sparseness problem.</S>
    <S sid="150" ssid="11">If current transmission probability doesn&#8217;t exist, we resort to its synonym transmission.</S>
    <S sid="151" ssid="12">In statistical sense, synonym transmissions are approximate.</S>
    <S sid="152" ssid="13">Take an example, the probability of P(A|B) doesn&#8217;t exist, but there has P(C|B), meanwhile, the word A and C are thesaurus according to /TONG YI CI CI LIN , then we use P(C|B) to replace P(A|B).</S>
  </SECTION>
  <SECTION title="5 Results of Evaluation" number="6">
    <S sid="153" ssid="1">We also test our hybrid model on IEER-99 neswire test data.</S>
    <S sid="154" ssid="2">The performance is shown in Table 4.</S>
    <S sid="155" ssid="3">Comparing Table 1 with 4, we find that the performance of the hybrid model increases remarkably.</S>
    <S sid="156" ssid="4">More specifically, the precision and the recall of PNs increase from 80.23% to 83.30% and from 89.55% to 92.28% respectively.</S>
    <S sid="157" ssid="5">The precision and recall of LNs increase from 45.05% to 82.18% and from 66.96% to 86.74% respectively.</S>
    <S sid="158" ssid="6">The precision and recall of ONs increase from 42.98% to 80.86% and from 61.45% to 72.09% respectively.</S>
    <S sid="159" ssid="7">The reason that the improvement of PNs is slighter than that of ONs and LNs is that the statistical information estimated from labeled corpus for PNs is good enough but not for LNs and ONs.</S>
    <S sid="160" ssid="8">Must be mentioned is that, in our evaluation, only NEs with both correct boundary and correct type label are considered as the correct recognitions, which is a little different from other evaluation systems.</S>
    <S sid="161" ssid="9">We also test our system on data set of sport, finance, news and entertainment domains.</S>
    <S sid="162" ssid="10">These test data are downloaded from Internet shown in Table 5 shows that the performance on financial domain is much lower.</S>
    <S sid="163" ssid="11">The reason is that, in financial domain, there are many stock names which are the abbreviation of organization names.</S>
    <S sid="164" ssid="12">Moreover, organization full name never appear in the text.</S>
    <S sid="165" ssid="13">So the system can&#8217;t recognize them as an organization name.</S>
    <S sid="166" ssid="14">However, on many occasions, they are recognized as person names.</S>
    <S sid="167" ssid="15">As a result, the precision of PNs declines, meanwhile, the precision and recall of ONs can&#8217;t be high.</S>
    <S sid="168" ssid="16">Based on the above analysis, we find that the main sources of errors in our system are as follows.</S>
    <S sid="169" ssid="17">First, we still have not found a good strategy for the abbreviation location names and organization names.</S>
    <S sid="170" ssid="18">Because abbreviative LNs and ONs sometimes appear before full LN, sometimes not, so the pool strategy can&#8217;t work well.</S>
    <S sid="171" ssid="19">Second, some famous organization names that always appear in the shape of abbreviation can&#8217;t be recognized as ON because the full name never appear such as /GaoTong, /Xinlang.</S>
    <S sid="172" ssid="20">However, these ONs are often recognized as PNs.</S>
    <S sid="173" ssid="21">Such errors are especially serious in finance domain shown Table 5.</S>
    <S sid="174" ssid="22">Third, many words can&#8217;t be found in /TONG YI CI CI LIN .</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="7">
    <S sid="175" ssid="1">Chinese NER is a more difficult task than English NER.</S>
    <S sid="176" ssid="2">Though many approaches have been tried, the result is still not satisfactory.</S>
    <S sid="177" ssid="3">In this paper, we present a hybrid algorithm of incorporating human knowledge into statistical model.</S>
    <S sid="178" ssid="4">Thus we only need a relative small-sized labeled corpus (onemonth&#8217;s Chinese People&#8217;s Daily tagged with NER tags at Peking University) and human knowledge, but can achieve better performance.</S>
    <S sid="179" ssid="5">The main contribution of this paper is putting forward an approach which can make up for the limitation of using the statistical model or human knowledge purely by combining them organically.</S>
    <S sid="180" ssid="6">Our lab was mainly devoted to cross-language information processing and its application.</S>
    <S sid="181" ssid="7">So in the future we will shift our algorithm to other languages.</S>
    <S sid="182" ssid="8">And fine-tune to a specific domain such as sports.</S>
  </SECTION>
  <SECTION title="ACKNOWLEDGEMENT" number="8">
    <S sid="183" ssid="1">This paper is supported by the National &#8220;973&#8221; project G1998030501A-06 and the Natural Science Foundation of China 60272041.</S>
  </SECTION>
</PAPER>
