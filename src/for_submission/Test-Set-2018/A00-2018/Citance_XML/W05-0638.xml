<PAPER>
  <S sid="0">Exploiting Full Parsing Information To Label Semantic Roles Using An Ensemble Of ME And SVM Via Integer Linear Programming</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.</S>
    <S sid="2" ssid="2">In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs.</S>
    <S sid="3" ssid="3">The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%.</S>
    <S sid="4" ssid="4">The ensemble of SVM and ME also boosts the F-score by 0.77%.</S>
    <S sid="5" ssid="5">Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">The Semantic Role Labeling problem can be formulated as a sentence tagging problem.</S>
    <S sid="7" ssid="2">A sentence can be represented as a sequence of words, as phrases (chunks), or as a parsing tree.</S>
    <S sid="8" ssid="3">The basic units of a sentence are words, phrases, and constituents in these representations, respectively.. Pradhan et al. (2004) established that Constituentby-Constituent (C-by-C) is better than Phrase-byPhrase (P-by-P), which is better than Word-byWord (W-by-W).</S>
    <S sid="9" ssid="4">This is probably because the boundaries of the constituents coincide with the arguments; therefore, C-by-C has the highest argument identification F-score among the three approaches.</S>
    <S sid="10" ssid="5">In addition, a full parsing tree also provides richer syntactic information than a sequence of chunks or words.</S>
    <S sid="11" ssid="6">Pradhan et al. (2004) compared the seven most common features as well as several features related to the target constituent&#8217;s parent and sibling constituents.</S>
    <S sid="12" ssid="7">Their experimental results show that using other constituents&#8217; information increases the F-score by 6%.</S>
    <S sid="13" ssid="8">Punyakanok et al. (2004) represent full parsing information as constraints in integer linear programs.</S>
    <S sid="14" ssid="9">Their experimental results show that using such information increases the argument classification accuracy by 1%.</S>
    <S sid="15" ssid="10">In this paper, we not only add more full parsing features to argument classification models, but also represent full parsing information as constraints in integer linear programs (ILP) to resolve label inconsistencies.</S>
    <S sid="16" ssid="11">We also build an ensemble of two argument classification models: Maximum Entropy and SVM by combining their argument classification results and applying them to the abovementioned ILPs.</S>
  </SECTION>
  <SECTION title="2 System Architecture" number="2">
    <S sid="17" ssid="1">Our SRL system is comprised of four stages: pruning, argument classification, classification model incorporation, and integer linear programming.</S>
    <S sid="18" ssid="2">This section describes how we build these stages, including the features used in training the argument classification models.</S>
    <S sid="19" ssid="3">When the full parsing tree of a sentence is available, only the constituents in the tree are considered as argument candidates.</S>
    <S sid="20" ssid="4">In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000).</S>
    <S sid="21" ssid="5">According to Punyakanok et al. (2005), the boundary agreement of Charniak is higher than that of Collins; therefore, we choose the Charniak parser&#8217;s results.</S>
    <S sid="22" ssid="6">However, there are two million nodes on the full parsing trees in the training corpus, which makes the training time of machine learning algorithms extremely long.</S>
    <S sid="23" ssid="7">Besides, noisy information from unrelated parts of a sentence could also affect the training of machine learning models.</S>
    <S sid="24" ssid="8">Therefore, our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are unlikely to be arguments.</S>
    <S sid="25" ssid="9">Applying pruning heuristics to the output of Charniak&#8217;s parser effectively eliminates 61% of the training data and 61.3% of the development data, while still achieves 93% and 85.5% coverage of the correct arguments in the training and development sets, respectively.</S>
    <S sid="26" ssid="10">This stage assigns the final labels to the candidates derived in Section 2.1.</S>
    <S sid="27" ssid="11">A multi-class classifier is trained to classify the types of the arguments supplied by the pruning stage.</S>
    <S sid="28" ssid="12">In addition, to reduce the number of excess candidates mistakenly output by the previous stage, these candidates can be labeled as null (meaning &#8220;not an argument&#8221;).</S>
    <S sid="29" ssid="13">The features used in this stage are as follows.</S>
    <S sid="30" ssid="14">We believe that information from related constituents in the full parsing tree helps in labeling the target constituent.</S>
    <S sid="31" ssid="15">Denote the target constituent by t. The following features are the most common baseline features of t&#8217;s parent and sibling constituents.</S>
    <S sid="32" ssid="16">For example, Parent/ Left Sibling/ Right Sibling Path denotes t&#8217;s parents&#8217;, left sibling&#8217;s, and right sibling&#8217;s Path features.</S>
  </SECTION>
  <SECTION title="Argument Classification Models" number="3">
    <S sid="33" ssid="1">We use all the features of the SVM-based and MEbased argument classification models.</S>
    <S sid="34" ssid="2">All SVM classifiers are realized using SVM-Light with a polynomial kernel of degree 2.</S>
    <S sid="35" ssid="3">The ME-based model is implemented based on Zhang&#8217;s MaxEnt toolkit1 and L-BFGS (Nocedal and Wright, 1999) method to perform parameter estimation.</S>
    <S sid="36" ssid="4">We now explain how we incorporate the SVMbased and ME-based argument classification models.</S>
    <S sid="37" ssid="5">After argument classification, we acquire two scoring matrices, PME and PSVM, respectively.</S>
    <S sid="38" ssid="6">Incorporation of these two models is realized by weighted summation of PME and PSVM as follows: We use P&#8217; for the objective coefficients of the ILP described in Section 2.4.</S>
    <S sid="39" ssid="7">To represent full parsing information as features, there are still several syntactic constraints on a parsing tree in the SRL problem.</S>
    <S sid="40" ssid="8">For example, on a path of the parsing tree, there can be only one constituent annotated as a non-null argument.</S>
    <S sid="41" ssid="9">However, it is difficult to encode this constraint in the argument classification models.</S>
    <S sid="42" ssid="10">Therefore, we apply integer linear programming to resolve inconsistencies produced in the argument classification stage.</S>
    <S sid="43" ssid="11">According to Punyakanok et al. (2004), given a set of constituents, S, and a set of semantic role labels, A, the SRL problem can be formulated as an ILP as follows: Let zia be the indicator variable that represents whether or not an argument, a, is assigned to any Si &#8712; S; and let pia = score(Si = a).</S>
    <S sid="44" ssid="12">The scoring matrix P composed of all pia is calculated by the argument classification models.</S>
    <S sid="45" ssid="13">The goal of this ILP is to find a set of assignments for all zia that maximizes the following function: piazia .</S>
    <S sid="46" ssid="14">Each Si&#8712; S should have one of these argument types, or no type (null).</S>
    <S sid="47" ssid="15">Therefore, we have zia =1.</S>
    <S sid="48" ssid="16">Next, we show how to transform the constraints in the filter function into linear equalities or inequalities, and use them in this ILP.</S>
    <S sid="49" ssid="17">Constraint I: No overlapping or embedding For arguments Sj1 , .</S>
    <S sid="50" ssid="18">.</S>
    <S sid="51" ssid="19">.</S>
    <S sid="52" ssid="20">, Sjk on the same path in a full parsing tree, only one argument can be assigned to an argument type.</S>
    <S sid="53" ssid="21">Thus, at least k &#8722; 1 arguments will be null, which is represented by &#966; in the following linear equality: Constraint II: No duplicate argument classes Within the same sentence, A0-A5 cannot appear more than once.</S>
    <S sid="54" ssid="22">The inequality for A0 is therefore:</S>
  </SECTION>
  <SECTION title="Constraint III: R-XXX arguments" number="4">
    <S sid="55" ssid="1">The linear inequalities that represent A0 and its reference type R-A0 are: The continued argument XXX has to occur before C-XXX.</S>
    <S sid="56" ssid="2">The linear inequalities for A0 are: For each verb, we look up its allowed roles.</S>
    <S sid="57" ssid="3">This constraint is represented by summing all the corresponding indicator variables to 0.</S>
  </SECTION>
  <SECTION title="3 Experiment Results" number="5">
    <S sid="58" ssid="1">The data, which is part of the PropBank corpus, consists of sections from the Wall Street Journal part of the Penn Treebank.</S>
    <S sid="59" ssid="2">All experiments were carried out using Section 2 to Section 21 for training, Section 24 for development, and Section 23 for testing.</S>
    <S sid="60" ssid="3">Unlike CoNLL-2004, part of the Brown corpus is also included in the test set.</S>
    <S sid="61" ssid="4">Table 1 shows that our system makes little difference to the development set and Test WSJ.</S>
    <S sid="62" ssid="5">However, due to the intrinsic difference between the WSJ and Brown corpora, our system performs better on Test WSJ than on Test Brown.</S>
    <S sid="63" ssid="6">From Table 2, we can see that the model with full parsing features outperforms the model without the features in all three performance matrices.</S>
    <S sid="64" ssid="7">After applying ILP, the performance is improved further.</S>
    <S sid="65" ssid="8">We also observe that SVM slightly outperforms ME.</S>
    <S sid="66" ssid="9">However, the hybrid argument classification model achieves the best results in all three metrics.</S>
  </SECTION>
  <SECTION title="4 Conclusion" number="6">
    <S sid="67" ssid="1">In this paper, we add more full parsing features to argument classification models, and represent full parsing information as constraints in ILPs to resolve labeling inconsistencies.</S>
    <S sid="68" ssid="2">We also integrate two argument classification models, ME and SVM, by combining their argument classification results and applying them to the above-mentioned ILPs.</S>
    <S sid="69" ssid="3">The results show full parsing information increases the total F-score by 1.34%.</S>
    <S sid="70" ssid="4">The ensemble of SVM and ME also boosts the F-score by 0.77%.</S>
    <S sid="71" ssid="5">Finally, our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.</S>
  </SECTION>
  <SECTION title="Acknowledgement" number="7">
    <S sid="72" ssid="1">We are indebted to Wen Shong Lin and Prof. Fu Chang for their invaluable advice in data pruning, which greatly speeds up the training of our machine learning models.</S>
  </SECTION>
</PAPER>
