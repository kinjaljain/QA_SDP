<PAPER>
	<S sid="0">Multilingual Dependency Parsing and Domain Adaptation using DeSR</S><ABSTRACT>
		<S sid="1" ssid="1">We describe our experiments using the DeSR parser in the multilingual and do main adaptation tracks of the CoNLL 2007 shared task.</S>
		<S sid="2" ssid="2">DeSR implements an incre mental deterministic Shift/Reduce parsing algorithm, using specific rules to handle non-projective dependencies.</S>
		<S sid="3" ssid="3">For the multi lingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language.</S>
		<S sid="4" ssid="4">For the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">Classifier-based dependency parsers (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) learn from an annotated corpus how to select an appropriate sequence of Shift/Reduce actions to construct the dependency tree for a sentence.</S>
			<S sid="6" ssid="6">Learning is based on techniques such as SVM (Vapnik 1998) or Memory Based Learning (Daelemans 2003), which provide high accuracy but are often computationally expensive.</S>
			<S sid="7" ssid="7">For the multilingual track in the CoNLL 2007 Shared Task, we employed a Shift/Reduce parser which uses a perceptron algorithm with second-order feature maps, in order to verify whether a simpler and faster algorithm can still achieve comparable accuracy.</S>
			<S sid="8" ssid="8">For the domain adaptation track we wished to explore the use of tree revisions in order to incorporate language knowledge from a new domain.</S>
	</SECTION>
	<SECTION title="Multilingual Track. " number="2">
			<S sid="9" ssid="1">The overall parsing algorithm is a deterministic classifier-based statistical parser, which extends the approach by Yamada and Matsumoto (2003), by using different reduction rules that ensure deterministic incremental processing of the input sentence and by adding specific rules for handling non-projective dependencies.</S>
			<S sid="10" ssid="2">The parser also performs dependency labeling within a single processing step.</S>
			<S sid="11" ssid="3">The parser is modular and can use several learning algorithms.</S>
			<S sid="12" ssid="4">The submitted runs used a second order Average Perceptron, derived from the multiclass perceptron of Crammer and Singer (2003).</S>
			<S sid="13" ssid="5">No additional resources were used.</S>
			<S sid="14" ssid="6">No pre processing or post-processing was used, except stemming for English, by means of the Snowball stemmer (Porter 2001).</S>
	</SECTION>
	<SECTION title="Deterministic Classifier-based Parsing. " number="3">
			<S sid="15" ssid="1">DeSR (Attardi, 2006) is an incremental determinis tic classifier-based parser.</S>
			<S sid="16" ssid="2">The parser constructs dependency trees employing a deterministic bot tom-up algorithm which performs Shift/Reduce actions while analyzing input sentences in left-to right order.</S>
			<S sid="17" ssid="3">Using a notation similar to (Nivre and Scholz, 2003), the state of the parser is represented by a 1112 quadruple ?S, I, T, A?, where S is the stack of past tokens, I is the list of (remaining) input tokens, T is a stack of temporary tokens and A is the arc rela tion for the dependency graph.</S>
			<S sid="18" ssid="4">Given an input string W, the parser is initialized to ?(), W, (), ()?, and terminates when it reaches a configuration ?S, (), (), A?.</S>
			<S sid="19" ssid="5">The three basic parsing rule schemas are as fol lows: ?S, n|I, T, A?</S>
			<S sid="20" ssid="6">Shift ?n|S, I, T, A?</S>
			<S sid="21" ssid="7">?s|S, n|I, T, A?</S>
			<S sid="22" ssid="8">Rightd ?S, n|I, T, A?{(s, d, n)}?</S>
			<S sid="23" ssid="9">?s|S, n|I, T, A?</S>
			<S sid="24" ssid="10">Leftd ?S, s|I, T, A?{(n, d, s)}?</S>
			<S sid="25" ssid="11">The schemas for the Left and Right rules are in stantiated for each dependency type d ? D, for a total of 2|D| + 1 rules.</S>
			<S sid="26" ssid="12">These rules perform both attachment and labeling.</S>
			<S sid="27" ssid="13">At each step the parser uses classifiers trained on a treebank corpus in order to predict which action to perform and which dependency label to as sign given the current configuration.</S>
	</SECTION>
	<SECTION title="Non-Projective Relations. " number="4">
			<S sid="28" ssid="1">For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head re peatedly, until the tree becomes pseudo-projective.</S>
			<S sid="29" ssid="2">A post-processing step is then required to restore the arcs to the proper heads.</S>
			<S sid="30" ssid="3">In DeSR non-projective dependencies are handled in a single step by means of the following ad ditional parsing rules, slightly different from those in (Attardi, 2006): ?s1|s2|S, n|I, T, A?</S>
			<S sid="31" ssid="4">Right2d ? S, s1|n|I, T, A?{(s2, d, n)}?</S>
			<S sid="32" ssid="5">?s1|s2|S, n|I, T, A?</S>
			<S sid="33" ssid="6">Left2d ?s2|S, s1|I, T, A?{(n, d, s2)}?</S>
			<S sid="34" ssid="7">?s1|s2|s3|S, n|I, T, A?</S>
			<S sid="35" ssid="8">Right3d ? S, s1|s2|n|I, T, A?{(s3, d, n)}?</S>
			<S sid="36" ssid="9">?s1|s2|s3|S, n|I, T, A?</S>
			<S sid="37" ssid="10">Left3d ?s2|s3|S, s1|I, T, A?{(n, d, s3)}?</S>
			<S sid="38" ssid="11">?s1|s2|S, n|I, T, A?</S>
			<S sid="39" ssid="12">Extract ?n|s1|S, I, s2|T, A?</S>
			<S sid="40" ssid="13">?S, I, s1|T, A?</S>
			<S sid="41" ssid="14">Insert ?s1|S, I, T, A?</S>
			<S sid="42" ssid="15">Left2, Right2 are similar to Left and Right, except that they create links crossing one intermediate node, while Left3 and Right3 cross two intermedi ate nodes.</S>
			<S sid="43" ssid="16">Notice that the RightX actions put back on the input the intervening tokens, allowing the parser to complete the linking of tokens whose processing had been delayed.</S>
			<S sid="44" ssid="17">Extract/Insert gener alize the previous rules by moving one token to the stack T and reinserting the top of T into S.</S>
	</SECTION>
	<SECTION title="Perceptron Learning and 2nd-Order. " number="5">
			<S sid="45" ssid="1">Feature Maps The software architecture of the DeSR parser is modular.</S>
			<S sid="46" ssid="2">Several learning algorithms are available, including SVM, Maximum Entropy, Memory Based Learning, Logistic Regression and a few variants of the perceptron algorithm.</S>
			<S sid="47" ssid="3">We obtained the best accuracy with a multiclass averaged perceptron classifier based on the ultraconservative formulation of Crammer and Singer (2003) with uniform negative updates.</S>
			<S sid="48" ssid="4">The classifier function is: { }xxF k k ?= ?maxarg)( where each parsing action k is associated with a weight vector ?k. To regularize the model the final weight vectors are computed as the average of all weight vectors posited during training.</S>
			<S sid="49" ssid="5">The number of learning iterations over the training data, which is the only adjustable parameter of the algorithm, was determined by cross-validation.</S>
			<S sid="50" ssid="6">In order to overcome the limitations of a linear perceptron, we introduce a feature map ?: IRd ? IRd(d+1)/2 that maps a feature vector x into a higher dimensional feature space consisting of all un ordered feature pairs: ?(x) = ?xixj | i = 1, ?, d, j = i, ?, d?</S>
			<S sid="51" ssid="7">In other words we expand the original representation in the input space with a feature map that generates all second-order feature combinations from each observation.</S>
			<S sid="52" ssid="8">We call this the 2nd-order model, where the inner products are computed as ?k ? ?(x), with ?k a vector of dimen sion d(d+1)/2.</S>
			<S sid="53" ssid="9">Applying a linear perceptron to this feature space corresponds to simulating a polyno mial kernel of degree two.</S>
			<S sid="54" ssid="10">A polynomial kernel of degree two for SVM was also used by Yamada and Matsumoto (2003).</S>
			<S sid="55" ssid="11">However, training SVMs on large data sets like those arising from a big training corpus was too 1113 computationally expensive, forcing them to resort to partitioning the training data (by POS) and to learn several models.</S>
			<S sid="56" ssid="12">Our implementation of the perceptron algorithm uses sparse data structures (hash maps) so that it can handle efficiently even large feature spaces in a single model.</S>
			<S sid="57" ssid="13">For example the feature space for the 2nd-order model for English contains over 21 million.</S>
			<S sid="58" ssid="14">Parsing unseen data can be performed at tens of sentences per second.</S>
			<S sid="59" ssid="15">More details on such aspects of the DeSR parser can be found in (Ci aramita and Attardi 2007).</S>
	</SECTION>
	<SECTION title="Tuning. " number="6">
			<S sid="60" ssid="1">The base parser was tuned on several parameters to optimize its accuracy as follows.</S>
			<S sid="61" ssid="2">6.1 Feature Selection.</S>
			<S sid="62" ssid="3">Given the different characteristics of languages and corpus annotations, it is worth while to select a different set of features for each language.</S>
			<S sid="63" ssid="4">For ex ample, certain corpora do not contain lemmas or morphological information so lexical information will be useful.</S>
			<S sid="64" ssid="5">Vice versa, when lemmas are present, lexical information might be avoided, reduc ing the size of the feature set.</S>
			<S sid="65" ssid="6">We performed a series of feature selection ex periments on each language, starting from a fairly comprehensive set of 43 features and trying all variants obtained by dropping a single feature.</S>
			<S sid="66" ssid="7">The best of these alternatives feature models was cho sen and the process iterated until no further gains were achieved.</S>
			<S sid="67" ssid="8">The score for the alternatives was computed on a development set of approximately 5000 tokens, extracted from a split of the original training corpus.</S>
			<S sid="68" ssid="9">Despite the process is not guaranteed to produce a global optimum, we noticed LAS improvements of up to 4 percentage points on some languages.</S>
			<S sid="69" ssid="10">The set of features to be used by DeSR is con trolled by a number of parameters supplied through a parameter file.</S>
			<S sid="70" ssid="11">Each parameter describes a fea ture and from which tokens to extract it.</S>
			<S sid="71" ssid="12">Tokens are referred through positive numbers for input tokens and negative numbers for tokens on the stack.</S>
			<S sid="72" ssid="13">For example PosFeatures -2 -1 0 1 2 3 means to use the POS tag of the first two tokens on the stack and of the first four tokens on the input.</S>
			<S sid="73" ssid="14">The parameter PosPrev refers to the POS of the preceding token in the original sentence, PosLeftChild refers to the POS of the left chil dren of a token, PastActions tells how many previous actions to include as features.</S>
			<S sid="74" ssid="15">The selection process was started from the fol lowing base feature model: LexFeatures -1 0 1 LemmaFeatures -2 -1 0 1 2 3 LemmaPrev -1 0 LemmaSucc -1 0 LemmaLeftChild -1 0 LemmaRightChild -1 MorphoFeatures -1 0 1 2 PosFeatures -2 -1 0 1 2 3 PosNext -1 0 PosPrev -1 0 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 DepFeatures -1 0 DepLeftChild -1 0 DepRightChild -1 PastActions 1 The selection process produced different variants for each language, sometimes suggesting dropping certain intermediate features, like the lemma of the third next input token in the case of Catalan: LemmaFeatures -2 -1 0 1 3 LemmaPrev 0 LemmaSucc -1 LemmaLeftChild 0 LemmaRightChild -1 PosFeatures -2 -1 0 1 2 3 PosPrev 0 PosSucc -1 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 MorphoFeatures 0 1 DepLeftChild -1 0 DepRightChild -1 For Italian, instead, we ran a series of tests in parallel using a set of manually prepared feature mod els.</S>
			<S sid="75" ssid="16">The best of these models achieved a LAS of 80.95%.</S>
			<S sid="76" ssid="17">The final run used this model with the addition of the morphological agreement feature discussed below.</S>
			<S sid="77" ssid="18">English was the only language for which no feature selection was done and for which lexical features 1114 were used.</S>
			<S sid="78" ssid="19">English is also the language where the official score is significantly lower than what we had been getting on our development set (90.01% UAS).</S>
			<S sid="79" ssid="20">6.2 Prepositional Attachment.</S>
			<S sid="80" ssid="21">Certain languages, such as Catalan, use detailed dependency labeling, that for instance distinguish between adverbials of location and time.</S>
			<S sid="81" ssid="22">We ex ploited this information by introducing a feature that captures the entity type of a child of the top word on the stack or in the input.</S>
			<S sid="82" ssid="23">During training a list of nouns occurring in the corpus as dependent on prepositions with label CCL (meaning ?com plement of location?</S>
			<S sid="83" ssid="24">for Catalan) was created and similarly for CCT (complement of time).</S>
			<S sid="84" ssid="25">The en tity type TIME is extracted as a feature depending on whether the noun occurs in the time list more than ? times than in the location list, and similarly for the feature LOCATION.</S>
			<S sid="85" ssid="26">was set to 1.5 in our experiments.</S>
			<S sid="86" ssid="27">6.3 Morphological Agreement.</S>
			<S sid="87" ssid="28">Certain languages require gender and number agreement between head and dependent.</S>
			<S sid="88" ssid="29">The feature MorphoAgreement is computed for such lan guages and provided noticeable accuracy improvements.</S>
			<S sid="89" ssid="30">For example, for Italian, the improvement was from: LAS: 80.95%, UAS: 85.03% to: LAS: 81.34%, UAS: 85.54% For Catalan, adding this feature we obtained an unofficial score of: LAS: 87.64%, UAS: 92.20% with respect to the official run: LAS: 86.86%, UAS: 91.41%</S>
	</SECTION>
	<SECTION title="Accuracy. " number="7">
			<S sid="90" ssid="1">Table 1 reports the accuracy scores in the multilin gual track.</S>
			<S sid="91" ssid="2">They are all considerably above the average and within 2% from the best for Catalan, 3% for Chinese, Greek, Italian and Turkish.</S>
	</SECTION>
	<SECTION title="Performance. " number="8">
			<S sid="92" ssid="1">The experiments were performed on a 2.4 Ghz AMD Opteron machine with 32 GB RAM.</S>
			<S sid="93" ssid="2">Train ing the parser using the 2nd-order perceptron on the English corpus required less than 3 GB of memory and about one hour for each iteration over the whole dataset.</S>
			<S sid="94" ssid="3">Parsing the English test set required 39.97 sec.</S>
			<S sid="95" ssid="4">For comparison, we tested the MST parser version 0.4.3 (Mstparser, 2007), configured for second-order, on the same data: training took 73.9 minutes to perform 10 iterations and parsing took 97.5 sec.</S>
			<S sid="96" ssid="5">MST parser achieved: LAS: 89.01%, UAS: 90.17%</S>
	</SECTION>
	<SECTION title="Error Analysis on Catalan. " number="9">
			<S sid="97" ssid="1">The parser achieved its best score on Catalan, so we performed an analysis on its output for this lan guage.</S>
			<S sid="98" ssid="2">Among the 42 dependency relations that the parser had to assign to a sentence, the largest num ber of errors occurred assigning CC (124), SP (33), CD (27), SUJ (26), CONJUNCT (22), SN (23).</S>
			<S sid="99" ssid="3">The submitted run for Catalan did not use the entity feature discussed earlier and indeed 67 er rors were due to assigning CCT or CCL instead of CC (generic complement of circumstance).</S>
			<S sid="100" ssid="4">How ever over half of these appear as underspecified annotation errors in the corpus rather than parser errors.</S>
			<S sid="101" ssid="5">By adding the ChildEntityType feature, which distinguishes better between CCT and CCL, the UAS improved, while the LAS dropped slightly, due to the effect of underspecified annota tions in the corpus: LAS: 87.22%, UAS: 91.71% Table 1.</S>
			<S sid="102" ssid="6">Multilingual track official scores.</S>
			<S sid="103" ssid="7">LAS UAS Task 1st DeSR Avg 1st DeSR Avg Arabic 76.52 72.66 68.34 86.09 82.53 78.84 Basque 76.92 69.48 68.06 82.80 76.86 75.15 Catalan 88.70 86.86 79.85 93.40 91.41 87.98 Chinese 84.69 81.50 76.59 88.94 86.73 81.98 Czech 80.19 77.37 70.12 86.28 83.40 77.56 English 89.61 85.85 80.95 90.63 86.99 82.67 Greek 76.31 73.92 70.22 84.08 80.75 77.78 Hungarian 80.27 76.81 71.49 83.55 81.81 76.34 Italian 84.40 81.34 78.06 87.91 85.54 82.45 Turkish 79.81 76.87 73.19 86.22 83.56 80.33 1115 A peculiar aspect of the original Catalan corpus was the use of a large number (195) of dependency labels.</S>
			<S sid="104" ssid="8">These labels were reduced to 42 in the ver sion used for CoNNL 2007, in order to make it comparable to other corpora.</S>
			<S sid="105" ssid="9">However, performing some preliminary experiments using the original Catalan collection with all 195 dependency labels, the DeSR parser achieved a significantly better score: LAS: 88.80%, UAS: 91.43% while with the modified one, the score dropped to: LAS: 84.55%, UAS: 89.38% This suggests that accuracy might improve for other languages as well if the training corpus was labeled with more precise dependencies.</S>
	</SECTION>
	<SECTION title="Adaptation Track. " number="10">
			<S sid="106" ssid="1">The adaptation track originally covered two do mains, the CHILDES and the Chemistry domain.</S>
			<S sid="107" ssid="2">The CHILDES (Brown, 1973; MacWhinney, 2000) consists of transcriptions of dialogues with children, typically short sentences of the kind: Would you like more grape juice ? That 's a nice box of books . Phrases are short, half of them are questions.</S>
			<S sid="108" ssid="3">The only difficulty that appeared from looking at the unlabeled collection supplied for training in the domain was the presence of truncated terms like goin (for going), d (for did), etc. However none of these unusually spelled words appeared in the test set, so a normal English parser performed reasonably well on this task.</S>
			<S sid="109" ssid="4">Because of certain in consistencies in the annotation guidelines, the organizers decided to make this task optional and hence we submitted just the parse produced by the parser trained for English.</S>
			<S sid="110" ssid="5">For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain (Kulick et al 2004) as well as a test set of 5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll).</S>
			<S sid="111" ssid="6">There were three sets of unlabeled documents: we chose the smallest (unlab1) consisting of over 300,000 tokens (11663 sentences).</S>
			<S sid="112" ssid="7">unlab1 was tokenized, POS and lemmas were added using our version of TreeTagger (Schmid, 1994), and lem mas replaced with stems, which had turned out to be more effective than lemmas.</S>
			<S sid="113" ssid="8">We call this set pchemtb_unlab1.conll.</S>
			<S sid="114" ssid="9">We trained the DeSR parser on English using english_ptb_train.conll, the WSJ PTB col lection provided for CoNLL 2007.</S>
			<S sid="115" ssid="10">This consists of WSJ sections 02-11, half of the usual set 02-23, for a total of 460,000 tokens with dependencies gener ated with the converter by Johansson and Nugues (2007).</S>
			<S sid="116" ssid="11">We added stems and produced a parser called DeSRwsj.</S>
			<S sid="117" ssid="12">By parsing eng-lish_pchem_test.conll with DeSRwsj we obtained pchemtb_test_base.desr, our base line for the task.</S>
			<S sid="118" ssid="13">By visual inspection using DgAnnotator (DgAnnotator, 2006), the parses looked generally correct.</S>
			<S sid="119" ssid="14">Most of the errors seemed due to improper handling of conjunctions and disjunctions.</S>
			<S sid="120" ssid="15">The collection in fact contains several phrases like: Specific antibodies raised against P450IIB1 , P450 IA1 or IA2 , P450IIE1 , and P450IIIA2 inhibited the activation in liver microsomes from rats pretreated with PB , BNF , INH and DEX respectively The parser did not seem to have much of a problem with terminology, possibly because the supplied gold POS were adequate.</S>
			<S sid="121" ssid="16">For the adaptation we proceeded as follows.</S>
			<S sid="122" ssid="17">We parsed pchemtb_unlab1.conll using DeSRwsj obtaining pchemtb_unlab1.desr.</S>
			<S sid="123" ssid="18">We then extracted a set of 12,500 sentences from ptb_train.conll and 7,500 sentences from pchemtb_unlab1.desr, creating a corpus of 20,000 sentences called combined.conll.</S>
			<S sid="124" ssid="19">In both cases the selection criteria was to choose sen tences shorter than 30 tokens.</S>
			<S sid="125" ssid="20">We then trained a low accuracy parser (called DesrCombined) on combined.conll, by using a 1st-order averaged perceptron.</S>
			<S sid="126" ssid="21">DesrCombined was used to parse english_ptb_train.conll, the original training corpus for English.</S>
			<S sid="127" ssid="22">By com paring this parse with the original, one can detect where such parser makes mistakes.</S>
			<S sid="128" ssid="23">The rationale for using an inaccurate parser is to obtain parses with many errors so that they form a suitably large training set for the next step: parser revision.</S>
			<S sid="129" ssid="24">We then used a parsing revision technique (At tardi and Ciaramita, 2007) to learn how to correct these errors, producing a parse reviser called DesrReviser.</S>
			<S sid="130" ssid="25">The revision technique consists of comparing the parse trees produced by the parser with the gold standard parse trees, from the annotated corpus.</S>
			<S sid="131" ssid="26">Where a difference is noted, a 1116 revision rule is determined to correct the mistake.</S>
			<S sid="132" ssid="27">Such rules consist in movements of a single link to a different head.</S>
			<S sid="133" ssid="28">Learning how to revise a parse tree consists in training a classifier on a set of training examples consisting of pairs ?(wi, d, wj), ti?, i.e. the link to be modified and the transformation rule to apply.</S>
			<S sid="134" ssid="29">Attardi and Ciaramita (2007) showed that 80% of the corrections can be typically dealt with just 20 tree revision rules.</S>
			<S sid="135" ssid="30">For the adaptation track we limited the training to errors recurring at least 20 times and to 30 rules.</S>
			<S sid="136" ssid="31">DesrReviser was then applied to pchemtb_test_base.desr producing pchemtb_test_rev.desr, our final submission.</S>
			<S sid="137" ssid="32">Many conjunction errors were corrected, in par ticular by moving the head of the sentence from a coordinate verb to the conjunction ?and?</S>
			<S sid="138" ssid="33">linking two coordinate phrases.</S>
			<S sid="139" ssid="34">The revision step produced an improvement of 0.42% LAS over the score achieved by using just the base DeSRwsj parser.</S>
			<S sid="140" ssid="35">Table 2 reports the official accuracy scores on the closed adaptation track.</S>
			<S sid="141" ssid="36">DeSR achieved a close second best UAS on the ptchemtb test set and third best on CHILDES.</S>
			<S sid="142" ssid="37">The results are quite encouraging, particularly considering that the revi sion step does not yet correct the dependency labels and that our base English parser had a lower rank in the multilingual track.</S>
			<S sid="143" ssid="38">LAS UAS Task 1st DeSR Avg 1st DeSR Avg CHILDES 61.37 58.67 57.89 Pchemtb 81.06 80.40 73.03 83.42 83.08 76.42 Table 2.</S>
			<S sid="144" ssid="39">Closed adaptation track scores.</S>
			<S sid="145" ssid="40">Notice that the adaptation process could be iter ated.</S>
			<S sid="146" ssid="41">Since the combination DeSRwsj+DesrReviser is a more accurate parser than DeSRwsj, we could use it again to parse pchemtb_unlab1.conll and so on.</S>
	</SECTION>
	<SECTION title="Conclusions. " number="11">
			<S sid="147" ssid="1">For performing multilingual parsing in the CoNLL 2007 shared task we employed DeSR, a classifier based Shift/Reduce parser.</S>
			<S sid="148" ssid="2">We used a second order averaged perceptron as classifier and achieved accuracy scores quite above the average in all lan guages.</S>
			<S sid="149" ssid="3">For proper comparison with other approaches, one should take into account that the parser is incremental and deterministic; hence it is typically faster than other non linear algorithms.</S>
			<S sid="150" ssid="4">For the adaptation track we used a novel ap proach, based on the technique of tree revision, applied to a parser trained on a corpus combining sentences from both the training and the adaptation domain.</S>
			<S sid="151" ssid="5">The technique achieved quite promising results and it also offers the interesting possibility of being iterated, allowing the parser to incorporate language knowledge from additional domains.</S>
			<S sid="152" ssid="6">Since the technique is applicable to any parser, we plan to test it also with more accurate English parsers.</S>
			<S sid="153" ssid="7">Acknowledgments.</S>
			<S sid="154" ssid="8">The following treebanks were used for training the parser: (Aduriz et al, 2003; B?hmov?</S>
			<S sid="155" ssid="9">et al, 2003; Chen et al, 2003; Ha ji?</S>
			<S sid="156" ssid="10">et al, 2004; Marcus et al, 1993; Mart?</S>
			<S sid="157" ssid="11">et al, 2002; Montemagni et al 2003; Oflazer et al, 2003; Prokopidis et al, 2005; Csendes et al, 2005).</S>
			<S sid="158" ssid="12">Ryan McDonald and Jason Baldridge made available mstparser and helped us using it.</S>
			<S sid="159" ssid="13">We grate fully acknowledge Hugo Zaragoza and Ricardo Baeza-Yates for supporting the first author during a sabbatical at Yahoo!</S>
			<S sid="160" ssid="14">Research Barcelona.</S>
	</SECTION>
</PAPER>
