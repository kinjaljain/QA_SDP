<PAPER>
  <S sid="0">Multilingual Dependency-based Syntactic and Semantic Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">dependency parser.</S>
    <S sid="2" ssid="2">In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li.</S>
    <S sid="3" ssid="3">2008.</S>
    <S sid="4" ssid="4">A cascaded syntactic and semantic dependency parsing system.</S>
    <S sid="5" ssid="5">In Jason Eisner.</S>
    <S sid="6" ssid="6">2000.</S>
    <S sid="7" ssid="7">Bilexical grammars and their cubicparsing algorithms.</S>
    <S sid="8" ssid="8">In in Probabilistic</S>
  </ABSTRACT>
  <SECTION title="1 System Architecture" number="1">
    <S sid="9" ssid="1">Our CoNLL 2009 Shared Task (Haji&#711;c et al., 2009): multilingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling.</S>
  </SECTION>
  <SECTION title="2 Syntactic Dependency Parsing" number="2">
    <S sid="10" ssid="1">We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS).</S>
    <S sid="11" ssid="2">On the other hand, keeping all possible labels for each arc made the decoding inefficient.</S>
    <S sid="12" ssid="3">Therefore, in the system of this year, we adopt approximate techniques to compromise, as shown in the following formulas. flbl arc, and the third parameter indicates the direction.</S>
    <S sid="13" ssid="4">L denotes the whole label set.</S>
    <S sid="14" ssid="5">Then we re-rank the labels by combining the bigram features, and choose K2-best labels.</S>
    <S sid="15" ssid="6">During decoding, we only use the K2 labels chosen for each arc (K2 &#191; K1 &lt; |L|).</S>
    <S sid="16" ssid="7">Following the Eisner (2000) algorithm, we use spans as the basic unit.</S>
    <S sid="17" ssid="8">A span is defined as a substring of the input sentence whose sub-tree is already produced.</S>
    <S sid="18" ssid="9">Only the start or end words of a span can link with other spans.</S>
    <S sid="19" ssid="10">In this way, the algorithm parses the left and the right dependence of a word independently, and combines them in the later stage.</S>
    <S sid="20" ssid="11">We follow McDonald (2006)&#8217;s implementation of first-order Eisner parsing algorithm by modifying its scoring method to incorporate high-order features.</S>
    <S sid="21" ssid="12">Our extended algorithm is shown in Algorithm 1.</S>
    <S sid="22" ssid="13">There are four different span-combining operations.</S>
    <S sid="23" ssid="14">Here we explain two of them that correspond to right-arc (s &lt; t), as shown in Figure 1 and 2.</S>
    <S sid="24" ssid="15">We follow the way of (McDonald, 2006) and (Carreras, 2007) to represent spans.</S>
    <S sid="25" ssid="16">The other two operations corresponding to left-arc are similar. an incomplete span.</S>
    <S sid="26" ssid="17">A complete span means that only the head word can link with other words further, noted as &#8220;&#8212;*&#8221; or &#8220;+&#8212;&#8221;.</S>
    <S sid="27" ssid="18">An incomplete span indicates that both the start and end words of the span will link with other spans in the future, noted as &#8220;--;&#8221; or &#8220;F--&#8221;.</S>
    <S sid="28" ssid="19">In this operation, we combine two smaller spans, sps&#8594;r and spr+1&#8592;t, into sps99Kt with adding arcs&#8594;t.</S>
    <S sid="29" ssid="20">As shown in the following formulas, the score of sps99Kt is composed of three parts: the score of sps&#8594;r, the score of spr+1&#8592;t, and the score of adding arcs&#8594;t.</S>
    <S sid="30" ssid="21">The score of arcs&#8594;t is determined by four different feature sets: unigram features, bigram features, sibling features and left grandchildren features (or inside grandchildren features, meaning that the grandchildren lie between s and t).</S>
    <S sid="31" ssid="22">Note that the sibling features are only related to the nearest sibling node of t, which is denoted as sck here.</S>
    <S sid="32" ssid="23">And the inside grandchildren features are related to all the children of t. This is different from the models used by Carreras (2007) and Johansson and Nugues (2008).</S>
    <S sid="33" ssid="24">They only used the left-most child of t, which is tck, here.</S>
    <S sid="34" ssid="25">In Figure 2 we combine sps99Kr and spr&#8594;t into sps&#8594;t, which explains line 10 in Algorithm 1.</S>
    <S sid="35" ssid="26">The score of sps&#8594;t also includes three parts, as shown in the following formulas.</S>
    <S sid="36" ssid="27">Although there is no new arc added in this operation, the third part is necessary because it reflects the right (or called outside) grandchildren information of arcs&#8594;r.</S>
    <S sid="37" ssid="28">As shown above, features used in our model can be decomposed into four parts: unigram features, bigram features, sibling features, and grandchildren features.</S>
    <S sid="38" ssid="29">Each part can be seen as two different sets: arc-related and label-related features, except sibling features, because we do not consider labels when using sibling features.</S>
    <S sid="39" ssid="30">Arc-related features can be understood as back-off of label-related features.</S>
    <S sid="40" ssid="31">Actually, label-related features are gained by simply attaching the label to the arc-features.</S>
    <S sid="41" ssid="32">The unigram and bigram features used in our model are similar to those of (Che et al., 2008), except that we use bigram label-related features.</S>
    <S sid="42" ssid="33">The sibling features we use are similar to those of (McDonald, 2006), and the grandchildren features are similar to those of (Carreras, 2007).</S>
  </SECTION>
  <SECTION title="3 Predicate Classification" number="3">
    <S sid="43" ssid="1">The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here.</S>
    <S sid="44" ssid="2">The task is divided into four steps:</S>
  </SECTION>
  <SECTION title="4 Semantic Role Labeling" number="4">
    <S sid="45" ssid="1">The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI).</S>
    <S sid="46" ssid="2">During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence to be each semantic role.</S>
    <S sid="47" ssid="3">We add a virtual role &#8220;NULL&#8221; (presenting none of roles is assigned) to the roles set, so we do not need semantic role identification stage anymore.</S>
    <S sid="48" ssid="4">For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role &#8220;NULL&#8221;).</S>
    <S sid="49" ssid="5">The features used in this stage are listed in Table 4.</S>
    <S sid="50" ssid="6">The probability of each word to be a semantic role for a predicate is given by the SRC stage.</S>
    <S sid="51" ssid="7">The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains.</S>
    <S sid="52" ssid="8">As we did in the last year&#8217;s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label &#8220;NULL&#8221;).</S>
    <S sid="53" ssid="9">C2: Roles with a small probability should never be labeled (except for the virtual role &#8220;NULL&#8221;).</S>
    <S sid="54" ssid="10">The threshold we use in our system is 0.3.</S>
    <S sid="55" ssid="11">C3: Statistics show that some roles (except for the virtual role &#8220;NULL&#8221;) usually appear once for a predicate.</S>
    <S sid="56" ssid="12">We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles&#8217; duplication for each single predicate.</S>
    <S sid="57" ssid="13">Table 1 shows the no-duplicate-roles for different languages.</S>
    <S sid="58" ssid="14">Our maximum entropy classifier is implemented with Maximum Entropy Modeling Toolkit1.</S>
    <S sid="59" ssid="15">The classifier parameters are tuned with the development data for different languages respectively. lp solve 5.52 is chosen as our ILP problem solver.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="60" ssid="1">We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul&#180;e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Haji&#711;c et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul&#180;e et al., 2008).</S>
    <S sid="61" ssid="2">Besides the closed challenge, we also submitted the open challenge results.</S>
    <S sid="62" ssid="3">Our open challenge strategy is very simple.</S>
    <S sid="63" ssid="4">We add the SRL development data of each language into their training data.</S>
    <S sid="64" ssid="5">The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data.</S>
    <S sid="65" ssid="6">Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models.</S>
    <S sid="66" ssid="7">During the peak time, Amazon&#8217;s EC2 (Elastic Compute Cloud)3 was used, too.</S>
    <S sid="67" ssid="8">Our system requires 15G memory at most and the longest training time is about 36 hours.</S>
    <S sid="68" ssid="9">During training the predicate classification (PC) and the semantic role labeling (SRL) models, golden syntactic dependency parsing results are used.</S>
    <S sid="69" ssid="10">Previous experiments show that the PC and SRL test results based on golden parse trees are slightly worse than that based on cross trained parse trees.</S>
    <S sid="70" ssid="11">It is, however, a pity that we have no enough time and machines to do cross training for so many languages.</S>
    <S sid="71" ssid="12">In order to examine the performance of the ILP based post inference (PI) for different languages, we adopt a simple PI strategy as baseline, which selects the most likely label (including the virtual label &#8220;NULL&#8221;) except for those duplicate non-virtual labels with lower probabilities (lower than 0.5).</S>
    <S sid="72" ssid="13">Table 2 shows their performance on development data.</S>
    <S sid="73" ssid="14">We can see that the ILP based post inference can improve the precision but decrease the recall.</S>
    <S sid="74" ssid="15">Except for Czech, almost all languages are improved.</S>
    <S sid="75" ssid="16">Among them, English benefits most.</S>
    <S sid="76" ssid="17">The final system results are shown in Table 3.</S>
    <S sid="77" ssid="18">Comparing with our CoNLL 2008 (Che et al., 2008) syntactic parsing results on English4, we can see that our new high-order model improves about 1%.</S>
    <S sid="78" ssid="19">For the open challenge, because we did not modify the syntactic training data, its results are the same as the closed ones.</S>
    <S sid="79" ssid="20">We can, therefore, examine the effect of the additional training data on SRL.</S>
    <S sid="80" ssid="21">We can see that along with the development data are added into the training data, the performance on the indomain test data is increased.</S>
    <S sid="81" ssid="22">However, it is interesting that the additional data is harmful to the ood test.</S>
  </SECTION>
  <SECTION title="6 Conclusion and Future Work" number="6">
    <S sid="82" ssid="1">Our CoNLL 2009 Shared Task system is composed of three cascaded components.</S>
    <S sid="83" ssid="2">The pseudoprojective high-order syntactic dependency model outperforms our CoNLL 2008 model (in English).</S>
    <S sid="84" ssid="3">The additional in-domain (devel) SRL data can help the in-domain test.</S>
    <S sid="85" ssid="4">However, it is harmful to the ood test.</S>
    <S sid="86" ssid="5">Our final system achieves promising results.</S>
    <S sid="87" ssid="6">In the future, we will study how to solve the domain adaptive problem and how to do joint learning between syntactic and semantic parsing.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="88" ssid="1">This work was supported by National Natural Science Foundation of China (NSFC) via grant 60803093, 60675034, and the &#8220;863&#8221; National HighTech Research and Development of China via grant 2008AA01Z144.</S>
  </SECTION>
</PAPER>
