<PAPER>
	<S sid="0">Parsing the SynTagRus Treebank of Russian</S><ABSTRACT>
		<S sid="1" ssid="1">We present the first results on parsing the SYNTAGRUS treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%.</S>
		<S sid="2" ssid="2">A feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features.</S>
		<S sid="3" ssid="3">We conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">Dependency-based syntactic parsing has become increasingly popular in computational linguistics in recent years.</S>
			<S sid="5" ssid="5">One of the reasons for the growinginterest is apparently the belief that dependency based representations should be more suitable for languages that exhibit free or flexible word order and where most of the clues to syntactic structure are found in lexical and morphological features, rather than in syntactic categories and word order configurations.</S>
			<S sid="6" ssid="6">Some support for this view can be found in the results from the CoNLL shared tasks on dependency parsing in 2006 and 2007, where a variety of data-driven methods for dependency parsing have been applied with encouraging resultsto languages of great typological diversity (Buch holz and Marsi, 2006; Nivre et al, 2007a).</S>
			<S sid="7" ssid="7">However, there are still important differences in parsing accuracy for different language types.</S>
			<S sid="8" ssid="8">For ? Joakim Nivre, Igor M. Boguslavsky, and LeonidL.</S>
			<S sid="9" ssid="9">Iomdin, 2008.</S>
			<S sid="10" ssid="10">Licensed under the Creative Com mons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid="11" ssid="11">Some rights reserved.example, Nivre et al (2007a) observe that the lan guages included in the 2007 CoNLL shared taskcan be divided into three distinct groups with re spect to top accuracy scores, with relatively low accuracy for richly inflected languages like Arabic and Basque, medium accuracy for agglutinating languages like Hungarian and Turkish, and high accuracy for more configurational languages like English and Chinese.</S>
			<S sid="12" ssid="12">A complicating factor in thiskind of comparison is the fact that the syntactic an notation in treebanks varies across languages, in such a way that it is very difficult to tease apart the impact on parsing accuracy of linguistic structure, on the one hand, and linguistic annotation, on the other.</S>
			<S sid="13" ssid="13">It is also worth noting that the majority of the data sets used in the CoNLL shared tasks arenot derived from treebanks with genuine depen dency annotation, but have been obtained through conversion from other kinds of annotation.</S>
			<S sid="14" ssid="14">Andthe data sets that do come with original depen dency annotation are generally fairly small, with less than 100,000 words available for training, thenotable exception of course being the Prague De pendency Treebank of Czech (Haji?c et al, 2001), which is one of the largest and most widely used treebanks in the field.This paper contributes to the growing literature on dependency parsing for typologically di verse languages by presenting the first results onparsing the Russian treebank SYNTAGRUS (Bo guslavsky et al, 2000; Boguslavsky et al, 2002).</S>
			<S sid="15" ssid="15">There are several factors that make this treebank an interesting resource in this context.</S>
			<S sid="16" ssid="16">First of all, it contains a genuine dependency annotation,theoretically grounded in the long tradition of dependency grammar for Slavic languages, repre sented by the work of Tesni`ere (1959) andMel??cuk (1988), among others.</S>
			<S sid="17" ssid="17">Secondly, with close to 641 500,000 tokens, the treebank is larger than most other available dependency treebanks and providesa good basis for experimental investigations using data-driven methods.</S>
			<S sid="18" ssid="18">Thirdly, the Russian language, which has not been included in previous ex perimental evaluations such as the CoNLL shared tasks, is a richly inflected language with free wordorder and thus representative of the class of lan guages that tend to pose problems for the currently available parsing models.</S>
			<S sid="19" ssid="19">Taken together, thesefactors imply that experiments using the SYNTA GRUS treebank may be able to shed further light on the complex interplay between language type,annotation scheme, and training set size, as determinants of parsing accuracy for data-driven depen dency parsers.</S>
			<S sid="20" ssid="20">The experimental parsing results presented in this paper have been obtained using MaltParser,a freely available system for data-driven depen dency parsing with state-of-the-art accuracy for most languages in previous evaluations (Buchholz and Marsi, 2006; Nivre et al, 2007a; Nivre et al, 2007b).</S>
			<S sid="21" ssid="21">Besides establishing a first benchmark forthe SYNTAGRUS treebank, we analyze the influence of different kinds of features on parsing ac curacy, showing conclusively that both lexical and morphological features are crucial for obtaininggood parsing accuracy.</S>
			<S sid="22" ssid="22">All results are based on in put with gold standard annotations, which means that the results can be seen to establish an upper bound on what can be achieved when parsing raw text.</S>
			<S sid="23" ssid="23">However, this also means that results are comparable to those from the CoNLL shared tasks,which have been obtained under the same condi tions.</S>
			<S sid="24" ssid="24">The rest of the paper is structured as follows.</S>
			<S sid="25" ssid="25">Section 2 introduces the SYNTAGRUS treebank, section 3 describes the MaltParser system used inthe experiments, and section 4 presents experimental results and analysis.</S>
			<S sid="26" ssid="26">Section 5 contains conclu sions and future work.</S>
	</SECTION>
	<SECTION title="The SYNTAGRUS Treebank. " number="2">
			<S sid="27" ssid="1">The Russian dependency treebank, SYNTAGRUS,is being developed by the Computational Linguistics Laboratory, Institute of Information Trans mission Problems, Russian Academy of Sciences.Currently the treebank contains over 32,000 sen tences (roughly 460,000 words) belonging to texts from a variety of genres (contemporary fiction, popular science, newspaper and journal articles dated between 1960 and 2008, texts of onlinenews, etc.) and it is growing steadily.</S>
			<S sid="28" ssid="2">It is an integral but fully autonomous part of the Russian Na tional Corpus developed in a nationwide research project and can be freely consulted on the Web (http://www.ruscorpora.ru/).</S>
			<S sid="29" ssid="3">Since Russian is a language with relatively freeword order, SYNTAGRUS adopted a dependency based annotation scheme, in a way parallel to the Prague Dependency Treebank (Haji?c et al, 2001).</S>
			<S sid="30" ssid="4">The treebank is so far the only corpus of Russiansupplied with comprehensive morphological anno tation and syntactic annotation in the form of acomplete dependency tree provided for every sen tence.</S>
			<S sid="31" ssid="5">Figure 1 shows the dependency tree for the sentence Naibol~xee vozmuwenie uqastnikov mitinga vyzval prodolawi$is rost cenna benzin, ustanavlivaemyh neftnymi kom panimi (It was the continuing growth of petrol prices set by oil companies that caused the greatest indignation of the participants of the meeting).</S>
			<S sid="32" ssid="6">Inthe dependency tree, nodes represent words (lemmas), annotated with parts of speech and morphological features, while arcs are labeled with syntac tic dependency types.</S>
			<S sid="33" ssid="7">There are over 65 distinct dependency labels in the treebank, half of which are taken from Mel??cuk?s Meaning?Text Theory (Mel??cuk, 1988).</S>
			<S sid="34" ssid="8">Dependency types that are used in figure 1 include: 1.</S>
			<S sid="35" ssid="9">predik (predicative), which, prototypically, represents the relation between the verbalpredicate as head and its subject as depen dent; 2.</S>
			<S sid="36" ssid="10">1-kompl (first complement), which denotes the relation between a predicate word as head and its direct complement as dependent;3.</S>
			<S sid="37" ssid="11">agent (agentive), which introduces the rela tion between a predicate word (verbal noun or verb in the passive voice) as head and its agent in the instrumental case as dependent; 4.</S>
			<S sid="38" ssid="12">kvaziagent (quasi-agentive), which relatesany predicate noun as head with its first syn tactic actant as dependent, if the latter is not eligible for being qualified as the noun?s agent; 5.</S>
			<S sid="39" ssid="13">opred (modifier), which connects a noun head with an adjective/participle dependent if the latter serves as an adjectival modifier to the noun; 642 Figure 1: A syntactically annotated sentence from the SYNTAGRUS treebank.</S>
			<S sid="40" ssid="14">6.</S>
			<S sid="41" ssid="15">predl (prepositional), which accounts for the relation between a preposition as head and a noun as dependent.</S>
			<S sid="42" ssid="16">Dependency trees in SYNTAGRUS may contain non-projective dependencies.</S>
			<S sid="43" ssid="17">Normally, one token corresponds to one node in the dependency tree.There are however a noticeable number of exceptions, the most important of which are the follow ing: 1.</S>
			<S sid="44" ssid="18">compound words like ptidestitany$i (fifty-storied), where one token corresponds to two or more nodes;2.</S>
			<S sid="45" ssid="19">so-called phantom nodes for the representa tion of hard cases of ellipsis, which do notcorrespond to any particular token in the sen tence; for example,  kupil rubaxku, a on galstuk (I bought a shirt and he a tie), which is expanded into  kupil rubaxku, a on kupil PHANTOM galstuk (I bought a shirt and he bought PHANTOM a tie); 3.</S>
			<S sid="46" ssid="20">multiword expressions like po kra$ine$i mere (at least), where several tokens correspond to one node.Syntactic annotation is performed semi automatically: sentences are first processed by the rule-based Russian parser of an advanced NLP system, ETAP-3 (Apresian et al, 2003) and then edited manually by linguists who handle errors of the parser as well as cases of ambiguitythat cannot be reliably resolved without extra linguistic knowledge.</S>
			<S sid="47" ssid="21">The parser processes raw sentences without prior part-of-speech tagging.</S>
			<S sid="48" ssid="22">Morphological annotation in SYNTAGRUS isbased on a comprehensive morphological dictio nary of Russian that counts about 130,000 entries(over 4 million word forms).</S>
			<S sid="49" ssid="23">The ETAP-3 mor phological analyzer uses the dictionary to produce morphological annotation of words belonging to the corpus, including lemma, part-of-speech tag and additional morphological features dependent on the part of speech: animacy, gender, number,case, degree of comparison, short form (of adjectives and participles), representation (of verbs), as pect, tense, mood, person, voice, composite form, and attenuation.</S>
			<S sid="50" ssid="24">Statistics for the version of SYNTAGRUS used for the experiments described in this paper are as follows: ? 32,242 sentences, belonging to the fiction genre (9.8%), texts of online news (12.4%), newspaper and journal articles (77.8%); ? 461,297 tokens, including expressions with non-alphabetical symbols (e.g., 10, 1.200, $333, +70C, #) but excluding punctuation; ? 31,683 distinct word types, of which 635 with a frequency greater than 100, 5041 greater than 10, and 18231 greater than 1; ? 3,414 sentences (10.3%) with non-projective 643 POS DEP MOR LEM LEX TOP + + + + + TOP?1 + HEAD(TOP) + + LDEP(TOP) + RDEP(TOP) + NEXT + + + + NEXT+1 + + + + NEXT+2 + NEXT+3 + LDEP(NEXT) + Table 1: History-based features (TOP = token on top of stack; NEXT = next token in input buffer;HEAD(w) = head of w; LDEP(w) = leftmost depen dent of w; RDEP(w) = leftmost dependent of w).dependencies and 3,934 non-projective de pendency arcs in total; ? 478 sentences (1.5%) containing phantom nodes and 631 phantom nodes in total.</S>
	</SECTION>
	<SECTION title="MaltParser. " number="3">
			<S sid="51" ssid="1">MaltParser (Nivre et al, 2007b) is a language independent system for data-driven dependency parsing, based on a transition-based parsing model (McDonald and Nivre, 2007).</S>
			<S sid="52" ssid="2">More precisely, the approach is based on four essential components: ? A transition-based deterministic algorithm for building labeled projective dependency graphs in linear time (Nivre, 2003).</S>
			<S sid="53" ssid="3">History-based feature models for predicting the next parser action (Black et al, 1992; Magerman, 1995; Ratnaparkhi, 1997).?</S>
			<S sid="54" ssid="4">Discriminative classifiers for mapping histo ries to parser actions (Kudo and Matsumoto, 2002; Yamada and Matsumoto, 2003).?</S>
			<S sid="55" ssid="5">Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005).</S>
			<S sid="56" ssid="6">In the following subsections, we briefly describe each of these four components in turn.</S>
			<S sid="57" ssid="7">3.1 Parsing Algorithm.</S>
			<S sid="58" ssid="8">The parser uses the deterministic algorithm for la beled dependency parsing first proposed by Nivre (2003).</S>
			<S sid="59" ssid="9">The algorithm builds a labeled dependencygraph in one left-to-right pass over the input, us ing a stack to store partially processed tokens and adding arcs using four elementary actions (where TOP is the token on top of the stack and NEXT is the next token): ? Shift: Push NEXT onto the stack.</S>
			<S sid="60" ssid="10">Reduce: Pop the stack.</S>
			<S sid="61" ssid="11">Right-Arc(r): Add an arc labeled r from TOP to NEXT; push NEXT onto the stack.</S>
			<S sid="62" ssid="12">Left-Arc(r): Add an arc labeled r from NEXT to TOP; pop the stack.</S>
			<S sid="63" ssid="13">Parser actions are predicted using a history-based feature model (section 3.2) and SVM classifiers (section 3.3).</S>
			<S sid="64" ssid="14">Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4).</S>
			<S sid="65" ssid="15">3.2 History-Based Feature Models.</S>
			<S sid="66" ssid="16">History-based parsing models rely on features ofthe derivation history to predict the next parser ac tion (Black et al, 1992).</S>
			<S sid="67" ssid="17">The features used are all symbolic and defined in terms of five different node attributes: ? POS = part of speech (atomic) ? DEP = dependency type ? MOR = morphological features (set) ? LEM = lemma ? LEX = word form Features of the type DEP have a special status inthat they are extracted during parsing from the partially built dependency graph and are updated dy namically during parsing.</S>
			<S sid="68" ssid="18">The other four feature types (LEX, LEM, POS, and MOR) are given as part of the input to the parser and remain static duringthe processing of a sentence.</S>
			<S sid="69" ssid="19">Of these four feature types, all except LEX presupposes that the in put has been preprocessed by a lemmatizer, tagger and morphological analyzer, respectively, but forthe experiments reported below we use gold stan dard annotation from the treebank.In order to study the influence of different features, we have experimented with different combinations of the five feature types, where the base line model contains only POS and DEP features, while more complex models add MOR, LEM, and LEX features in different combinations.</S>
			<S sid="70" ssid="20">The exact 644 features included for each feature type are shown in table 1, where rows denote tokens in a parserconfiguration (defined relative to the stack, the re maining input, and the partially built dependency graph), and where columns correspond to feature types.</S>
			<S sid="71" ssid="21">The selection of features in each group wastuned on a development set as described in sec tion 4.</S>
			<S sid="72" ssid="22">3.3 Discriminative Classifiers.</S>
			<S sid="73" ssid="23">We use support vector machines (Vapnik, 1995) to predict the next parser action from a feature vector representing the history.</S>
			<S sid="74" ssid="24">More specifically, we use LIBSVM (Chang and Lin, 2001) with a quadratic kernel K(x i , x j ) = (?x T i x j + r) 2and the builtin one-versus-all strategy for multi-class classifica tion.</S>
			<S sid="75" ssid="25">Symbolic features are converted to numericalfeatures using the standard technique of binariza tion, and we split the set values of MOR features into their atomic components.</S>
			<S sid="76" ssid="26">In order to speed up training, we also divide the training data into smaller bins according to the feature POS of NEXT, and train separate classifiers on each bin.</S>
			<S sid="77" ssid="27">3.4 Pseudo-Projective Parsing.</S>
			<S sid="78" ssid="28">Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser.</S>
			<S sid="79" ssid="29">We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r?h, where r is the original label and h is the label of the original head in the non-projective dependency graph.</S>
			<S sid="80" ssid="30">Non-projective dependencies can be recoveredby an inverse transformation applied to the dependency graph output by the parser, using a left-to right, top-down, breadth-first search, guided by the extended arc labels r?h assigned by the parser.</S>
	</SECTION>
	<SECTION title="Experiments. " number="4">
			<S sid="81" ssid="1">In this section we describe the first experiments onparsing the SYNTAGRUS treebank using a data driven parser.</S>
			<S sid="82" ssid="2">The experimental setup is described in section 4.1, while the experimental results are presented and discussed in section 4.2.</S>
			<S sid="83" ssid="3">4.1 Experimental Setup.</S>
			<S sid="84" ssid="4">All experiments have been performed on the version of SYNTAGRUS described in section 2, con Model Count LAS UAS Base = POS + DEP 46506 60.2 76.0 B1 = Base + MOR 46506 73.0 84.5 B2 = Base + LEM 46506 75.5 84.6 B3 = Base + LEX 46506 74.5 84.6 BM1 = B1 + LEM 46506 82.3 89.0 BM2 = B1 + LEX 46506 81.0 88.8 All = B1 + LEM + LEX 46506 82.3 89.1 Table 2: Parsing accuracy for different feature models on the final test set (Count = Number of tokens in the test set, LAS = Labeled attachment score, UAS = Unlabeled attachment score).</S>
			<S sid="85" ssid="5">verted to the CoNLL data format (Buchholz and Marsi, 2006).</S>
			<S sid="86" ssid="6">1 The available data were divided into 80% for training, 10% for development, and 10% for final testing, using a pseudo-randomized split.</S>
			<S sid="87" ssid="7">The development set was used for tuningparameters of the parsing algorithm and pseudoprojective parsing technique, and for feature selec tion within the feature groups not included in the baseline model (i.e., MOR, LEM, and LEX).</S>
			<S sid="88" ssid="8">The test set was used for evaluating the finally selected models once.The evaluation metrics used are labeled attach ment score (LAS) ? the percentage of tokens that are assigned the correct head and dependency type?</S>
			<S sid="89" ssid="9">and unlabeled attachment score (UAS) ? the per centage of tokens that are assigned the correct head (regardless of dependency type).</S>
			<S sid="90" ssid="10">In addition, wepresent precision and recall for non-projective de pendencies.</S>
			<S sid="91" ssid="11">Punctuation tokens are excluded in all scores, but phantom tokens are included.</S>
			<S sid="92" ssid="12">We use McNemar?s test for statistical significance.</S>
			<S sid="93" ssid="13">4.2 Results and Discussion.</S>
			<S sid="94" ssid="14">Table 2 gives the parsing accuracy for different fea ture models on the held-out test set, measured aslabeled attachment score (LAS) and unlabeled at tachment score (UAS).</S>
			<S sid="95" ssid="15">With respect to LAS, there are statistically significant differences between allmodels except BM1 and All (p  0.01).</S>
			<S sid="96" ssid="16">With respect to UAS, there are statistically significant dif ferences between four groups, such that {Base}  {B1, B2, B3}  {BM2}  {BM1, All}, but there 1 Since SYNTAGRUS only distinguishes ten different parts of speech (not counting morphological features), the fieldsCPOSTAG and POSTAG in the CoNLL format ? for coarse grained and fine-grained parts of speech ? were given the same content.</S>
			<S sid="97" ssid="17">645 are no differences within these groups.</S>
			<S sid="98" ssid="18">2 Looking at the results for different models, we see that while the baseline model (Base) achievesa modest 60.2% LAS and 76.0% UAS, the addi tion of only one additional feature group (B1?B3)boosts unlabeled accuracy by close to ten percent age points and labeled accuracy by up to fifteenpercentage points.</S>
			<S sid="99" ssid="19">Somewhat surprisingly, the dif ferences between models B1?B3 are very small,and only differences with respect to LAS are statis tically significant, which may be taken to suggest that morphological and lexical features capture thesame type of information.</S>
			<S sid="100" ssid="20">However, this hypothe sis is clearly refuted by the results for models BM1 and BM2, where the addition of lexical features on top of morphological features gives a further gain in LAS of eight to ten percentage points (and over four percentage points for UAS).</S>
			<S sid="101" ssid="21">Comparing the use of raw word forms (LEX) and lemmas (LEM) as lexical features, we see a slightadvantage for the latter, at least for labeled accuracy.</S>
			<S sid="102" ssid="22">However, it must be remembered that the experiments are based on gold standard input anno tation, which probably leads to an overestimation of the value of LEM features.</S>
			<S sid="103" ssid="23">Finally, it is worth noting that including both LEX and LEM features does not result in a significant improvement over the model with only LEM features, which may be a sign of saturation, although this may again change in the presence of noisy LEM features.</S>
			<S sid="104" ssid="24">The experimental results show conclusively that both morphological and lexical features are crucial for achieving high parsing accuracy.</S>
			<S sid="105" ssid="25">It may seem that they are most important for labeled accuracy, where the gain in absolute percentage points is the greatest with respect to the baseline, but it must be remembered that the unlabeled scores start at ahigher level, thus leaving less room for improve ment.</S>
			<S sid="106" ssid="26">In fact, the total error reduction from Base to All is over 50% for both LAS and UAS.</S>
			<S sid="107" ssid="27">Table 3 gives a more detailed picture of parsing performance for the best model (All), by breaking down both LAS and UAS by the part-of-speech tag of the dependent.</S>
			<S sid="108" ssid="28">We note that accuracy is higherthan average for nouns (S), adjectives (A), parti cles (PART), and reasonably good for verbs (V).</S>
			<S sid="109" ssid="29">For prepositions (PR), conjunctions (CONJ), and adverbs (ADV), accuracy is considerably lower,which may be attributed to attachment ambigui 2 For the difference BM2  BM1, 0.01  p  0.05; for all other differences, p  0.01.</S>
			<S sid="110" ssid="30">Part of Speech Count LAS UAS S (noun) 7303 86.7 93.3 A (adjective) 7024 92.8 94.2 V (verb) 6946 81.9 85.8 PR (preposition) 5302 60.0 79.0 CONJ (conjunction) 2998 76.1 80.7 ADV (adverb) 2855 72.3 83.3 PART (particle) 1833 88.1 89.6 NUM (numeral) 807 88.7 93.6 NID (foreign word) 142 76.5 91.5 COM (compound) 32 93.8 96.9 P (proposition word) 7 57.1 85.7 INTJ (interjection) 5 0.0 20.0 Table 3: Accuracy by part of speech on the final test set for All features (Count = Number of tokens in the test set, LAS = Labeled attachment score, UAS = Unlabeled attachment score).</S>
			<S sid="111" ssid="31">ties.</S>
			<S sid="112" ssid="32">It is also worth noting that both prepositions and adverbs have considerably higher UAS thanLAS (almost twenty percentage points for prepositions), which shows that even when they are at tached correctly they are are often mislabeled.</S>
			<S sid="113" ssid="33">The remaining parts of speech are too infrequent to warrant any conclusions.Looking specifically at non-projective dependencies, we find that the best model has a la beled precision of 68.8 and a labeled recall of 31.4.</S>
			<S sid="114" ssid="34">The corresponding unlabeled figures are 73.3 and 33.4.</S>
			<S sid="115" ssid="35">3 This confirms the results of previous studiesshowing that the pseudo-projective parsing technique used by MaltParser tends to give high pre cision ? given that non-projective dependencies are among the most difficult to parse correctly ? but rather low recall (McDonald and Nivre, 2007).</S>
			<S sid="116" ssid="36">It is also worth mentioning that phantom tokens,i.e., empty tokens inserted for the analysis of cer tain elliptical constructions (see section 2), have a labeled precision of 82.4 and a labeled recall of 82.8 (89.2 and 89.6 unlabeled), which is very close to the average accuracy, despite being very infrequent.</S>
			<S sid="117" ssid="37">However, it must be remembered that these tokens were given as part of the input in these experiments.</S>
			<S sid="118" ssid="38">In order to correctly analysethese tokens and their dependencies when pars ing raw text, they would have to be recovered in a pre-processing phase along the lines of Dienes 3The precision is the percentage of non-projective depen dencies predicted by the parser that were correct, while the recall is the percentage of true non-projective dependencies that were correctly predicted by the parser.</S>
			<S sid="119" ssid="39">646 and Dubey (2003).Summing up, the main result of the experimental evaluation is that both morphological and lexical features are crucial for attaining high accuracy when training and evaluating on the repre sentations found in the SYNTAGRUS treebank of Russian.</S>
			<S sid="120" ssid="40">With regard to morphological features this is in line with a number of recent studiesshowing the importance of morphology for parsing languages with less rigid word order, includ ing work on Spanish (Cowan and Collins, 2005), Hebrew (Tsarfaty, 2006; Tsarfaty and Sima?an, 2007), Turkish (Eryigit et al, 2006), and Swedish (?vrelid and Nivre, 2007).</S>
			<S sid="121" ssid="41">With regard to lexical features, the situation ismore complex in that there are a number of stud ies questioning the usefulness of lexical features in statistical parsing and arguing that equivalentor better results can be achieved with unlexical ized models provided that linguistic categories can be split flexibly into more fine-grained categories, either using hand-crafted splits, as in the seminalwork of Klein and Manning (2003), or using hid den variables and unsupervised learning, as in the more recent work by Petrov et al (2006), amongothers.</S>
			<S sid="122" ssid="42">There are even studies showing that lexicalization can be harmful when parsing richly in flected languages like German (Dubey and Keller, 2003) and Turkish (Eryi?git and Oflazer, 2006).</S>
			<S sid="123" ssid="43">However, it is worth noting that most of these results have been obtained either for models ofconstituency-based parsing or for models of de pendency parsing suffering from sparse data.</S>
			<S sid="124" ssid="44">4 In the experiments presented here, we have used a transition-based model for dependency parsingthat has much fewer parameters than state-of-the art probabilistic models for constituency parsing.</S>
			<S sid="125" ssid="45">Moreover, we have been able to use a relatively large training set, thereby minimizing the effect ofsparseness for lexical features.</S>
			<S sid="126" ssid="46">We therefore con jecture that the beneficial effect of lexical features on parsing accuracy will generalize to other richly inflected languages when similar conditions hold.</S>
			<S sid="127" ssid="47">As far as we know, these are the first results for a large-scale data-driven parser for Russian.</S>
			<S sid="128" ssid="48">There do exist several rule-based parsers for Russian, such as the ETAP-3 parser (Apresian et al, 2003) and a Link Grammar parser, 5 as well as a prototypeof a hybrid system based on the ETAP-3 parser en 4The latter case applies to the probabilistic model of de pendency parsing explored by Eryi?git and Oflazer (2006).</S>
			<S sid="129" ssid="49">5 http://sz.ru/parser/ riched with statistics extracted from SYNTAGRUS(Boguslavsky et al, 2003; Chardin, 2004), but differences in both input format and output representations make it difficult to compare the perfor mance directly.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="5">
			<S sid="130" ssid="1">We have presented the first results on parsing theSYNTAGRUS treebank of Russian using a data driven dependency parser.</S>
			<S sid="131" ssid="2">Besides establishing a first benchmark for the SYNTAGRUS treebank, we have analyzed the influence of different kindsof features on parsing accuracy, showing conclu sively that both lexical and morphological features are crucial for obtaining good parsing accuracy.</S>
			<S sid="132" ssid="3">We hypothesize that this result can be generalized to other richly inflected languages, provided that sufficient amounts of data are available.Future work includes a deeper analysis of the in fluence of individual features, both morphological and lexical, as well as an evaluation of the parserunder more realistic conditions without gold stan dard annotation in the input.</S>
			<S sid="133" ssid="4">This will require notonly automatic morphological analysis and disambiguation but also a mechanism for inserting so called phantom tokens in elliptical constructions.</S>
			<S sid="134" ssid="5">Acknowledgments We want to thank Ivan Chardin for initiating this collaboration and Jens Nilsson for converting the SYNTAGRUS data to the CoNLL format.</S>
			<S sid="135" ssid="6">We aregrateful to the Russian Foundation of Basic Re search for partial support of this research (grant no. 07-06-00339).</S>
	</SECTION>
</PAPER>
