<PAPER>
	<S sid="0">Structured Prediction Models via the Matrix-Tree Theorem</S><ABSTRACT>
		<S sid="1" ssid="1">This paper provides an algorithmic framework for learning statistical models involv ing directed spanning trees, or equivalently non-projective dependency structures.</S>
		<S sid="2" ssid="2">We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff?s Matrix-Tree Theorem.</S>
		<S sid="3" ssid="3">To demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers.</S>
		<S sid="4" ssid="4">The new training methods give improvements in accuracy over perceptron-trained models.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">Learning with structured data typically involvessearching or summing over a set with an exponen tial number of structured elements, for example theset of all parse trees for a given sentence.</S>
			<S sid="6" ssid="6">Meth ods for summing over such structures include theinside-outside algorithm for probabilistic context free grammars (Baker, 1979), the forward-backward algorithm for hidden Markov models (Baum et al., 1970), and the belief-propagation algorithm for graphical models (Pearl, 1988).</S>
			<S sid="7" ssid="7">These algorithmscompute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex struc tures (e.g., the EM algorithm (Baker, 1979; Baumet al, 1970), contrastive estimation (Smith and Eis ner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al, 2004; Taskar et al, 2004a)).This paper describes inside-outside-style algo rithms for the case of directed spanning trees.</S>
			<S sid="8" ssid="8">Thesestructures are equivalent to non-projective depen dency parses (McDonald et al, 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlyingspanning tree.</S>
			<S sid="9" ssid="9">Unlike the case for projective depen dency structures, partition functions and marginals for non-projective trees cannot be computed usingdynamic-programming methods such as the inside outside algorithm.</S>
			<S sid="10" ssid="10">In this paper we describe howthese quantities can be computed by adapting a wellknown result in graph theory: Kirchhoff?s Matrix Tree Theorem (Tutte, 1984).</S>
			<S sid="11" ssid="11">A na??ve application of the theorem yields O(n4) and O(n6) algorithms for computation of the partition function and marginals,respectively.</S>
			<S sid="12" ssid="12">However, our adaptation finds the par tition function and marginals in O(n3) time using simple matrix determinant and inversion operations.We demonstrate an application of the new inference algorithm to non-projective dependency pars ing.</S>
			<S sid="13" ssid="13">Specifically, we show how to implement two popular supervised learning approaches for this task: globally-normalized log-linear models andmax-margin models.</S>
			<S sid="14" ssid="14">Log-linear estimation critically depends on the calculation of partition func tions and marginals, which can be computed by our algorithms.</S>
			<S sid="15" ssid="15">For max-margin models, Bartlettet al (2004) have provided a simple training algorithm, based on exponentiated-gradient (EG) up dates, that requires computation of marginals and can thus be implemented within our framework.</S>
			<S sid="16" ssid="16">Both of these methods explicitly minimize the loss incurred when parsing the entire training set.</S>
			<S sid="17" ssid="17">This contrasts with the online learning algorithms used inprevious work with spanning-tree models (McDon ald et al, 2005b).We applied the above two marginal-based training algorithms to six languages with varying de grees of non-projectivity, using datasets obtained from the CoNLL-X shared task (Buchholz andMarsi, 2006).</S>
			<S sid="18" ssid="18">Our experimental framework com pared three training approaches: log-linear models, max-margin models, and the averaged perceptron.</S>
			<S sid="19" ssid="19">Each of these was applied to both projective and non-projective parsing.</S>
			<S sid="20" ssid="20">Our results demonstrate thatmarginal-based training yields models which out 141 perform those trained using the averaged perceptron.</S>
			<S sid="21" ssid="21">In summary, the contributions of this paper are: 1.</S>
			<S sid="22" ssid="22">We introduce algorithms for inside-outside-.</S>
			<S sid="23" ssid="23">style calculations for directed spanning trees, orequivalently non-projective dependency struc tures.</S>
			<S sid="24" ssid="24">These algorithms should have wide applicability in learning problems involving spanning-tree structures.log-linear training of dependency parsing mod els, and show improvements in accuracy when compared to averaged-perceptron training.</S>
			<S sid="25" ssid="25">3.</S>
			<S sid="26" ssid="26">We also train max-margin models for depen-.</S>
			<S sid="27" ssid="27">dency parsing via an EG algorithm (Bartlett et al, 2004).</S>
			<S sid="28" ssid="28">The experiments presented here constitute the first application of this algorithmto a large-scale problem.</S>
			<S sid="29" ssid="29">We again show im proved performance over the perceptron.</S>
			<S sid="30" ssid="30">The goal of our experiments is to give a rigorouscomparative study of the marginal-based training algorithms and a highly-competitive baseline, the av eraged perceptron, using the same feature sets for all approaches.</S>
			<S sid="31" ssid="31">We stress, however, that the purpose of this work is not to give competitive performance on the CoNLL data sets; this would require further engineering of the approach.</S>
			<S sid="32" ssid="32">Similar adaptations of the Matrix-Tree Theoremhave been developed independently and simultane ously by Smith and Smith (2007) andMcDonald and Satta (2007); see Section 5 for more discussion.</S>
	</SECTION>
	<SECTION title="Background. " number="2">
			<S sid="33" ssid="1">2.1 Discriminative Dependency Parsing.</S>
			<S sid="34" ssid="2">Dependency parsing is the task of mapping a sentence x to a dependency structure y. Given a sentence x with n words, a dependency for that sen tence is a tuple (h,m) where h ? [0 . . .</S>
			<S sid="35" ssid="3">n] is the index of the head word in the sentence, and m ? [1 . . .</S>
			<S sid="36" ssid="4">n] is the index of a modifier word.</S>
			<S sid="37" ssid="5">The valueh = 0 is a special root-symbol that may only ap pear as the head of a dependency.</S>
			<S sid="38" ssid="6">We use D(x) to refer to all possible dependencies for a sentence x: D(x) = {(h,m) : h ? [0 . . .</S>
			<S sid="39" ssid="7">n],m ? [1 . . .</S>
			<S sid="40" ssid="8">n]}.</S>
			<S sid="41" ssid="9">A dependency parse is a set of dependenciesthat forms a directed tree, with the sentence?s root symbol as its root.</S>
			<S sid="42" ssid="10">We will consider both projective Projective Non-projective Single Root 1 30 2Heroot saw her 1 30 2Heroot saw her Multi Root 1 30 2Heroot saw her 1 30 2Heroot saw herFigure 1: Examples of the four types of dependency struc tures.</S>
			<S sid="43" ssid="11">We draw dependency arcs from head to modifier.</S>
			<S sid="44" ssid="12">trees, where dependencies are not allowed to cross,and non-projective trees, where crossing dependen cies are allowed.</S>
			<S sid="45" ssid="13">Dependency annotations for somelanguages, for example Czech, can exhibit a signifi cant number of crossing dependencies.</S>
			<S sid="46" ssid="14">In addition, we consider both single-root and multi-root trees.</S>
			<S sid="47" ssid="15">In a single-root tree y, the root-symbol has exactly one child, while in a multi-root tree, the root-symbol has one or more children.</S>
			<S sid="48" ssid="16">This distinction is relevant as our training sets include both single-root corpora (in which all trees are single-root structures) andmulti-root corpora (in which some trees are multi root structures).The two distinctions described above are orthog onal, yielding four classes of dependency structures; see Figure 1 for examples of each kind of structure.We use T sp (x) to denote the set of all possible projective single-root dependency structures for a sen tence x, and T snp(x) to denote the set of single-root non-projective structures for x. The sets T mp (x) andT mnp (x) are defined analogously for multi-root struc tures.</S>
			<S sid="49" ssid="17">In contexts where any class of dependency structures may be used, we use the notation T (x) as a placeholder that may be defined as T sp (x), T s np(x), T mp (x) or T m np (x).Following McDonald et al (2005a), we use a discriminative model for dependency parsing.</S>
			<S sid="50" ssid="18">Fea tures in the model are defined through a function f(x, h,m) which maps a sentence x together with a dependency (h,m) to a feature vector in Rd. A feature vector can be sensitive to any properties of the triple (x, h,m).</S>
			<S sid="51" ssid="19">Given a parameter vector w, the optimal dependency structure for a sentence x is y?(x;w) = argmax y?T (x) ?</S>
			<S sid="52" ssid="20">(h,m)?y w ? f(x, h,m) (1) where the set T (x) can be defined as T sp (x), T s np(x), T mp (x) or T m np (x), depending on the type of parsing.</S>
			<S sid="53" ssid="21">142The parameters w will be learned from a train ing set {(xi, yi)}Ni=1 where each xi is a sentence andeach yi is a dependency structure.</S>
			<S sid="54" ssid="22">Much of the previous work on learningw has focused on training lo cal models (see Section 5).</S>
			<S sid="55" ssid="23">McDonald et al (2005a;2005b) trained global models using online algo rithms such as the perceptron algorithm or MIRA.</S>
			<S sid="56" ssid="24">In this paper we consider training algorithms basedon work in conditional random fields (CRFs) (Laf ferty et al, 2001) and max-margin methods (Taskar et al, 2004a).</S>
			<S sid="57" ssid="25">2.2 Three Inference Problems.</S>
			<S sid="58" ssid="26">This section highlights three inference problems which arise in training and decoding discriminativedependency parsers, and which are central to the ap proaches described in this paper.</S>
			<S sid="59" ssid="27">Assume that we have a vector ? with values?h,m ? R for all (h,m) ? D(x); these values cor respond to weights on the different dependencies inD(x).</S>
			<S sid="60" ssid="28">Define a conditional distribution over all de pendency structures y ? T (x) as follows: P (y |x;?)</S>
			<S sid="61" ssid="29">= exp {?</S>
			<S sid="62" ssid="30">(h,m)?y ?h,m } Z(x;?)</S>
			<S sid="63" ssid="31">(2) Z(x;?)</S>
			<S sid="64" ssid="32">= ? y?T (x) exp ? ?</S>
			<S sid="65" ssid="33">(h,m)?y ?h,m ? ?</S>
			<S sid="66" ssid="34">(3) The function Z(x;?)</S>
			<S sid="67" ssid="35">is commonly referred to as the partition function.</S>
			<S sid="68" ssid="36">Given the distribution P (y |x;?), we can define the marginal probability of a dependency (h,m) as ?h,m(x;?)</S>
			<S sid="69" ssid="37">= ? y?T (x) : (h,m)?y P (y |x;?)</S>
			<S sid="70" ssid="38">The inference problems are then as follows: Problem 1: Decoding: Find argmaxy?T (x) ?</S>
			<S sid="71" ssid="39">(h,m)?y ?h,mProblem 2: Computation of the Partition Func tion: Calculate Z(x;?).</S>
			<S sid="72" ssid="40">Problem 3: Computation of the Marginals: For all (h,m) ? D(x), calculate ?h,m(x;?).Note that all three problems require a maximization or summation over the set T (x), which is ex ponential in size.</S>
			<S sid="73" ssid="41">There is a clear motivation for being able to solve Problem 1: by setting ?h,m = w ? f(x, h,m), the optimal dependency structure y?(x;w) (see Eq.</S>
			<S sid="74" ssid="42">1) can be computed.</S>
			<S sid="75" ssid="43">In this paper the motivation for solving Problems 2 and 3 arises from training algorithms for discriminative models.</S>
			<S sid="76" ssid="44">As we will describe in Section 4, both log-linear and max-margin models can be trained via methods that make direct use of algorithms for Problems 2 and 3.</S>
			<S sid="77" ssid="45">In the case of projective dependency structures (i.e., T (x) defined as T sp (x) or T m p (x)), there arewell-known algorithms for all three inference problems.</S>
			<S sid="78" ssid="46">Decoding can be carried out using Viterbistyle dynamic-programming algorithms, for example the O(n3) algorithm of Eisner (1996).</S>
			<S sid="79" ssid="47">Com putation of the marginals and partition function can also be achieved in O(n3) time, using a variant of the inside-outside algorithm (Baker, 1979) applied to the Eisner (1996) data structures (Paskin, 2001).</S>
			<S sid="80" ssid="48">In the non-projective case (i.e., T (x) defined as T snp(x) or T mnp (x)), McDonald et al (2005b) de scribe how the CLE algorithm (Chu and Liu, 1965;Edmonds, 1967) can be used for decoding.</S>
			<S sid="81" ssid="49">How ever, it is not possible to compute the marginalsand partition function using the inside-outside algo rithm.</S>
			<S sid="82" ssid="50">We next describe a method for computing these quantities in O(n3) time using matrix inverse and determinant operations.</S>
	</SECTION>
	<SECTION title="Spanning-tree inference using the. " number="3">
			<S sid="83" ssid="1">Matrix-Tree Theorem In this section we present algorithms for computing the partition function and marginals, as defined inSection 2.2, for non-projective parsing.</S>
			<S sid="84" ssid="2">We first re iterate the observation of McDonald et al (2005a) that non-projective parses correspond to directed spanning trees on a complete directed graph of n nodes, where n is the length of the sentence.</S>
			<S sid="85" ssid="3">The above inference problems thus involve summation over the set of all directed spanning trees.</S>
			<S sid="86" ssid="4">Note thatthis set is exponentially large, and there is no obvious method for decomposing the sum into dynamicprogramming-like subproblems.</S>
			<S sid="87" ssid="5">This section de scribes how a variant of Kirchhoff?s Matrix-Tree Theorem (Tutte, 1984) can be used to evaluate the partition function and marginals efficiently.In what follows, we consider the single-root set ting (i.e., T (x) = T snp(x)), leaving the multi-root 143 case (i.e., T (x) = T mnp (x)) to Section 3.3.</S>
			<S sid="88" ssid="6">For a sentence x with n words, define a complete directed graph G on n nodes, where each node correspondsto a word in x, and each edge corresponds to a de pendency between two words in x. Note thatG doesnot include the root-symbol h = 0, nor does it ac count for any dependencies (0,m) headed by the root-symbol.</S>
			<S sid="89" ssid="7">We assign non-negative weights to the edges of this graph, yielding the following weighted adjacency matrix A(?)</S>
			<S sid="90" ssid="8">Rn?n, for h,m = 1 . . .</S>
			<S sid="91" ssid="9">n: Ah,m(?)</S>
			<S sid="92" ssid="10">= { 0, if h = m exp {?h,m} , otherwise To account for the dependencies (0,m) headed by the root-symbol, we define a vector of root-selection scores r(?)</S>
			<S sid="93" ssid="11">Rn, form = 1 . . .</S>
			<S sid="94" ssid="12">n: rm(?)</S>
			<S sid="95" ssid="13">= exp {?0,m} Let the weight of a dependency structure y ? T snp(x) be defined as: ?(y;?)</S>
			<S sid="96" ssid="14">= rroot(y)(?)</S>
			<S sid="97" ssid="15">(h,m)?y : h 6=0 Ah,m(?)</S>
			<S sid="98" ssid="16">Here, root(y) = m : (0,m) ? y is the child of the root-symbol; there is exactly one such child, since y ? T snp(x).</S>
			<S sid="99" ssid="17">Eq.</S>
			<S sid="100" ssid="18">2 and 3 can be rephrased as: P (y |x;?)</S>
			<S sid="101" ssid="19">= ?(y;?)</S>
			<S sid="102" ssid="20">Z(x;?)</S>
			<S sid="103" ssid="21">(4) Z(x;?)</S>
			<S sid="104" ssid="22">= ? y?T snp(x) ?(y;?)</S>
			<S sid="105" ssid="23">(5)In the remainder of this section, we drop the nota tional dependence on x for brevity.</S>
			<S sid="106" ssid="24">The original Matrix-Tree Theorem addressed theproblem of counting the number of undirected span ning trees in an undirected graph.</S>
			<S sid="107" ssid="25">For the models we study here, we require a sum of weighted and directed spanning trees.</S>
			<S sid="108" ssid="26">Tutte (1984) extended theMatrix-Tree Theorem to this case.</S>
			<S sid="109" ssid="27">We briefly sum marize his method below.</S>
			<S sid="110" ssid="28">First, define the Laplacian matrix L(?)</S>
			<S sid="111" ssid="29">Rn?n of G, for h,m = 1 . . .</S>
			<S sid="112" ssid="30">n: Lh,m(?)</S>
			<S sid="113" ssid="31">= { ?n h?=1Ah?,m(?)</S>
			<S sid="114" ssid="32">if h = m ?Ah,m(?)</S>
			<S sid="115" ssid="33">otherwise Second, for a matrix X , let X(h,m) be the minor of X with respect to row h and column m; i.e., the determinant of the matrix formed by deleting row h and columnm fromX . Finally, define the weight of any directed spanning tree of G to be the product of the weights Ah,m(?)</S>
			<S sid="116" ssid="34">for the edges in that tree.</S>
			<S sid="117" ssid="35">Theorem 1 (Tutte, 1984, p. 140).</S>
			<S sid="118" ssid="36">Let L(?)</S>
			<S sid="119" ssid="37">be the Laplacian matrix of G. Then L(m,m)(?)</S>
			<S sid="120" ssid="38">is equal to the sum of the weights of all directed spanning treesof G which are rooted at m. Furthermore, the mi nors vary only in sign when traversing the columns of the Laplacian (Tutte, 1984, p. 150): ?h,m : (?1)h+mL(h,m)(?)</S>
			<S sid="121" ssid="39">= L(m,m)(?)</S>
			<S sid="122" ssid="40">(6) 3.1 Partition functions via matrix determinants.</S>
			<S sid="123" ssid="41">From Theorem 1, it directly follows that L(m,m)(?)</S>
			<S sid="124" ssid="42">= ? y?U(m) ?</S>
			<S sid="125" ssid="43">(h,m)?y : h 6=0 Ah,m(?)</S>
			<S sid="126" ssid="44">where U(m) = {y ? T snp : root(y) = m}.</S>
			<S sid="127" ssid="45">A na??ve method for computing the partition function is therefore to evaluate Z(?)</S>
			<S sid="128" ssid="46">= n?</S>
			<S sid="129" ssid="47">m=1 rm(?)L(m,m)(?)</S>
			<S sid="130" ssid="48">The above would require calculating n determinants, resulting in O(n4) complexity.</S>
			<S sid="131" ssid="49">However, as we show below Z(?)</S>
			<S sid="132" ssid="50">may be obtained in O(n3) time using a single determinant evaluation.</S>
			<S sid="133" ssid="51">Define a newmatrix L?(?)</S>
			<S sid="134" ssid="52">to beL(?)</S>
			<S sid="135" ssid="53">with the first row replaced by the root-selection scores: L?h,m(?)</S>
			<S sid="136" ssid="54">= { rm(?)</S>
			<S sid="137" ssid="55">h = 1 Lh,m(?)</S>
			<S sid="138" ssid="56">h &gt; 1This matrix allows direct computation of the parti tion function, as the following proposition shows.</S>
			<S sid="139" ssid="57">Proposition 1 The partition function in Eq.</S>
			<S sid="140" ssid="58">5 is given by Z(?)</S>
			<S sid="141" ssid="59">= |L?(?)|.</S>
			<S sid="142" ssid="60">Proof: Consider the row expansion of |L?(?)| with respect to row 1: |L?(?)| = n?</S>
			<S sid="143" ssid="61">m=1 (?1)1+mL?1,m(?)L?(1,m)(?)</S>
			<S sid="144" ssid="62">= n?</S>
			<S sid="145" ssid="63">m=1 (?1)1+mrm(?)L(1,m)(?)</S>
			<S sid="146" ssid="64">= n?</S>
			<S sid="147" ssid="65">m=1 rm(?)L(m,m)(?)</S>
			<S sid="148" ssid="66">= Z(?)</S>
			<S sid="149" ssid="67">The second line follows from the construction of L?(?), and the third line follows from Eq.</S>
			<S sid="150" ssid="68">6.</S>
			<S sid="151" ssid="69">144 3.2 Marginals via matrix inversion.</S>
			<S sid="152" ssid="70">The marginals we require are given by ?h,m(?)</S>
			<S sid="153" ssid="71">= 1 Z(?)</S>
			<S sid="154" ssid="72">y?T snp : (h,m)?y ?(y;?)</S>
			<S sid="155" ssid="73">To calculate these marginals efficiently for all values of (h,m) we use a well-known identity relating the log partition-function to marginals ?h,m(?)</S>
			<S sid="156" ssid="74">= ? logZ(?)</S>
			<S sid="157" ssid="75">??h,mSince the partition function in this case has a closed form expression (i.e., the determinant of a matrix constructed from ?), the marginals can also obtained in closed form.</S>
			<S sid="158" ssid="76">Using the chain rule, the derivative of the log partition-function in Proposition 1 is ?h,m(?)</S>
			<S sid="159" ssid="77">= ? log |L?(?)| ??h,m = n?</S>
			<S sid="160" ssid="78">h?=1 n?</S>
			<S sid="161" ssid="79">m?=1 ? log |L?(?)| ?L?h?,m?(?)</S>
			<S sid="162" ssid="80">?L?h?,m?(?)</S>
			<S sid="163" ssid="81">??h,m To perform the derivative, we use the identity ? log |X| ?X = ( X?1 )T and the fact that ?L?h?,m?(?)/??h,m is nonzero for only a few h?,m?.</S>
			<S sid="164" ssid="82">Specifically, when h = 0, the marginals are given by ?0,m(?)</S>
			<S sid="165" ssid="83">= rm(?)</S>
			<S sid="166" ssid="84">[ L??1(?)</S>
			<S sid="167" ssid="85">] m,1 and for h &gt; 0, the marginals are given by ?h,m(?)</S>
			<S sid="168" ssid="86">= (1 ? ?1,m)Ah,m(?)</S>
			<S sid="169" ssid="87">[ L??1(?)</S>
			<S sid="170" ssid="88">] m,m ?</S>
			<S sid="171" ssid="89">(1 ? ?h,1)Ah,m(?)</S>
			<S sid="172" ssid="90">[ L??1(?)</S>
			<S sid="173" ssid="91">] m,hwhere ?h,m is the Kronecker delta.</S>
			<S sid="174" ssid="92">Thus, the com plexity of evaluating all the relevant marginals is dominated by the matrix inversion, and the total complexity is therefore O(n3).</S>
			<S sid="175" ssid="93">3.3 Multiple Roots.</S>
			<S sid="176" ssid="94">In the case of multiple roots, we can still compute the partition function and marginals efficiently.</S>
			<S sid="177" ssid="95">In fact, the derivation of this case is simpler than for single-root structures.</S>
			<S sid="178" ssid="96">Create an extended graph G?</S>
			<S sid="179" ssid="97">which augmentsG with a dummy root node that has edges pointing to all of the existing nodes, weighted by the appropriate root-selection scores.</S>
			<S sid="180" ssid="98">Note that there is a bijection between directed spanning treesofG?</S>
			<S sid="181" ssid="99">rooted at the dummy root and multi-root struc tures y ? T mnp (x).</S>
			<S sid="182" ssid="100">Thus, Theorem 1 can be used to compute the partition function directly: construct aLaplacian matrix L(?)</S>
			<S sid="183" ssid="101">for G?</S>
			<S sid="184" ssid="102">and compute the minor L(0,0)(?).</S>
			<S sid="185" ssid="103">Since this minor is also a determi nant, the marginals can be obtained analogously to the single-root case.</S>
			<S sid="186" ssid="104">More concretely, this technique corresponds to defining the matrix L?(?)</S>
			<S sid="187" ssid="105">as L?(?)</S>
			<S sid="188" ssid="106">= L(?)</S>
			<S sid="189" ssid="107">+ diag(r(?)) where diag(v) is the diagonal matrix with the vector v on its diagonal.</S>
			<S sid="190" ssid="108">3.4 Labeled Trees.</S>
			<S sid="191" ssid="109">The techniques above extend easily to the case where dependencies are labeled.</S>
			<S sid="192" ssid="110">For a model with L different labels, it suffices to define the edge and root scores as Ah,m(?)</S>
			<S sid="193" ssid="111">= ?L `=1 exp {?h,m,`} and rm(?)</S>
			<S sid="194" ssid="112">= ?L `=1 exp {?0,m,`}.</S>
			<S sid="195" ssid="113">The partitionfunction over labeled trees is obtained by operat ing on these values as described previously, and the marginals are given by an application of the chain rule.</S>
			<S sid="196" ssid="114">Both inference problems are solvable in O(n3 + Ln2) time.</S>
	</SECTION>
	<SECTION title="Training Algorithms. " number="4">
			<S sid="197" ssid="1">This section describes two methods for parameter estimation that rely explicitly on the computation of the partition function and marginals.</S>
			<S sid="198" ssid="2">4.1 Log-Linear Estimation.</S>
			<S sid="199" ssid="3">In conditional log-linear models (Johnson et al, 1999; Lafferty et al, 2001), a distribution over parse trees for a sentence x is defined as follows: P (y |x;w) = exp {?</S>
			<S sid="200" ssid="4">(h,m)?y w ? f(x, h,m) } Z(x;w) (7) where Z(x;w) is the partition function, a sum over T sp (x), T s np(x), T m p (x) or T m np (x).</S>
			<S sid="201" ssid="5">We train the model using the approach described by Sha and Pereira (2003).</S>
			<S sid="202" ssid="6">Assume that we have a training set {(xi, yi)}Ni=1.</S>
			<S sid="203" ssid="7">The optimal parameters 145 are taken to be w?</S>
			<S sid="204" ssid="8">= argminw L(w) where L(w) = ?C N?</S>
			<S sid="205" ssid="9">i=1 logP (yi |xi;w) + 1 2 ||w||2 The parameterC &gt; 0 is a constant dictating the level of regularization in the model.Since L(w) is a convex function, gradient de scent methods can be used to search for the global minimum.</S>
			<S sid="206" ssid="10">Such methods typically involve repeated computation of the loss L(w) and gradient ?L(w)?w ,requiring efficient implementations of both func tions.</S>
			<S sid="207" ssid="11">Note that the log-probability of a parse is logP (y |x;w) = ?</S>
			<S sid="208" ssid="12">(h,m)?y w ? f(x, h,m)?</S>
			<S sid="209" ssid="13">logZ(x;w)so that the main issue in calculating the loss func tion L(w) is the evaluation of the partition functions Z(xi;w).</S>
			<S sid="210" ssid="14">The gradient of the loss is given by ?L(w) ?w = w ? C N?</S>
			<S sid="211" ssid="15">i=1 ?</S>
			<S sid="212" ssid="16">(h,m)?yi f(xi, h,m) + C N?</S>
			<S sid="213" ssid="17">i=1 ?</S>
			<S sid="214" ssid="18">(h,m)?D(xi) ?h,m(xi;w)f(xi, h,m) where ?h,m(x;w) = ? y?T (x) : (h,m)?y P (y |x;w) is the marginal probability of a dependency (h,m).</S>
			<S sid="215" ssid="19">Thus, the main issue in the evaluation of the gradient is the computation of the marginals ?h,m(xi;w).Note that Eq.</S>
			<S sid="216" ssid="20">7 forms a special case of the log linear distribution defined in Eq.</S>
			<S sid="217" ssid="21">2 in Section 2.2.</S>
			<S sid="218" ssid="22">If we set ?h,m = w ? f(x, h,m) then we have P (y |x;w) = P (y |x;?), Z(x;w) = Z(x;?), and ?h,m(x;w) = ?h,m(x;?).</S>
			<S sid="219" ssid="23">Thus in the projectivecase the inside-outside algorithm can be used to cal culate the partition function and marginals, therebyenabling training of a log-linear model; in the non projective case the algorithms in Section 3 can be used for this purpose.</S>
			<S sid="220" ssid="24">4.2 Max-Margin Estimation.</S>
			<S sid="221" ssid="25">The second learning algorithm we consider is the large-margin approach for structured prediction (Taskar et al, 2004a; Taskar et al, 2004b).</S>
			<S sid="222" ssid="26">Learning in this framework again involves minimization of a convex function L(w).</S>
			<S sid="223" ssid="27">Let the margin for parse tree y on the i?th training example be defined as mi,y(w) = ?</S>
			<S sid="224" ssid="28">(h,m)?yi w?f(xi, h,m) ? ?</S>
			<S sid="225" ssid="29">(h,m)?y w?f(xi, h,m) The loss function is then defined as L(w) = C N?</S>
			<S sid="226" ssid="30">i=1 max y?T (xi) (Ei,y ?mi,y(w)) + 1 2 ||w||2 where Ei,y is a measure of the loss?or number of errors?for parse y on the i?th training sentence.</S>
			<S sid="227" ssid="31">In this paper we take Ei,y to be the number of incorrect dependencies in the parse tree y when compared to the gold-standard parse tree yi.The definition of L(w) makes use of the expression maxy?T (xi) (Ei,y ?mi,y(w)) for the i?th train ing example, which is commonly referred to as the hinge loss.</S>
			<S sid="228" ssid="32">Note that Ei,yi = 0, and also thatmi,yi(w) = 0, so that the hinge loss is always non negative.</S>
			<S sid="229" ssid="33">In addition, the hinge loss is 0 if and only ifmi,y(w) ? Ei,y for all y ? T (xi).</S>
			<S sid="230" ssid="34">Thus the hinge loss directly penalizes margins mi,y(w) which are less than their corresponding losses Ei,y. Figure 2 shows an algorithm for minimizingL(w) that is based on the exponentiated-gradient al gorithm for large-margin optimization described by Bartlett et al (2004).</S>
			<S sid="231" ssid="35">The algorithm maintains a set of weights ?i,h,m for i = 1 . . .</S>
			<S sid="232" ssid="36">N, (h,m) ? D(xi),which are updated example-by-example.</S>
			<S sid="233" ssid="37">The algo rithm relies on the repeated computation of marginal values ?i,h,m, which are defined as follows:1 ?i,h,m = ? y?T (xi) : (h,m)?y P (y |xi) (8) P (y |xi) = exp {?</S>
			<S sid="234" ssid="38">(h,m)?y ?i,h,m } ? y??T (xi) exp {?</S>
			<S sid="235" ssid="39">(h,m)?y? ?i,h,m }A similar definition is used to derive marginal val ues ??i,h,m from the values ? ?</S>
			<S sid="236" ssid="40">i,h,m. Computation of the ? and ??</S>
			<S sid="237" ssid="41">values is again inference of the form described in Problem 3 in Section 2.2, and can be1Bartlett et al (2004) write P (y |xi) as ?i,y . The ?i,y variables are dual variables that appear in the dual objective func tion, i.e., the convex dual of L(w).</S>
			<S sid="238" ssid="42">Analysis of the algorithmshows that as the ?i,h,m variables are updated, the dual vari ables converge to the optimal point of the dual objective, and the parameters w converge to the minimum of L(w).</S>
			<S sid="239" ssid="43">146 Inputs: Training examples {(xi, yi)}Ni=1.</S>
			<S sid="240" ssid="44">Parameters: Regularization constant C, starting point ?, number of passes over training set T . Data Structures: Real values ?i,h,m and li,h,m for i = 1 . . .</S>
			<S sid="241" ssid="45">N, (h,m) ? D(xi).</S>
			<S sid="242" ssid="46">Learning rate ?.</S>
			<S sid="243" ssid="47">Initialization: Set learning rate ? = 1C . Set ?i,h,m = ? for (h,m) ? yi, and ?i,h,m = 0 for (h,m) /?</S>
			<S sid="244" ssid="48">yi.</S>
			<S sid="245" ssid="49">Set li,h,m = 0 for (h,m) ? yi, and li,h,m = 1 for (h,m) /?</S>
			<S sid="246" ssid="50">yi.</S>
			<S sid="247" ssid="51">Calculate initial parameters as w = C ? i ?</S>
			<S sid="248" ssid="52">(h,m)?D(xi) ?i,h,mf(xi, h,m) where ?i,h,m = (1?</S>
			<S sid="249" ssid="53">li,h,m ??i,h,m) and the ?i,h,m values are calculated from the ?i,h,m values as described in Eq.</S>
			<S sid="250" ssid="54">8.</S>
			<S sid="251" ssid="55">Algorithm: Repeat T passes over the training set, where each pass is as follows: Set obj = 0 For i = 1 . . .</S>
			<S sid="252" ssid="56">N ? For all (h,m) ? D(xi): ??i,h,m = ?i,h,m + ?C (li,h,m +w ? f(xi, h,m)) ? For example i, calculate marginals ?i,h,m from ?i,h,m values, and marginals ??i,h,m from ??i,h,m values (see Eq.</S>
			<S sid="253" ssid="57">8).</S>
			<S sid="254" ssid="58">Update the parameters: w = w + C ?</S>
			<S sid="255" ssid="59">(h,m)?D(xi) ?i,h,mf(xi, h,m) where ?i,h,m = ?i,h,m ? ??i,h,m, ? For all (h,m) ? D(xi), set ?i,h,m = ??i,h,m ? Set obj = obj + C ?</S>
			<S sid="256" ssid="60">(h,m)?D(xi) li,h,m??i,h,m Set obj = obj ? ||w|| 2 2 . If obj has decreased compared to last iteration, set ? = ?2 . Output: Parameter values w. Figure 2: The EG Algorithm for Max-Margin Estimation.The learning rate ? is halved each time the dual objective function (see (Bartlett et al, 2004)) fails to increase.</S>
			<S sid="257" ssid="61">In our experi ments we chose ? = 9, which was found to work well during development of the algorithm.achieved using the inside-outside algorithm for pro jective structures, and the algorithms described in Section 3 for non-projective structures.</S>
	</SECTION>
	<SECTION title="Related Work. " number="5">
			<S sid="258" ssid="1">Global log-linear training has been used in the con text of PCFG parsing (Johnson, 2001).</S>
			<S sid="259" ssid="2">Riezler et al (2004) explore a similar application of log-linear models to LFG parsing.</S>
			<S sid="260" ssid="3">Max-margin learning has been applied to PCFG parsing by Taskar et al (2004b).</S>
			<S sid="261" ssid="4">They show that this problem has a QP dual of polynomial size, where the dual variables correspond to marginal probabilities of CFG rules.</S>
			<S sid="262" ssid="5">A similar QP dual may be obtained for max-marginprojective dependency parsing.</S>
			<S sid="263" ssid="6">However, for nonprojective parsing, the dual QP would require an ex ponential number of constraints on the dependency marginals (Chopra, 1989).</S>
			<S sid="264" ssid="7">Nevertheless, alternative optimization methods like that of Tsochantaridis et al.</S>
			<S sid="265" ssid="8">(2004), or the EGmethod presented here, can still be applied.</S>
			<S sid="266" ssid="9">The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al, 2004; Y. Cheng, 2005).</S>
			<S sid="267" ssid="10">Non-local (i.e., classification ofentire trees) training methods were used by McDon ald et al (2005a), who employed online learning.</S>
			<S sid="268" ssid="11">Dependency parsing accuracy can be improved by allowing second-order features, which considermore than one dependency simultaneously.</S>
			<S sid="269" ssid="12">McDonald and Pereira (2006) define a second-order depen dency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007)defines a second-order model that allows grandparent and sibling interactions.</S>
			<S sid="270" ssid="13">Both authors give poly time algorithms for exact projective parsing.</S>
			<S sid="271" ssid="14">Byadapting the inside-outside algorithm to these models, partition functions and marginals can be computed for second-order projective structures, allowing log-linear and max-margin training to be ap plied via the framework developed in this paper.</S>
			<S sid="272" ssid="15">For higher-order non-projective parsing, however, computational complexity results (McDonald and Pereira, 2006; McDonald and Satta, 2007) indicate that exact solutions to the three inference problemsof Section 2.2 will be intractable.</S>
			<S sid="273" ssid="16">Exploration of ap proximate second-order non-projective inference is a natural avenue for future research.</S>
			<S sid="274" ssid="17">Two other groups of authors have independently and simultaneously proposed adaptations of theMatrix-Tree Theorem for structured inference on di rected spanning trees (McDonald and Satta, 2007; Smith and Smith, 2007).</S>
			<S sid="275" ssid="18">There are some algorithmic differences between these papers and ours.</S>
			<S sid="276" ssid="19">First, we define both multi-root and single-root algorithms, whereas the other papers only consider multi-root 147 parsing.</S>
			<S sid="277" ssid="20">This distinction can be important as oneoften expects a dependency structure to have ex actly one child attached to the root-symbol, as is the case in a single-root structure.</S>
			<S sid="278" ssid="21">Second, McDonald and Satta (2007) propose an O(n5) algorithm for computing the marginals, as opposed to the O(n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves.</S>
			<S sid="279" ssid="22">In addition to the algorithmic differences, both groups of authors consider applications of the Matrix-Tree Theorem which we have not discussed.</S>
			<S sid="280" ssid="23">For example, both papers propose minimum-riskdecoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees.</S>
			<S sid="281" ssid="24">In this paper we used EG training methods onlyfor max-margin models (Bartlett et al, 2004).</S>
			<S sid="282" ssid="25">How ever, Globerson et al (2007) have recently shown how EG updates can be applied to efficient training of log-linear models.</S>
	</SECTION>
	<SECTION title="Experiments on Dependency Parsing. " number="6">
			<S sid="283" ssid="1">In this section, we present experimental results applying our inference algorithms for dependencyparsing models.</S>
			<S sid="284" ssid="2">Our primary purpose is to estab lish comparisons along two relevant dimensions: projective training vs. non-projective training, and marginal-based training algorithms vs. the averagedperceptron.</S>
			<S sid="285" ssid="3">The feature representation and other rel evant dimensions are kept fixed in the experiments.</S>
			<S sid="286" ssid="4">6.1 Data Sets and Features.</S>
			<S sid="287" ssid="5">We used data from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006).</S>
			<S sid="288" ssid="6">In our experiments, we used a subset consisting of six languages; Table 1 gives details of the data sets used.2 For each language we created a validation set that was a subset of the CoNLL-X2Our subset includes the two languages with the lowest ac curacy in the CoNLL-X evaluations (Turkish and Arabic), thelanguage with the highest accuracy (Japanese), the most nonprojective language (Dutch), a moderately non-projective lan guage (Slovene), and a highly projective language (Spanish).</S>
			<S sid="289" ssid="7">All languages but Spanish have multi-root parses in their data.</S>
			<S sid="290" ssid="8">We are grateful to the providers of the treebanks that constituted the data of our experiments (Hajic?</S>
			<S sid="291" ssid="9">et al, 2004; van der Beek et al., 2002; Kawata and Bartels, 2000; Dz?eroski et al, 2006; Civit and Mart??, 2002; Oflazer et al, 2003).</S>
			<S sid="292" ssid="10">language %cd train val.</S>
			<S sid="293" ssid="11">test Arabic 0.34 49,064 5,315 5,373 Dutch 4.93 178,861 16,208 5,585 Japanese 0.70 141,966 9,495 5,711 Slovene 1.59 22,949 5,801 6,390 Spanish 0.06 78,310 11,024 5,694 Turkish 1.26 51,827 5,683 7,547 Table 1: Information for the languages in our experiments.The 2nd column (%cd) is the percentage of crossing dependen cies in the training and validation sets.</S>
			<S sid="294" ssid="12">The last three columns report the size in tokens of the training, validation and test sets.</S>
			<S sid="295" ssid="13">training set for that language.</S>
			<S sid="296" ssid="14">The remainder of eachtraining set was used to train the models for the dif ferent languages.</S>
			<S sid="297" ssid="15">The validation sets were used totune the meta-parameters (e.g., the value of the regularization constantC) of the different training algo rithms.</S>
			<S sid="298" ssid="16">We used the official test sets and evaluation script from the CoNLL-X task.</S>
			<S sid="299" ssid="17">All of the results that we report are for unlabeled dependency parsing.3 The non-projective models were trained on theCoNLL-X data in its original form.</S>
			<S sid="300" ssid="18">Since the pro jective models assume that the dependencies in thedata are non-crossing, we created a second training set for each language where non-projective de pendency structures were automatically transformed into projective structures.</S>
			<S sid="301" ssid="19">All projective models were trained on these new training sets.4 Our feature space is based on that of McDonald et al (2005a).5 6.2 Results.</S>
			<S sid="302" ssid="20">We performed experiments using three training al gorithms: the averaged perceptron (Collins, 2002), log-linear training (via conjugate gradient descent), and max-margin training (via the EG algorithm).Each of these algorithms was trained using pro jective and non-projective methods, yielding six training settings per language.</S>
			<S sid="303" ssid="21">The different training algorithms have various meta-parameters, which we optimized on the validation set for each language/training-setting combination.</S>
			<S sid="304" ssid="22">The 3Our algorithms also support labeled parsing (see Section 3.4).</S>
			<S sid="305" ssid="23">Initial experiments with labeled models showed the same trend that we report here for unlabeled parsing, so for simplicity we conducted extensive experiments only for unlabeled parsing.4The transformations were performed by running the projective parser with score +1 on correct dependencies and -1 oth erwise: the resulting trees are guaranteed to be projective and to have a minimum loss with respect to the correct tree.</S>
			<S sid="306" ssid="24">Note that only the training sets were transformed.</S>
			<S sid="307" ssid="25">5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features.</S>
			<S sid="308" ssid="26">148 Perceptron Max-Margin Log-Linear p np p np p np Ara 71.74 71.84 71.74 72.99 73.11 73.67 Dut 77.17 78.83 76.53 79.69 76.23 79.55 Jap 91.90 91.78 92.10 92.18 91.68 91.49 Slo 78.02 78.66 79.78 80.10 78.24 79.66 Spa 81.19 80.02 81.71 81.93 81.75 81.57 Tur 71.22 71.70 72.83 72.02 72.26 72.62 Table 2: Test data results.</S>
			<S sid="309" ssid="27">The p and np columns show results with projective and non-projective training respectively.</S>
			<S sid="310" ssid="28">Ara Dut Jap Slo Spa Tur AV P 71.74 78.83 91.78 78.66 81.19 71.70 79.05 E 72.99 79.69 92.18 80.10 81.93 72.02 79.82 L 73.67 79.55 91.49 79.66 81.57 72.26 79.71Table 3: Results for the three training algorithms on the differ ent languages (P = perceptron, E = EG, L = log-linear models).</S>
			<S sid="311" ssid="29">AV is an average across the results for the different languages.</S>
			<S sid="312" ssid="30">averaged perceptron has a single meta-parameter, namely the number of iterations over the training set.</S>
			<S sid="313" ssid="31">The log-linear models have two meta-parameters: the regularization constant C and the number of gradient steps T taken by the conjugate-gradientoptimizer.</S>
			<S sid="314" ssid="32">The EG approach also has two meta parameters: the regularization constant C and the number of iterations, T .6 For models trained usingnon-projective algorithms, both projective and non projective parsing was tested on the validation set, and the highest scoring of these two approaches was then used to decode test data sentences.Table 2 reports test results for the six training sce narios.</S>
			<S sid="315" ssid="33">These results show that for Dutch, which isthe language in our data that has the highest number of crossing dependencies, non-projective train ing gives significant gains over projective trainingfor all three training methods.</S>
			<S sid="316" ssid="34">For the other lan guages, non-projective training gives similar or even improved performance over projective training.</S>
			<S sid="317" ssid="35">Table 3 gives an additional set of results, which were calculated as follows.</S>
			<S sid="318" ssid="36">For each of the three training methods, we used the validation set results to choose between projective and non-projectivetraining.</S>
			<S sid="319" ssid="37">This allows us to make a direct com parison of the three training algorithms.</S>
			<S sid="320" ssid="38">Table 3 6We trained the perceptron for 100 iterations, and chose the iteration which led to the best score on the validation set.</S>
			<S sid="321" ssid="39">Note that in all of our experiments, the best perceptron results were actually obtained with 30 or fewer iterations.</S>
			<S sid="322" ssid="40">For the log-linear and EG algorithms we tested a number of values for C, and for each value of C ran 100 gradient steps or EG iterations, finally choosing the best combination of C and T found in validation.</S>
			<S sid="323" ssid="41">shows the results of this comparison.7 The results show that log-linear and max-margin models both give a higher average accuracy than the perceptron.</S>
			<S sid="324" ssid="42">For some languages (e.g., Japanese), the differences from the perceptron are small; however for otherlanguages (e.g., Arabic, Dutch or Slovene) the im provements seen are quite substantial.</S>
	</SECTION>
	<SECTION title="Conclusions. " number="7">
			<S sid="325" ssid="1">This paper describes inference algorithms forspanning-tree distributions, focusing on the funda mental problems of computing partition functionsand marginals.</S>
			<S sid="326" ssid="2">Although we concentrate on loglinear and max-margin estimation, the inference al gorithms we present can serve as black-boxes in many other statistical modeling techniques.</S>
			<S sid="327" ssid="3">Our experiments suggest that marginal-basedtraining produces more accurate models than per ceptron learning.</S>
			<S sid="328" ssid="4">Notably, this is the first large-scale application of the EG algorithm, and shows that it is a promising approach for structured learning.</S>
			<S sid="329" ssid="5">In line with McDonald et al (2005b), we confirmthat spanning-tree models are well-suited to depen dency parsing, especially for highly non-projective languages such as Dutch.</S>
			<S sid="330" ssid="6">Moreover, spanning-treemodels should be useful for a variety of other prob lems involving structured data.</S>
			<S sid="331" ssid="7">AcknowledgmentsThe authors would like to thank the anonymous reviewers for their constructive comments.</S>
			<S sid="332" ssid="8">In addition, the authors gratefully acknowledge the follow ing sources of support.</S>
			<S sid="333" ssid="9">Terry Koo was funded by a grant from the NSF (DMS-0434222) and a grantfrom NTT, Agmt.</S>
			<S sid="334" ssid="10">Dtd.</S>
			<S sid="335" ssid="11">6/21/1998.</S>
			<S sid="336" ssid="12">Amir Globerson was supported by a fellowship from the Roth schild Foundation - Yad Hanadiv.</S>
			<S sid="337" ssid="13">Xavier Carreraswas supported by the Catalan Ministry of Innova tion, Universities and Enterprise, and a grant from NTT, Agmt.</S>
			<S sid="338" ssid="14">Dtd.</S>
			<S sid="339" ssid="15">6/21/1998.</S>
			<S sid="340" ssid="16">Michael Collins was funded by NSF grants 0347631 and DMS-0434222.</S>
			<S sid="341" ssid="17">7We ran the sign test at the sentence level to measure the statistical significance of the results aggregated across the six languages.</S>
			<S sid="342" ssid="18">Out of 2,472 sentences total, log-linear models gave improved parses over the perceptron on 448 sentences, and worse parses on 343 sentences.</S>
			<S sid="343" ssid="19">The max-margin method gave improved/worse parses for 500/383 sentences.</S>
			<S sid="344" ssid="20">Both results are significant with p ? 0.001.</S>
			<S sid="345" ssid="21">149</S>
	</SECTION>
</PAPER>
