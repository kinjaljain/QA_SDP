<PAPER>
	<S sid="0">A Two-Stage Parser for Multilingual Dependency Parsing</S><ABSTRACT>
		<S sid="1" ssid="1">We present a two-stage multilingual de pendency parsing system submitted to the Multilingual Track of CoNLL-2007.</S>
		<S sid="2" ssid="2">The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem.</S>
		<S sid="3" ssid="3">We describe the features used ineach stage.</S>
		<S sid="4" ssid="4">For four languages with different values of ROOT, we design some spe cial features for the ROOT labeler.</S>
		<S sid="5" ssid="5">Then we present evaluation results and error analyses focusing on Chinese.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">The CoNLL-2007 shared tasks include two tracks: the Multilingual Track and Domain AdaptationTrack(Nivre et al, 2007).</S>
			<S sid="7" ssid="7">We took part the Multi lingual Track of all ten languages provided by the CoNLL-2007 shared task organizers(Hajic?</S>
			<S sid="8" ssid="8">et al, 2004; Aduriz et al, 2003; Mart??</S>
			<S sid="9" ssid="9">et al, 2007; Chen et al, 2003; Bo?hmova?</S>
			<S sid="10" ssid="10">et al, 2003; Marcus et al, 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al, 2005; Montemagni et al, 2003; Oflazer et al, 2003) . In this paper, we describe a two-stage parsingsystem consisting of an unlabeled parser and a sequence labeler, which was submitted to the Multilingual Track.</S>
			<S sid="11" ssid="11">At the first stage, we use the pars ing model proposed by (Nivre, 2003) to assign thearcs between the words.</S>
			<S sid="12" ssid="12">Then we obtain a dependency parsing tree based on the arcs.</S>
			<S sid="13" ssid="13">At the sec ond stage, we use a SVM-based approach(Kudo and Matsumoto, 2001) to tag the dependency label foreach arc. The labeling is treated as a sequence la beling problem.</S>
			<S sid="14" ssid="14">We design some special features for tagging the labels of ROOT for Arabic, Basque, Czech, and Greek, which have different labels forROOT.</S>
			<S sid="15" ssid="15">The experimental results show that our ap proach can provide higher scores than average.</S>
	</SECTION>
	<SECTION title="Two-Stage Parsing. " number="2">
			<S sid="16" ssid="1">2.1 The Unlabeled Parser.</S>
			<S sid="17" ssid="2">The unlabeled parser predicts unlabeled directed de pendencies.</S>
			<S sid="18" ssid="3">This parser is primarily based on theparsing models described by (Nivre, 2003).</S>
			<S sid="19" ssid="4">The algorithm makes a dependency parsing tree in one left to-right pass over the input, and uses a stack to store the processed tokens.</S>
			<S sid="20" ssid="5">The behaviors of the parser are defined by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): ? Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack.</S>
			<S sid="21" ssid="6">Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stack.</S>
			<S sid="22" ssid="7">Reduce(RE): Pop the stack.</S>
			<S sid="23" ssid="8">Shift(SH): Push NEXT onto the stack.Although (Nivre et al, 2006) used the pseudoprojective approach to process non-projective dependencies, here we only derive projective depen dency tree.</S>
			<S sid="24" ssid="9">We use MaltParser(Nivre et al, 2006) 1129 V0.41 to implement the unlabeled parser, and use the SVM model as the classifier.</S>
			<S sid="25" ssid="10">More specifically, the MaltParser use LIBSVM(Chang and Lin, 2001)with a quadratic kernel and the built-in one-versus all strategy for multi-class classification.</S>
			<S sid="26" ssid="11">2.1.1 Features for Parsing The MaltParser is a history-based parsing model, which relies on features of the derivation history to predict the next parser action.</S>
			<S sid="27" ssid="12">We represent thefeatures extracted from the fields of the data repre sentation, including FORM, LEMMA, CPOSTAG, POSTAG, and FEATS.</S>
			<S sid="28" ssid="13">We use the features for all languages that are listed as follows: ? The FORM features: the FORM of TOP and NEXT, the FORM of the token immediately before NEXT in original input string, and the FORM of the head of TOP.</S>
			<S sid="29" ssid="14">The LEMMA features: the LEMMA of TOPand NEXT, the LEMMA of the token immedi ately before NEXT in original input string, and the LEMMA of the head of TOP.</S>
			<S sid="30" ssid="15">The CPOS features: the CPOSTAG of TOP and NEXT, and the CPOSTAG of next left token of the head of TOP.</S>
			<S sid="31" ssid="16">The POS features: the POSTAG of TOP andNEXT, the POSTAG of next three tokens after NEXT, the POSTAG of the token immedi ately before NEXT in original input string, the POSTAG of the token immediately below TOP,and the POSTAG of the token immediately af ter rightmost dependent of TOP.</S>
			<S sid="32" ssid="17">The FEATS features: the FEATS of TOP and NEXT.</S>
			<S sid="33" ssid="18">But note that the fields LEMMA and FEATS are not available for all languages.</S>
			<S sid="34" ssid="19">2.2 The Sequence Labeler.</S>
			<S sid="35" ssid="20">2.2.1 The Sequence Problem We denote by x = x 1 , ..., xn a sentence with n words and by y a corresponding dependency tree.</S>
			<S sid="36" ssid="21">A dependency tree is represented from ROOT to leaves 1The tool is available at http://w3.msi.vxu.se/?nivre/research/MaltParser.html with a set of ordered pairs (i, j) ? y in which xj is a dependent and xi is the head.</S>
			<S sid="37" ssid="22">We have produced the dependency tree y at the first stage.</S>
			<S sid="38" ssid="23">In this stage, we assign a label l (i,j) to each pair.</S>
			<S sid="39" ssid="24">As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem.</S>
			<S sid="40" ssid="25">Suppose that we consider a head xi withdependents xj1, ..., xjM . We then consider the la bels of (i, j1), ..., (i, jM) as a sequence.</S>
			<S sid="41" ssid="26">We use the model to find the solution: lmax = arg max l s(l, i, y, x) (1)And we consider a first-order Markov chain of la bels.We used the package YamCha (V0.33)2 to imple ment the SVM model for labeling.</S>
			<S sid="42" ssid="27">YamCha is apowerful tool for sequence labeling(Kudo and Mat sumoto, 2001).</S>
			<S sid="43" ssid="28">2.2.2 Features for LabelingAfter the first stage, we know the unlabeled de pendency parsing tree for the input sentence.</S>
			<S sid="44" ssid="29">This information forms the basis for part of the features of the second stage.</S>
			<S sid="45" ssid="30">For the sequence labeler, we define the individual features, the pair features, the verb features, the neighbor features, and the position features.</S>
			<S sid="46" ssid="31">All the features are listed as follows: ? The individual features: the FORM, the LEMMA, the CPOSTAG, the POSTAG, and the FEATS of the parent and child node.?</S>
			<S sid="47" ssid="32">The pair features: the direction of depen dency, the combination of lemmata of the parent and child node, the combination of parent?s LEMMA and child?s CPOSTAG, the combination of parent?s CPOSTAG and child?s LEMMA, and the combination of FEATS of parent and child.</S>
			<S sid="48" ssid="33">The verb features: whether the parent or child is the first or last verb in the sentence.</S>
			<S sid="49" ssid="34">The neighbor features: the combination of CPOSTAG and LEMMA of the left and right neighbors of the parent and child, number of children, CPOSTAG sequence of children.</S>
			<S sid="50" ssid="35">2YamCha is available at http://chasen.org/?taku/software/yamcha/ 1130 ? The position features: whether the child is the first or last word in the sentence and whetherthe child is the first word of left or right of par ent.</S>
			<S sid="51" ssid="36">2.2.3 Features for the Root Labeler Because there are four languages have different labels for root, we define the features for the root labeler.</S>
			<S sid="52" ssid="37">The features are listed as follows: ? The individual features: the FORM, the LEMMA, the CPOSTAG, the POSTAG, and the FEATS of the parent and child node.</S>
			<S sid="53" ssid="38">The verb features: whether the child is the first or last verb in the sentence.</S>
			<S sid="54" ssid="39">The neighbor features: the combination of CPOSTAG and LEMMA of the left and right neighbors of the parent and child, number of children, CPOSTAG sequence of children.</S>
			<S sid="55" ssid="40">The position features: whether the child is the first or last word in the sentence and whetherthe child is the first word of left or right of par ent.</S>
	</SECTION>
	<SECTION title="Evaluation Results. " number="3">
			<S sid="56" ssid="1">We evaluated our system in the Multilingual Track for all languages.</S>
			<S sid="57" ssid="2">For the unlabeled parser, we chosethe parameters for the MaltParser based on perfor mance from a held-out section of the training data.</S>
			<S sid="58" ssid="3">We also chose the parameters for Yamcha based on performance from training data.Our official results are shown at Table 1.</S>
			<S sid="59" ssid="4">Performance is measured by labeled accuracy and unlabeled accuracy.</S>
			<S sid="60" ssid="5">These results showed that our two stage system can achieve good performance.</S>
			<S sid="61" ssid="6">For all languages, our system provided better results than average performance of all the systems(Nivre et al, 2007).</S>
			<S sid="62" ssid="7">Compared with top 3 scores, our system provided slightly worse performance.</S>
			<S sid="63" ssid="8">The reasonsmay be that we just used projective parsing algorithms while all languages except Chinese have non projective structure.</S>
			<S sid="64" ssid="9">Another reason was that we did not tune good parameters for the system due to lack of time.</S>
			<S sid="65" ssid="10">Data Set LA UA Arabic 74.65 83.49 Basque 72.39 78.63 Catalan 86.66 90.87 Chinese 81.24 85.91 Czech 73.69 80.14 English 83.81 84.91 Greek 74.42 81.16 Hungarian 75.34 79.25 Italian 82.04 85.91 Turkish 76.31 81.92 average 78.06 83.22Table 1: The results of proposed approach.</S>
			<S sid="66" ssid="11">LABELED ATTACHMENT SCORE(LA) and UNLA BELED ATTACHMENT SCORE(UA)</S>
	</SECTION>
	<SECTION title="General Error Analysis. " number="4">
			<S sid="67" ssid="1">4.1 Chinese.</S>
			<S sid="68" ssid="2">For Chinese, the system achieved 81.24% on labeled accuracy and 85.91% on unlabeled accuracy.</S>
			<S sid="69" ssid="3">Wealso ran the MaltParser to provide the labels.</S>
			<S sid="70" ssid="4">Besides the same features, we added the DEPREL fea tures: the dependency type of TOP, the dependency type of the token leftmost of TOP, the dependencytype of the token rightmost of TOP, and the de pendency type of the token leftmost of NEXT.</S>
			<S sid="71" ssid="5">The labeled accuracy of MaltParser was 80.84%, 0.4% lower than our system.Some conjunctions, prepositions, and DE3 attached to their head words with much lower ac curacy: 74% for DE, 76% for conjunctions, and 71% for prepositions.</S>
			<S sid="72" ssid="6">In the test data, these words formed 19.7%.</S>
			<S sid="73" ssid="7">For Chinese parsing, coordinationand preposition phrase attachment were hard prob lems.</S>
			<S sid="74" ssid="8">(Chen et al, 2006) defined the special features for coordinations for chunking.</S>
			<S sid="75" ssid="9">In the future, we plan to define some special features for these words.</S>
			<S sid="76" ssid="10">Now we focused words where most of the errors occur as Table 2 shows.</S>
			<S sid="77" ssid="11">For ??/DE?, there was 32.4% error rate of 383 occurrences.</S>
			<S sid="78" ssid="12">And most ofthem were assigned incorrect labels between ?property?</S>
			<S sid="79" ssid="13">and ?predication?: 45 times for ?property?</S>
			<S sid="80" ssid="14">instead of ?predication?</S>
			<S sid="81" ssid="15">and 20 times for ?predica tion?</S>
			<S sid="82" ssid="16">instead of ?property?.</S>
			<S sid="83" ssid="17">For examples, ??/DE?</S>
			<S sid="84" ssid="18">3including ??/?/?/??.</S>
			<S sid="85" ssid="19">1131 num any head dep both ?/ DE 383 124 35 116 27 a/ C 117 38 36 37 35 ?/ P 67 20 6 19 5 ??/ N 31 10 8 4 2 ?/ V 72 8 8 8 8 Table 2: The words where most of errors occur in Chinese data.</S>
			<S sid="86" ssid="20">in ???/?/??/??(popular TV channel)?</S>
			<S sid="87" ssid="21">was to be tagged as ?property?</S>
			<S sid="88" ssid="22">instead of ?predication?, while ??/DE?</S>
			<S sid="89" ssid="23">in ????/?/??(volunteer of museum)?</S>
			<S sid="90" ssid="24">was to be tagged as ?predication?</S>
			<S sid="91" ssid="25">insteadof ?property?.</S>
			<S sid="92" ssid="26">It was very hard to tell the labels be tween the words around ???.</S>
			<S sid="93" ssid="27">Humans can make the distinction between property and predication for ???, because we have background knowledge of the words.</S>
			<S sid="94" ssid="28">So if we can incorporate the additional knowledge for the system, the system may assign the correct label.</S>
			<S sid="95" ssid="29">For ?a/C?, it was hard to assign the head, 36 wrong head of all 38 errors.</S>
			<S sid="96" ssid="30">It often appeared at coordination expressions.</S>
			<S sid="97" ssid="31">For example, the head of ?a? at ??/?/?/?/a/?/?/?/??/(Besides extreme cool and too amazing)?</S>
			<S sid="98" ssid="32">was ????, and the head of ?a? at ????/??/?/??/a/?/?</S>
			<S sid="99" ssid="33">?/?/??(Give the visitors solid and methodical knowledge)?</S>
			<S sid="100" ssid="34">was ????.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="5">
			<S sid="101" ssid="1">In this paper, we presented our two-stage depen dency parsing system submitted to the Multilingual Track of CoNLL-2007 shared task.</S>
			<S sid="102" ssid="2">We used Nivre?smethod to produce the dependency arcs and the se quence labeler to produce the dependency labels.</S>
			<S sid="103" ssid="3">The experimental results showed that our system can provide good performance for all languages.</S>
	</SECTION>
</PAPER>
