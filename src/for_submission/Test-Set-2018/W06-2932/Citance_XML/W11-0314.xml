<PAPER>
  <S sid="0">ULISSE: an Unsupervised Algorithm for Detecting Reliable Dependency Parses</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper we present ULISSE, an unsupervised linguistically&#8211;driven algorithm to select reliable parses from the output of a dependency parser.</S>
    <S sid="2" ssid="2">Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains.</S>
    <S sid="3" ssid="3">In all cases, ULISSE appears to outperform the baseline algorithms.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">While the accuracy of state&#8211;of&#8211;the&#8211;art parsers is increasing more and more, this is still not enough for their output to be used in practical NLP&#8211;based applications.</S>
    <S sid="5" ssid="2">In fact, when applied to real&#8211;world texts (e.g. the web or domain&#8211;specific corpora such as bio&#8211;medical literature, legal texts, etc.) their accuracy decreases significantly.</S>
    <S sid="6" ssid="3">This is a real problem since it is broadly acknowledged that applications such as Information Extraction, Question Answering, Machine Translation, and so on can benefit significantly from exploiting the output of a syntactic parser.</S>
    <S sid="7" ssid="4">To overcome this problem, over the last few years a growing interest has been shown in assessing the reliability of automatically produced parses: the selection of high quality parses represents nowadays a key and challenging issue.</S>
    <S sid="8" ssid="5">The number of studies devoted to detecting reliable parses from the output of a syntactic parser is spreading.</S>
    <S sid="9" ssid="6">They mainly differ with respect to the kind of selection algorithm they exploit.</S>
    <S sid="10" ssid="7">Depending on whether training data, machine learning classifiers or external parsers are exploited, existing algorithms can be classified into i) supervised&#8211;based, ii) ensemble&#8211;based and iii) unsupervised&#8211;based methods.</S>
    <S sid="11" ssid="8">The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types.</S>
    <S sid="12" ssid="9">Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser.</S>
    <S sid="13" ssid="10">Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.</S>
    <S sid="14" ssid="11">), whereas Ravi et al. (2008) exploited an external constituency parser to extract text&#8211;based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy.</S>
    <S sid="15" ssid="12">The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble&#8211;based methods.</S>
    <S sid="16" ssid="13">Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data.</S>
    <S sid="17" ssid="14">However, a widely acknowledged problem of both supervised&#8211;based and ensemble&#8211;based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser.</S>
    <S sid="18" ssid="15">To our knowledge, Reichart and Rappoport (2009a) are the first to address the task of high quality parse selection by resorting to an unsupervised&#8211; based method.</S>
    <S sid="19" ssid="16">The underlying idea is that syntactic structures that are frequently created by a parser are more likely to be correct than structures produced less frequently.</S>
    <S sid="20" ssid="17">For this purpose, their PUPA (POS&#8211; based Unsupervised Parse Assessment Algorithm) uses statistics about POS tag sequences of parsed sentences produced by an unsupervised constituency parser.</S>
    <S sid="21" ssid="18">In this paper, we address this unsupervised scenario with two main novelties: unlike Reichart and Rappoport (2009a), a) we address the reliable parses selection task using an unsupervised method in a supervised parsing scenario, and b) we operate on dependency&#8211;based representations.</S>
    <S sid="22" ssid="19">Similarly to Reichart and Rappoport (2009a) we exploit text internal statistics: but whereas they rely on features that are closely related to constituency representations, we use linguistic features which are dependency&#8211; motivated.</S>
    <S sid="23" ssid="20">The proposed algorithm has been evaluated for selecting reliable parses from English and Italian corpora; to our knowledge, this is the first time that such a task has been applied to a less resourced language such as Italian.</S>
    <S sid="24" ssid="21">The paper is organised as follows: in Section 2 we illustrate the ULISSE algorithm; sections 3 and 4 are devoted to the used parsers and baselines.</S>
    <S sid="25" ssid="22">Section 5 describes the experiments and discusses achieved results.</S>
  </SECTION>
  <SECTION title="2 The ULISSE Algorithm" number="2">
    <S sid="26" ssid="1">The ULISSE (Unsupervised LInguiStically&#8211;driven Selection of dEpendency parses) algorithm takes as input a set of parsed sentences and it assigns to each dependency tree a score quantifying its reliability.</S>
    <S sid="27" ssid="2">It operates in two different steps: 1) it collects statistics about a set of linguistically&#8211;motivated features extracted from a corpus of parsed sentences; 2) it calculates a quality (or reliability) score for each analyzed sentence using the feature statistics extracted from the whole corpus.</S>
    <S sid="28" ssid="3">The features exploited by ULISSE are all linguistically motivated and rely on the dependency tree structure.</S>
    <S sid="29" ssid="4">Different criteria guided their selection.</S>
    <S sid="30" ssid="5">First, as pointed out in Roark et al. (2007), we needed features which could be reliably identified within the automatic output of a parser.</S>
    <S sid="31" ssid="6">Second, we focused on dependency structures that are widely agreed in the literature a) to reflect sentences&#8217; syntactic and thus parsing complexity and b) to impose a high cognitive load on the parsing of a complete sentence.</S>
    <S sid="32" ssid="7">Here follows the list of features used in the experiments reported in this paper, which turned out to be the most effective ones for the task at hand.</S>
    <S sid="33" ssid="8">Parse tree depth: this feature is a reliable indicator of sentence complexity due to the fact that, with sentences of approximately the same length, parse tree depth can be indicative of increased sentence complexity (Yngve, 1960; Frazier, 1985; Gibson, 1998; Nenkova, 2010).</S>
    <S sid="34" ssid="9">Depth of embedded complement &#8216;chains&#8217;: this feature is a subtype of the previous one, focusing on the depth of chains of embedded complements, either prepositional complements or nominal and adjectival modifiers.</S>
    <S sid="35" ssid="10">Long chains of embedded complements make the syntactic structure more complex and their analysis much more difficult.</S>
    <S sid="36" ssid="11">Arity of verbal predicates: this feature refers to the number of dependency links sharing the same verbal head.</S>
    <S sid="37" ssid="12">Here, there is no obvious relation between the number of dependents and sentence complexity: both a small number and a high number of dependents can make the sentence processing quite complex, although for different reasons (elliptical constructions in the former case, a high number of modifiers in the latter).</S>
    <S sid="38" ssid="13">Verbal roots: this feature counts the number of verbal roots with respect to number of all sentence roots in the target corpus.</S>
    <S sid="39" ssid="14">Subordinate vs main clauses: subordination is generally considered to be an index of structural complexity in language.</S>
    <S sid="40" ssid="15">Two distinct features are considered for monitoring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause.</S>
    <S sid="41" ssid="16">It is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post&#8211;verbal rather than in pre&#8211;verbal position (Miller, 1998).</S>
    <S sid="42" ssid="17">Length of dependency links: McDonald and Nivre (2007) report that statistical parsers have a drop in accuracy when analysing long distance dependencies.</S>
    <S sid="43" ssid="18">This is in line with Lin (1996) and Gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies.</S>
    <S sid="44" ssid="19">Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent.</S>
    <S sid="45" ssid="20">Dependency link plausibility (henceforth, ArcPOSFeat): this feature is used to calculate the plausibility of a dependency link given the part&#8211;of&#8211;speech of the dependent and the head, by also considering the PoS of the head father and the dependency linking the two.</S>
    <S sid="46" ssid="21">The quality score (henceforth, QS) of parsed sentences results from a combination of the weights associated with the monitored features.</S>
    <S sid="47" ssid="22">ULISSE is modular and can use several weights combination strategies, which may be customised with respect to the specific task exploiting the output of ULISSE.</S>
    <S sid="48" ssid="23">For this study, QS is computed as a simple product of the individual feature weights.</S>
    <S sid="49" ssid="24">This follows from the necessity to recognize high quality parses within the input set of parsed sentences: the product combination strategy is able to discard low quality parse trees even in presence of just one low weight feature.</S>
    <S sid="50" ssid="25">Therefore, QS for each sentence i in the set of input parsed sentences I is QS(Si) = Hny_1 Weight(Si, fy), where Si is the i&#8211;th sentence of I, n is the total number of selected features and Weight(Si, fy) is the computed weight for the y&#8211;th feature.</S>
    <S sid="51" ssid="26">Selected features can be divided into two classes, depending on whether they are computed with respect to each sentence and averaged over all sentences in the target corpus (global features), or they are computed with respect to individual dependency links and averaged over all of them (local features).</S>
    <S sid="52" ssid="27">The latter is the case of the ArcPOSFeat feature, whereas the all other ones represent global features.</S>
    <S sid="53" ssid="28">For the global features, the Weight(Si, fy) is defined as: where V (fy) is the value of the y&#8211;th feature (extracted from Si), L(Si) is the length of the sentence Si, range(L(Si), r) defines a range covering values from L(Si) &#8722; r and L(Si) + r, F(V (fy), range(L(Si), r)) is the frequency of V (fy) in all sentences in I that has a value of length in range(L(Si), r1) and |range(L(Si), r)| is the total number of sentences in I with length in range(L(Si), r).</S>
    <S sid="54" ssid="29">For what concerns the local feature ArcPOSFeat, ULISSE assigns a weight for each arc in Si: in principle different strategies can be used to compute a unique weight for this feature for Si.</S>
    <S sid="55" ssid="30">Here, the sentence weight for the feature ArcPOSFeat is computed as the minimum weight among the weights of all arcs of Si.</S>
    <S sid="56" ssid="31">Therefore, Weight(Si, ArcPOSFeat) = min{weight((Pd, Ph, t)),V(Pd, Ph, t) E Si}, where the triple (Pd, Ph, t) is an arc in Si in which Pd is the POS of the dependent, Ph is the POS of the syntactic head and t is the type of the dependency relation and weight((Pd, Ph, t)) is the weight of the specific arc (Pd, Ph, t).</S>
    <S sid="57" ssid="32">The individual arc weight is computed as follows: where F(x) is the frequency of x in I, X is a variable and (arc1 arc2) represent two consecutive arcs in the tree.</S>
  </SECTION>
  <SECTION title="3 The Parsers" number="3">
    <S sid="58" ssid="1">ULISSE was tested against the output of two really different data&#8211;driven parsers: the first&#8211;order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm.</S>
    <S sid="59" ssid="2">The former is a graph&#8211;based parser (following the so&#8211; called &#8220;all&#8211;pairs&#8221; approach Buchholz et al. (2006)) where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for a maximum spanning tree in a directed graph.</S>
    <S sid="60" ssid="3">The latter is a Shift&#8211;Reduce parser (following a &#8220;stepwise&#8221; approach, Buchholz et al. (2006)), where the parser is trained and learns the sequence of parsing actions required to build the parse tree.</S>
    <S sid="61" ssid="4">Although both parser models show a similar accuracy, McDonald and Nivre (2007) demonstrate that the two types of models exhibit different behaviors.</S>
    <S sid="62" ssid="5">Their analysis exemplifies how different the two parsers behave when their accuracies are compared with regard to some linguistic features of the analyzed sentences.</S>
    <S sid="63" ssid="6">To mention only a few, the Shift&#8211; Reduce parser tends to perform better on shorter sentences, while the MST parser guarantees a higher accuracy in identifying long distance dependencies.</S>
    <S sid="64" ssid="7">As regards the identification of dependency types, the MST parser shows a better ability to identify the dependents of the sentences&#8217; roots whereas the Shift&#8211;Reduce tends to better recognize specific relations (e.g.</S>
    <S sid="65" ssid="8">Subject and Object).</S>
    <S sid="66" ssid="9">McDonald and Nivre (2007) describe how the systems&#8217; behavioral differences are due to the different parsing algorithms implemented by the Shift&#8211; Reduce and the MST parsing models.</S>
    <S sid="67" ssid="10">The Shift Reduce parser constructs a dependency tree by performing a sequence of parser actions or transitions through a greedy parsing strategy.</S>
    <S sid="68" ssid="11">As a result of this parsing procedure, a Shift Reduce parser creates shorter arcs before longer arcs.</S>
    <S sid="69" ssid="12">The latter could be the reason for the lower accuracy in identifying longer arcs when compared to the MST parser.</S>
    <S sid="70" ssid="13">This also influences a lower level of accuracy in the analysis of longer sentences that usually contain longer arcs than shorter sentences.</S>
    <S sid="71" ssid="14">The MST parser&#8217;s ability to analyze both short and long arcs is invariant as it employs a graph-based parsing method where every possible arc is considered in the construction of the dependency tree.</S>
  </SECTION>
  <SECTION title="4 The Baselines" number="4">
    <S sid="72" ssid="1">Three different increasingly complex baseline models were used to evaluate the performance of ULISSE.</S>
    <S sid="73" ssid="2">The first baseline is constituted by a Random Selection (RS) of sentences from the test sets.</S>
    <S sid="74" ssid="3">This baseline is calculated in terms of the scores of the parser systems on the test set.</S>
    <S sid="75" ssid="4">The second baseline is represented by the Sentence Length (SL), starting from the assumption, demonstrated by McDonald and Nivre (2007), that long sentences are harder to analyse using statistical dependency parsers than short ones.</S>
    <S sid="76" ssid="5">This is a strong unsupervised baseline based on raw text features, ranking the parser results from the shortest sentence to the longest one.</S>
    <S sid="77" ssid="6">The third and most advanced baseline, exploiting parse features, is the PUPA algorithm (Reichart and Rappoport, 2007a).</S>
    <S sid="78" ssid="7">PUPA uses a set of parsed sentences to compute the statistics on which its scores are based.</S>
    <S sid="79" ssid="8">The PUPA algorithm operates on a constituency based representation and collects statistics about the POS tags of the words in the yield of the constituent and of the words in the yields of neighboring constituents.</S>
    <S sid="80" ssid="9">The sequences of POS tags that are more frequent in target corpus receive higher scores after proper regularization is applied to prevent potential biases.</S>
    <S sid="81" ssid="10">Therefore, the final score assigned to a constituency tree results from a combination of the scores of its extracted sequences of POSs.</S>
    <S sid="82" ssid="11">In order to use PUPA as a baseline, we implemented a dependency&#8211;based version, hencefoth referred to as dPUPA. dPUPA uses the same score computation of PUPA and collects statistics about sequences of POS tags: the difference lies in the fact that in this case the POS sequences are not extracted from constituency trees but rather from dependency trees.</S>
    <S sid="83" ssid="12">To be more concrete, rather than representing a sentence as a collection of constituency&#8211;based sequences of POSs, dPUPA represents each sentence as a collection of sequences of POSs covering all identified dependency subtrees.</S>
    <S sid="84" ssid="13">In particular, each dependency tree is represented as the set of all subtrees rooted by non&#8211;terminal nodes.</S>
    <S sid="85" ssid="14">Each subtree is then represented as the sequence of POS tags of the words in the subtree (reflecting the word order of the original sentence) integrated with the POS of the leftmost and rightmost in the sentence (NULL when there are no neighbors).</S>
    <S sid="86" ssid="15">Figure 1 shows the example of the dependency tree for the sentence I will give you the ball.</S>
    <S sid="87" ssid="16">If we consider the subtree rooted by give (in the dotted circle), the resulting POS sequence is as follows: POS2 POS3 POS4 POS5 POSE NULL, where POS3 POS4 POS5 POSE is the sequence of POS tags in the subtree, POS2 is the left neighbor POS tag and NULL marks the absence of a right neighbor.</S>
  </SECTION>
  <SECTION title="5 Experiments and Results" number="5">
    <S sid="88" ssid="1">The experiments were organised as follows: a target corpus was automatically POS tagged (Dell&#8217;Orletta, 2009) and dependency&#8211;parsed; the ULISSE and baseline algorithms of reliable parse selection were run on the POS&#8211;tagged and dependency&#8211;parsed target corpus in order to identify high quality parses; results achieved by the selection algorithms were evaluated with respect to a subset of the target corpus of about 5,000 word&#8211;tokens (henceforth referred to as &#8220;test set&#8221;) for which gold-standard annotation was available.</S>
    <S sid="89" ssid="2">Different sets of experiments were devised to test the robustness of our algorithm.</S>
    <S sid="90" ssid="3">They were performed with respect to i) the output of the parsers described in Section 3, ii) two different languages, iii) different domains.</S>
    <S sid="91" ssid="4">For what concerns the languages, we chose Italian and English for two main reasons.</S>
    <S sid="92" ssid="5">First of all, they pose different challenges to a parser since they are characterised by quite different syntactic features.</S>
    <S sid="93" ssid="6">For instance, Italian, as opposed to English, is characterised by a relatively free word order (especially for what concerns subject and object relations with respect to the verb) and by the possible absence of an overt subject.</S>
    <S sid="94" ssid="7">Secondly, as it is shown in Section 5.1, Italian is a less resourced language with respect to English.</S>
    <S sid="95" ssid="8">This is a key issue, since as demonstrated by Reichart and Rappoport (2007b) and McClosky et al. (2008), small and big treebanks pose different problems in the reliable parses selection.</S>
    <S sid="96" ssid="9">Last but not least, we aimed at demonstrating that ULISSE can be successfully used not only with texts belonging to the same domain as the parser training corpus.</S>
    <S sid="97" ssid="10">For this purpose, ULISSE was tested on a target corpus of Italian legislative texts, whose automatic linguistic analysis poses domain&#8211;specific challenges (Venturi, 2010).</S>
    <S sid="98" ssid="11">Out&#8211;of&#8211;domain experiments are being carried out also for English.</S>
    <S sid="99" ssid="12">The Italian corpora Both parsers were trained on ISST&#8211;TANL2, a dependency annotated corpus used in Evalita&#8217;093, an evaluation campaign carried out for Italian (Bosco et al., 2009).</S>
    <S sid="100" ssid="13">ISST&#8211;TANL includes 3,109 sentences (71,285 tokens) and consists of articles from newspapers and periodicals.</S>
    <S sid="101" ssid="14">Two different target corpora were used for the in&#8211;domain and out&#8211;of&#8211;domain experiments.</S>
    <S sid="102" ssid="15">For the former, we used a corpus of 1,104,237 sentences (22,830,739 word&#8211;tokens) of newspapers texts which was extracted from the CLIC-ILC Corpus (Marinelli et al., 2003); for the legal domain, we used a collection of Italian legal texts (2,697,262 word&#8211;tokens; 97,564 sentences) regulating a variety of domains, ranging from environment, human rights, disability rights, freedom of expression to privacy, age disclaimer, etc.</S>
    <S sid="103" ssid="16">In the two experiments, the test sets were represented respectively by: a) the test set used in the Evalita&#8217;09 evaluation campaign, constituted by 260 sentences and 5,011 tokens from newpapers text; b) a set of 102 sentences (corresponding to 5,691 tokens) from legal texts.</S>
    <S sid="104" ssid="17">The English corpora For the training of parsers we used the dependency&#8211;based version of Sections 2&#8211;11 of the Wall Street Journal partition of the Penn Treebank (Marcus et al., 2003), which was developed for the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007): it includes 447,000 word tokens and about 18,600 sentences.</S>
    <S sid="105" ssid="18">As target data we took a corpus of news, specifically the whole Wall Street Journal Section of the Penn Treebank4, from which the portion of text corresponding to the training corpus was removed; the English target corpus thus includes 39,285,425 tokens (1,625,606 sentences).</S>
    <S sid="106" ssid="19">For testing we used the test set of the CoNLL 2007 Shared Task, corresponding to a subset of Section 23 of the Wall Street Journal partition of the Penn Treebank (5,003 tokens, 214 sentences).</S>
    <S sid="107" ssid="20">Performances of the ULISSE algorithm have been evaluated i) with respect to the accuracy of ranked parses and ii) in terms of Precision and Recall.</S>
    <S sid="108" ssid="21">First, for each experiment we evaluated how the ULISSE algorithm and the baselines classify the sentences in the test set with respect to the &#8220;Labelled Attachment Score&#8221; (LAS) obtained by the parsers, i.e. the percentage of tokens for which it has predicted the correct head and dependency relation.</S>
    <S sid="109" ssid="22">In particular, we computed the LAS score of increasingly wider top lists of k tokens, where k ranges from 500 word tokens to the whole size of the test set (with a step size of 500 word tokens, i.e. k=500, k=1000, k=1500, etc.).</S>
    <S sid="110" ssid="23">As regards ii), we focused on the set of ranked sentences showing a LAS &gt; &#945;.</S>
    <S sid="111" ssid="24">Since imposing a 100% LAS was too restrictive, for each experiment we defined a different &#945; threshold taking into account the performance of each parser across the different languages and domains.</S>
    <S sid="112" ssid="25">In particular, we took the top 25% and 50% of the list of ranked sentences and calculated Precision and Recall for each of them.</S>
    <S sid="113" ssid="26">To this specific end, a parse tree showing a LAS &gt; &#945; is considered as a trustworthy analysis.</S>
    <S sid="114" ssid="27">Precision has been computed as the ratio of the number of trustworthy analyses over the total number of sentences in each top list.</S>
    <S sid="115" ssid="28">Recall has been computed as the ratio of the number of trustworthy analyses which have been retrieved over the total number of trustworthy analyses in the whole test set.</S>
    <S sid="116" ssid="29">In order to test how the ULISSE algorithm is able to select reliable parses by relying on parse features rather than on raw text features, we computed the accuracy score (LAS) of a subset of the top list of sentences parsed by both parsers and ranked by ULISSE: in particular, we focused on those sentences which were not shared by the MST and DeSR top lists.</S>
    <S sid="117" ssid="30">We will refer to the performed experiments as follows: &#8220;IT in&#8211;domain&#8221; and &#8220;IT out&#8211;of&#8211;domain&#8221; for the Italian experiments using respectively the ISST&#8211; TANL test set (henceforth ISST TS) and the LegalCorpus test set (henceforth Legal TS); &#8220;EN in&#8211; domain&#8221; for the English experiment using the PTB test set (PTB TS).</S>
    <S sid="118" ssid="31">As a starting point let us consider the accuracy of DeSR and MST parsers on the whole test sets, reported in Table 1.</S>
    <S sid="119" ssid="32">The accuracy has been computed in terms of LAS and of Unlabelled Attachment Score (UAS), i.e. the percentage of tokens with a correctly identified syntactic head.</S>
    <S sid="120" ssid="33">It can be noticed that the performance of the two parsers is quite similar for Italian (i.e. wrt ISST TS and Legal TS), whereas there is a 2.3% difference between the MST and DeSR accuracy as far as English is concerned.</S>
    <S sid="121" ssid="34">The plots in Figure 2 show the LAS of parses ranked by ULISSE and the baselines across the different experiments.</S>
    <S sid="122" ssid="35">Each plot reports the results of a single experiment: plots in the same row report the LAS of DeSR and MST parsers with respect to the same test set.</S>
    <S sid="123" ssid="36">In all experiments, ULISSE turned out to be the best ranking algorithm since it appears to select top lists characterised by higher LAS scores than the baselines.</S>
    <S sid="124" ssid="37">As Figure 2 shows, all ranking algorithms perform better than Random Selection (RS), i.e. all top lists (for each k value) show a LAS higher than the accuracy of DeSR and MST parsers on the whole test sets.</S>
    <S sid="125" ssid="38">In the EN in&#8211;domain experiment, the difference between the results of ULISSE and the other ranking algorithms is smaller than in the corresponding Italian experiment, a fact resulting from the higher accuracy of DeSR and MST parsers (i.e.</S>
    <S sid="126" ssid="39">LAS 85.95% and 88.25% respectively) on the PTB TS.</S>
    <S sid="127" ssid="40">It follows that, for example, the first top list (with k=500) of the SL baseline has a LAS accuracy of 93.36% and 93.96% respectively for DeSR and MST: even in this case, ULISSE outperforms all baselines.</S>
    <S sid="128" ssid="41">This is also the case in the IT out&#8211;of&#8211;domain experiment.</S>
    <S sid="129" ssid="42">As reported in Table 1, parsing legal texts is a quite challenging task due to a number of domain&#8211;specific peculiarities at the level of syntax: this is testified by the average sentence length which in the Legal TS is 56 word tokens.</S>
    <S sid="130" ssid="43">Nevertheless, ULISSE is able also in this case to highly rank long sentences showing a high LAS.</S>
    <S sid="131" ssid="44">For example, while in the first top list of 500 word tokens the sentences parsed by DeSR and ordered by SL have an average sentence length of 24 words and a LAS of 79.37%, ULISSE includes in the same top list longer sentences (with average sentence length = 29) with a higher LAS (82.72%).</S>
    <S sid="132" ssid="45">Also dPUPA ranks in the same top list quite long sentences (with 27 average sentence length), but compared to ULISSE it shows a lower LAS (i.e.</S>
    <S sid="133" ssid="46">73.56%).</S>
    <S sid="134" ssid="47">Results in Table 2 show that in the top 25% of the ranked sentences with a LAS &gt; &#945; ULISSE has the highest Precision and Recall in all experiments.</S>
    <S sid="135" ssid="48">We believe that the low performance of dPUPA with respect to all other ranking algorithms can be due to the fact that PUPA is based on constituency&#8211;specific features that once translated in terms of dependency structures may be not so effective.</S>
    <S sid="136" ssid="49">In order to show that the ranking of sentences does not follow from raw text features but rather from parse features, we evaluated the accuracy of parsed sentences that are not&#8211;shared by MST and DeSR top&#8211;lists selected by ULISSE.</S>
    <S sid="137" ssid="50">For each test set we selected a different top list: a set of 100 sentences in the IT and EN in&#8211;domain experiments and of 50 sentences in the IT out&#8211;of&#8211;domain experiment.</S>
    <S sid="138" ssid="51">For each of them we have a different number of not&#8211;shared sentences: 24, 15 and 16 in the IT in&#8211;domain, IT out&#8211;of&#8211;domain and EN in&#8211;domain experiments respectively.</S>
    <S sid="139" ssid="52">Table 3 reports the LAS of DeSR and MST for these sentences: it can be observed that the LAS of not&#8211;shared sentences in the DeSR top list is always higher than the LAS assigned by the same parser to the not&#8211;shared sentences in the MST top list, and viceversa.</S>
    <S sid="140" ssid="53">For instance, in the English experiment the LAS achieved by DeSR on the not&#8211;shared top list is higher (86.50) than the LAS of DeSR on the not&#8211;shared MST top list (83.37); viceversa, the LAS of MST on the not&#8211; shared DeSR top list is higher (86.74) than the LAS of MST on the not&#8211;shared MST top list (90.39).</S>
    <S sid="141" ssid="54">The unique exception is MST in the IT out&#8211;of&#8211;domain experiment, but the difference in terms of LAS between the parses is not statistically relevant (p&#8211;value &lt; 0.05).</S>
    <S sid="142" ssid="55">These results demonstrate that ULISSE is able to select parsed sentences on the basis of the reliability of the analysis produced by each parser.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="143" ssid="1">ULISSE is an unsupervised linguistically&#8211;driven method to select reliable parses from the output of dependency parsers.</S>
    <S sid="144" ssid="2">To our knowledge, it represents the first unsupervised ranking algorithm operating on dependency representations which are more and more gaining in popularity and are arguably more useful for some applications than constituency parsers.</S>
    <S sid="145" ssid="3">ULISSE shows a promising performance against the output of two supervised parsers selected for their behavioral differences.</S>
    <S sid="146" ssid="4">In all experiments, ULISSE outperforms all baselines, including dPUPA and Sentence Length (SL), the latter representing a very strong baseline selection method in a supervised scenario, where parsers have a very high performance with short sentences.</S>
    <S sid="147" ssid="5">The fact of carrying out the task of reliable parse selection in a supervised scenario represents an important novelty: however, the unsupervised nature of ULISSE could also be used in an unsupervised scenario (Reichart and Rappoport, 2010).</S>
    <S sid="148" ssid="6">Current direction of research include a careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and effectivess of the set of linguistic features.</S>
    <S sid="149" ssid="7">This study is being carried out with a specific view to NLP tasks which might benefit from the ULISSE algorithm.</S>
    <S sid="150" ssid="8">This is the case, for instance, of the domain adaptation task in a self&#8211;training scenario (McClosky et al., 2006), of the treebank construction process by minimizing the human annotators&#8217; efforts (Reichart and Rappoport, 2009b), of n&#8211;best ranking methods for machine translation (Zhang, 2006).</S>
  </SECTION>
</PAPER>
