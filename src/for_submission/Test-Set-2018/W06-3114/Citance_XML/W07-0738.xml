<PAPER>
  <S sid="0">Linguistic Features for Automatic Evaluation of Heterogenous MT Systems</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, may not be a reliable MT quality indicator.</S>
    <S sid="2" ssid="2">This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon.</S>
    <S sid="3" ssid="3">The reason is that, while MT quality aspects are its scope to the lexical dimension.</S>
    <S sid="4" ssid="4">In this work, we suggest using metrics which take into account linguistic features at more abstract levels.</S>
    <S sid="5" ssid="5">We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Most metrics used in the context of Automatic Machine Translation (MT) Evaluation are based on the assumption that &#8216;acceptable&#8217; translations tend to share the lexicon (i.e., word forms) in a predefined set of manual reference translations.</S>
    <S sid="7" ssid="2">This assumption works well in many cases.</S>
    <S sid="8" ssid="3">However, several results in recent MT evaluation campaigns have cast some doubts on its general validity.</S>
    <S sid="9" ssid="4">For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001).</S>
    <S sid="10" ssid="5">In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.)</S>
    <S sid="11" ssid="6">BLEU may not be a reliable MT quality indicator.</S>
    <S sid="12" ssid="7">The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one.</S>
    <S sid="13" ssid="8">Indeed, the underlying cause is much simpler.</S>
    <S sid="14" ssid="9">In general, lexical similarity is nor a sufficient neither a necessary condition so that two sentences convey the same meaning.</S>
    <S sid="15" ssid="10">On the contrary, natural languages are expressive and ambiguous at different levels.</S>
    <S sid="16" ssid="11">Consequently, the similarity between two sentences may involve different dimensions.</S>
    <S sid="17" ssid="12">In this work, we hypothesize that, in order to &#8216;fairly&#8217; evaluate MT systems based on different paradigms, similarities at more abstract linguistic levels must be analyzed.</S>
    <S sid="18" ssid="13">For that purpose, we have compiled a rich set of metrics operating at the lexical, syntactic and shallow-semantic levels (see Section 2).</S>
    <S sid="19" ssid="14">We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3).</S>
    <S sid="20" ssid="15">We show that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than those produced by metrics which limit their scope to the lexical dimension, specially when the systems under evaluation are of a different nature.</S>
  </SECTION>
  <SECTION title="2 A Heterogeneous Metric Set" number="2">
    <S sid="21" ssid="1">For our experiments, we have compiled a representative set of metrics1 at different linguistic levels.</S>
    <S sid="22" ssid="2">We have resorted to several existing metrics, and we have also developed new ones.</S>
    <S sid="23" ssid="3">Below, we group them according to the level at which they operate.</S>
    <S sid="24" ssid="4">Most of the current metrics operate at the lexical level.</S>
    <S sid="25" ssid="5">We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001).</S>
    <S sid="26" ssid="6">NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002).</S>
    <S sid="27" ssid="7">GTM We set to 1 the value of the e parameter (Melamed et al., 2003).</S>
    <S sid="28" ssid="8">METEOR We run all modules: &#8216;exact&#8217;, &#8216;porterstem&#8217;, &#8216;wn stem&#8217; and &#8216;wn synonymy&#8217;, in that order (Banerjee and Lavie, 2005).</S>
    <S sid="29" ssid="9">ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length).</S>
    <S sid="30" ssid="10">Stemming is enabled (Lin and Och, 2004a). mWER We use 1 &#8722; mWER (Nie&#223;en et al., 2000). mPER We use 1 &#8722; mPER (Tillmann et al., 1997).</S>
    <S sid="31" ssid="11">Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations).</S>
    <S sid="32" ssid="12">Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998).</S>
    <S sid="33" ssid="13">Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures.</S>
    <S sid="34" ssid="14">We have defined what we call &#8216;linguistic elements&#8217; (LEs).</S>
    <S sid="35" ssid="15">LEs are linguistic units, structures, or relationships, such that a sentence may be partially seen as a &#8216;bag&#8217; of LEs.</S>
    <S sid="36" ssid="16">Possible kinds of LEs are: word forms, parts-of-speech, dependency relationships, syntactic phrases, named entities, semantic roles, etc.</S>
    <S sid="37" ssid="17">Each LE may consist, in its turn, of one or more LEs, which we call &#8216;items&#8217; inside the LE.</S>
    <S sid="38" ssid="18">For instance, a &#8216;phrase&#8217; LE may consist of &#8216;phrase&#8217; items, &#8216;part-ofspeech&#8217; (PoS) items, &#8216;word form&#8217; items, etc.</S>
    <S sid="39" ssid="19">Items may be also combinations of LEs.</S>
    <S sid="40" ssid="20">For instance, a &#8216;phrase&#8217; LE may be seen as a sequence of &#8216;wordform:PoS&#8217; items.</S>
    <S sid="41" ssid="21">We are interested in comparing linguistic structures, and linguistic units.</S>
    <S sid="42" ssid="22">LEs allow for comparisons at different granularity levels, and from different viewpoints.</S>
    <S sid="43" ssid="23">For instance, we might compare the semantic structure of two sentences (i.e., which actions, semantic arguments and adjuncts exist) or we might compare lexical units according to the semantic role they play inside the sentence.</S>
    <S sid="44" ssid="24">For that purpose, we use two very simple kinds of similarity measures over LEs: &#8216;Overlapping&#8217; and &#8216;Matching&#8217;.</S>
    <S sid="45" ssid="25">We provide a general definition: Overlapping between items inside LEs, according to their type.</S>
    <S sid="46" ssid="26">Formally: where t is the LE type2, itemst(s) refers to the set of items occurring inside LEs of type t in sentence s, countref(i, t) denotes the number of times item i appears in the reference translation inside a LE of type t, and count&#65533;hyp(i, t) denotes the number of times i appears in the candidate translation inside a LE of type t, limited by the number of times i appears in the reference translation inside a LE of type t. Thus, &#8216;Overlapping&#8217; provides a rough measure of the proportion of items inside elements of a certain type which have been &#8216;successfully&#8217; translated.</S>
    <S sid="47" ssid="27">We also introduce a coarser metric, &#8216;Overlapping(*)&#8217;, which considers the uniformly averaged &#8216;overlapping&#8217; over all types: where T is the set of types.</S>
    <S sid="48" ssid="28">2LE types vary according to the specific LE class.</S>
    <S sid="49" ssid="29">For instance, in the case of Named Entities types may be &#8216;PER&#8217; (i.e., person), &#8216;LOC&#8217; (i.e., location), &#8216;ORG&#8217; (i.e., organization), etc.</S>
    <S sid="50" ssid="30">Matching between items inside LEs, according to their type.</S>
    <S sid="51" ssid="31">Its definition is analogous to the &#8216;Overlapping&#8217; definition, but in this case the relative order of the items is important.</S>
    <S sid="52" ssid="32">All items inside the same element are considered as a single unit (i.e., a sequence in left-to-right order).</S>
    <S sid="53" ssid="33">In other words, we are computing the proportion of &#8216;fully&#8217; translated elements, according to their type.</S>
    <S sid="54" ssid="34">We also introduce a coarser metric, &#8216;Matching(*)&#8217;, which considers the uniformly averaged &#8216;Matching&#8217; over all types.</S>
    <S sid="55" ssid="35">Metrics based on shallow parsing (SP&#8217;) analyze similarities at the level of PoS-tagging, lemmatization, and base phrase chunking.</S>
    <S sid="56" ssid="36">Outputs and references are automatically annotated using stateof-the-art tools.</S>
    <S sid="57" ssid="37">PoS-tagging and lemmatization are provided by the SVMTool package (Gim&#180;enez and M`arquez, 2004), and base phrase chunking is provided by the Phreco software (Carreras et al., 2005).</S>
    <S sid="58" ssid="38">Tag sets for English are derived from the Penn Treebank (Marcus et al., 1993).</S>
    <S sid="59" ssid="39">We instantiate &#8216;Overlapping&#8217; over parts-of-speech and chunk types.</S>
    <S sid="60" ssid="40">The goal is to capture the proportion of lexical items correctly translated, according to their shallow syntactic realization: SP-Op-t Lexical overlapping according to the partof-speech &#8216;t&#8217;.</S>
    <S sid="61" ssid="41">For instance, &#8216;SP-O,-NN&#8217; roughly reflects the proportion of correctly translated singular nouns.</S>
    <S sid="62" ssid="42">We also introduce a coarser metric, &#8216;SP-Op-*&#8217; which computes average overlapping over all parts-of-speech.</S>
    <S sid="63" ssid="43">SP-Oc-t Lexical overlapping according to the chunk type &#8216;t&#8217;.</S>
    <S sid="64" ssid="44">For instance, &#8216;SP-Oc-NP&#8217; roughly reflects the successfully translated proportion of noun phrases.</S>
    <S sid="65" ssid="45">We also introduce a coarser metric, &#8216;SP-Oc-*&#8217; which considers the average overlapping over all chunk types.</S>
    <S sid="66" ssid="46">At a more abstract level, we use the NIST metric (Doddington, 2002) to compute accumulated/individual scores over sequences of: Lemmas &#8211; SP-NIST(i)l-n Parts-of-speech &#8211; SP-NIST(i)p-n Base phrase chunks &#8211; SP-NIST(i)c-n For instance, &#8216;SP-NISTl-5&#8217; corresponds to the accumulated NIST score for lemma n-grams up to length 5, whereas &#8216;SP-NISTi,-5&#8217; corresponds to the individual NIST score for PoS 5-grams.</S>
    <S sid="67" ssid="47">We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea (2005) and Amig&#180;o et al. (2006) based on dependency and constituency parsing.</S>
    <S sid="68" ssid="48">DP&#8217; metrics capture similarities between dependency trees associated to automatic and reference translations.</S>
    <S sid="69" ssid="49">Dependency trees are provided by the MINIPAR dependency parser (Lin, 1998).</S>
    <S sid="70" ssid="50">Similarities are captured from different viewpoints: DP-HWC(i)-l This metric corresponds to the HWC metric presented by Liu and Gildea (2005).</S>
    <S sid="71" ssid="51">All head-word chains are retrieved.</S>
    <S sid="72" ssid="52">The fraction of matching head-word chains of a given length, &#8216;l&#8217;, is computed.</S>
    <S sid="73" ssid="53">We have slightly modified this metric in order to distinguish three different variants according to the type of items headword chains may consist of: Lexical forms &#8211; DP-HWC(i)w-l Grammatical categories &#8211; DP-HWC(i)c-l Grammatical relationships &#8211; DP-HWC(i)r-l Average accumulated scores up to a given chain length may be used as well.</S>
    <S sid="74" ssid="54">For instance, &#8216;DP-HWCiw-4&#8217; retrieves the proportion of matching length-4 word-chains, whereas &#8216;DP-HWCw4&#8217; retrieves average accumulated proportion of matching word-chains up to length-4.</S>
    <S sid="75" ssid="55">Analogously, &#8216;DP-HWCc-4&#8217;, and &#8216;DP-HWCr-4&#8217; compute average accumulated proportion of category/relationship chains up to length-4.</S>
    <S sid="76" ssid="56">DP-Ol|Oc|Or These metrics correspond exactly to the LEVEL, GRAM and TREE metrics introduced by Amig6 et al. (2006).</S>
    <S sid="77" ssid="57">DP-Ol-l Overlapping between words hanging at level &#8216;l&#8217;, or deeper.</S>
    <S sid="78" ssid="58">DP-Oc-t Overlapping between words directly hanging from terminal nodes (i.e. grammatical categories) of type &#8216;t&#8217;.</S>
    <S sid="79" ssid="59">DP-Or-t Overlapping between words ruled by non-terminal nodes (i.e. grammatical relationships) of type &#8216;t&#8217;.</S>
    <S sid="80" ssid="60">Node types are determined by grammatical categories and relationships defined by MINIPAR.</S>
    <S sid="81" ssid="61">For instance, &#8216;DP-Or-s&#8217; reflects lexical overlapping between subtrees of type &#8216;s&#8217; (subject).</S>
    <S sid="82" ssid="62">&#8216;DPOc-A&#8217; reflects lexical overlapping between terminal nodes of type &#8216;A&#8217; (Adjective/Adverbs).</S>
    <S sid="83" ssid="63">&#8216;DP-Ol-4&#8217; reflects lexical overlapping between nodes hanging at level 4 or deeper.</S>
    <S sid="84" ssid="64">Additionally, we consider three coarser metrics (&#8216;DP-Ol*&#8217;, &#8216;DP-Oc-*&#8217; and &#8216;DP-Or-*&#8217;) which correspond to the uniformly averaged values over all levels, categories, and relationships, respectively.</S>
    <S sid="85" ssid="65">&#8216;CP&#8217; metrics capture similarities between constituency parse trees associated to automatic and reference translations.</S>
    <S sid="86" ssid="66">Constituency trees are provided by the Charniak-Johnson&#8217;s Max-Ent reranking parser (Charniak and Johnson, 2005).</S>
    <S sid="87" ssid="67">CP-STM(i)-l This metric corresponds to the STM metric presented by Liu and Gildea (2005).</S>
    <S sid="88" ssid="68">All syntactic subpaths in the candidate and the reference trees are retrieved.</S>
    <S sid="89" ssid="69">The fraction of matching subpaths of a given length, &#8216;l&#8217;, is computed.</S>
    <S sid="90" ssid="70">For instance, &#8216;CP-STMi-5&#8217; retrieves the proportion of length-5 matching subpaths.</S>
    <S sid="91" ssid="71">Average accumulated scores may be computed as well.</S>
    <S sid="92" ssid="72">For instance, &#8216;CP-STM-9&#8217; retrieves average accumulated proportion of matching subpaths up to length-9.</S>
    <S sid="93" ssid="73">We have designed two new families of metrics, &#8216;NE&#8217; and &#8216;SR&#8217;, which are intended to capture similarities over Named Entities (NEs) and Semantic Roles (SRs), respectively.</S>
    <S sid="94" ssid="74">&#8216;NE&#8217; metrics analyze similarities between automatic and reference translations by comparing the NEs which occur in them.</S>
    <S sid="95" ssid="75">Sentences are automatically annotated using the BIOS package (Surdeanu et al., 2005).</S>
    <S sid="96" ssid="76">BIOS requires at the input shallow parsed text, which is obtained as described in Section 2.3.</S>
    <S sid="97" ssid="77">See the list of NE types in Table 1.</S>
    <S sid="98" ssid="78">We define two types of metrics: NE-Oe-t Lexical overlapping between NEs according to their type t. For instance, &#8216;NE-Oe-PER&#8217; reflects lexical overlapping between NEs of type &#8216;PER&#8217; (i.e., person), which provides a rough estimate of the successfully translated proportion of person names.</S>
    <S sid="99" ssid="79">The &#8216;NE-Oe-*&#8217; metric considers the average lexical overlapping over all NE types.</S>
    <S sid="100" ssid="80">This metric includes the NE type &#8216;O&#8217; (i.e., Not-a-NE).</S>
    <S sid="101" ssid="81">We introduce another variant, &#8216;NE-Oe-**&#8217;, which considers only actual NEs.</S>
    <S sid="102" ssid="82">NE-Me-t Lexical matching between NEs according to their type t. For instance, &#8216;NE-Me-LOC&#8217; reflects the proportion of fully translated NEs of type &#8216;LOC&#8217; (i.e., location).</S>
    <S sid="103" ssid="83">The &#8216;NE-Me-*&#8217; metric considers the average lexical matching over all NE types, this time excluding type &#8216;O&#8217;.</S>
    <S sid="104" ssid="84">Other authors have measured MT quality over NEs in the recent literature.</S>
    <S sid="105" ssid="85">In particular, the &#8216;NEMe-*&#8217; metric is similar to the &#8216;NEE&#8217; metric defined by Reeder et al. (2001).</S>
    <S sid="106" ssid="86">SR&#8217; metrics analyze similarities between automatic and reference translations by comparing the SRs (i.e., arguments and adjuncts) which occur in them.</S>
    <S sid="107" ssid="87">Sentences are automatically annotated using the SwiRL package (M`arquez et al., 2005).</S>
    <S sid="108" ssid="88">This package requires at the input shallow parsed text enriched with NEs, which is obtained as described in Section 2.5.1.</S>
    <S sid="109" ssid="89">See the list of SR types in Table 2.</S>
    <S sid="110" ssid="90">We define three types of metrics: SR-Or-t Lexical overlapping between SRs according to their type t. For instance, &#8216;SR-O,.-A0&#8217; reflects lexical overlapping between &#8216;A0&#8217; arguments.</S>
    <S sid="111" ssid="91">&#8216;SR-Or-*&#8217; considers the average lexical overlapping over all SR types.</S>
    <S sid="112" ssid="92">SR-Mr-t Lexical matching between SRs according to their type t. For instance, the metric &#8216;SR-M,.-AM-MOD&#8217; reflects the proportion of fully translated modal adjuncts.</S>
    <S sid="113" ssid="93">The &#8216;SR-Mr-*&#8217; metric considers the average lexical matching over all SR types.</S>
    <S sid="114" ssid="94">SR-Or This metric reflects &#8216;role overlapping&#8217;, i.e.. overlapping between semantic roles independently from their lexical realization.</S>
    <S sid="115" ssid="95">Note that in the same sentence several verbs, with their respective SRs, may co-occur.</S>
    <S sid="116" ssid="96">However, the metrics described above do not distinguish between SRs associated to different verbs.</S>
    <S sid="117" ssid="97">In order to account for such a distinction we introduce a more restrictive version of these metrics (&#8216;SR-M,.,,-t&#8217;, &#8216;SR-O,.,,-t&#8217;, &#8216;SR-Mrv-*&#8217;, &#8216;SR-Orv-*&#8217;, and &#8216;SR-Orv&#8217;), which require SRs to be associated to the same verb.</S>
  </SECTION>
  <SECTION title="3 Experimental Work" number="3">
    <S sid="118" ssid="1">In this section, we study the behavior of some of the metrics described in Section 2, according to the linguistic level at which they operate.</S>
    <S sid="119" ssid="2">We have selected a set of coarse-grained metric variants (i.e., accumulated/average scores over linguistic units and structures of different kinds)3.</S>
    <S sid="120" ssid="3">We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006).</S>
    <S sid="121" ssid="4">We distinguish different evaluation contexts.</S>
    <S sid="122" ssid="5">In Section 3.1, we study the case of a single reference translation being available.</S>
    <S sid="123" ssid="6">In principle, this scenario should diminish the reliability of metrics based on lexical matching alone, and favour metrics based on deeper linguistic features.</S>
    <S sid="124" ssid="7">In Section 3.2, we study the case of several reference translations available.</S>
    <S sid="125" ssid="8">This scenario should alleviate the deficiencies caused by the shallowness of metrics based on lexical matching.</S>
    <S sid="126" ssid="9">We also analyze separately the case of homogeneous&#8217; systems (i.e., all systems being of the same nature), and the case of heterogenous&#8217; systems (i.e., there exist systems based on different paradigms).</S>
    <S sid="127" ssid="10">As to the metric meta-evaluation criterion, the two most prominent criteria are: Human Acceptability Metrics are evaluated on the basis of correlation with human evaluators.</S>
    <S sid="128" ssid="11">Human Likeness Metrics are evaluated in terms of descriptive power, i.e., their ability to distinguish between human and automatic translations (Lin and Och, 2004b; Amig&#180;o et al., 2005).</S>
    <S sid="129" ssid="12">In our case, metrics are evaluated on the basis of &#8216;Human Acceptability&#8217;.</S>
    <S sid="130" ssid="13">Specifically, we use Pearson correlation coefficients between metric scores and the average sum of adequacy and fluency assessments at the document level.</S>
    <S sid="131" ssid="14">The reason is that meta-evaluation based on &#8216;Human Likeness&#8217; requires the availability of heterogenous test beds (i.e., representative sets of automatic outputs and human references), which, unfortunately, is not the case of all the tasks under study.</S>
    <S sid="132" ssid="15">First, because most translation systems are statistical.</S>
    <S sid="133" ssid="16">Second, because in most cases only one reference translation is available.</S>
    <S sid="134" ssid="17">We use some of the test beds corresponding to the &#147;NAACL 2006 Workshop on Statistical Machine Translation&#8221; (WMT 2006) (Koehn and Monz, 2006).</S>
    <S sid="135" ssid="18">Since linguistic features described in Section 2 are so far implemented only for the case of English being the target language, among the 12 translation tasks available, we studied only the 6 tasks corresponding to the Foreign-to-English direction.</S>
    <S sid="136" ssid="19">A single reference translation is available.</S>
    <S sid="137" ssid="20">System outputs consist of 2000 and 1064 sentences for the &#8216;indomain&#8217; and &#8216;out-of-domain&#8217; test beds, respectively.</S>
    <S sid="138" ssid="21">In each case, human assessments on adequacy and fluency are available for a subset of systems and sentences.</S>
    <S sid="139" ssid="22">Table 3 shows the number of sentences assessed in each case.</S>
    <S sid="140" ssid="23">Each sentence was evaluated by two different human judges.</S>
    <S sid="141" ssid="24">System scores have been obtained by averaging over all sentence scores. in out sys French-to-English 2,247 1,274 11/14 German-to-English 2,401 1,535 10/12 Spanish-to-English 1,944 1,070 11/15 Table 3: WMT 2006.</S>
    <S sid="142" ssid="25">&#8216;in&#8217; and &#8216;out&#8217; columns show the number of sentences assessed for the &#8216;indomain&#8217; and &#8216;out-of-domain&#8217; subtasks.</S>
    <S sid="143" ssid="26">The &#8216;sys&#8217; column shows the number of systems counting on human assessments with respect to the total number of systems which presented to each task.</S>
    <S sid="144" ssid="27">In four of the six translation tasks under study, all the systems are statistical except Systran&#8217;, which is rule-based.</S>
    <S sid="145" ssid="28">This is the case of the German/Frenchto-English in-domain/out-of-domain tasks.</S>
    <S sid="146" ssid="29">Table 4 shows correlation with human assessments for some metric representatives at different linguistic levels.</S>
    <S sid="147" ssid="30">Systems.</S>
    <S sid="148" ssid="31">French-to-English (fr2en) / German-toEnglish (de2en), in-domain and out-of-domain.</S>
    <S sid="149" ssid="32">Although the four cases are different, we have identified several regularities.</S>
    <S sid="150" ssid="33">For instance, BLEU and, in general, all metrics based on lexical matching alone, except METEOR, obtain significantly lower levels of correlation than metrics based on deeper linguistic similarities.</S>
    <S sid="151" ssid="34">The problem with lexical metrics is that they are unable to capture the actual quality of the &#8216;Systran&#8217; system.</S>
    <S sid="152" ssid="35">Interestingly, METEOR obtains a higher correlation, which, in the case of French-to-English, rivals the top-scoring metrics based on deeper linguistic features.</S>
    <S sid="153" ssid="36">The reason, however, does not seem to be related to its additional linguistic operations (i.e., stemming or synonymy lookup), but rather to the METEOR matching strategy itself (unigram precision/recall).</S>
    <S sid="154" ssid="37">Metrics at the shallow syntactic level are in the same range of lexical metrics.</S>
    <S sid="155" ssid="38">At the properly syntactic level, metrics obtain in most cases high correlation coefficients.</S>
    <S sid="156" ssid="39">However, the &#8216;DP-HWCw-4&#8217; metric, which, although from the viewpoint of dependency relationships, still considers only lexical matching, obtains a lower level of correlation.</S>
    <S sid="157" ssid="40">This reinforces the idea that metrics based on rewarding long n-grams matchings may not be a reliable quality indicator in these cases.</S>
    <S sid="158" ssid="41">At the level of shallow semantics, while &#8216;NE&#8217; metrics are not equally useful in all cases, &#8216;SR&#8217; metrics prove very effective.</S>
    <S sid="159" ssid="42">For instance, correlation attained by &#8216;SR-Or-*&#8217; reveals that it is important to translate lexical items according to the semantic role they play inside the sentence.</S>
    <S sid="160" ssid="43">Moreover, correlation attained by the &#8216;SR-Mr-*&#8217; metric is a clear indication that in order to achieve a high quality, it is important to &#8216;fully&#8217; translate &#8216;whole&#8217; semantic structures (i.e., arguments/adjuncts).</S>
    <S sid="161" ssid="44">The existence of all the semantic structures (&#8216;SR-Or&#8217;), specially associated to the same verb (&#8216;SR-Orv&#8217;), is also important.</S>
    <S sid="162" ssid="45">In the two remaining tasks, Spanish-to-English in-domain/out-of-domain, all the systems are statistical.</S>
    <S sid="163" ssid="46">Table 5 shows correlation with human assessments for some metric representatives.</S>
    <S sid="164" ssid="47">In this case, BLEU proves very effective, both in-domain and out-of-domain.</S>
    <S sid="165" ssid="48">Indeed, all metrics based on lexical matching obtain high levels of correlation with human assessments.</S>
    <S sid="166" ssid="49">However, still metrics based on deeper linguistic analysis attain in most cases higher correlation coefficients, although not as significantly higher as in the case of heterogeneous systems.</S>
    <S sid="167" ssid="50">We study the case reported by Callison-Burch et al. (2006) in the context of the Arabic-to-English exercise of the &#8220;2005 NIST MT Evaluation Campaign&#148;4 (Le and Przybocki, 2005).</S>
    <S sid="168" ssid="51">In this case all systems are statistical but &#8216;LinearB&#8217;, a human-aided MT system (Callison-Burch, 2005).</S>
    <S sid="169" ssid="52">Five reference translations are available.</S>
    <S sid="170" ssid="53">System outputs consist of 1056 sentences.</S>
    <S sid="171" ssid="54">We obtained permission5 to use 7 system outputs.</S>
    <S sid="172" ssid="55">For six of these systems we counted on a subjective manual evaluation based on adequacy and fluency for a subset of 266 sentences (i.e., 1596 sentences were assessed).</S>
    <S sid="173" ssid="56">Each sentence was evaluated by two different human judges.</S>
    <S sid="174" ssid="57">System scores have been obtained by averaging over all sentence scores.</S>
    <S sid="175" ssid="58">Table 6 shows the level of correlation with human assessments for some metric representatives (see &#8216;ALL&#8217; column).</S>
    <S sid="176" ssid="59">In this case, lexical metrics obtain extremely low levels of correlation.</S>
    <S sid="177" ssid="60">Again, the problem is that lexical metrics are unable to capture the actual quality of &#8216;LinearB&#8217;.</S>
    <S sid="178" ssid="61">At the shallow syntactic level, only metrics which do not consider any lexical information (&#8216;SP-NISTp-5&#8217; and &#8216;SP-NISTc5&#8217;) attain a significantly higher quality.</S>
    <S sid="179" ssid="62">At the properly syntactic level, all metrics attain a higher correlation.</S>
    <S sid="180" ssid="63">At the shallow semantic level, again, while &#8216;NE&#8217; metrics are not specially useful, &#8216;SR&#8217; metrics prove very effective.</S>
    <S sid="181" ssid="64">On the other hand, if we remove &#8216;LinearB&#8217; (see ercise.</S>
    <S sid="182" ssid="65">&#8216;ALL&#8217; refers to the evaluation of all systems.</S>
    <S sid="183" ssid="66">&#8216;SMT&#8217; refers to the evaluation of statistical systems alone (i.e., removing &#8216;LinearB&#8217;).</S>
    <S sid="184" ssid="67">&#8216;SMT&#8217; column), lexical metrics attain a much higher correlation, in the same range of metrics based on deeper linguistic information.</S>
    <S sid="185" ssid="68">However, still metrics based on syntactic parsing, and semantic roles, exhibit a slightly higher quality.</S>
  </SECTION>
  <SECTION title="4 Conclusions" number="4">
    <S sid="186" ssid="1">We have presented a comparative study on the behavior of a wide set of metrics for automatic MT evaluation at different linguistic levels (lexical, shallow-syntactic, syntactic, and shallow-semantic) under different scenarios.</S>
    <S sid="187" ssid="2">We have shown, through empirical evidence, that linguistic features at more abstract levels may provide more reliable system rankings, specially when the systems under evaluation do not share the same lexicon.</S>
    <S sid="188" ssid="3">We strongly believe that future MT evaluation campaigns should benefit from these results, by including metrics at different linguistic levels.</S>
    <S sid="189" ssid="4">For instance, the following set could be used: { &#8216;DP-HWCr-4&#8217;, &#8216;DP-Oc-*&#8217;, &#8216;DP-Ol-*&#8217;, &#8216;DP-Or-*&#8217;, &#8216;CPSTM-9&#8217;, &#8216;SR-Or-*&#8217;, &#8216;SR-Orv&#8217; } All these metrics are among the top-scoring in all the translation tasks studied.</S>
    <S sid="190" ssid="5">However, none of these metrics provides, in isolation, a &#8216;global&#8217; measure of quality.</S>
    <S sid="191" ssid="6">Indeed, all these metrics focus on &#8216;partial&#8217; aspects of quality.</S>
    <S sid="192" ssid="7">We believe that, in order to perform &#8216;global&#8217; evaluations, different quality dimensions should be integrated into a single measure of quality.</S>
    <S sid="193" ssid="8">With that purpose, we are currently exploring several metric combination strategies.</S>
    <S sid="194" ssid="9">Preliminary results, based on the QUEEN measure inside the QARLA Framework (Amig&#180;o et al., 2005), indicate that metrics at different linguistic levels may be robustly combined.</S>
    <S sid="195" ssid="10">Experimental results also show that metrics requiring linguistic analysis seem very robust against parsing errors committed by automatic linguistic processors, at least at the document level.</S>
    <S sid="196" ssid="11">That is very interesting, taking into account that, while reference translations are supposedly well formed, that is not always the case of automatic translations.</S>
    <S sid="197" ssid="12">However, it remains pending to test the behaviour at the sentence level, which could be very useful for error analysis.</S>
    <S sid="198" ssid="13">Moreover, relying on automatic processors implies two other important limitations.</S>
    <S sid="199" ssid="14">First, these tools are not available for all languages.</S>
    <S sid="200" ssid="15">Second, usually they are too slow to allow for massive evaluations, as required, for instance, in the case of system development.</S>
    <S sid="201" ssid="16">In the future, we plan to incorporate more accurate, and possibly faster, linguistic processors, also for languages other than English, as they become publicly available.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="5">
    <S sid="202" ssid="1">This research has been funded by the Spanish Ministry of Education and Science, projects OpenMT (TIN2006-15307-C03-02) and TRANGRAM (TIN2004-07925-C03-02).</S>
    <S sid="203" ssid="2">We are recognized as a Quality Research Group (2005 SGR00130) by DURSI, the Research Department of the Catalan Government.</S>
    <S sid="204" ssid="3">Authors are thankful to the WMT organizers for providing such valuable test beds.</S>
    <S sid="205" ssid="4">Authors are also thankful to Audrey Le (from NIST), and to the 2005 NIST MT Evaluation Campaign participants who agreed to share their system outputs and human assessments for the purpose of this research.</S>
    <S sid="206" ssid="5">Audrey Le and Mark Przybocki.</S>
    <S sid="207" ssid="6">2005.</S>
    <S sid="208" ssid="7">NIST 2005 machine translation evaluation official results.</S>
    <S sid="209" ssid="8">Technical report, NIST, August.</S>
  </SECTION>
</PAPER>
