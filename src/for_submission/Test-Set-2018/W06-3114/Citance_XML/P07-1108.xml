<PAPER>
  <S sid="0">Pivot Language Approach for Phrase-Based Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper proposes a novel method for phrase-based statistical machine translation by using pivot language.</S>
    <S sid="2" ssid="2">To conduct transbetween languages with a small bilingual corpus, we bring in a third which is named the lan- For and there exist bilingual corpora.</S>
    <S sid="3" ssid="3">Using only bilingual corpora, we can build a model for The advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair.</S>
    <S sid="4" ssid="4">Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for French-Spanish translation.</S>
    <S sid="5" ssid="5">Moreover, with small bilingual corpus available, our method can further improve the translaquality by using the additional bilingual corpora.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993).</S>
    <S sid="7" ssid="2">These methods need large bilingual corpora.</S>
    <S sid="8" ssid="3">However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems.</S>
    <S sid="9" ssid="4">To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language.</S>
    <S sid="10" ssid="5">To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corpora for language pairs Lf-Lp and Lp-Le.</S>
    <S sid="11" ssid="6">With the Lf-Lp and Lp-Le bilingual corpora, we can build a translation model for Lf-Le by using Lp as the pivot language.</S>
    <S sid="12" ssid="7">We name the translation model pivot model.</S>
    <S sid="13" ssid="8">The advantage of this method lies in that we can conduct translation between Lf and Le even if there is no bilingual corpus available for this language pair.</S>
    <S sid="14" ssid="9">Moreover, if a small corpus is available for Lf-Le, we build another translation model, which is named standard model.</S>
    <S sid="15" ssid="10">Then, we build an interpolated model by performing linear interpolation on the standard model and the pivot model.</S>
    <S sid="16" ssid="11">Thus, the interpolated model can employ both the small LfLe corpus and the large Lf-Lp and Lp-Le corpora.</S>
    <S sid="17" ssid="12">We perform experiments on the Europarl corpus (Koehn, 2005).</S>
    <S sid="18" ssid="13">Using BLEU (Papineni et al., 2002) as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the standard model trained with 5,000 Lf-Le sentence pairs for French-Spanish translation.</S>
    <S sid="19" ssid="14">The translation quality is comparable with that of the model trained with a bilingual corpus of 30,000 LfLe sentence pairs.</S>
    <S sid="20" ssid="15">Moreover, translation quality is further boosted by using both the small Lf-Le bilingual corpus and the large Lf-Lp and Lp-Le corpora.</S>
    <S sid="21" ssid="16">Experimental results on Chinese-Japanese translation also indicate that our method achieves satisfactory results using English as the pivot language.</S>
    <S sid="22" ssid="17">The remainder of this paper is organized as follows.</S>
    <S sid="23" ssid="18">In section 2, we describe the related work.</S>
    <S sid="24" ssid="19">Section 3 briefly introduces phrase-based SMT.</S>
    <S sid="25" ssid="20">Section 4 and Section 5 describes our method for phrase-based SMT using pivot language.</S>
    <S sid="26" ssid="21">We describe the experimental results in sections 6 and 7.</S>
    <S sid="27" ssid="22">Lastly, we conclude in section 8.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="28" ssid="1">Our method is mainly related to two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources.</S>
    <S sid="29" ssid="2">For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003).</S>
    <S sid="30" ssid="3">These methods only used the available dictionaries to perform word by word translation.</S>
    <S sid="31" ssid="4">In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language.</S>
    <S sid="32" ssid="5">Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004).</S>
    <S sid="33" ssid="6">Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT.</S>
    <S sid="34" ssid="7">Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment.</S>
    <S sid="35" ssid="8">Borin (2000) used multilingual corpora to increase alignment coverage.</S>
    <S sid="36" ssid="9">Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality.</S>
    <S sid="37" ssid="10">Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on.</S>
    <S sid="38" ssid="11">For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources.</S>
    <S sid="39" ssid="12">Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources.</S>
    <S sid="40" ssid="13">A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005).</S>
    <S sid="41" ssid="14">This task focused on languages with scarce resources.</S>
    <S sid="42" ssid="15">For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results.</S>
    <S sid="43" ssid="16">In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora.</S>
    <S sid="44" ssid="17">Our method does not need language-dependent resources or deep linguistic processing.</S>
    <S sid="45" ssid="18">Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available.</S>
  </SECTION>
  <SECTION title="3 Phrase-Based SMT" number="3">
    <S sid="46" ssid="1">According to the translation model presented in (Koehn et al., 2003), given a source sentence f , the best target translation ebest can be obtained according to the following model &#955; is the strength of the lexical weight.</S>
    <S sid="47" ssid="2">Where i and d denote phrase translation probability and distortion probability, respectively.</S>
    <S sid="48" ssid="3">4Phrase-Based SMT Via Pivot Language This section will introduce the method that performs phrase-based SMT for the language pair LfLe by using the two bilingual corpora of and With the two additional bilingual corpora, we train two translation models for and respectively.</S>
    <S sid="49" ssid="4">Based on these two models, we build a pivot translation model for with as a pivot language.</S>
    <S sid="50" ssid="5">According to equation (2), the phrase translation probability and the lexical weight are language dependent.</S>
    <S sid="51" ssid="6">We will introduce them in sections 4.1 an d 4.2, respectively.</S>
    <S sid="52" ssid="7">Using the Lf-Lp and Lp-Le bilingual corpora, we train two phrase translation probabilities &#966;(f i  |pi) and &#966;(pi  |ei), where pi is the phrase in the pivot language Lp.</S>
    <S sid="53" ssid="8">Given the phrase translation probabilities &#966;(f i  |pi) and &#966;(pi  |ei) , we obtain the phrase translation probability The phrase translation probability &#966;(f i  |pi , ei ) does not depend on the phase ei in the language Le, since it is estimated from the Lf-Lp bilingual corpus.</S>
    <S sid="54" ssid="9">Thus, equation (3) can be rewritten as Given a phrase pair (f , e) and a word alignment a between the source word positions i =1,..., n and the target word positions j =1,... , m , the lexical weight can be estimated according to the following method (Koehn et al., 2003). and a2 represent the word alignment inrespectively, then the alignment information a inside (f , e) can be obtained as shown in (6).</S>
    <S sid="55" ssid="10">An example is shown in Figure 1. om the induced phrase pairs.</S>
    <S sid="56" ssid="11">We name this method phrase method.</S>
    <S sid="57" ssid="12">If we use K to denote the number of the induced phrase pairs, we estimate the co-occurring frequency of the word pair (f , e) according to the following model.</S>
    <S sid="58" ssid="13">Where &#966;k (f  |e) is the phrase translation probability for phrase pair k . &#948;(x, y) =1 if x = y; otherwise, &#948;(x, y) = 0 .</S>
    <S sid="59" ssid="14">Thus, lexical translation probability can be estimated as in (8).</S>
    <S sid="60" ssid="15">&amp;( } (6) With the induced alignment information, this paper proposes a method to estimate the probability directly fr ability w(f e) using the method described in (Wang et al., 2006), which is shown in (9).</S>
    <S sid="61" ssid="16">We named it word method in this paper. probabilities, and sim(f , e; p) is the crosslanguage word similari | | ty.</S>
    <S sid="62" ssid="17">In order to estimate the lexical weight, we first need to obtain the alignment information a between the two phrases f and e , andthen estimate the lexical translation probability w(f e) according to the alignment information.</S>
    <S sid="63" ssid="18">The alignment information of the phrase pair (f , e) can be induced fr | om the two phrase pairs (f , p) and (p, e) .</S>
    <S sid="64" ssid="19">We also estimate the lexical translation probWhere w(f</S>
  </SECTION>
  <SECTION title="5 Interpolated Model" number="4">
    <S sid="65" ssid="1">If we have a small Lf-Le bilingual corpus, we can employ this corpus to estimate a translation model as described in section 3.</S>
    <S sid="66" ssid="2">However, this model may perform poorly due to the sparseness of the data.</S>
    <S sid="67" ssid="3">In order to improve its performance, we can employ the additional and bilingual corpora.</S>
    <S sid="68" ssid="4">Moreover, we can use more than one pivot language to improve the translation performance if the corresponding bilingual corpora exist.</S>
    <S sid="69" ssid="5">Different pivot lan i j a nomena, and improve translation quality for the desired language pair Lf-Le in different ways.</S>
    <S sid="70" ssid="6">If we include n pivot languages, n pivot models can be estimated using the method as described in section 4.</S>
    <S sid="71" ssid="7">In order to combine these n pivot models with the standard model trained with the Lf-Le corpus, we use the linear interpolation method.</S>
    <S sid="72" ssid="8">The phrase translation probability and the lexical weight are estimated as shown in (10) and (11), respectively.</S>
    <S sid="73" ssid="9">Where &#966;0 (f  |e) and p,,,0 (f  |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. &#966;i (f  |e) and p ,,,i (f  |e, a) ( i =1,... , n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. &#945;i and &#946;i are the interpolation coefficients.</S>
  </SECTION>
  <SECTION title="6 Experiments on the Europarl Corpus" number="5">
    <S sid="74" ssid="1">A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006).</S>
    <S sid="75" ssid="2">The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German.</S>
    <S sid="76" ssid="3">The shared task performed translation between English and the other three languages.</S>
    <S sid="77" ssid="4">In our work, we perform translation from French to the other three languages.</S>
    <S sid="78" ssid="5">We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language.</S>
    <S sid="79" ssid="6">In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language.</S>
    <S sid="80" ssid="7">Table 1 shows the information about the bilingual training data.</S>
    <S sid="81" ssid="8">In the table, &amp;quot;Fr&amp;quot;, &amp;quot;En&amp;quot;, &amp;quot;Es&amp;quot;, and &amp;quot;De&amp;quot; denotes &amp;quot;French&amp;quot;, &amp;quot;English&amp;quot;, &amp;quot;Spanish&amp;quot;, and &amp;quot;German&amp;quot;, respectively.</S>
    <S sid="82" ssid="9">For the language pairs Lf-Le not including English, the bilingual corpus is extracted from Lf-English and English-Le since Europarl corpus is a multilingual corpus.</S>
    <S sid="83" ssid="10">For the language models, we use the same data provided in the shared task.</S>
    <S sid="84" ssid="11">We also use the same development set and test set provided by the shared task.</S>
    <S sid="85" ssid="12">The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language.</S>
    <S sid="86" ssid="13">To perform phrase-based SMT, we use Koehn's training scripts1 and the Pharaoh decoder (Koehn, 2004).</S>
    <S sid="87" ssid="14">We run the decoder with its default settings and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set.</S>
    <S sid="88" ssid="15">The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002).</S>
    <S sid="89" ssid="16">And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores.</S>
    <S sid="90" ssid="17">As described in section 4, we employ two methods to estimate the lexical weight in the translation model.</S>
    <S sid="91" ssid="18">In order to compare the two methods, we translate from French to Spanish, using English as the pivot language.</S>
    <S sid="92" ssid="19">We use the French-English and English-Spanish corpora described in Table 1 as training data.</S>
    <S sid="93" ssid="20">During training, before estimating the Spanish to French phrase translation probability, we filter those French-English and EnglishSpanish phrase pairs whose translation probabilities are below a fixed threshold 0.001.2 The translation results are shown in Table 2.</S>
    <S sid="94" ssid="21">The phrase method proposed in this paper performs better than the word method proposed in (Wang et al., 2006).</S>
    <S sid="95" ssid="22">This is because our method uses phrase translation probability as a confidence weight to estimate the lexical translation probability.</S>
    <S sid="96" ssid="23">It strengthens the frequently aligned pairs and weakens the infrequently aligned pairs.</S>
    <S sid="97" ssid="24">Thus, the following sections will use the phrase method to estimate the lexical weight.</S>
    <S sid="98" ssid="25">This section describes the translation results by using only one pivot language.</S>
    <S sid="99" ssid="26">For the language pair French and Spanish, we use English as the pivot language.</S>
    <S sid="100" ssid="27">The entire French-English and English-Spanish corpora as described in section 4 are used to train a pivot model for French-Spanish.</S>
    <S sid="101" ssid="28">As described in section 5, if we have a small LfLe bilingual corpus and large Lf-Lp and Lp-Le bilingual corpora, we can obtain interpolated models.</S>
    <S sid="102" ssid="29">In order to conduct the experiments, we randomly select 5K, 10K, 20K, 30K, 40K, 50K, and 100K sentence pairs from the French-Spanish corpus.</S>
    <S sid="103" ssid="30">Using each of these corpora, we train a standard translation model.</S>
    <S sid="104" ssid="31">For each standard model, we interpolate it with the pivot model to get an interpolated model.</S>
    <S sid="105" ssid="32">The interpolation weights are tuned using the development set.</S>
    <S sid="106" ssid="33">For all the interpolated models, we set &#945;0 = 0.9 , &#945;1 = 0.</S>
    <S sid="107" ssid="34">1 , ,Q0 = 0.9 , and ,Q1 = 0.</S>
    <S sid="108" ssid="35">1.</S>
    <S sid="109" ssid="36">We test the three kinds of models on both the indomain and out-of-domain test sets.</S>
    <S sid="110" ssid="37">The results are shown in Figures 2 and 3.</S>
    <S sid="111" ssid="38">The pivot model achieves BLEU scores of 0.3212 and 0.2098 on the in-domain and out-ofdomain test set, respectively.</S>
    <S sid="112" ssid="39">It achieves an absolute improvement of 0.05 on both test sets (16.92% and 35.35% relative) over the standard model trained with 5,000 French-Spanish sentence pairs.</S>
    <S sid="113" ssid="40">And the performance of the pivot models are comparable with that of the standard models trained with 20,000 and 30,000 sentence pairs on the indomain and out-of-domain test set, respectively.</S>
    <S sid="114" ssid="41">When the French-Spanish training corpus is increased, the standard models quickly outperform the pivot model.</S>
    <S sid="115" ssid="42">When only a very small French-Spanish bilingual corpus is available, the interpolated method can greatly improve the translation quality.</S>
    <S sid="116" ssid="43">For example, when only 5,000 French-Spanish sentence pairs are available, the interpolated model outperforms the standard model by achieving a relative improvement of 17.55%, with the BLEU score improved from 0.2747 to 0.3229.</S>
    <S sid="117" ssid="44">With 50,000 French-Spanish sentence pairs available, the interpolated model significantly3 improves the translation quality by achieving an absolute improvement of 0.01 BLEU.</S>
    <S sid="118" ssid="45">When the FrenchSpanish training corpus increases to 100,000 sentence pairs, the interpolated model achieves almost the same result as the standard model.</S>
    <S sid="119" ssid="46">This indicates that our pivot language method is suitable for the language pairs with small quantities of training data available.</S>
    <S sid="120" ssid="47">Besides experiments on French-Spanish translation, we also conduct translation from French to English and French to German, using German and English as the pivot language, respectively.</S>
    <S sid="121" ssid="48">The results on the in-domain test set4 are shown in Figures 4 and 5.</S>
    <S sid="122" ssid="49">The tendency of the results is similar to that in Figure 2.</S>
    <S sid="123" ssid="50">For French to Spanish translation, we also introduce German as a pivot language besides English.</S>
    <S sid="124" ssid="51">Using these two pivot languages, we build two different pivot models, and then perform linear interpolation on them.</S>
    <S sid="125" ssid="52">The interpolation weights for the English pivot model and the German pivot model are set to 0.6 and 0.4 respectively5.</S>
    <S sid="126" ssid="53">The translation results on the in-domain test set are 0.3212, 0.3077, and 0.3355 for the pivot models using English, German, and both German and English as pivot languages, respectively.</S>
    <S sid="127" ssid="54">With the pivot model using both English and German as pivot languages, we interpolate it with the standard models trained with French-Spanish corpora of different sizes as described in the above section.</S>
    <S sid="128" ssid="55">The comparison of the translation results among the interpolated models, standard models, and the pivot model are shown in Figure 6.</S>
    <S sid="129" ssid="56">It can be seen that the translation results can be further improved by using more than one pivot language.</S>
    <S sid="130" ssid="57">The pivot model &amp;quot;Pivot-En+De&amp;quot; using two pivot languages achieves an absolute improvement of 0.06 (22.13% relative) as compared with the standard model trained with 5,000 sentence pairs.</S>
    <S sid="131" ssid="58">And it achieves comparable translation result as compared with the standard model trained with 30,000 French-Spanish sentence pairs.</S>
    <S sid="132" ssid="59">The results in Figure 6 also indicate the interpolated models using two pivot languages achieve the best results of all.</S>
    <S sid="133" ssid="60">Significance test shows that the interpolated models using two pivot languages significantly outperform those using one pivot language when less than 50,000 French-Spanish sentence pairs are available.</S>
    <S sid="134" ssid="61">In all of the above results, the corpora used to train the pivot models are not changed.</S>
    <S sid="135" ssid="62">In order to examine the effect of the size of the pivot corpora, we decrease the French-English and EnglishFrench corpora.</S>
    <S sid="136" ssid="63">We randomly select 200,000 and 400,000 sentence pairs from both of them to train two pivot models, respectively.</S>
    <S sid="137" ssid="64">The translation results on the in-domain test set are 0.2376, 0.2954, and 0.3212 for the pivot models trained with 200,000, 400,000, and the entire French-English and English-Spanish corpora, respectively.</S>
    <S sid="138" ssid="65">The results of the interpolated models and the standard models are shown in Figure 7.</S>
    <S sid="139" ssid="66">The results indicate that the larger the training corpora used to train the pivot model are, the better the translation quality is.</S>
  </SECTION>
  <SECTION title="7 Experiments on Chinese to Japanese Translation" number="6">
    <S sid="140" ssid="1">In section 6, translation results on the Europarl multilingual corpus indicate the effectiveness of our method.</S>
    <S sid="141" ssid="2">To investigate the effectiveness of our method by using independently sourced parallel corpora, we conduct Chinese-Japanese translation using English as a pivot language in this section, where the training data are not limited to a specific domain.</S>
    <S sid="142" ssid="3">The data used for this experiment is the same as those used in (Wang et al., 2006).</S>
    <S sid="143" ssid="4">There are 21,977, 329,350, and 160,535 sentence pairs for the language pairs Chinese-Japanese, Chinese-English, and English-Japanese, respectively.</S>
    <S sid="144" ssid="5">The development data and testing data include 500 and 1,000 Chinese sentences respectively, with one reference for each sentence.</S>
    <S sid="145" ssid="6">For Japanese language model training, we use about 100M bytes Japanese corpus.</S>
    <S sid="146" ssid="7">The translation result is shown in Figure 8.</S>
    <S sid="147" ssid="8">The pivot model only outperforms the standard model trained with 2,500 sentence pairs.</S>
    <S sid="148" ssid="9">This is because (1) the corpora used to train the pivot model are smaller as compared with the Europarl corpus; (2) the training data and the testing data are not limited to a specific domain; (3) The languages are not closely related.</S>
    <S sid="149" ssid="10">The interpolated models significantly outperform the other models.</S>
    <S sid="150" ssid="11">When only 5,000 sentence pairs are available, the BLEU score increases relatively by 20.53%.</S>
    <S sid="151" ssid="12">With the entire (21,977 pairs) Chinese-Japanese available, the interpolated model relatively increases the BLEU score by 5.62%, from 0.1708 to 0.1804.</S>
  </SECTION>
  <SECTION title="8 Conclusion" number="7">
    <S sid="152" ssid="1">This paper proposed a novel method for phrasebased SMT on language pairs with a small bilingual corpus by bringing in pivot languages.</S>
    <S sid="153" ssid="2">To perform translation between Lf and Le, we bring in a pivot language Lp, via which the large corpora of Lf-Lp and Lp-Le can be used to induce a translation model for Lf-Le.</S>
    <S sid="154" ssid="3">The advantage of this method is that it can perform translation between the language pair Lf-Le even if no bilingual corpus for this pair is available.</S>
    <S sid="155" ssid="4">Using BLEU as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 sentence pairs for FrenchSpanish translation.</S>
    <S sid="156" ssid="5">And the translation quality is comparable with that of the model directly trained with 30,000 French-Spanish sentence pairs.</S>
    <S sid="157" ssid="6">The results also indicate that using more pivot languages leads to better translation quality.</S>
    <S sid="158" ssid="7">With a small bilingual corpus available for Lf-Le, we built a translation model, and interpolated it with the pivot model trained with the large Lf-Lp and Lp-Le bilingual corpora.</S>
    <S sid="159" ssid="8">The results on both the Europarl corpus and Chinese-Japanese translation indicate that the interpolated models achieve the best results.</S>
    <S sid="160" ssid="9">Results also indicate that our pivot language approach is suitable for translation on language pairs with a small bilingual corpus.</S>
    <S sid="161" ssid="10">The less the Lf-Le bilingual corpus is, the bigger the improvement is.</S>
    <S sid="162" ssid="11">We also performed experiments using Lf-Lp and Lp-Le corpora of different sizes.</S>
    <S sid="163" ssid="12">The results indicate that using larger training corpora to train the pivot model leads to better translation quality.</S>
  </SECTION>
</PAPER>
