<PAPER>
  <S sid="0">Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs.</S>
    <S sid="2" ssid="2">One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages.</S>
    <S sid="3" ssid="3">Although pivoting is a robust technique, it introduces some low quality translations.</S>
    <S sid="4" ssid="4">In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT.</S>
    <S sid="5" ssid="5">The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table.</S>
    <S sid="6" ssid="6">We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">One of the main issues in statistical machine translation (SMT) is the scarcity of parallel data for many language pairs especially when the source and target languages are morphologically rich.</S>
    <S sid="8" ssid="2">A common SMT solution to the lack of parallel data is to pivot the translation through a third language (called pivot or bridge language) for which there exist abundant parallel corpora with the source and target languages.</S>
    <S sid="9" ssid="3">The literature covers many pivoting techniques.</S>
    <S sid="10" ssid="4">One of the best performing techniques, phrase pivoting (Utiyama and Isahara, 2007), builds an induced new phrase table between the source and target.</S>
    <S sid="11" ssid="5">One of the main issues of this technique is that the size of the newly created pivot phrase table is very large (Utiyama and Isahara, 2007).</S>
    <S sid="12" ssid="6">Moreover, many of the produced phrase pairs are of low quality which affects the translation choices during decoding and the overall translation quality.</S>
    <S sid="13" ssid="7">In this paper, we introduce language independent features to determine the quality of the pivot phrase pairs between source and target.</S>
    <S sid="14" ssid="8">We show positive results (0.6 BLEU points) on Persian-Arabic SMT.</S>
    <S sid="15" ssid="9">Next, we briefly discuss some related work.</S>
    <S sid="16" ssid="10">We then review two common pivoting strategies and how we use them in Section 3.</S>
    <S sid="17" ssid="11">This is followed by our approach to using connectivity strength features in Section 4.</S>
    <S sid="18" ssid="12">We present our experimental results in Section 5.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="19" ssid="1">Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al., 2008; Bertoldi et al., 2008; Habash and Hu, 2009).</S>
    <S sid="20" ssid="2">The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual corpora.</S>
    <S sid="21" ssid="3">Pivoting has been explored for closely related languages (Haji&#711;c et al., 2000) as well as unrelated languages (Koehn et al., 2009; Habash and Hu, 2009).</S>
    <S sid="22" ssid="4">Many different pivot strategies have been presented in the literature.</S>
    <S sid="23" ssid="5">The following three are perhaps the most common.</S>
    <S sid="24" ssid="6">The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to the target language The second strategy is based on phrase pivoting (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009).</S>
    <S sid="25" ssid="7">In phrase pivoting, a new source-target phrase table (translation model) is induced from source-pivot and pivottarget phrase tables.</S>
    <S sid="26" ssid="8">Lexical weights and translation probabilities are computed from the two translation models.</S>
    <S sid="27" ssid="9">The third strategy is to create a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008).</S>
    <S sid="28" ssid="10">In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007).</S>
    <S sid="29" ssid="11">We extend phrase table scores with two other features that are language independent.</S>
    <S sid="30" ssid="12">Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008).</S>
    <S sid="31" ssid="13">Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation.</S>
    <S sid="32" ssid="14">To our knowledge, there hasn&#8217;t been a lot of work on Persian and Arabic as a language pair.</S>
    <S sid="33" ssid="15">The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K&#168;opr&#168;u, 2010).</S>
  </SECTION>
  <SECTION title="3 Pivoting Strategies" number="3">
    <S sid="34" ssid="1">In this section, we review the two pivoting strategies that are our baselines.</S>
    <S sid="35" ssid="2">We also discuss how we overcome the large expansion of source-totarget phrase pairs in the process of creating a pivot phrase table.</S>
    <S sid="36" ssid="3">In sentence pivoting, English is used as an interface between two separate phrase-based MT systems; Persian-English direct system and EnglishArabic direct system.</S>
    <S sid="37" ssid="4">Given a Persian sentence, we first translate the Persian sentence from Persian to English, and then from English to Arabic.</S>
    <S sid="38" ssid="5">In phrase pivoting (sometimes called triangulation or phrase table multiplication), we train a Persianto-Arabic and an English-Arabic translation models, such as those used in the sentence pivoting technique.</S>
    <S sid="39" ssid="6">Based on these two models, we induce a new Persian-Arabic translation model.</S>
    <S sid="40" ssid="7">Since we build our models on top of Moses phrase-based SMT (Koehn et al., 2007), we need to provide the same set of phrase translation probability distributions.1 We follow Utiyama and Isahara (2007) in computing the probability distributions.</S>
    <S sid="41" ssid="8">The following are the set of equations used to compute the lexical probabilities (&#966;) and the phrase probabilities (pw) where f is the Persian source phrase. e is the English pivot phrase that is common in both Persian-English translation model and EnglishArabic translation model. a is the Arabic target phrase.</S>
    <S sid="42" ssid="9">We also build a Persian-Arabic reordering table using the same technique but we compute the reordering weights in a similar manner to Henriquez et al. (2010).</S>
    <S sid="43" ssid="10">As discussed earlier, the induced PersianArabic phrase and reordering tables are very large.</S>
    <S sid="44" ssid="11">Table 1 shows the amount of parallel corpora used to train the Persian-English and the EnglishArabic and the equivalent phrase table sizes compared to the induced Persian-Arabic phrase table.2 We introduce a basic filtering technique discussed next to address this issue and present some baseline experiments to test its performance in Section 5.3.</S>
    <S sid="45" ssid="12">The main idea of the filtering process is to select the top [n] English candidate phrases for each Persian phrase from the Persian-English phrase table and similarly select the top [n] Arabic target phrases for each English phrase from the EnglishArabic phrase table and then perform the pivoting process described earlier to create a pivoted Persian-Arabic phrase table.</S>
    <S sid="46" ssid="13">To select the top candidates, we first rank all the candidates based on the log linear scores computed from the phrase translation probabilities and lexical weights multiplied by the optimized decoding weights then we pick the top [n] pairs.</S>
    <S sid="47" ssid="14">We compare the different pivoting strategies and various filtering thresholds in Section 5.3.</S>
  </SECTION>
  <SECTION title="4 Approach" number="4">
    <S sid="48" ssid="1">One of the main challenges in phrase pivoting is the very large size of the induced phrase table.</S>
    <S sid="49" ssid="2">It becomes even more challenging if either the source or target language is morphologically rich.</S>
    <S sid="50" ssid="3">The number of translation candidates (fanout) increases due to ambiguity and richness (discussed in more details in Section 5.2) which in return increases the number of combinations between source and target phrases.</S>
    <S sid="51" ssid="4">Since the only criteria of matching between the source and target phrase is through a pivot phrase, many of the induced phrase pairs are of low quality.</S>
    <S sid="52" ssid="5">These phrase pairs unnecessarily increase the search space and hurt the overall quality of translation.</S>
    <S sid="53" ssid="6">To solve this problem, we introduce two language-independent features which are added to the log linear space of features in order to determine the quality of the pivot phrase pairs.</S>
    <S sid="54" ssid="7">We call these features connectivity strength features.</S>
    <S sid="55" ssid="8">Connectivity Strength Features provide two scores, Source Connectivity Strength (SCS) and Target Connectivity Strength (TCS).</S>
    <S sid="56" ssid="9">These two scores are similar to precision and recall metrics.</S>
    <S sid="57" ssid="10">They depend on the number of alignment links between words in the source phrase to words of the target phrase.</S>
    <S sid="58" ssid="11">SCS and TSC are defined in equations 1 and 2 where S = {i : 1 &#8804; i &#8804; S} is the set of source words in a given phrase pair in the pivot phrase table and T = {j : 1 &#8804; j &#8804; T} is the set of the equivalent target words.</S>
    <S sid="59" ssid="12">The word alignment between S and T is defined as We get the alignment links by projecting the alignments of source-pivot to the pivot-target phrase pairs used in pivoting.</S>
    <S sid="60" ssid="13">If the source-target phrase pair are connected through more than one pivot phrase, we take the union of the alignments.</S>
    <S sid="61" ssid="14">In contrast to the aggregated values represented in the lexical weights and the phrase probabilities, connectivity strength features provide additional information by counting the actual links between the source and target phrases.</S>
    <S sid="62" ssid="15">They provide an independent and direct approach to measure how good or bad a given phrase pair are connected.</S>
    <S sid="63" ssid="16">Figure 1 and 2 are two examples (one good, one bad) Persian-Arabic phrase pairs in a pivot phrase table induced by pivoting through English.3 In the first example, each Persian word is aligned to an Arabic word.</S>
    <S sid="64" ssid="17">The meaning is preserved in both phrases which is reflected in the SCS and TCS scores.</S>
    <S sid="65" ssid="18">In the second example, only one Persian word in aligned to one Arabic word in the equivalent phrase and the two phrases conveys two different meanings.</S>
    <S sid="66" ssid="19">The English phrase is not a good translation for either, which leads to this bad pairing.</S>
    <S sid="67" ssid="20">This is reflected in the SCS and TCS scores.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="68" ssid="1">In this section, we present a set of baseline experiments including a simple filtering technique to overcome the huge expansion of the pivot phrase table.</S>
    <S sid="69" ssid="2">Then we present our results in using connectivity strength features to improve Persian-Arabic pivot translation quality.</S>
    <S sid="70" ssid="3">In our pivoting experiments, we build two SMT models.</S>
    <S sid="71" ssid="4">One model to translate from Persian to English and another model to translate from English to Arabic.</S>
    <S sid="72" ssid="5">The English-Arabic parallel corpus is about 2.8M sentences (&#8776;60M words) available from LDC4 and GALE5 constrained data.</S>
    <S sid="73" ssid="6">We use an in-house Persian-English parallel corpus of about 170K sentences and 4M words.</S>
    <S sid="74" ssid="7">Word alignment is done using GIZA++ (Och and Ney, 2003).</S>
    <S sid="75" ssid="8">For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data.</S>
    <S sid="76" ssid="9">We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002).</S>
    <S sid="77" ssid="10">For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011).</S>
    <S sid="78" ssid="11">All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007).</S>
    <S sid="79" ssid="12">We use MERT (Och, 2003) for decoding weight optimization.</S>
    <S sid="80" ssid="13">For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIST MT evaluation test set (MT04).</S>
    <S sid="81" ssid="14">The optimized weights are used for ranking and filtering (discussed in Section 3.3).</S>
    <S sid="82" ssid="15">We use a maximum phrase length of size 8 across all models.</S>
    <S sid="83" ssid="16">We report results on an inhouse Persian-Arabic evaluation set of 536 sentences with three references.</S>
    <S sid="84" ssid="17">We evaluate using BLEU-4 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007).</S>
    <S sid="85" ssid="18">In this section we present our motivation and choice for preprocessing Arabic, Persian, English data.</S>
    <S sid="86" ssid="19">Both Arabic and Persian are morphologically complex languages but they belong to two different language families.</S>
    <S sid="87" ssid="20">They both express richness and linguistic complexities in different ways.</S>
    <S sid="88" ssid="21">One aspect of Arabic&#8217;s complexity is its various attachable clitics and numerous morphological features (Habash, 2010).</S>
    <S sid="89" ssid="22">We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our experiments.</S>
    <S sid="90" ssid="23">We use MADA v3.1 (Habash and Rambow, 2005; Habash et al., 2009) to tokenize the Arabic text.</S>
    <S sid="91" ssid="24">We only evaluate on detokenized and orthographically correct (enriched) output following the work of El Kholy and Habash (2010b).</S>
    <S sid="92" ssid="25">Persian on the other hand has a relatively simple nominal system.</S>
    <S sid="93" ssid="26">There is no case system and words do not inflect with gender except for a few animate Arabic loanwords.</S>
    <S sid="94" ssid="27">Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix lm+ +hA and sometimes J+ +An, or one of the Arabic plural markers.</S>
    <S sid="95" ssid="28">Verbal morphology is very complex in Persian.</S>
    <S sid="96" ssid="29">Each verb has a past and present root and many verbs have attached prefix that is regarded part of the root.</S>
    <S sid="97" ssid="30">A verb in Persian inflects for 14 different tense, mood, aspect, person, number and voice combination values (Rasooli et al., 2013).</S>
    <S sid="98" ssid="31">We use Perstem (Jadidinejad et al., 2010) for segmenting Persian text.</S>
    <S sid="99" ssid="32">English, our pivot language, is quite different from both Arabic and Persian.</S>
    <S sid="100" ssid="33">English is poor in morphology and barely inflects for number and tense, and for person in a limited context.</S>
    <S sid="101" ssid="34">English preprocessing simply includes down-casing, separating punctuation and splitting off &#8220;&#8217;s&#8221;.</S>
    <S sid="102" ssid="35">We compare the performance of sentence pivoting against phrase pivoting with different filtering thresholds.</S>
    <S sid="103" ssid="36">The results are presented in Table 2.</S>
    <S sid="104" ssid="37">In general, the phrase pivoting outperforms the sentence pivoting even when we use a small filtering threshold of size 100.</S>
    <S sid="105" ssid="38">Moreover, the higher the threshold the better the performance but with a diminishing gain.</S>
    <S sid="106" ssid="39">We use the best performing setup across the rest of the experiments.</S>
    <S sid="107" ssid="40">In this experiment, we test the performance of adding the connectivity strength features (+Conn) to the best performing phrase pivoting model The results in Table 3 show that we get a nice improvement of &#8776;0.6/0.5 (BLEU/METEOR) points by adding the connectivity strength features.</S>
    <S sid="108" ssid="41">The differences in BLEU scores between this setup and all other systems are statistically significant above the 95% level.</S>
    <S sid="109" ssid="42">Statistical significance is computed using paired bootstrap resampling (Koehn, 2004).</S>
  </SECTION>
  <SECTION title="6 Conclusion and Future Work" number="6">
    <S sid="110" ssid="1">We presented an experiment showing the effect of using two language independent features, source connectivity score and target connectivity score, to improve the quality of pivot-based SMT.</S>
    <S sid="111" ssid="2">We showed that these features help improving the overall translation quality.</S>
    <S sid="112" ssid="3">In the future, we plan to explore other features, e.g., the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases.</S>
    <S sid="113" ssid="4">We also plan to explore language specific features which could be extracted from some seed parallel data, e.g., syntactic and morphological compatibility of the source and target phrase pairs.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="114" ssid="1">The work presented in this paper was possible thanks to a generous research grant from Science Applications International Corporation (SAIC).</S>
    <S sid="115" ssid="2">The last author (Sawaf) contributed to the effort while he was at SAIC.</S>
    <S sid="116" ssid="3">We would like to thank M. Sadegh Rasooli and Jon Dehdari for helpful discussions and insights into Persian.</S>
    <S sid="117" ssid="4">We also thank the anonymous reviewers for their insightful comments.</S>
  </SECTION>
</PAPER>
