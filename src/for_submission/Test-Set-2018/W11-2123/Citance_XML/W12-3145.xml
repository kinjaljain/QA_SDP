<PAPER>
  <S sid="0">Kriya - The SFU System for Translation Task at WMT-12</S>
  <ABSTRACT>
    <S sid="1" ssid="1">models for morpheme segmentation and morphology Transactions on Speech and Language 4(1):3:1&#8211;3:34, February.</S>
    <S sid="2" ssid="2">Kenneth Heafield.</S>
    <S sid="3" ssid="3">2011.</S>
    <S sid="4" ssid="4">KenLM: Faster and smaller model queries.</S>
    <S sid="5" ssid="5">In of the Sixth on Statistical Machine pages 187&#8211;197.</S>
  </ABSTRACT>
  <SECTION title="1 Baseline Systems" number="1">
    <S sid="6" ssid="1">Our shared task submissions are trained in the hierarchical phrase-based model (Chiang, 2007) framework.</S>
    <S sid="7" ssid="2">Specifically, we use Kriya (Sankaran et al., 2012) - our in-house Hiero-style system for training and decoding.</S>
    <S sid="8" ssid="3">We now briefly explain the baseline systems in French-English and English-Czech language pairs.</S>
    <S sid="9" ssid="4">We use GIZA++ for word alignments and the Moses (Koehn et al., 2007) phrase-extractor for extracting the initial phrases.</S>
    <S sid="10" ssid="5">The translation models are trained using the rule extraction module in Kriya.</S>
    <S sid="11" ssid="6">In both cases, we pre-processed the training data by running it through the usual pre-processing pipeline of tokenization and lowercasing.</S>
    <S sid="12" ssid="7">For French-English baseline system, we trained a simplified hierarchical phrase-based model where the right-hand side can have at most one nonterminal (denoted as 1NT) instead of the usual two non-terminal (2NT) model.</S>
    <S sid="13" ssid="8">In our earlier experiments we found the 1NT model to perform comparably to the 2NT model for close language pairs such as French-English (Sankaran et al., 2012) at the same time resulting in a smaller model.</S>
    <S sid="14" ssid="9">We used the shared-task training data consisting of Europarl (v7), News commentary and UN documents for training the translation models having a total of 15 M sentence pairs (we did not use the Fr-En Giga parallel corpus for the training).</S>
    <S sid="15" ssid="10">We trained a 5-gram language model for English using the English Gigaword (v4).</S>
    <S sid="16" ssid="11">For English-Czech, we trained a standard Hiero model that has up to two non-terminals on the righthand side.</S>
    <S sid="17" ssid="12">We used the Europarl (v7), news commentary and CzEng (v0.9) corpora having 7.95M sentence pairs for training translation models.</S>
    <S sid="18" ssid="13">We trained a 5-gram language model using the Czech side of the parallel corpora and did not use the Czech monolingual corpus.</S>
    <S sid="19" ssid="14">The baseline systems use the following 8 standard Hiero features: rule probabilities p(e|f) and p(f|e); lexical weights pl(e|f) and pl(f|e); word penalty, phrase penalty, language model and glue rule penalty.</S>
    <S sid="20" ssid="15">The kriya decoder is based on a modified CYK algorithm similar to that of Chiang (2007).</S>
    <S sid="21" ssid="16">We use a novel approach in computing the language model (LM) scores in Kriya, which deserves a mention here.</S>
    <S sid="22" ssid="17">The CKY decoder in Hiero-style systems can freely combine target hypotheses generated in intermediate cells with hierarchical rules in the higher cells.</S>
    <S sid="23" ssid="18">Thus the generation of the target hypotheses are fragmented and out of order in Hiero, compared to the left to right order preferred by n-gram language models.</S>
    <S sid="24" ssid="19">This leads to challenges in estimating LM scores for partial target hypotheses and this is typically addressed by adding a sentence initial marker (&lt;s&gt;) to the beginning of each derivation path.1 Thus the language model scores for the hypothesis in the intermediate cell are approximated, with the true language model score (taking into account sentence boundaries) being computed in the last cell that spans the entire source sentence.</S>
    <S sid="25" ssid="20">Kriya uses a novel idea for computing LM scores: for each of the target hypothesis fragment, it finds the best position for the fragment in the final sentence and uses the corresponding score.</S>
    <S sid="26" ssid="21">Specifically, we compute three different scores corresponding to the three states where the fragment can end up in the final sentence, viz. sentence initial, middle and final and choose the best score.</S>
    <S sid="27" ssid="22">Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) &lt;s&gt; tf, (ii) tf and (iii) tf &lt;/s&gt; and use the best score (only) for pruning.2 While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states.</S>
    <S sid="28" ssid="23">Our earlier experiments showed significant reduction in search errors due to this approach, in addition to a small but consistent increase in BLEU score (Sankaran et al., 2012).</S>
  </SECTION>
  <SECTION title="2 French-English System" number="2">
    <S sid="29" ssid="1">In addition to the baseline system, we also trained separate systems for News and Non-News genres for applying ensemble decoding (Razmara et al., 2012).</S>
    <S sid="30" ssid="2">The news genre system was trained only using the news-commentary corpus (about 137K sen1Alternately systems add sentence boundary markers (&lt;s&gt; and &lt;/s&gt;) to the training data so that they are explicitly present in the translation and language models.</S>
    <S sid="31" ssid="3">While this can speed up the decoding as the cube pruning is more aggressive, it also limits the applicability of rules having the boundary contexts.</S>
    <S sid="32" ssid="4">2This ensures the the LM score estimates are never underestimated for pruning.</S>
    <S sid="33" ssid="5">We retain the LM score for fragment (case ii) for estimating the score for the full candidate sentence later. tence pairs) and the non-news genre system was trained on the Europarl and UN documents data (14.8M sentence pairs).</S>
    <S sid="34" ssid="6">The ensemble decoding framework combines the models of these two systems dynamically when decoding the testset.</S>
    <S sid="35" ssid="7">The idea is to effectively use the small amount of news genre data in order to maximize the performance on the news-based testsets.</S>
    <S sid="36" ssid="8">In the following sections, we explain in broader detail how this system combination technique works as well as the details of this experiment and the evaluation results.</S>
    <S sid="37" ssid="9">In the ensemble decoding framework we view translation task as a domain mixing problem involving news and non-news genres.</S>
    <S sid="38" ssid="10">The official training data is from two major sources: news-commentary data and Europarl/UN data and we hope to exploit the distinctive nature of the two genres.</S>
    <S sid="39" ssid="11">Given that the news data is smaller comparing to parliamentary proceedings data, we could tune the ensemble decoding to appropriately boost the weight for the news genre mode during decoding.</S>
    <S sid="40" ssid="12">The ensemble decoding approach (Razmara et al., 2012) takes advantage of multiple translation models with the goal of constructing a system that outperforms all the component models.</S>
    <S sid="41" ssid="13">The key strength of this system combination method is that the systems are combined dynamically at decode time.</S>
    <S sid="42" ssid="14">This enables the decoder to pick the best hypotheses for each span of the input.</S>
    <S sid="43" ssid="15">In ensemble decoding, given a number of translation systems which are already trained and tuned, all of the hypotheses from component models are used in order to translate a sentence.</S>
    <S sid="44" ssid="16">The scores of such rules are combined in the decoder (i.e.</S>
    <S sid="45" ssid="17">CKY) using various mixture operations to assign a single score to them.</S>
    <S sid="46" ssid="18">Depending on the mixture operation used for combining the scores, we would get different mixture scores.</S>
    <S sid="47" ssid="19">Ensemble decoding extends the log-linear framework which is found in state-of-the-art machine translation systems.</S>
    <S sid="48" ssid="20">Specifically, the probability of a phrase-pair (e, f) in the ensemble model is: &#65533; where &#8853; denotes the mixture operation between two or more model scores.</S>
    <S sid="49" ssid="21">Mixture operations receive two or more scores (probabilities) and return the mixture score (probability).</S>
    <S sid="50" ssid="22">In this section, we explore different options for this mixture operation.</S>
    <S sid="51" ssid="23">Weighted Sum (wsum): in wsum the ensemble probability is proportional to the weighted sum of all individual model probabilities. where m denotes the index of component models, M is the total number of them and &#955;i is the weight for component i.</S>
    <S sid="52" ssid="24">Weighted Max (wmax): where the ensemble score is the weighted max of all model scores.</S>
    <S sid="53" ssid="25">The criteria for choosing a model for each cell, &#968;( f, n), could be based on: Max: for each cell, the model that has the highest weighted top-rule score wins: Sum: Instead of comparing only the score of the top rules, the model with the highest weighted sum of the probability of the rules wins (taking into account the ttl(translation table limit) limit on the number of rules suggested by each model for each cell): The probability of each phrase-pair (e, f) is computed as: Product (prod): in prod, the probability of the ensemble model or a rule is computed as the product of the probabilities of all components (or equally the sum of log-probabilities).</S>
    <S sid="54" ssid="26">When using this mixture operation, ensemble decoding would be a generalization of the loglinear framework over multiple models.</S>
    <S sid="55" ssid="27">Product models can also make use of weights to control the contribution of each component.</S>
    <S sid="56" ssid="28">These models are generally known as Logarithmic Opinion Pools (LOPs) where: Model Switching: in model switching, each cell in the CKY chart gets populated only by rules from one of the models and the other models&#8217; rules are discarded.</S>
    <S sid="57" ssid="29">This is based on the hypothesis that each component model is an expert on different parts of sentence.</S>
    <S sid="58" ssid="30">In this method, we need to define a binary indicator function &#948;( 1, m) for each span and component model.</S>
    <S sid="59" ssid="31">Since log-linear models usually look for the best derivation, they do not need to normalize the scores to form probabilities.</S>
    <S sid="60" ssid="32">Therefore, the scores that different models assign to each phrase-pair may not be in the same scale.</S>
    <S sid="61" ssid="33">Therefore, mixing their scores might wash out the information in one (or some) of the models.</S>
    <S sid="62" ssid="34">We applied a heuristic to deal with this problem where the scores are normalized over a shorter list.</S>
    <S sid="63" ssid="35">So the list of rules coming from each model for a certain cell in the CKY chart is normalized before getting mixed with other phrase-table rules.</S>
    <S sid="64" ssid="36">However, experiments showed using normalized scores hurts the BLEU score radically.</S>
    <S sid="65" ssid="37">So we use the normalized scores only for pruning and for mixing the actual scores are used.</S>
    <S sid="66" ssid="38">As a more principled way, we used a toolkit, CONDOR (Vanden Berghen and Bersini, 2005), to optimize the weights of our component models on a dev-set.</S>
    <S sid="67" ssid="39">CONDOR, which is publicly available, is a direct optimizer based on Powell&#8217;s algorithm that does not require explicit gradient information for the objective function.</S>
    <S sid="68" ssid="40">As mentioned earlier all the experiments reported for French-English use a simpler Hiero translation model having at most one non-terminal (1NT) on the right-hand side.</S>
    <S sid="69" ssid="41">We use 7567 sentence pairs from news-tests 2008 through 2010 for tuning and use news-test 2011 for testing in addition to the 2012 test data.</S>
    <S sid="70" ssid="42">The feature weights were tuned using MERT (Och, 2003) and we report the devset (IBM) BLEU scores and the testset BLEU scores computed using the official evaluation script (mteval-v11b.pl).</S>
    <S sid="71" ssid="43">The results for the French-English experiments are reported in Table 1.</S>
    <S sid="72" ssid="44">We note that both baseline Hiero model and the model trained from the nonnews genre get comparable BLEU scores.</S>
    <S sid="73" ssid="45">The news genre model however gets a lesser BLEU score and this is to be expected due to the very small training data available for this genre.</S>
    <S sid="74" ssid="46">Table 2 shows the results of applying various mixture operations on the devset and testset, both in normalized (denoted by Norm.) and un-normalized settings (denoted by Base).</S>
    <S sid="75" ssid="47">We present results for these mixture operations using uniform weights (i.e. untuned weights) and for PROD we also present the results using the weights optimized by CONDOR.</S>
    <S sid="76" ssid="48">Most of the mixture operations outperform the Test11 BLEU of the baseline models (shown in Table 1) even with uniform (untuned) weights.</S>
    <S sid="77" ssid="49">We took the best performing operation (i.e.</S>
    <S sid="78" ssid="50">PROD) and tuned its component weights using our optimizer which lead to 0.26 points improvement over its uniform-weight version.</S>
    <S sid="79" ssid="51">The last row in Table 1 reports the BLEU score for this mixture operation with the tuned weights on the Test-12 dataset and it is marginally less than the baseline model.</S>
    <S sid="80" ssid="52">While this is disappointing, this also runs counter to our empirical results from other datasets.</S>
    <S sid="81" ssid="53">We are currently investigating this aspect as we hope to improve the robustness and applicability of our ensemble approach for different datasets and language pairs.</S>
  </SECTION>
  <SECTION title="3 English-Czech System" number="3">
    <S sid="82" ssid="1">For English-Czech, we additionally experimented using morphologically segmented versions of the Czech side of the parallel data, since previous work (Clifton and Sarkar, 2011) has shown that segmentation of morphologically rich languages can aid translation.</S>
    <S sid="83" ssid="2">To derive the segmentation, we built an unsupervised morphological segmentation model using the Morfessor toolkit (Creutz and Lagus, 2007).</S>
    <S sid="84" ssid="3">Morfessor uses minimum description length criteria to train a HMM-based segmentation model.</S>
    <S sid="85" ssid="4">Varying the perplexity threshold in Morfessor does not segment more word types, but rather oversegments the same word types.</S>
    <S sid="86" ssid="5">We hand tuned the model parameters over training data size and perplexity; these control the granularity and coverage of the segmentations.</S>
    <S sid="87" ssid="6">Specifically, we trained different segmenter models on varying sets of most frequent words and different perplexities and identified two sets that performed best based on a separate heldout set.</S>
    <S sid="88" ssid="7">These two sets correspond to 500k most frequent words and a perplexity of 50 (denoted SM1) and 10k most frequent words and a perplexity of 20 (denoted SM2).</S>
    <S sid="89" ssid="8">We then used these two models to segment the entire data set and generate two different segmented training sets.</S>
    <S sid="90" ssid="9">These models had the best combination of segmentation coverage of the training data and largest segments, since we found empirically that smaller segments were less meaningful in the translation model.</S>
    <S sid="91" ssid="10">The SM2 segmentation segmented more words than SM1, but more frequently segmented words into single-character units.</S>
    <S sid="92" ssid="11">For example, the Czech word &#8216;dlaebn&#180;&#305;&#8217; is broken into the useful components &#8216;dlaeb + n&#180;&#305;&#8217; by SM1, but is oversegmented into &#8216;dl + a + e + b + n&#180;&#305;&#8217; by SM2.</S>
    <S sid="93" ssid="12">However, SM1 fails to find a segmentation at all for the related word &#8216;dlaebn&#180;&#305;mi&#8217;, while SM2 breaks it up similiarly with an additional suffix: &#8216;dl + a + e + b + n&#180;&#305; + mi&#8217;.</S>
    <S sid="94" ssid="13">With these segmentation models, we segmented the target side of the training and dev data before training the translation model.</S>
    <S sid="95" ssid="14">Similarly, we also train segmented language models corresponding to the two sets SM1 and SM2.</S>
    <S sid="96" ssid="15">The MERT tuning step uses the segmented dev-set reference to evaluate the segmented hypotheses generated by the decoder for optimizing the weights for the BLEU score.</S>
    <S sid="97" ssid="16">However for evaluating the test-set, we stitched the segments in the decoder output back into unsegmented forms in a post-processing step, before performing evaluation against the original unsegmented references.</S>
    <S sid="98" ssid="17">The hypotheses generated by the decoder can have incomplete dangling segments where one or more prefixes and/or suffixes are missing.</S>
    <S sid="99" ssid="18">While these dangling segments could be handled in a different way, we use a simple heuristic of ignoring the segment marker &#8217;+&#8217; by just removing the segment marker.</S>
    <S sid="100" ssid="19">In next section, we report the results of using the unsegmented model as well as its segmented counterparts.</S>
    <S sid="101" ssid="20">In the English-Czech experiments, we used the same datasets for the dev and test sets as in FrenchEnglish experiments (dev: news-tests 2008, 2009, 2010 with 7567 sentence pairs and test: newstest2011 with 3003 sentence pairs).</S>
    <S sid="102" ssid="21">Similarly, MERT (Och, 2003) has been used to tune the feature weights and we report the BLEU scores of two testsets computed using the official evaluation script (mteval-v11b.pl).</S>
    <S sid="103" ssid="22">Table 3.2 shows the results of different segmentation schemes on the WMT-11 and WMT-12 test-sets.</S>
    <S sid="104" ssid="23">SM1 slightly outperformed the other two models in Test-11, however the unsegmented model performed best in Test-12, though marginally.</S>
    <S sid="105" ssid="24">We are currently investigating this and are also considering the possibility employing the idea of morpheme prediction in the post-decoding step in combination with this morpheme-based translation as suggested by Clifton and Sarkar (2011).</S>
  </SECTION>
  <SECTION title="4 Conclusion" number="4">
    <S sid="106" ssid="1">We submitted systems in two language pairs FrenchEnglish and English-Czech for WMT-12 shared task.</S>
    <S sid="107" ssid="2">In French-English, we experimented the ensemble decoding framework that effectively utilizes the small amount of news genre data to improve the performance in the testset belonging to the same genre.</S>
    <S sid="108" ssid="3">We obtained a moderate gain of 0.4 BLEU points with the ensemble decoding over the baseline system in newstest-2011.</S>
    <S sid="109" ssid="4">For newstest-2012, it performs comparably to that of the baseline and we are presently investigating the lack of improvement in newstest-2012.</S>
    <S sid="110" ssid="5">For Cz-En, We found that the BLEU scores do not substantially differ from each other and also the minor differences are not consistent for Test-11 and Test-12.</S>
  </SECTION>
</PAPER>
