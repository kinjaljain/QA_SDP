<PAPER>
  <S sid="0">Improving Translation Model by Monolingual Data</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation.</S>
    <S sid="2" ssid="2">This method called &#8220;reverse self-training&#8221; improves the decoder&#8217;s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting.</S>
    <S sid="3" ssid="3">We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words.</S>
    <S sid="4" ssid="4">We also provide a description of the systems we</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Like any other statistical NLP task, SMT relies on sizable language data for training.</S>
    <S sid="6" ssid="2">However the parallel data required for MT are a very scarce resource, making it difficult to train MT systems of decent quality.</S>
    <S sid="7" ssid="3">On the other hand, it is usually possible to obtain large amounts of monolingual data.</S>
    <S sid="8" ssid="4">In this paper, we attempt to make use of the monolingual data to reduce the sparseness of surface forms, an issue typical for morphologically rich languages.</S>
    <S sid="9" ssid="5">When MT systems translate into such languages, the limited size of parallel data often causes the situation where the output should include a word form never observed in the training data.</S>
    <S sid="10" ssid="6">Even though the parallel data do contain the desired word in other forms, a standard phrase-based decoder has no way of using it to generate the correct translation.</S>
    <S sid="11" ssid="7">Reverse self-training addresses this problem by incorporating the available monolingual data in the translation model.</S>
    <S sid="12" ssid="8">This paper builds upon the idea outlined in Bojar and Tamchyna (2011), describing how this technique was incorporated in the WMT Shared Task and extending the experimental evaluation of reverse self-training in several directions &#8211; the examined language pairs (Section 4.2), data size (Section 4.3) and back-off techniques (Section 4.4).</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="13" ssid="1">The idea of using monolingual data for improving the translation model has been explored in several previous works.</S>
    <S sid="14" ssid="2">Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains.</S>
    <S sid="15" ssid="3">In their experiments, the most effective approach was to train a new translation model from &#8220;fake&#8221; parallel data consisting of target-side monolingual data and their machine translation into the source language by a baseline system.</S>
    <S sid="16" ssid="4">Ueffing et al. (2007) used a boot-strapping technique to extend translation models using monolingual data.</S>
    <S sid="17" ssid="5">They gradually translated additional source-side sentences and selectively incorporated them and their translations in the model.</S>
    <S sid="18" ssid="6">Our technique also bears a similarity to de Gispert et al. (2005), in that we try to use a back-off for surface forms to generalize our model and produce translations with word forms never seen in the original parallel data.</S>
    <S sid="19" ssid="7">However, instead of a rulebased approach, we take advantage of the available data and learn these forms statistically.</S>
    <S sid="20" ssid="8">We are therefore not limited to verbs, but our system is only able to generate surface forms observed in the target-side monolingual data.</S>
  </SECTION>
  <SECTION title="3 Reverse Self-Training" number="3">
    <S sid="21" ssid="1">Figure 1 illustrates the core of the method.</S>
    <S sid="22" ssid="2">Using available parallel data, we first train an MT system to translate from the target to the source language.</S>
    <S sid="23" ssid="3">Since we want to gather new word forms from the monolingual data, this reverse model needs the ability to translate them.</S>
    <S sid="24" ssid="4">For that purpose we use a factored translation model (Koehn and Hoang, 2007) with two alternative decoding paths: form&#8212;form and back-off&#8212;form.</S>
    <S sid="25" ssid="5">We experimented with several options for the back-off (simple stemming by truncation or full lemmatization), see Section 4.4.</S>
    <S sid="26" ssid="6">The decoder can thus use a less sparse representation of words if their exact forms are not available in the parallel data.</S>
    <S sid="27" ssid="7">We use this reverse model to translate (much larger) target-side monolingual data into the source language.</S>
    <S sid="28" ssid="8">We preserve the word alignments of the phrases as used in the decoding so we directly obtain the word alignment in the new &#8220;parallel&#8221; corpus.</S>
    <S sid="29" ssid="9">This gives us enough information to proceed with the standard MT system training &#8211; we extract and score the phrases consistent with the constructed word alignment and create the phrase table.</S>
    <S sid="30" ssid="10">We combine this enlarged translation model with a model trained on the true parallel data and use Minimum Error Rate Training (Och, 2003) to find the balance between the two models.</S>
    <S sid="31" ssid="11">The final model has four separate components &#8211; two language models (one trained on parallel and one on monolingual data) and the two translation models.</S>
    <S sid="32" ssid="12">We do not expect the translation quality to improve simply because more data is included in training &#8211; by adding translations generated using known data, the model could gain only new combinations of known words.</S>
    <S sid="33" ssid="13">However, by using a back-off to less sparse units (e.g. lemmas) in the factored target&#8212;source translation, we enable the decoder to produce previously unseen surface forms.</S>
    <S sid="34" ssid="14">These translations are then included in the model, reducing the data sparseness of the target-side surface forms.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="35" ssid="1">We used common tools for phrase-based translation &#8211; Moses (Koehn et al., 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments.</S>
    <S sid="36" ssid="2">For reverse self-training, we needed Moses to also output word alignments between source sentences and their translations.</S>
    <S sid="37" ssid="3">As we were not able to make the existing version of this feature work, we added a new option and re-implemented this funcionality.</S>
    <S sid="38" ssid="4">We rely on automatic translation quality evaluation throughout our paper, namely the wellestablished BLEU metric (Papineni et al., 2002).</S>
    <S sid="39" ssid="5">We estimate 95% confidence bounds for the scores as described in Koehn (2004).</S>
    <S sid="40" ssid="6">We evaluated our translations on lower-cased sentences.</S>
    <S sid="41" ssid="7">Aside from the WMT 2011 Translation Task data, we also used several additional data sources for the experiments aimed at evaluating various aspects of reverse self-training.</S>
    <S sid="42" ssid="8">JRC-Acquis We used the JRC-Acquis 3.0 corpus (Steinberger et al., 2006) mainly because of the number of available languages.</S>
    <S sid="43" ssid="9">This corpus contains a large amount of legislative texts of the European Union.</S>
    <S sid="44" ssid="10">The fact that all data in the corpus come from a single, very narrow domain has two effects &#8211; models trained on this corpus perform mostly very well in that domain (as documented e.g.</S>
    <S sid="45" ssid="11">in Koehn et al. (2009)), but fail when translating ordinary texts such as news or fiction.</S>
    <S sid="46" ssid="12">Sentences in this corpus also tend to be rather long (e.g.</S>
    <S sid="47" ssid="13">30 words on average for English).</S>
    <S sid="48" ssid="14">CzEng CzEng 0.9 (Bojar and &#711;Zabokrtsk&#180;y, 2009) is a parallel richly annotated Czech-English corpus.</S>
    <S sid="49" ssid="15">It contains roughly 8 million parallel sentences from a variety of domains, including European regulations (about 34% of tokens), fiction (15%), news (3%), technical texts (10%) and unofficial movie subtitles (27%).</S>
    <S sid="50" ssid="16">We do not make much use of the rich annotation in this paper, however we did experiment with using Czech lemmas (included in the annotation) as the back-off factor for reverse self-training.</S>
    <S sid="51" ssid="17">In order to determine how successful our approach is across languages, we experimented with Czech, Finnish, German and Slovak as target languages.</S>
    <S sid="52" ssid="18">All of them have a rich morphology in some sense.</S>
    <S sid="53" ssid="19">We limited our selection of source languages to English, French and German because our method focuses on the target language anyway.</S>
    <S sid="54" ssid="20">We did however combine the languages with respect to the richness of their vocabulary &#8211; the source language has less word forms in almost all cases.</S>
    <S sid="55" ssid="21">Czech and Slovak are very close languages, sharing a large portion of vocabulary and having a very similar grammar.</S>
    <S sid="56" ssid="22">There are many inflectional rules for verbs, nouns, adjectives, pronouns and numerals.</S>
    <S sid="57" ssid="23">Sentence structure is exhibited by various agreement rules which often apply over long distance.</S>
    <S sid="58" ssid="24">Most of the issues commonly associated with rich morphology are clearly observable in these languages.</S>
    <S sid="59" ssid="25">German also has some inflection, albeit much less complex.</S>
    <S sid="60" ssid="26">The main source of German vocabulary size are the compound words.</S>
    <S sid="61" ssid="27">Finnish serves as an example of agglutinative languages well-known for the abundance of word forms.</S>
    <S sid="62" ssid="28">Table 1 contains the summary of our experimental results.</S>
    <S sid="63" ssid="29">Here, only the JRC-Acquis corpus was used for training, development and evaluation.</S>
    <S sid="64" ssid="30">For every language pair, we extracted the first 10 percent of the parallel corpus and used them as the parallel data.</S>
    <S sid="65" ssid="31">The last 70 percent of the same corpus were our &#8220;monolingual&#8221; data.</S>
    <S sid="66" ssid="32">We used a separate set of 1000 sentences for the development and another 1000 for testing.</S>
    <S sid="67" ssid="33">Sentence counts of the corpora are shown in the columns Corpus Size Para and Mono.</S>
    <S sid="68" ssid="34">The table also shows the ratio between observed vocabulary size of the target and source language.</S>
    <S sid="69" ssid="35">Except for the German-*Czech language pair, the ratios are higher than 1.</S>
    <S sid="70" ssid="36">The Baseline column contains the BLEU score of a system trained solely on the parallel data (i.e. the first 10 percent).</S>
    <S sid="71" ssid="37">A 5-gram language model was used.</S>
    <S sid="72" ssid="38">The &#8220;+Mono LM&#8221; scores were achieved by adding a 5-gram language model trained on the monolingual data as a separate component (its weight was determined by MERT).</S>
    <S sid="73" ssid="39">The last column contains the scores after adding the translation model self-trained on target monolingual data.</S>
    <S sid="74" ssid="40">This model was also added as another component and the weights associated with it were found by MERT.</S>
    <S sid="75" ssid="41">For the back-off in the reverse self-training, we used a simple suffix-trimming heuristic suitable for fusional languages: cut off the last three characters of each word always keeping at least the first three characters.</S>
    <S sid="76" ssid="42">This heuristic reduces the vocabulary size to a half for Czech and Slovak but it is much less effective for Finish and German (Table 2), as can be expected from their linguistic properties.</S>
    <S sid="77" ssid="43">We did not use any linguistic tools, such as morphological analyzers, in this set of experiments.</S>
    <S sid="78" ssid="44">We see the main point of this section in illustrating the applicability of our technique on a wide range of languages, including languages for which such tools are not available.</S>
    <S sid="79" ssid="45">We encountered problems when using MERT to balance the weights of the four model components.</S>
    <S sid="80" ssid="46">Our model consisted of 14 features &#8211; one for each language model, five for each translation model (phrase probability and lexical weight for both directions and phrase penalty), word penalty and distortion penalty.</S>
    <S sid="81" ssid="47">The extra 5 weights of the reversely trained translation model caused MERT to diverge in some cases.</S>
    <S sid="82" ssid="48">Since we used the mert-moses.pl script for tuning and kept the default parameters, MERT ran for 25 iterations and stopped.</S>
    <S sid="83" ssid="49">As a result, even though our method seemed to improve translation performance in most language pairs, several experiments contradicted this observation.</S>
    <S sid="84" ssid="50">We simply reran the final tuning procedure in these cases and were able to achieve an improvement in BLEU as well.</S>
    <S sid="85" ssid="51">These language pairs are marked with a &#8217;*&#8217; sign in Table 1.</S>
    <S sid="86" ssid="52">A possible explanation for this behaviour of MERT is that the alternative decoding paths add a lot of possible derivations that generate the same string.</S>
    <S sid="87" ssid="53">To validate our hypothesis we examined a diverging run of MERT for English--+Czech translation with two translation models.</S>
    <S sid="88" ssid="54">Our n-best lists contained the best 100 derivations for each trans0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8 3 Vocabulary size ratio lated sentence from the development data.</S>
    <S sid="89" ssid="55">On average (over all 1000 sentences and over all runs), the n-best list only contained 6.13 different translations of a sentence.</S>
    <S sid="90" ssid="56">The result of the same calculation applied on the baseline run of MERT (which converged in 9 iterations) was 34.85 hypotheses.</S>
    <S sid="91" ssid="57">This clear disproportion shows that MERT had much less information when optimizing our model.</S>
    <S sid="92" ssid="58">Overall, reverse self-training seems helpful for translating into morphologically rich languages.</S>
    <S sid="93" ssid="59">We achieved promising gains in BLEU, even over the baseline including a language model trained on the monolingual data.</S>
    <S sid="94" ssid="60">The improvement ranges from roughly 0.3 (e.g.</S>
    <S sid="95" ssid="61">German-*Czech) to over 1 point (English-*German) absolute.</S>
    <S sid="96" ssid="62">This result also indicates that suffix trimming is a quite robust heuristic, useful for a variety of language types.</S>
    <S sid="97" ssid="63">Figure 2 illustrates the relationship between vocabulary size ratio of the language pair and the improvement in translation quality.</S>
    <S sid="98" ssid="64">Although the points are distributed quite irregularly, a certain tendency towards higher gains with higher ratios is observable.</S>
    <S sid="99" ssid="65">We assume that reverse self-training is most useful in cases where a single word form in the source language can be translated as several forms in the target language.</S>
    <S sid="100" ssid="66">A higher ratio between vocabulary sizes suggests that these cases happen more often, thus providing more space for improvement using our method.</S>
    <S sid="101" ssid="67">We conducted a series of English-to-Czech experiments with fixed parallel data and a varying size of monolingual data.</S>
    <S sid="102" ssid="68">We used the CzEng corpus, 500 thousand parallel sentences and from 500 thousand up to 5 million monolingual sentences.</S>
    <S sid="103" ssid="69">We used two separate sets of 1000 sentences from CzEng for development and evaluation.</S>
    <S sid="104" ssid="70">Our results are summarized in Figure 3.</S>
    <S sid="105" ssid="71">The gains in BLEU become more significant as the size of included monolingual data increases.</S>
    <S sid="106" ssid="72">The highest improvement can be observed when the data are largest &#8211; over 3 points absolute.</S>
    <S sid="107" ssid="73">Figure 4 shows an example of the impact on translation quality &#8211; the &#8220;Mono&#8221; data are 5 million sentences.</S>
    <S sid="108" ssid="74">When evaluated from this point of view, our method can also be seen as a way of considerably improving translation quality for languages with little available parallel data.</S>
    <S sid="109" ssid="75">We also experimented with varying size of parallel data (500 thousand to 5 million sentences) and its effect on reverse self-training contribution.</S>
    <S sid="110" ssid="76">The size of monolingual data was always 5 million sentences.</S>
    <S sid="111" ssid="77">We first measured the percentage of test data word forms covered by the training data.</S>
    <S sid="112" ssid="78">We calculated the value for parallel data and for the combination of parallel and monolingual data.</S>
    <S sid="113" ssid="79">For word forms that appeared only in the monolingual data, a different form of the word had to be contained in the parallel data (so that the model can learn it through the backoff heuristic) in order to be counted in.</S>
    <S sid="114" ssid="80">The difference between the first and second value can simply be thought of as the upper-bound estimation of reverse self-training contribution.</S>
    <S sid="115" ssid="81">Figure 5 shows the results along with BLEU scores achieved in translation experiments following this scenario.</S>
    <S sid="116" ssid="82">Our technique has much greater effect for small parallel data sizes; the amount of newly learned word forms declines rapidly as the size grows.</S>
    <S sid="117" ssid="83">Similarly, improvement in BLEU score decreases quickly and becomes negligible around 2 million parallel sentences.</S>
    <S sid="118" ssid="84">We experimented with several options for the backoff factor in English&#8212;*Czech translation.</S>
    <S sid="119" ssid="85">Data from training section of CzEng were used, 1 million parallel sentences and another 5 million sentences as target-side monolingual data.</S>
    <S sid="120" ssid="86">As in the previous section, the sizes of our development and evaluation sets were a thousand sentences.</S>
    <S sid="121" ssid="87">CzEng annotation contains lexically disambiguated word lemmas, an appealing option for our purposes.</S>
    <S sid="122" ssid="88">We also tried trimming the last 3 characters of each word, keeping at least the first 3 characters intact.</S>
    <S sid="123" ssid="89">Stemming of each word to four characters was also evaluated (Stem-4).</S>
    <S sid="124" ssid="90">Table 3 summarizes our results.</S>
    <S sid="125" ssid="91">The last column shows the vocabulary size compared to original vocabulary size, estimated on lower-cased words.</S>
    <S sid="126" ssid="92">We are not surprised by stemming performing the worst &#8211; the equivalence classes generated by this simple heuristic are too broad.</S>
    <S sid="127" ssid="93">Using lemmas seems optimal from the linguistic point of view, however suffix trimming outperformed this approach in our experiments.</S>
    <S sid="128" ssid="94">We feel that finding well-performing back-off techniques for other languages merits further research.</S>
    <S sid="129" ssid="95">We submitted systems that used reverse selftraining (cu-tamchyna) for English-*Czech and English-*German language pairs.</S>
    <S sid="130" ssid="96">Our parallel data for German were constrained to the provided set (1.9 million sentences).</S>
    <S sid="131" ssid="97">For Czech, we used the training sections of CzEng and the supplied WMT11 News Commentary data (7.3 million sentences in total).</S>
    <S sid="132" ssid="98">In case of German, we only used the supplied monolingual data, for Czech we used a large collection of texts for language modelling (i.e. unconstrained).</S>
    <S sid="133" ssid="99">The reverse self-training used only the constrained data &#8211; 2.3 million sentences in German and 2.2 in Czech.</S>
    <S sid="134" ssid="100">In case of Czech, we only used the News monolingual data from 2010 and 2011 for reverse self-training &#8211; we expected that recent data from the same domain as the test set would improve translation performance the most.</S>
    <S sid="135" ssid="101">We achieved mixed results with these systems &#8211; for translation into German, reverse self-training did not improve translation performance.</S>
    <S sid="136" ssid="102">For Czech, we were able to achieve a small gain, even though the reversely translated data contained less sentences than the parallel data.</S>
    <S sid="137" ssid="103">Our BLEU scores were also affected by submitting translation outputs without normalized punctuation and with a slightly different tokenization.</S>
    <S sid="138" ssid="104">In this scenario, a lot of parallel data were available and we did not manage to prepare a reversely trained model from larger monolingual data.</S>
    <S sid="139" ssid="105">Both of these factors contributed to the inconclusive results.</S>
    <S sid="140" ssid="106">Table 4 shows case-insensitive BLEU scores as calculated in the official evaluation.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="5">
    <S sid="141" ssid="1">We introduced a technique for exploiting monolingual data to improve the quality of translation into morphologically rich languages.</S>
    <S sid="142" ssid="2">We carried out experiments showing improvements in BLEU when using our method for translating into Czech, Finnish, German and Slovak with small parallel data.</S>
    <S sid="143" ssid="3">We discussed the issues of including similar translation models as separate components in MERT.</S>
    <S sid="144" ssid="4">We showed that gains in BLEU score increase with growing size of monolingual data.</S>
    <S sid="145" ssid="5">On the other hand, growing parallel data size diminishes the effect of our method quite rapidly.</S>
    <S sid="146" ssid="6">We also documented our experiments with several back-off techniques for English to Czech translation.</S>
    <S sid="147" ssid="7">Finally, we described our primary submissions to the WMT 2011 Shared Translation Task.</S>
  </SECTION>
</PAPER>
