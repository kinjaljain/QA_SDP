<PAPER>
  <S sid="0">K-Best Combination of Syntactic Parsers</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers.</S>
    <S sid="2" ssid="2">The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model.</S>
    <S sid="3" ssid="3">As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features.</S>
    <S sid="4" ssid="4">For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm.</S>
    <S sid="5" ssid="5">Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model.</S>
    <S sid="6" ssid="6">Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages.</S>
    <S sid="8" ssid="2">In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al.</S>
    <S sid="9" ssid="3">2006; Petrov and Klein 2007).</S>
    <S sid="10" ssid="4">In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity.</S>
    <S sid="11" ssid="5">Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper).</S>
    <S sid="12" ssid="6">Therefore, it is natural to combine the two models for better parsing performance.</S>
    <S sid="13" ssid="7">Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and &#381;abokrtsk&#253; 2005; Sagae and Lavie 2006) and promising performance improvements have been reported.</S>
    <S sid="14" ssid="8">In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance.</S>
    <S sid="15" ssid="9">This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level.</S>
    <S sid="16" ssid="10">In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers.</S>
    <S sid="17" ssid="11">In this paper, we propose a linear model-based general framework for multiple parsers combination.</S>
    <S sid="18" ssid="12">The proposed framework leverages on the strengths of previous system combination and reranking methods and is open to any type of features.</S>
    <S sid="19" ssid="13">In particular, it is capable of utilizing the logarithm of the parse tree probability from each individual parser while previous combination methods are unable to use this feature since the probabilities from different parsers are not comparable.</S>
    <S sid="20" ssid="14">In addition, we experiment on k-best combination while previous methods are only verified on 1-best combination.</S>
    <S sid="21" ssid="15">Finally, we apply our method in combining outputs from both the lexicalized and un-lexicalized parsers while previous methods only carry out experiments on multiple lexicalized parsers.</S>
    <S sid="22" ssid="16">We also compare two learning algorithms in tuning the feature weights for the linear model.</S>
    <S sid="23" ssid="17">We perform extensive experiments on the Chinese and English Penn Treebank corpus.</S>
    <S sid="24" ssid="18">Experimental results show that our final results, an F-Score of 92.62 on English and 85.45 on Chinese, outperform the previously best-reported systems by 0.52 point and 1.21 point, respectively.</S>
    <S sid="25" ssid="19">This convincingly demonstrates the effectiveness of our proposed framework.</S>
    <S sid="26" ssid="20">Our study also shows that the simulated-annealing algorithm (Kirkpatrick et al. 1983) is more effective than the perceptron algorithm (Collins 2002) for feature weight tuning.</S>
    <S sid="27" ssid="21">The rest of this paper is organized as follows.</S>
    <S sid="28" ssid="22">Section 2 briefly reviews related work.</S>
    <S sid="29" ssid="23">Section 3 discusses our method while section 4 presents the feature weight tuning algorithm.</S>
    <S sid="30" ssid="24">In Section 5, we report our experimental results and then conclude in Section 6.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="31" ssid="1">As discussed in the previous section, system combination and re-ranking are two techniques to improve parsing performance by postprocessing parsers&#8217; k-best outputs.</S>
    <S sid="32" ssid="2">Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees.</S>
    <S sid="33" ssid="3">According to the second scheme, it breaks each parse tree into constituents, calculates the count of each constituent, then applies the majority voting to decide which constituent would appear in the final tree.</S>
    <S sid="34" ssid="4">Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination.</S>
    <S sid="35" ssid="5">Zeman and &#381;abokrtsk&#253; (2005) study four combination techniques, including voting, stacking, unbalanced combining and switching, for constituent selection on Czech dependency parsing.</S>
    <S sid="36" ssid="6">Promising results have been reported in all the above three prior work.</S>
    <S sid="37" ssid="7">Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper.</S>
    <S sid="38" ssid="8">Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper.</S>
    <S sid="39" ssid="9">Finally, Zeman and &#381;abokrtsk&#253; (2005) reports great improvements over each individual parsers and show that a parser with very low accuracy can also help to improve the performance of a highly accurate parser.</S>
    <S sid="40" ssid="10">However, there are two major limitations in these prior works.</S>
    <S sid="41" ssid="11">First, only one-best output from each individual parsers are utilized.</S>
    <S sid="42" ssid="12">Second, none of these works uses the parse probability of each parse tree output from the individual parser.</S>
    <S sid="43" ssid="13">Regarding the parser re-ranking, Collins (2000) proposes a dozen of feature types to re-rank kbest outputs of a single head-driven parser.</S>
    <S sid="44" ssid="14">He uses these feature types to extract around half a million different features on the training set, and then examine two loss functions, MRF and Boosting, to do feature selection.</S>
    <S sid="45" ssid="15">Charniak and Johnson (2005) generate a more accurate k-best output and adopt MaxEnt method to estimate the feature weights for more than one million features extracted from the training set.</S>
    <S sid="46" ssid="16">Huang (2008) further improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list.</S>
    <S sid="47" ssid="17">The reranking techniques also achieve great improvement over the original individual parser.</S>
    <S sid="48" ssid="18">Collins (2002) improves the F1 score from 88.2% to 89.7%, while Charniak and Johnson (2005) improve from 90.3% to 91.4%.</S>
    <S sid="49" ssid="19">This latter work was then further improved by Huang (2008) to 91.7%, by utilizing the benefit of forest structure.</S>
    <S sid="50" ssid="20">However, one of the limitations of these techniques is the huge number of features which makes the training very expensive and inefficient in space and memory usage.</S>
  </SECTION>
  <SECTION title="3 K-best Combination of Lexicalized and Un-Lexicalized Parsers with Model Probabilities" number="3">
    <S sid="51" ssid="1">In this section, we first introduce our proposed kbest combination framework.</S>
    <S sid="52" ssid="2">Then we apply this framework to the combination of two state-ofthe-art lexicalized and un-lexicalized parsers with an additional feature inspired by traditional combination techniques.</S>
    <S sid="53" ssid="3">Our proposed framework consists of the following steps: annotation model&#8217;s confidence for this tree.</S>
    <S sid="54" ssid="4">This enables our method to effectively utilize the confidence measure of all the individual models without any bias.</S>
    <S sid="55" ssid="5">Without this re-evaluation step, the previous combination methods are unable to utilize the various model scores.</S>
    <S sid="56" ssid="6">The above is the linear function used in our method, where t is the tree to be evaluated, to are the model confidence scores (in this paper, we use logarithm of the parse tree probability) from the N models, to are their weights, &#8242; to &#8242; are the L additional features, &#8242; to &#8242; are their weights.</S>
    <S sid="57" ssid="7">In this paper, we employ two individual parsing model scores and only one additional feature.</S>
    <S sid="58" ssid="8">Let be the head-driven model score, be the latent-annotation model score, &#8242; be the constituent count feature and &#8242; is the weight of feature &#8242; .</S>
    <S sid="59" ssid="9">The term &#8220;confidence&#8221; was used in prior parser combination studies to refer to the accuracy of each individual parser.</S>
    <S sid="60" ssid="10">This reflects how much we can trust the parse output of each parser.</S>
    <S sid="61" ssid="11">In this paper, we use the term &#8220;confidence&#8221; to refer to the logarithm of the tree probability computed by each model, which is a direct measurement of the model&#8217;s confidence on the target tree being the best or correct parse output.</S>
    <S sid="62" ssid="12">In fact, the feature weight i in our linear model functions similarly as the traditional &#8220;confidence&#8221;.</S>
    <S sid="63" ssid="13">However, we do not directly use parser&#8217;s accuracy as its value.</S>
    <S sid="64" ssid="14">Instead we tune it automatically on development set to optimize it against the parsing performance directly.</S>
    <S sid="65" ssid="15">In the following, we introduce the state-of-the-art head-driven lexicalized and latent-annotation un-lexicalized models (which are used as two individual models in this paper), and describe how they compute the tree probability briefly.</S>
    <S sid="66" ssid="16">Head-driven model is one of the most representative lexicalized models.</S>
    <S sid="67" ssid="17">It attaches the head word to each non-terminal and views the generation of each rule as a Markov process first from father to head child, and then to the head child&#8217;s left and right siblings.</S>
    <S sid="68" ssid="18">Take following rule r as example, is the rule&#8217;s left hand side (i.e. father label), is the head child, is M&#8217;s left sibling and is M&#8217;s right sibling.</S>
    <S sid="69" ssid="19">Let h be M&#8217;s head word, the probability of this rule is The probability of a tree is just the product of the probabilities of all the rules in it.</S>
    <S sid="70" ssid="20">The above is the general framework of head-driven model.</S>
    <S sid="71" ssid="21">For a specific model, there may be some additional features and modification.</S>
    <S sid="72" ssid="22">For example, the model2 in Collins (1999) introduces subcategorization and model3 introduces gap as additional features.</S>
    <S sid="73" ssid="23">Charniak (2000)&#8217;s model introduces pre-terminal as additional features.</S>
    <S sid="74" ssid="24">The latent-annotation model (Matsuzaki et al. 2005; Petrov et al.</S>
    <S sid="75" ssid="25">2006) is one of the most effective un-lexicalized models.</S>
    <S sid="76" ssid="26">Briefly speaking, latent-annotation model views each non-terminal in the Treebank as a non-terminal followed by a set of latent variables, and uses EM algorithms to automatically learn the latent variables&#8217; probability functions to maximize the probability of the given training data.</S>
    <S sid="77" ssid="27">Take the following binarized rule as example, could be viewed as the set of rules The process of computing the probability of a normal tree is to first binarized all the rules in it, and then replace each rule to the corresponding set of rules with latent variables.</S>
    <S sid="78" ssid="28">Now the previous tree becomes a packed forest (Klein and Manning 2001; Petrov et al. 2007) in the latentannotation model, and its probability is the inside probability of the root node.</S>
    <S sid="79" ssid="29">This model is quite different from the head-driven model in which the probability of a tree is just the product all the rules&#8217; probability.</S>
    <S sid="80" ssid="30">Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006).</S>
    <S sid="81" ssid="31">A constituent is a non-terminal node covering a special span.</S>
    <S sid="82" ssid="32">For example, &#8220;NP[2,4]&#8221; means a constituent labelled as &#8220;NP&#8221; which covers the span from the second word to the fourth word.</S>
    <S sid="83" ssid="33">If we have 100 trees and NP[2,4] appears in 60 of them, then its constituent count is 60.</S>
    <S sid="84" ssid="34">For each tree, its constituent count is the sum of all the counts of its constituent.</S>
    <S sid="85" ssid="35">However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall.</S>
    <S sid="86" ssid="36">To solve this issue, Sagae and Lavie (2006) use a threshold to balance them.</S>
    <S sid="87" ssid="37">For any constituent, we calculate its count if and only if it appears more than X times in the k-best trees; otherwise we set it as 0.</S>
    <S sid="88" ssid="38">In this paper, we normalize this feature by dividing the constituent count by the number of k-best.</S>
    <S sid="89" ssid="39">Note that the threshold value and the additional feature value are not independent.</S>
    <S sid="90" ssid="40">Once the threshold changes, the feature value has to be recalculated.</S>
    <S sid="91" ssid="41">In conclusion, we have four parameters to estimate: two model score weights, one additional feature weight and a threshold for the additional feature.</S>
  </SECTION>
  <SECTION title="4 Parameter Estimation" number="4">
    <S sid="92" ssid="1">We adopt the minimum error rate principle to tune the feature weights by minimizing the error rate (i.e. maximizing the F1 score) on the development set.</S>
    <S sid="93" ssid="2">In our study, we implement and compare two algorithms, the simulated-annealing style algorithm and the average perceptron algorithm.</S>
    <S sid="94" ssid="3">Simulated-annealing algorithm has been proved to be a powerful and efficient algorithm in solving NP problem (&#268;ern&#253; 1985).</S>
    <S sid="95" ssid="4">Fig 1 is the pseudo code of the simulated-annealing algorithm that we apply.</S>
    <S sid="96" ssid="5">In a single iteration (line 4-11), the simulated algorithm selects some random points (the Markov link) for hill climbing.</S>
    <S sid="97" ssid="6">However, it accepts some bad points with a threshold probability controlled by the annealing temperature (line 710).</S>
    <S sid="98" ssid="7">The hill climbing nature gives this algorithm the ability of converging at local maximal point and the random nature offers it the chance to jump from some local maximal points to global maximal point.</S>
    <S sid="99" ssid="8">We do a slight modification to save the best parameter so far across all the finished iterations and let it be the initial point for upcoming iterations (line 12-17).</S>
    <S sid="100" ssid="9">RandomNeighbour(p) is the function to generate a random neighbor for the p (the four-tuple parameter to be estimated).</S>
    <S sid="101" ssid="10">F1(p) is the function to calculate the F1 score over the entire test set.</S>
    <S sid="102" ssid="11">Given a fixed parameter p, it selects the candidate tree with best score for each sentence and computes the F1 score with the PARSEVAL metrics.</S>
    <S sid="103" ssid="12">Another algorithm we apply is the averaged perceptron algorithm.</S>
    <S sid="104" ssid="13">Fig 2 is the pseudo code of this algorithm.</S>
    <S sid="105" ssid="14">Averaged perceptron is an online algorithm.</S>
    <S sid="106" ssid="15">It iterates through each instance.</S>
    <S sid="107" ssid="16">In each instance, it selects the candidate answer with the maximum function score.</S>
    <S sid="108" ssid="17">Then it updates the weight by the margin of feature value between the select answer and the oracle answer (line 5-9).</S>
    <S sid="109" ssid="18">After each iteration, it does average to generate a new weight (line 10).</S>
    <S sid="110" ssid="19">The averaged perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks (Collins 2002).</S>
    <S sid="111" ssid="20">However, it needs a slightly modification to adapt to our problem.</S>
    <S sid="112" ssid="21">Since the threshold and the constituent count are not independent, they are not linear separable.</S>
    <S sid="113" ssid="22">In this case, the perceptron algorithm cannot be guaranteed to converge.</S>
    <S sid="114" ssid="23">To solve this issue, we introduce an outer loop (line 2) to iterate through the value range of threshold with a fixed step length and in the inner loop we use perceptron to estimate the other three parameters.</S>
    <S sid="115" ssid="24">Finally we select the final parameter which has maximum F1 score across all the iteration (line 14-17).</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="116" ssid="1">We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 5.0 and WSJ English Treebank 3.0 (Marcus et al. 1993) as shown in Table 1.</S>
    <S sid="117" ssid="2">We use Satoshi Sekine and Michael Collins&#8217; EVALB script modified by David Ellis for accuracy evaluation.</S>
    <S sid="118" ssid="3">We use Charniak&#8217;s parser (Charniak 2000) and Berkeley&#8217;s parser (Petrov and Klein 2007) as the two individual parsers, where Charniak&#8217;s parser represents the best performance of the lexicalized model and the Berkeley&#8217;s parser represents the best performance of the un-lexicalized model.</S>
    <S sid="119" ssid="4">We retrain both of them according to the division in Table.</S>
    <S sid="120" ssid="5">1.</S>
    <S sid="121" ssid="6">The number of EM iteration process for Berkeley&#8217;s parser is set to 5 on English and 6 on Chinese.</S>
    <S sid="122" ssid="7">Both the Charniak&#8217;s parser and Berkeley&#8217;s parser provide function to evaluate an input parse tree&#8217;s probability and output the logarithm of the probability.</S>
    <S sid="123" ssid="8">This sub-section examines the effectiveness of our proposed methods.</S>
    <S sid="124" ssid="9">The experiment is set up as follows: 1) for each sentence in the dev and test sets, we generate 50-best from Charniak&#8217;s parser (Charniak 2000) and Berkeley&#8217;s parser (Petrov and Klein 2007), respectively; 2) the two 50-best trees are merged together and duplication was removed; 3) we tune the parameters on the dev set and test on the test set.</S>
    <S sid="125" ssid="10">(Without specific statement, we use simulated-annealing as default weight tuning algorithm.)</S>
    <S sid="126" ssid="11">The results are shown in Table 2 and Table 3.</S>
    <S sid="127" ssid="12">&#8220;P&#8221; means precision, &#8220;R&#8221; means recall and &#8220;F&#8221; is the F1-measure (all is in % percentage metrics); &#8220;Charniak&#8221; represents the parser of (Charniak 2000), &#8220;Berkeley&#8221; represents the parser of (Petrov and Klein 2007), &#8220;Comb.&#8221; represents the combination of the two parsers.</S>
    <S sid="128" ssid="13">From Table 2 and Table 3, we can see our method outperforms the single systems in all test cases with all the three evaluation metrics.</S>
    <S sid="129" ssid="14">Using the entire Chinese test set, our method improves the performance by 2.3 (85.45-83.13) point in F1-Score, representing 13.8% error rate reduction.</S>
    <S sid="130" ssid="15">Using the entire English test set, our method improves the performance by 1.7 (91.43-89.70) point in F1-Score, representing 16.5% error rate reduction.</S>
    <S sid="131" ssid="16">These improvements convincingly demonstrate the effectiveness of our method.</S>
    <S sid="132" ssid="17">Fig 3 and Fig.</S>
    <S sid="133" ssid="18">4 show the relationship between F1 score and the number of K-best used when doing combination on Chinese and English respectively.</S>
    <S sid="134" ssid="19">From Fig 3 and Fig.</S>
    <S sid="135" ssid="20">4, we could see that the F1 score first increases with the increasing of K (there are some vibration points, this may due to statistical noise) and reach the peak when K is around 30-50, then it starts to drop.</S>
    <S sid="136" ssid="21">It shows that k-best list did provide more information than one-best and thus can help improve the accuracy; however more k-best list may also contain more noises and these noises may hurt the final combination quality.</S>
    <S sid="137" ssid="22">In this subsection, we examine how different of the 50-best trees generated from Charnriak&#8217;s parser (head-driven model) (Charnriak, 2000) and Berkeley&#8217;s parser (latent-annotation model) (Petrov and Klein, 2007).</S>
    <S sid="138" ssid="23">Table 4 reports the statistics on the 50-best output for Chinese and English test set.</S>
    <S sid="139" ssid="24">Since for some short sentences the parser cannot generate up to 50 best trees, the average number of trees is less than 50 for each sentence.</S>
    <S sid="140" ssid="25">Each cell reports the total number of trees generated over the entire test set followed by the average count for each sentence in bracket.</S>
    <S sid="141" ssid="26">&#8220;Total&#8221; means simply combine the number of trees from the two parsers while &#8220;Unique&#8221; means the number after removing the duplicated trees for each sentence.</S>
    <S sid="142" ssid="27">In the last row, we report the averaged redundant rate for each sentence, which is derived by dividing the figures in the row &#8220;Duplicated&#8221; by those in the row &#8220;Total&#8221;.</S>
    <S sid="143" ssid="28">The small redundant rate clearly suggests that the two parsing models are quite different and are complementary to each other.</S>
    <S sid="144" ssid="29">The k-best oracle score is the upper bound of the quality of the k-best trees.</S>
    <S sid="145" ssid="30">Table 5 reports the oracle score for the 50-best of the two individual parsers and our method.</S>
    <S sid="146" ssid="31">Similar to Table 4, Table 5 shows again that the two models are complementary to each other and our method is able to take the strength of the two models.</S>
    <S sid="147" ssid="32">One of the advantages of our method that we claim is that we can utilize the feature of the model confidence score (logarithm of the parse tree probability).</S>
    <S sid="148" ssid="33">Table 6 shows that all the three features contribute to the final accuracy improvement.</S>
    <S sid="149" ssid="34">Even if we only use the &#8220;B+C&#8221; confidence scores, it also outperforms the baseline individual parser (as reported in Table 2 and Table 3) greatly.</S>
    <S sid="150" ssid="35">All these together clearly verify the effective of the model confidence feature and our method can effectively utilize this feature.</S>
    <S sid="151" ssid="36">Feat.</S>
    <S sid="152" ssid="37">I B+C B+C+I Lang Chinese 82.34 84.67 85.45 English 90.20 91.02 91.43 Table 6.</S>
    <S sid="153" ssid="38">F1 score on 50-best combination with different feature configuration.</S>
    <S sid="154" ssid="39">&#8220;I&#8221; means the constituent count, &#8220;B&#8221; means Berkeley parser confidence score and &#8220;C&#8221; means Charniak parser confidence score.</S>
    <S sid="155" ssid="40">In this sub-section, we compare the two weight tuning algorithms on 50-best combination tasks on both Chinese and English.</S>
    <S sid="156" ssid="41">Dan Bikel&#8217;s randomized parsing evaluation comparator (Bikel 2004) was used to do significant test on precision and recall metrics.</S>
    <S sid="157" ssid="42">The results are shown in Table 7.</S>
    <S sid="158" ssid="43">We can see, simulated annealing outperforms the averaged perceptron significantly in both precision (p&lt;0.005) and recall (p&lt;0.05) metrics of Chinese task and precision (p&lt;0.005) metric of English task.</S>
    <S sid="159" ssid="44">Though averaged perceptron got slightly better recall score on English task, it is not significant according to the p-value (p&gt;0.2).</S>
    <S sid="160" ssid="45">From table 8, we could see the simulated annealing algorithm is around 2-4 times slower than averaged perceptron algorithm.</S>
  </SECTION>
  <SECTION title="5.6 Performance-Enhanced Individual Parsers on English" number="6">
    <S sid="161" ssid="1">For Charniak&#8217;s lexicalized parser, there are two techniques to improve its performance.</S>
    <S sid="162" ssid="2">One is reranking as explained in section 2.</S>
    <S sid="163" ssid="3">The other is the self-training (McClosky et al. 2006) which first parses and reranks the NANC corpus, and then use them as additional training data to retrain the model.</S>
    <S sid="164" ssid="4">In this sub-section, we apply our method to combine the Berkeley parser and the enhanced Charniak parser by using the new model confidence score output from the enhanced Charniak parser.</S>
    <S sid="165" ssid="5">Table 9 and Table 10 show that the Charniak parser enhanced by re-ranking and self-training is able to help to further improve the performance of our method.</S>
    <S sid="166" ssid="6">This is because that the enhanced Charniak parser provides more accurate model confidence score.</S>
    <S sid="167" ssid="7">Table 11 and table 12 compare our method with the other state-of-the-art methods; we use I, B, R, S and C to denote individual model (Charniak 2000; Collins 2000; Bod 2003; Petrov and Klein 2007), bilingual-constrained model (Burkett and Klein 2008)1, re-ranking model (Charniak and Johnson 2005, Huang 2008), self-training model (David McClosky 2006) and combination model (Sagae and Lavie 2006) respectively.</S>
    <S sid="168" ssid="8">The two tables clearly show that our method advance the state-of-the-art results on both Chinese and English syntax parsing.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="7">
    <S sid="169" ssid="1">In this paper, we propose a linear model-based general framework for multiple parser combination.</S>
    <S sid="170" ssid="2">Compared with previous methods, our method is able to use diverse features, including logarithm of the parse tree probability calculated by the individual systems.</S>
    <S sid="171" ssid="3">We verify our method by combining the two representative parsing models, lexicalized model and un-lexicalized model, on both Chinese and English.</S>
    <S sid="172" ssid="4">Experimental results show our method is very effective and advance the state-of-the-art results on both Chinese and English syntax parsing.</S>
    <S sid="173" ssid="5">In the future, we will explore more features and study the forest-based combination methods for syntactic parsing.</S>
  </SECTION>
  <SECTION title="Acknowledgement" number="8">
    <S sid="174" ssid="1">We would like to thank Prof. Hwee Tou Ng for his help and support; Prof. Charniak for his suggestion on doing the experiments with the selftrained parser and David McCloksy for his help on the self-trained model; Yee Seng Chan and the anonymous reviewers for their valuable comments.</S>
  </SECTION>
</PAPER>
