<PAPER>
  <S sid="0">Ensemble Models for Dependency Parsing: Cheap and Good?</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking.</S>
    <S sid="2" ssid="2">In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models.</S>
    <S sid="3" ssid="3">This study proves that fast and accurate ensemble parsers can be built with minimal effort.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Several ensemble models have been proposed for the parsing of syntactic dependencies.</S>
    <S sid="5" ssid="2">These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell&#8217;Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell&#8217;Orletta, 2009).</S>
    <S sid="6" ssid="3">In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006).</S>
    <S sid="7" ssid="4">The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999).</S>
    <S sid="8" ssid="5">While all these models achieved good performance, the previous work has left several questions unanswered.</S>
    <S sid="9" ssid="6">Here we answer the following questions, in the context of English dependency parsing:</S>
  </SECTION>
  <SECTION title="2 Setup" number="2">
    <S sid="10" ssid="1">In our experiments we used the syntactic dependencies from the CoNLL 2008 shared task corpus (Surdeanu et al., 2008).</S>
    <S sid="11" ssid="2">We used seven base parsing models in this paper: six are variants of the Malt parser1 and the seventh is the projective version of MSTParser that uses only first-order features2 (or MST for short).</S>
    <S sid="12" ssid="3">The six Malt parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre&#8217;s arceager (AE), Nivre&#8217;s arc-standard (AS), and Covington&#8217;s non-projective model (CN)), and the parsing direction (left to right (-*) or right to left (+-)), similar to (Hall et al., 2007).</S>
    <S sid="13" ssid="4">The parameters of the Malt models were set to the values reported in (Hall et al., 2007).</S>
    <S sid="14" ssid="5">The MST parser was used with the default configuration.</S>
    <S sid="15" ssid="6">Table 1 shows the performance of these models in the development and test partitions.</S>
  </SECTION>
  <SECTION title="3 Experiments" number="3">
    <S sid="16" ssid="1">The most common approach for combining independently-trained models at parsing time is to assign each candidate dependency a score based on the number of votes it received from the base parsers.</S>
    <S sid="17" ssid="2">Considering that parsers specialize in different phenomena, these votes can be weighted by different criteria.</S>
    <S sid="18" ssid="3">To understand the importance of such weighting strategies we compare several voting approaches in Table 2: in the &#8220;unweighted&#8221; strategy all votes have the same weight; in all other strategies each vote is assigned a value equal to the accuracy of the given parser in the particular instance of the context considered, e.g., in the &#8220;weighted by POS of modifier&#8221; model we use the accuracies of the base models for each possible part-of-speech (POS) tag of a modifier token.</S>
    <S sid="19" ssid="4">In the table we show results as more base parsers are added to the ensemble (we add parsers in the order given by Table 1).</S>
    <S sid="20" ssid="5">The results in Table 2 indicate that weighting strategies do not have an important contribution to overall performance.</S>
    <S sid="21" ssid="6">The only approach that outperforms the LAS score of the unweighted voting model is the model that weighs parsers by their accuracy for a given modifier POS tag, but the improvement is marginal.</S>
    <S sid="22" ssid="7">On the other hand, the number of base parsers in the ensemble pool is crucial: performance generally continues to improve as more base parsers are considered.</S>
    <S sid="23" ssid="8">The best ensemble uses 6 out of the 7 base parsers.3 It is often argued that the best way to re-score candidate dependencies is not through voting but rather through a meta-classifier that selects candidate dependencies based on their likelihood of belonging to the correct tree.</S>
    <S sid="24" ssid="9">Unlike voting, a metaclassifier can combine evidence from multiple contexts (such as the ones listed in Table 2).</S>
    <S sid="25" ssid="10">However, in our experiments such a meta-classifier4 did not offer any gains over the much simpler unweighted voting strategy.</S>
    <S sid="26" ssid="11">We explain these results as follows: the meta-classifier can potentially help only when it proposes dependencies that disagree with the majority vote.</S>
    <S sid="27" ssid="12">We call such dependencies minority dependencies.5 For a given parser and context instance (e.g., a modifier POS), we define precision of minority dependencies as the ratio of minority dependencies in this group that are correct.</S>
    <S sid="28" ssid="13">Obviously, a 3We drew similar conclusions when we replaced voting with the re-parsing algorithms from the next sub-section.</S>
    <S sid="29" ssid="14">4We implemented a L2-regularized logistic regression classifier using as features: identifiers of the base models, POS tags of head and modifier, labels of dependencies, length of dependencies, length of sentence, and combinations of the above.</S>
    <S sid="30" ssid="15">5(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers. group of minority dependencies provides beneficial signal only if its precision is larger than 50%.</S>
    <S sid="31" ssid="16">Table 3 lists the total number of minority dependencies in groups with precision larger than 50% for all our base parsers and the most representative features.</S>
    <S sid="32" ssid="17">The table shows that the number of minority dependencies with useful signal is extremely low.</S>
    <S sid="33" ssid="18">All in all, it accounts for less than 0.7% of all dependencies in the development corpus.</S>
    <S sid="34" ssid="19">To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006; Hall et al., 2007).6 However, it is not clear that this step is necessary.</S>
    <S sid="35" ssid="20">In other words, how many sentences are not wellformed if one uses a simple word-by-word voting scheme?</S>
    <S sid="36" ssid="21">To answer this, we analyzed the output of our best word-by-word voting scheme (six base parsers weighted by the POS of the modifier).</S>
    <S sid="37" ssid="22">The results for both in-domain and out-of-domain testing corpora are listed in Table 4.</S>
    <S sid="38" ssid="23">The table shows that the percentage of badly-formed trees is relatively large: almost 10% out of domain.</S>
    <S sid="39" ssid="24">This indicates that the focus on algorithms that guarantee well-formed trees is justified.</S>
    <S sid="40" ssid="25">However, it is not clear how the Eisner algorithm, which has runtime complexity of O(n3) (n &#8211; number of tokens per sentence), compares against approximate re-parsing algorithms that have lower runtime complexity.</S>
    <S sid="41" ssid="26">One such algorithm was proposed by Attardi and Dell&#8217;Orletta (2009).</S>
    <S sid="42" ssid="27">The algorithm, which has a runtime complexity of O(n), builds dependency trees using a greedy top-down strategy, i.e., it starts by selecting the highest-scoring root node, then the highest-scoring children, etc.</S>
    <S sid="43" ssid="28">We compare these algorithms against the word-by-word voting scheme in Table 5.7 The results show that both algorithms pay a small penalty for guaranteeing well-formed trees.</S>
    <S sid="44" ssid="29">This performance drop is statistically significant out of domain.</S>
    <S sid="45" ssid="30">On the other hand, the difference between the Eisner and Attardi algorithms is not statistically significant out of domain.</S>
    <S sid="46" ssid="31">This experiment proves that approximate re-parsing algorithms are a better choice for practical purposes, i.e., ensemble parsing in domains different from the training material of the base models.</S>
    <S sid="47" ssid="32">Recent work has shown that the combination of base parsers at learning time, e.g., through stacking, yields considerable benefits (Nivre and McDonald, 2008; Attardi and Dell&#8217;Orletta, 2009).</S>
    <S sid="48" ssid="33">However, it is unclear how these approaches compare against the simpler ensemble models, which combine parsers only at runtime.</S>
    <S sid="49" ssid="34">To enable such a comparison, we reimplemented the best stacking model from (Nivre and McDonald, 2008) &#8211; MSTMalt &#8211; which trains a variant of the MSTParser that uses additional features extracted from the output of a Malt parser.</S>
    <S sid="50" ssid="35">In Table 6, we compare this stacking approach against four variants of our ensemble models.</S>
    <S sid="51" ssid="36">The superscript in the ensemble name indicates the runtime complexity of the model (O(n3) or O(n)).</S>
    <S sid="52" ssid="37">The cubic-time models use all base parsers from Table 1 and the Eisner algorithm for re-parsing.</S>
    <S sid="53" ssid="38">The lineartime models use only Malt-based parsers and the Attardi algorithm for re-parsing.</S>
    <S sid="54" ssid="39">The subscript in the model names indicates the percentage of available base parsers used, e.g., ensemble350% uses only the first three parsers from Table 1.</S>
    <S sid="55" ssid="40">These results show that MSTMalt is statistically equivalent to an ensemble that uses MST and two Malt variants, and both our ensemble100% models are significantly better than MSTMalt.</S>
    <S sid="56" ssid="41">While this comparison is somewhat unfair (MSTMalt uses two base models, whereas our ensemble models use at least three) it does illustrate that the advantages gained from combining parsers at learning time can be easily surpassed by runtime combination models that have access to more base parsers.</S>
    <S sid="57" ssid="42">Considering that variants of shift-reduce parsers can be generated with minimal effort (e.g., by varying the parsing direction, learning algorithms, etc.) and combining models at runtime is simpler than combining them at learning time, we argue that runtime parser combination is a more attractive approach.</S>
    <S sid="58" ssid="43">In Table 7 we compare our best ensemble models against the top two systems of the CoNLL-2008 shared task evaluation.</S>
    <S sid="59" ssid="44">The table indicates that our best ensemble model ranks second, outperforming significantly 19 other systems.</S>
    <S sid="60" ssid="45">The only model performing better than our ensemble is a parser that uses higher-order features and has a higher runtime complexity (O(n4)) (Johansson and Nugues, 2008).</S>
    <S sid="61" ssid="46">While this is certainly proof of the importance of higher-order features, it also highlights a pragmatic conclusion: in out-of-domain corpora, an ensemble of models that use only first-order features achieves performance that is within 1% LAS of much more complex models.</S>
  </SECTION>
  <SECTION title="4 Conclusions" number="4">
    <S sid="62" ssid="1">This study unearthed several non-intuitive yet important observations about ensemble models for dependency parsing.</S>
    <S sid="63" ssid="2">First, we showed that the diversity of base parsers is more important than complex learning models for parser combination, i.e., (a) ensemble models that combine several base parsers at runtime performs significantly better than a state-ofthe-art model that combines two parsers at learning time, and (b) meta-classification does not outperform unsupervised voting schemes for the re-parsing of candidate dependencies when six base models are available.</S>
    <S sid="64" ssid="3">Second, we showed that well-formed dependency trees can be guaranteed without significant performance loss by linear-time approximate re-parsing algorithms.</S>
    <S sid="65" ssid="4">And lastly, our analysis indicates that unweighted voting performs as well as weighted voting for the re-parsing of candidate dependencies.</S>
    <S sid="66" ssid="5">Considering that different base models are easy to generate, this work proves that ensemble parsers that are both accurate and fast can be rapidly developed with minimal effort.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="5">
    <S sid="67" ssid="1">This material is based upon work supported by the Air Force Research Laboratory (AFRL) under prime contract no.</S>
    <S sid="68" ssid="2">FA8750-09-C-0181.</S>
    <S sid="69" ssid="3">Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).</S>
    <S sid="70" ssid="4">We thank Johan Hall, Joakim Nivre, Ryan McDonald, and Giuseppe Attardi for their help in understanding details of their models.</S>
  </SECTION>
</PAPER>
