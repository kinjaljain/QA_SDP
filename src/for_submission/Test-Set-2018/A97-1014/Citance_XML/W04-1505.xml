<PAPER>
  <S sid="0">Fast Deep-Linguistic Statistical Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present and evaluate an implemented statistical minimal parsing strategy exploiting DG charateristics to permit fast, robust, deeplinguistic analysis of unrestricted text, and compare its probability model to (Collins, 1999) and an adaptation, (Dubey and Keller, 2003).</S>
    <S sid="2" ssid="2">We show that DG allows for the expression of the majority of English LDDs in a context-free way and offers simple yet powerful statistical models.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="3" ssid="1">We present a fast, deep-linguistic statistical parser that profits from DG characteristics and that uses am minimal parsing strategy.</S>
    <S sid="4" ssid="2">First, we rely on finite-state based approaches as long as possible, secondly where parsing is necessary we keep it context-free as long as possible1.</S>
    <S sid="5" ssid="3">For low-level syntactic tasks, tagging and baseNP chunking is used, parsing only takes place between heads of chunks.</S>
    <S sid="6" ssid="4">Robust, successful parsers (Abney, 1995; Collins, 1999) have shown that this division of labour is particularly attractive for DG.</S>
    <S sid="7" ssid="5">Deep-linguistic, Formal Grammar parsers have carefully crafted grammars written by professional linguists.</S>
    <S sid="8" ssid="6">But unrestricted real-world texts still pose a problem to NLP systems that are based on Formal Grammars.</S>
    <S sid="9" ssid="7">Few handcrafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al., 2002), (Burke et al., 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge.</S>
    <S sid="10" ssid="8">The typical problems can be grouped as follows.</S>
    <S sid="11" ssid="9">Grammar complexity Fully comprehensive grammars are difficult to maintain and consid'Non-subject WH-question pronouns and support verbs cannot be treated context-free with our approach.</S>
    <S sid="12" ssid="10">We use a simple pre-parsing step to analyze them erably increase parsing complexity.</S>
    <S sid="13" ssid="11">Parsing complexity Typical formal grammar parser complexity is much higher than the O(n3) for CFG.</S>
    <S sid="14" ssid="12">The complexity of some formal grammars is still unknown.2 Parsing algorithms able to treat completely unrestricted long-distance dependencies are NPcomplete (Neuhaus and Broker, 1997).</S>
    <S sid="15" ssid="13">Ranking Returning all syntactically possible analyses for a sentence is not what is expected of a syntactic analyzer.</S>
    <S sid="16" ssid="14">A clear indication of preference is needed.</S>
    <S sid="17" ssid="15">Pruning In order to keep search spaces manageable it is necessary to discard unconvincing alternatives already during the parsing process.</S>
    <S sid="18" ssid="16">A number of robust statistical parsers that offer solutions to these problems have become available (Charniak, 2000; Collins, 1999; Henderson, 2003).</S>
    <S sid="19" ssid="17">In a statistical parser, the ranking of intermediate structures occurs naturally and based on empirical grounds, while most rule-based systems rely on ad hoc heuristics.</S>
    <S sid="20" ssid="18">With an aggressive beam for parse-time pruning (so in our parser), real-world parsing time can be reduced to near-linear.</S>
    <S sid="21" ssid="19">If one were to assume a constantly full fixed beam, or uses an oracle (Nivre, 2004) it is linear in practice3.</S>
    <S sid="22" ssid="20">Also worst-case complexity for exhaustive parsing is low, as these parsers are CFGbased (Eisner, 2000)4.</S>
    <S sid="23" ssid="21">But they typically produce CFG constituency data as output, trees that do not express long-distance dependencies.</S>
    <S sid="24" ssid="22">Although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in Treebanks such as the Penn Treebank (Marcus et al., 1993), most statistical Treebank trained parsers fully or largely ignore them5, which entails two problems: first, the training cannot profit from valuable annotation data.</S>
    <S sid="25" ssid="23">Second, the extraction of long-distance dependencies (LDD) and the mapping to shallow semantic representations is not always possible from the output of these parsers.</S>
    <S sid="26" ssid="24">This limitation is aggravated by a lack of co-indexation information and parsing errors across an LDD.</S>
    <S sid="27" ssid="25">In fact, some syntactic relations cannot be recovered on configurational grounds only.</S>
    <S sid="28" ssid="26">For these reasons, (Johnson, 2002) refers to them as &amp;quot;half-grammars&amp;quot;.</S>
    <S sid="29" ssid="27">An approach that relies heavily on DG characteristics is explored in this paper.</S>
    <S sid="30" ssid="28">It uses a hand-written DG grammar and a lexicalized probability model.</S>
    <S sid="31" ssid="29">It combines the low complexity of a CFG parser, the pruning and ranking advantages of statistical parsers and the ability to express the majority of LDDs of Formal Grammars.</S>
    <S sid="32" ssid="30">After presenting the DG benefits, we define our DG and introduce our statistical model.</S>
    <S sid="33" ssid="31">Then, we give an evaluation.</S>
    <S sid="34" ssid="32">2 The Benefit of DG Characteristics In addition to some obvious benefits, such as the integration of chunking and parsing (Abney, 1995), where a chunk largely corresponds to a nucleus (Tesniere, 1959), or that in an endocentric theory projection can never fail, we present eight characteristics in more detail, which in their combination allow us to treat the majority of English long-distance dependencies (LDD) in our DG parser Pro3Gres in a context-fee way.</S>
    <S sid="35" ssid="33">5(Collins, 1999) Model 2 uses some of the functional labels, and Model 3 some long-distance dependencies The ten most frequent types of empty nodes cover more than 60,000 of the approximately 64,000 empty nodes of sections 2-21 of the Penn Treebank.</S>
    <S sid="36" ssid="34">Table 1, reproduced from (Johnson, 2002) [line numbers and counts from the whole Treebank added], gives an overview.</S>
    <S sid="37" ssid="35">The fact that traditional DG does not know empty nodes allows a DG parser to use the efficient 0(n3) CYK algorithm.</S>
    <S sid="38" ssid="36">Only content words can be nuclei in a traditional DG.</S>
    <S sid="39" ssid="37">This means that empty units, empty complementizers and empty relative pronouns [lines 4,5,9,10] pose no problem for DG as they are optional, non-head material.</S>
    <S sid="40" ssid="38">For example, a complementizer is an optional dependent of the subordinated verb.</S>
    <S sid="41" ssid="39">Moved clauses [line 6] are mostly PPs or clausal complements of verbs of utterance.</S>
    <S sid="42" ssid="40">Only verbs of utterance allow subject-verb inversion in affirmative clauses [line 8].</S>
    <S sid="43" ssid="41">Our hand-written grammar provides rules with appropriate restrictions for them, allowing an inversion of the &amp;quot;canonical&amp;quot; dependency direction under welldefined conditions, distinguishing between ordre lineaire (linear precedence(LP)) and ordre structural (immediate dominance(ID)).</S>
    <S sid="44" ssid="42">Fronted positions are available locally to the verb in a theory that does not posit a distinction between internal and external arguments.</S>
    <S sid="45" ssid="43">The fact that dependencies are often labeled is a main difference between DG and constituency.</S>
    <S sid="46" ssid="44">We exploit this by using dedicated labels to model a range of constituency LDDs, relations spanning several constituency levels, including empty nodes and functional Penn Treebank labels, by a purely local DG relation6.</S>
    <S sid="47" ssid="45">The selective mapping patterns for MLE counts of passive subjects and control subjects from the Penn Treebank, the most frequent NP traces [line 1], are e.g.</S>
    <S sid="48" ssid="46">(@ stands for arbitrary nestedness): Our approach employs finite-state approximations of long-distance dependencies, described in (Schneider, 2003) for DG and (Cahill et al., 2004) for Lexical Functional Grammar (LFG)It leaves empty nodes underspecified but largely recoverable.</S>
    <S sid="49" ssid="47">Table 2 gives an overview of important dependencies.</S>
    <S sid="50" ssid="48">While multistratal DGs exist and several dependency levels can be distinguished (Mel'cuk, 1988) we follow a conservative view close to the original (Tesniere, 1959), which basically parses directly for a simple LFG f-structure without needing a c-structure detour.</S>
    <S sid="51" ssid="49">6 I addition to taking less decisions due to the gained high-level shallowness, it is ensured that the lexical information that matters is available in one central place, allowing the parser to take one well-informed decision instead of several brittle decisions plagued by sparseness.</S>
    <S sid="52" ssid="50">Collapsing deeply nested structures into a single dependency relation is less complex but has a similar effect as selecting what goes in to the parse history in historybased approaches.</S>
    <S sid="53" ssid="51">DG theory often conceives of DG structures as graphs instead of trees (Hudson, 1984).</S>
    <S sid="54" ssid="52">A statistical lexicalized post-processing module in Pro3Gres transforms selected subtrees into graphs, e.g. in order to express control.</S>
    <S sid="55" ssid="53">Pro3Gres is currently being applied in a Question Answering system specifically targeted at technical domains (Rinaldi et al., 2004b).</S>
    <S sid="56" ssid="54">One of the main advantages of a DG parser such as Pro3Gres over other parsing approaches is that a mapping from the syntactic layer to a semantic layer (meaning representation) is partly simplified (Molly et al., 2000).</S>
    <S sid="57" ssid="55">The possible functional changes of a word called translations (Tesniere, 1959) are an exception to endocentricity.</S>
    <S sid="58" ssid="56">They are an important contribution to a traceless theory.</S>
    <S sid="59" ssid="57">Gerunds (after winning/VBG the race) or infinitives [line 2] may function as nouns, obviating the need for an empty subject.</S>
    <S sid="60" ssid="58">In nounless NPs such as the poor, adjectives function as nouns, obviating the need for an empty noun head.</S>
    <S sid="61" ssid="59">Participles may function as adjectives (Western industrialized/VBN countries), again obviating the need for an empty subject.</S>
  </SECTION>
  <SECTION title="3 The Statistical Dependency Model" number="2">
    <S sid="62" ssid="1">Most successful deep-linguistic Dependency Parsers (Lin, 1998; Tapanainen and Jarvinen, 1997) do not have a statistical base.</S>
    <S sid="63" ssid="2">But one DG advantage is precisely that it offers simple but powerful statistical Maximum Likelihood Estimation (MLE) models.</S>
    <S sid="64" ssid="3">We now define our DG and the probability model.</S>
    <S sid="65" ssid="4">The rules of a context-free, unlabeled DG are equivalent to binary-branching CFG rewrite rules in which the head and the mother node are isomorphic.</S>
    <S sid="66" ssid="5">When converting DG structures to CFG, the order of application of these rules is not necessarily known, but in a labeled DG, the set of rules can specify the order (Covington, 1994).</S>
    <S sid="67" ssid="6">Fig.</S>
    <S sid="68" ssid="7">1 shows such two structures, equivalent except for the absence of functional labels in CFG.</S>
    <S sid="69" ssid="8">Subj (but not PP) has been used in this example conversion to specify the application order, hence we get a repetition of the eat/V node, mirroring a traditional CFG S and VP distinction.</S>
    <S sid="70" ssid="9">In a binary CFG, any two constituents A and B which are adjacent during parsing are candiMany relations are only allowed towards one direction, the left/right factor is absent for them.</S>
    <S sid="71" ssid="10">Typical distances mainly depend on the relation.</S>
    <S sid="72" ssid="11">Objects usually immediately follow the verb, while a PP attached to the verb may easily follow only at the second or third position, after the object and other PPs etc.</S>
    <S sid="73" ssid="12">By application of the chain rule and assuming that distance is independent of the lexical heads we get: dates for the RHS of a rewrite rule.</S>
    <S sid="74" ssid="13">As terminal types we use word tags.</S>
    <S sid="75" ssid="14">In DG, one of these is isomorphic to the LHS, i.e. the head.</S>
    <S sid="76" ssid="15">This grammar is also a Bare Phrase Structure grammar known from Minimalism (Chomsky, 1995).</S>
    <S sid="77" ssid="16">Research on PCFG and PP-attachment has shown the importance of probabilizing on lexical heads (a and b). e.g. for the Verb-PP attachment relation pobj (following (Collins and Brooks, 1995) including the description noun7) The distance (measured in chunks) between a head and a dependent is a limiting factor for the probability of a dependency between them.</S>
    <S sid="78" ssid="17">We now explore Pro3Gres' main probability model by comparing it to (Collins, 1999), and an adaptation of it, (Dubey and Keller, 2003).</S>
    <S sid="79" ssid="18">We will first consider the non-generative Model 1 (Collins, 1999).</S>
    <S sid="80" ssid="19">Both (Collins, 1999) Model 1 and Pro3Gres are mainly dependency-based statistical parsers over heads of chunks, a close relation can thus be expected.</S>
    <S sid="81" ssid="20">The (Collins, 1999) Model 1 MLE estimation is: P(RI(a, atag), (b, btag), dist) &#65533;&#65533; Differences in comparison to (8) are: (Collins, 1999) Model 2 extends the parser to include a complement/adjunct distinction for NPs and subordinated clauses, and it includes a subcategorisation frame model.</S>
    <S sid="82" ssid="21">For the subcategorisation-dependent generation of dependencies in Model 2, first the probabilities of the possible subcat frames are calculated and the selected subcat frame is added as a condition.</S>
    <S sid="83" ssid="22">Once a subcategorized constituent has been found, it is removed from the subcat frame, ensuring that non-subcategorized constituents cannot be attached as complement, which is one of the two major function of a subcat frame.</S>
    <S sid="84" ssid="23">The other major function of a subcat frame is to find all the subcategorized constituents.</S>
    <S sid="85" ssid="24">In order to ensure this, the probability when a rewrite rule can stop expanding is calculated.</S>
    <S sid="86" ssid="25">Importantly, the probability of a rewrite rule with a non-empty subcat frame to stop expanding is low, the probability of a rewrite rule with an empty subcat frame to stop expanding is high.</S>
    <S sid="87" ssid="26">Pro3Gres includes a complement/adjunct distinction for NPs.</S>
    <S sid="88" ssid="27">The examples given in support of the subcategorisation frame model in (Collins, 1999) Model 2 are dealt with by the hand-written grammar in Pro3Gres.</S>
    <S sid="89" ssid="28">Every complement relation type, namely subj, obj, obj2, sentobj, can only occur once per verb, which ensures one of the two major functions of a subcat frame, that non-subcategorized constituents cannot be attached as complements.</S>
    <S sid="90" ssid="29">This amounts to keeping separate subcat frames for each relation type, where the selection of the appropriate frame and removing the found constituent coincide, which has the advantage of a reduced search space: no hypothesized, but unfound subcat frame elements need to be managed.</S>
    <S sid="91" ssid="30">As for the second major function of subcat frames &#8212; to ensure that if possible all subcategorized constituents are found &#8212; the same principle applies: selection of subcat frame and removing of found constituents coincide; lexical information on the verb argument candidate is available at frame selection time already.</S>
    <S sid="92" ssid="31">This implies that Collins Model 2 takes an unnecessary detour.</S>
    <S sid="93" ssid="32">As for the probability of stopping the expansion of a rule &#8212; since DG rules are always binary &#8212; it is always 0 before and 1 after the attachment.</S>
    <S sid="94" ssid="33">But what is needed in place of interrelations of constituents of the same rewrite rule is proper cooperation of the different subcat types.</S>
    <S sid="95" ssid="34">For example, the grammar rules only allow a noun to be obj2 once obj has been found, or a verb is required to have a subject unless it is non-finite or a participle, or all objects need to be closer to the verb than a subordinate clause.</S>
    <S sid="96" ssid="35">(Dubey and Keller, 2003) address the question whether models such as Collins also improve performance on freer word order languages, in their case German.</S>
    <S sid="97" ssid="36">German is considerably more inflectional which means that discarding functional information is more harmful, and which explains why the NEGRA annotation has been conceived to be quite flat (Skut et al., 1997).</S>
    <S sid="98" ssid="37">(Dubey and Keller, 2003) observe that models such as Collins when applied directly perform worse than an unlexicalized PCFG baseline.</S>
    <S sid="99" ssid="38">The fact that learning curves converge early indicates that this is not mainly a sparse data effect.</S>
    <S sid="100" ssid="39">They suggest a linguistically motivated change, which is shown to outperform the baseline.</S>
    <S sid="101" ssid="40">The (Collins, 1999) Model 2 rule generation model for P &#8212;&#65533; Lm...L1HR1...Rn, is Dubey &amp; Keller suggest the following change in order to respect the NEGRA flatness: Ph is left unchanged, but Pl and Pr are conditioned on the preceding sister instead of on the head: Their new model performs considerably better and also outperforms the unlexicalized baseline.</S>
    <S sid="102" ssid="41">The authors state that \[u]sing sister-head relationships is a way of counteracting the flatness of the grammar productions; it implicitly adds binary branching to the grammar.&amp;quot; (ibid.).</S>
    <S sid="103" ssid="42">DG is binary branching by definition; adding binary branching implicitly converts the CFG rules into an ad-hoc DG.</S>
    <S sid="104" ssid="43">Whether the combination ((Chomsky, 1995) merge) of two binary constituents directly projects to a &amp;quot;real&amp;quot; CFG rule LHS or an implicit intermediate constituent does not matter.</S>
    <S sid="105" ssid="44">Observations did not use the NEGRA GR labels has to arise when discussing a strongly inflectional language such as German.</S>
    <S sid="106" ssid="45">&#8226; The use of a generative model, calculating the probability of a rule and ultimately the probability of producing a sentence given the grammar only has theoretical advantages.</S>
    <S sid="107" ssid="46">For practical purposes, modeling parsetime decision probabilities is as valid.</S>
    <S sid="108" ssid="47">With these observations in mind, we can compare Pro3Gres to (Dubey and Keller, 2003).</S>
    <S sid="109" ssid="48">As for the Base-NP Model, Pro3Gres only respects the best tagging &amp; chunking result reported to it &#8212; a major source of errors (see section 4).</S>
    <S sid="110" ssid="49">In DG, projection (although not expansion) is deterministic.</S>
    <S sid="111" ssid="50">H and P are usually isomorphic, if not Tesniere-translations are rulebased.</S>
    <S sid="112" ssid="51">Since in DG, only lexical nodes are categories, P=t(P).</S>
    <S sid="113" ssid="52">Ph is thus l(h), the prior, we ignore it for maximizing.</S>
    <S sid="114" ssid="53">In analogy, also category (L/R) and their tags are identical.</S>
    <S sid="115" ssid="54">The revised formula is If a DG rule is head-right, P is Li or Ri, if it is head-left, P is Li_1 or Ri_1, respectively.</S>
    <S sid="116" ssid="55">&amp;quot;In primarily right-branching languages such as English or German (i-1) actually amounts to being the head in the majority of, but not all cases.</S>
    <S sid="117" ssid="56">In a more functional DG perspective such as the one taken in Pro3Gres, these languages turn out to be less right-branching, however, with prepositions or determiners analyzed as markers to the nominal head or complementizers or relative pronouns as markers to the verbal head of the subclause.</S>
    <S sid="118" ssid="57">Headedness and not direction matters.</S>
    <S sid="119" ssid="58">Li/Ri is replaced by Hi and L/Ri-1=i+1 by H'.</S>
    <S sid="120" ssid="59">H' is understood to be the DG dependent, although, as mentioned, H' could also be the DG head in this implicit ad-hoc DG.</S>
    <S sid="121" ssid="60">P(t(Hi)jt(Hi),t(H0i)) is a projection or attachment grammar model modeling the unlexicalized probability of t(H) and t(H') participating in a binary rule with t(H) as head &#8212; the merge probability in Bare Phrase Structure (Chomsky, 1995); an unlabeled version of (4).</S>
    <S sid="122" ssid="61">P(t(Hi),l(Hi)jt(Hi),t(H0i),l(H0i)) is a lexicalized version of the same projection or attachment grammar model; P(t(Hi),l(Hi)jt(Hi), t(H0i),l(H0i, d(i))) in addition conditions on the distance9.</S>
    <S sid="123" ssid="62">Pro3Gres expresses the unlexicalized rules by licensing grammar rules for relation R. Tags are not used in Pro3Gres' model, because semantic backoffs and tag-based licensing rules are used.</S>
    <S sid="124" ssid="63">The Pro3Gres main MLE estimation (8) (l(H) = a, l(H0) = b) differs from (10) by using labeled DG, and thus from the Dubey &amp; Keller Model by using a consistent functional DG.</S>
  </SECTION>
  <SECTION title="4 Evaluation" number="3">
    <S sid="125" ssid="1">(Lin, 1995; Carroll et al., 1999) suggest evaluating on the linguistically meaningful level of dependency relations.</S>
    <S sid="126" ssid="2">Two such evaluations are reported now.</S>
    <S sid="127" ssid="3">First, a general-purpose evaluation using a hand-compiled gold standard corpus (Carroll et al., 1999), which contains the grammatical relation data of 500 random sentences from the Susanne corpus.</S>
    <S sid="128" ssid="4">The performance (table 3), according to (Preiss, 2003), is similar to a large selection of statistical parsers and a grammatical relation finder.</S>
    <S sid="129" ssid="5">Relations involving LDDs form part of these relations.</S>
    <S sid="130" ssid="6">A selection of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the relative clause pronoun (RclSubjA).</S>
    <S sid="131" ssid="7">Secondly, to answer how the parser performs over domains markedly different to the training corpus, to test whether terminology is the key to a successful parsing system, and to assess the impact of chunking errors, the parser has been applied to the GENIA corpus (Kim et al., 2003), 2000 MEDLINE abstracts of more than 400,000 words describing the results of Biomedical research, which is annotated for multi-word terms and thus contains near-perfect chunking.</S>
    <S sid="132" ssid="8">100 random sentences from the GENIA corpus have been manually annotated and compared to the parser output (Rinaldi et al., 2004a).</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="4">
    <S sid="133" ssid="1">We have discussed how DG allows the expression of the majority of LDDs in a contextfree way and shown that DG allows for simple but powerful statistical models.</S>
    <S sid="134" ssid="2">An evaluation shows that the performance of its implementation is state-of-the-art10.</S>
    <S sid="135" ssid="3">Its parsing speed of about 300,000 words per hour is very good for a deep-linguistic parser and makes it fast enough for unlimited application.</S>
  </SECTION>
</PAPER>
