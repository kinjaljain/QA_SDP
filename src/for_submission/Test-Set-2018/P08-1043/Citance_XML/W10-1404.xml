<PAPER>
  <S sid="0">Application of Different Techniques to Dependency Parsing of Basque</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a set of experiments on dependency parsing of the Basque Dependency Treebank (BDT).</S>
    <S sid="2" ssid="2">The present work has examined several directions that try to explore the rich set of morphosyntactic features in the BDT: i) experimenting the impact of morphological features, ii) application of dependency tree transformations, iii) application of a two-stage parsing scheme (stacking), and iv) combinations of the individual experiments.</S>
    <S sid="3" ssid="3">All the tests were conducted using MaltParser (Vivre et al., 2007a), a freely available and state of the art dependency parser generator.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">This paper presents several experiments performed on dependency parsing of the Basque Dependency Treebank (BDT, Aduriz et al., 2003).</S>
    <S sid="5" ssid="2">Basque can be briefly described as a morphologically rich language with free constituent order of the main sentence elements with respect to the main verb.</S>
    <S sid="6" ssid="3">This work has been developed in the context of dependency parsing exemplified by the CoNLL Shared Task on Dependency Parsing in years 2006 and 2007 (Nivre et al., 2007b), where several systems competed analyzing data from a typologically varied range of 19 languages.</S>
    <S sid="7" ssid="4">The treebanks for all languages were standardized using a previously agreed CoNLL-X format (see Figure 1).</S>
    <S sid="8" ssid="5">An early version of the BDT (BDT I) was one of the evaluated treebanks, which will allow a comparison with our results.</S>
    <S sid="9" ssid="6">One of the conclusions of the CoNLL 2007 workshop (Nivre et al., 2007a) was that there is a class of languages, those that combine a relatively free word order with a high degree of inflection, that obtained the worst scores.</S>
    <S sid="10" ssid="7">This asks for the development of new methods and algorithms that will help to reach the parsing performance of the more studied languages, as English.</S>
    <S sid="11" ssid="8">In this work, we will take the opportunity of having a new fresh version of the BDT, (BDT II henceforth), which is the result of an extension (three times bigger than the original one), and its redesign (see section 3.2).</S>
    <S sid="12" ssid="9">Using MaltParser, a freely available and state of the art dependency parser for all the experiments (Nivre et al., 2007a), this paper will concentrate on the application of different techniques to the task of parsing this new treebank, with the objective of giving a snapshot that can show the expected gains of each technique, together with some of their combinations.</S>
    <S sid="13" ssid="10">Some of the techniques have already been evaluated with other languages/treebanks or BDT I, while others have been adapted or extended to deal with specific aspects of the Basque language or the Basque Treebank.</S>
    <S sid="14" ssid="11">We will test the following: (V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj = clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing., OBJ:3P: object in 3rd person pl, coord = coordination, auxmod = auxiliary, ncsubj = non-clausal subject, ncmod = non-clausal modifier). addition of new features to the input of the second-stage parser, in the form of morphosyntactic features propagated through the first parser&#8217;s dependency tree and also as the addition of contextual features (such as category or dependency relation of parent, grandparent, and descendants).</S>
  </SECTION>
  <SECTION title="2 Related work" number="2">
    <S sid="15" ssid="1">Until recently, many works on treebank parsing have been mostly dedicated to languages with poor morphology, as exemplified by the Penn English Treebank.</S>
    <S sid="16" ssid="2">As the availability of treebanks for typologically different languages has increased, there has been a growing interest towards research on extending the by now standard algorithms and methods to the new languages and treebanks (Tsarfaty et al., 2009).</S>
    <S sid="17" ssid="3">For example, Collins et al. (1999) adapted Collins&#8217; parser to Czech, a highlyinflected language.</S>
    <S sid="18" ssid="4">Cowan and Collins (2005) apply the same parser to Spanish, concluding that the inclusion of morphological information improves the analyzer.</S>
    <S sid="19" ssid="5">Eryi&#287;it et al. (2008) experiment the use of several types of morphosyntactic information in Turkish, showing how the richest the information improves precision.</S>
    <S sid="20" ssid="6">They also show that using morphemes as the unit of analysis (instead of words) gets better results, as a result of the agglutinative nature of Turkish, where each wordform contains several morphemes that can be individually relevant for parsing.</S>
    <S sid="21" ssid="7">Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach.</S>
    <S sid="22" ssid="8">This is in accord with our experiment of dividing words into morphemes and transforming the tree accordingly (see section 4.2).</S>
    <S sid="23" ssid="9">Since the early times of treebank-based parsing systems, a lot of effort has been devoted to aspects of preprocessing trees in order to improve the results (Collins, 1999).</S>
    <S sid="24" ssid="10">When applied to dependency parsing, several works (Nilsson et al., 2007; Bengoetxea and Gojenola, 2009a) have concentrated on modifying the structure of the dependency tree, changing its original shape.</S>
    <S sid="25" ssid="11">For example, Nilsson et al. (2007) present the application of pseudoprojective, verbal group and coordination transformations to several languages/treebanks using MaltParser, showing that they improve the results.</S>
    <S sid="26" ssid="12">Another interesting research direction has examined the application of a two-stage parser, where the second parser tries to improve upon the result of a first parser.</S>
    <S sid="27" ssid="13">For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers.</S>
    <S sid="28" ssid="14">This experiment can be seen as an instance of stacked learning, which was also tested on dependency parsing of several languages in (Martins et al., 2008) with significant improvements over the base parser.</S>
  </SECTION>
  <SECTION title="3 Resources" number="3">
    <S sid="29" ssid="1">This section will describe the main resources that have been used in the experiments.</S>
    <S sid="30" ssid="2">First, subsection 3.1 will describe the Basque Dependency Treebank, which has increased its size from 55,469 tokens in its original version to more than 150,000, while subsection 3.2 will present the main characteristics of MaltParser, a state of the art and datadriven dependency parser.</S>
    <S sid="31" ssid="3">Basque can be described as an agglutinative language that presents a high power to generate inflected word-forms, with free constituent order of sentence elements with respect to the main verb.</S>
    <S sid="32" ssid="4">The BDT can be considered a pure dependency treebank from its original design, due mainly to the syntactic characteristics of Basque. come that-has and go that-has tell did boy-the The boy told them that he has come and gone Figure 1 contains an example of a sentence (1), annotated in the CoNLL-X format.</S>
    <S sid="33" ssid="5">The text is organized in eight tab-separated columns: wordnumber, form, lemma, category, subcategory, morphological features, and the dependency relation (headword + dependency).</S>
    <S sid="34" ssid="6">The information in Figure 1 has been simplified due to space reasons, as typically the Features column will contain many morphosyntactic' features (case, number, type of subordinated sentence, ...), which are relevant for parsing.</S>
    <S sid="35" ssid="7">The first version of the Basque Dependency Treebank contained 55,469 tokens forming 3,700 sentences (Aduriz et al., 2003).</S>
    <S sid="36" ssid="8">This treebank was used as one of the evaluated treebanks in the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007b).</S>
    <S sid="37" ssid="9">Our work will make use of the second version of the BDT (BDT II), which is the consequence of a process of extension and redesign of the original requirements: 1 We will use the term morphosyntactic to name the set of features attached to each word-form, which by the agglutinative nature of Basque correspond to both morphology and syntax. minished from 2.9% in the original treebank to 1.3% in the new version.</S>
    <S sid="38" ssid="10">&#8226; The annotation follows a stand-off markup approach, inspired on TEI-P4 (Artola et al., 2005).</S>
    <S sid="39" ssid="11">There was a conversion process from a set of interconnected XML files to the CoNLL-X format of the present experiments.</S>
    <S sid="40" ssid="12">Although the different characteristics and size of the two treebank versions do not allow a strict comparison, our preliminary experiments showed that the results on both treebanks were similar regarding our main evaluation criterion (Labeled Attachment Score, or LAS).</S>
    <S sid="41" ssid="13">In the rest of the paper we will only use the new BDT II.</S>
    <S sid="42" ssid="14">MaltParser (Nivre et al. 2007a) is a state of the art dependency parser that has been successfully applied to typologically different languages and treebanks.</S>
    <S sid="43" ssid="15">While several variants of the base parser have been implemented, we will use one of its standard versions (MaltParser version 1.3).</S>
    <S sid="44" ssid="16">The parser obtains deterministically a dependency tree in linear-time in a single pass over the input using two main data structures: a stack of partially analyzed items and the remaining input sequence.</S>
    <S sid="45" ssid="17">To determine the best action at each parsing step, the parser uses history-based feature models and discriminative machine learning.</S>
    <S sid="46" ssid="18">In all the following experiments, we will make use of a SVM classifier.</S>
    <S sid="47" ssid="19">The specification of the configuration used for learning can in principle include any kind of column in Figure 1 (such as word-form, lemma, category, subcategory or morphological features), together with a feature function.</S>
    <S sid="48" ssid="20">This means that a learning model can be described as a series of (column, function) pairs, where column represents the name of a column in Figure 1, and function makes reference to the parser&#8217;s main data structures.</S>
    <S sid="49" ssid="21">For example, the two pairs (Word, Stack[0]), and (Word, Stack[1]) represent two features that correspond to the word-forms on top and next to top elements of the stack, respectively, while (POSTAG, Input[0]) represents the POS category of the first token in the remaining input sequence.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="50" ssid="1">The following subsections will present three types of techniques that will be tested with the aim of improving the results of the syntactic analyzer.</S>
    <S sid="51" ssid="2">Subsection 4.1 presents the process of fine-tuning the rich set of available morphosyntactic features.</S>
    <S sid="52" ssid="3">Then, 4.2 will describe the application of three types of tree transformations, while subsection 4.3 will examine the application of propagating syntactic features through a first-stage dependency tree, a process that can also be seen as an application of stacked learning, as tested in (Nivre and McDonald, 2008; Martins et al., 2008) The original CoNLL-X format uses 10 different columns (see Figure 12), grouping the full set of morphosyntactic features in a single column.</S>
    <S sid="53" ssid="4">We will experiment the effect of individual features, following two steps: &#8226; First, we tested the effect of incorporating each individual lexical feature, concluding that there were two features that individually gave significant performance increases.</S>
    <S sid="54" ssid="5">They were syntactic case, which is relevant for marking a word&#8217;s syntactic function (or, equivalently, the type of dependency relation), and subordination type (REL henceforth).</S>
    <S sid="55" ssid="6">This REL feature appears in verb-ending morphemes that specify a type of subordinated sentence, such as in relative, completive, or indirect interrogative clauses.</S>
    <S sid="56" ssid="7">The feature is, therefore, relevant for establishing the main structure of a sentence, helping to delimit main and subordinated clauses, and it is also crucial for determining the dependency relation between the subordinated sentence and the main verb (head).</S>
    <S sid="57" ssid="8">&#8226; Then, we separated these features in two independent columns, grouping the remaining features under the Features column.</S>
    <S sid="58" ssid="9">This way, Maltparser&#8217;s learning specification can be more fine-grained, in terms of three morphosyntactic feature sets (CASE, REL and the rest, see Table 2).</S>
    <S sid="59" ssid="10">This will allow us testing learning models with different configurations for each column, instead of treating the full set of features as a whole.</S>
    <S sid="60" ssid="11">So, we will have the possibility of experimenting with richer contexts (that is, advancing the Stack and/or Input3 functions for each feature).</S>
    <S sid="61" ssid="12">Tree transformations have long been applied with the objective of improving parsing results (Collins, 1999; Nilsson et al., 2007).</S>
    <S sid="62" ssid="13">The general process consists of the following steps: pendent transformation already tested in several languages (Nivre and Nilsson, 2005).</S>
    <S sid="63" ssid="14">This transformation is totally language independent, and can be considered a standard transformation.</S>
    <S sid="64" ssid="15">Its performance on the first version of BDT had been already tested (Hall et al., 2007), giving significant improvements This is in accordance with BDT I having a 2.9% of non-projective arcs.</S>
    <S sid="65" ssid="16">&#8226; Coordination (TC).</S>
    <S sid="66" ssid="17">The transformation on coordinated sentences can be considered general (Nilsson et al., 2007) but it is also language dependent, as it depends on the specific configurations present in each language, mainly the set of coordination conjunctions and the types of elements that can be coordinated, together with their morphosyntactic properties (such as head initial or final).</S>
    <S sid="67" ssid="18">Coordination in BDT (both versions) is annotated in the so called Prague Style (PS, see Figure 2), where the conjunction is taken as the head, and the conjuncts depend on it.</S>
    <S sid="68" ssid="19">Nilsson et al. (2007) advocate the Mel&#180;cuk style (MS) for parsing Czech, taking the first conjunct as the head, and creating a chain where each element depends on the preceding one.</S>
    <S sid="69" ssid="20">Basque is a head final language, where many important syntactic features, like case or subordinating morphemes are located at the end of constituents.</S>
    <S sid="70" ssid="21">For that reason, Bengoetxea and Gojenola (2009a) proposed MS-sym, a symmetric variation of MS in which the coordinated elements will be dependents of the last conjunct (which will be the head, see Figure 2).</S>
    <S sid="71" ssid="22">&#8226; Transformation of subordinated sentences (TS).</S>
    <S sid="72" ssid="23">They are formed in Basque by attaching the corresponding morphemes to the auxiliary verbs.</S>
    <S sid="73" ssid="24">However, in BDT (I and II) the verbal elements are organized around the main verb (semantic head) while the syntactic head corresponds to the subordination morpheme, which appears usually attached to the auxiliary.</S>
    <S sid="74" ssid="25">Its main consequence is that the elements bearing the relevant information for parsing are situated far in the tree with respect to their head.</S>
    <S sid="75" ssid="26">In Figure 3, we see that the morpheme &#8211;la, indicating a subordinated completive sentence, appears down in the tree, and this could affect the correct attachment to the main verb (esan).</S>
    <S sid="76" ssid="27">Figure 4 shows the effect of transforming the original tree in Figure 3.</S>
    <S sid="77" ssid="28">The subordination morpheme (-la) is separated from the auxiliary verb (da), and is &#8220;promoted&#8221; as the syntactic head of the subordinated sentence.</S>
    <S sid="78" ssid="29">New arcs are created from the main verb (etorri) to the morpheme (which is now the head), and also a new dependency relation (SUB).</S>
    <S sid="79" ssid="30">Overall, the projectivization transformation (TP) is totally language-independent.</S>
    <S sid="80" ssid="31">TC (coordination) can be considered in the middle, as it depends on the general characteristics of the language.</S>
    <S sid="81" ssid="32">Finally, the transformation of subordinated sentences (TS) is specific to the treebank and intrinsecally linked to the agglutinative nature of Basque.</S>
    <S sid="82" ssid="33">Bengoetxea and Gojenola (2009a) also found that the order of transformations can be relevant.</S>
    <S sid="83" ssid="34">Their best system, after applying all the transformations, obtained a 76.80% LAS on BDT I (2.24% improvement over a baseline of 74.52%) on the test set.</S>
    <S sid="84" ssid="35">We include these already evaluated transformations in the present work with two objectives in mind: Bengoetxea and Gojenola (2009b) tested the effect of propagating several morphosyntactic feature values after a first parsing phase, as in classical unification-based grammars, as a means of propagating linguistic information through syntax trees.</S>
    <S sid="85" ssid="36">They applied three types of feature propagation of the morphological feature values: a) from auxiliary verbs to the main verb (verb groups) b) propagation of case and number from post-modifiers to the head noun (noun phrases) c) from the last conjunct to the conjunction (coordination).</S>
    <S sid="86" ssid="37">This was done mainly because Basque is head final, and relevant features are located at the end of constituents.</S>
    <S sid="87" ssid="38">Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first.</S>
    <S sid="88" ssid="39">Martins et al. (2008) specify the following steps: on D&#8722;l = D \ Dl.</S>
    <S sid="89" ssid="40">Then use gl to output predictions for the (unseen) partition Dl.</S>
    <S sid="90" ssid="41">At the end, we have an augmented dataset D` = D + new set of stacked/propagated features.</S>
    <S sid="91" ssid="42">In our tests, it was enough with two partitions (L = 2), as experiments with L &gt; 2 did not give any significant improvement.</S>
    <S sid="92" ssid="43">Figure 5 shows the types of information that can be added to each target element.</S>
    <S sid="93" ssid="44">The token X can take several kinds of information from its children (A and B) or his parent (H).</S>
    <S sid="94" ssid="45">The information that is propagated can vary, including part of speech, morphosyntactic features or the dependency relations between X and its children/parent.</S>
    <S sid="95" ssid="46">We can roughly classify the stacked features in two different sets: different dependency tree configurations (see Figure 5), similarly to (Nivre and McDonald, 2008; Martins et al., 2008).</S>
    <S sid="96" ssid="47">Among them, we will test the inclusion of several features (dependency relation, category and morphosyntactic features) from the following: parent, grandparent, siblings, and children.</S>
    <S sid="97" ssid="48">In the present work, we have devised the following experiments: contrast to (Bengoetxea and Gojenola, 2009b), who used the enriched gold data as D` directly, we will test Martins et al.&#8217;s proposal, in which the level 1 parser will be able to learn on the errors of the level 0 parser.</S>
    <S sid="98" ssid="49">&#8226; We will extend these experiments with the use of different parser features (Nivre and McDonald, 2008; Martins et al., 2008).</S>
    <S sid="99" ssid="50">Finally, we will combine the different techniques.</S>
    <S sid="100" ssid="51">An important point is to determine whether the techniques are independent (and accumulative) or it could also be that they can serve as alternative treatments to deal with the same phenomena.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="101" ssid="1">BDT I was used at the CoNLL 2007 Shared Task, where many systems competed on it (Nivre et al., 2007b).</S>
    <S sid="102" ssid="2">We will use Labeled Attachment Score (LAS) as the evaluation measure: the percentage of correct arcs (both dependency relation and head) over all arcs, with respect to the gold standard.</S>
    <S sid="103" ssid="3">Table 1 shows the best CoNLL 2007 results on BDT I.</S>
    <S sid="104" ssid="4">The best system obtained a score of 76.94%, combining six variants of MaltParser, and competing with 19 systems.</S>
    <S sid="105" ssid="5">Carreras (2007) and Titov and Henderson (2007) obtained the second and third positions, respectively.</S>
    <S sid="106" ssid="6">We consider the last two lines in Table 1 as our baselines, which consist in applying a single MaltParser version (Hall et al., 2007), that obtained the fifth position at CoNLL 2007.</S>
    <S sid="107" ssid="7">Although Hall et al. (2007) applied the projectivization transformation (TP), we will not use it in our baseline because we want to evaluate the effect of multiple techniques over a base parser.</S>
    <S sid="108" ssid="8">Although we could not use the subset of BDT II corresponding to BDT I, we run4 a test with a set of sentences the size of BDT I.</S>
    <S sid="109" ssid="9">As could be expected, the three-fold increase in the new treebank gives a 2.35% improvement over BDT I.</S>
    <S sid="110" ssid="10">For evaluation, we divided the treebank in three sets, corresponding to training, development, and test (80%, 10%, and 10%, respectively).</S>
    <S sid="111" ssid="11">All the experiments were done on the development set, leaving the best systems for the final test.</S>
    <S sid="112" ssid="12">Table 3 shows the results for the basic systems employing each of the techniques advanced in Section 4.</S>
    <S sid="113" ssid="13">As a first result, we see that a new step of reengineering MaltParser&#8217;s learning configuration was rewarding (see row 2 in Table 3), as morphosyntactic features were more finely specified with respect to the most relevant features.</S>
    <S sid="114" ssid="14">Table 2 presents the baseline and the best learning models.</S>
    <S sid="115" ssid="15">We see that advancing the input lookahead for CASE and REL gives an increase of 0.82 points.</S>
    <S sid="116" ssid="16">Looking at the transformations (rows 3 to 7), the new Treebank BDT II obtains results similar to those described in (Bengoetxea and Gojenola, 2009a).</S>
    <S sid="117" ssid="17">As could be expected from the reduction of non-projective arcs (from 2.9% to 1.3%), the gains of TP are proportionally lower than in BDT I.</S>
    <S sid="118" ssid="18">Also, we can observe that Ts alone worsens the baseline, but it gives the best results when combined with the rest (rows 6 and 7).</S>
    <S sid="119" ssid="19">This can be explained because Ts creates new non-projective arcs, so it is effective only if TP is applied later.</S>
    <S sid="120" ssid="20">The transformation on coordination (TC) alone does not get better results, but when combined with TP and Ts gives the best results.</S>
    <S sid="121" ssid="21">Applying feature propagation and stacking (see rows 9-17), we can see that most of the individual techniques (rows 9-14) give improvements over the baseline.</S>
    <S sid="122" ssid="22">When combining what we defined as 5 This experiment was possible due to the fact that MaltParser&#8217;s functionality was extended, allowing the specification of new columns/features, as the first versions of MaltParser only permitted a single column that included all the features. linguistic features (those morphosyntactic features propagated by the application of three linguistic principles), we can see that their combination seems accumulative (row 15).</S>
    <S sid="123" ssid="23">The parser features also give a significant improvement individually (rows 12-14), but, when combined either among themselves (row 16) or with the linguistic features (row 17), their effect does not seem to be additive.</S>
    <S sid="124" ssid="24">After getting significant improvements on the individual techniques and some of their combinations, we took a further step to integrate different techniques.</S>
    <S sid="125" ssid="25">An important aspect that must be taken into account is that the combination is not trivial all the times.</S>
    <S sid="126" ssid="26">For example, we have seen (section 5.1) that combinations of the three kinds of tree transformations must be defined having in mind the possible side-effects of any previous transformation.</S>
    <S sid="127" ssid="27">When combining different techniques, care must be taken to avoid any incompatibility.</S>
    <S sid="128" ssid="28">For that reason we only tested some possibilities.</S>
    <S sid="129" ssid="29">Rows 18-21 show some of the combined experiments.</S>
    <S sid="130" ssid="30">Combination of feature optimization with the pseudoprojective transformation yields an accumulative improvement (row 18).</S>
    <S sid="131" ssid="31">However, the combination of all the tree transformations with FO (row 19) does not accumulate.</S>
    <S sid="132" ssid="32">This can be due to the fact that feature optimization already cancelled the effect of the transformation on coordination and subordinated sentences, or otherwise it could also need a better exploration of their interleaved effect.</S>
    <S sid="133" ssid="33">Finally, row 21 shows that feature optimization, the pseudoprojective transformation and feature propagation are also accumulative, giving the best results.</S>
    <S sid="134" ssid="34">The relations among the rest of the transformations deserve future examination, as the results do not allow us to extract a precise conclusion.</S>
  </SECTION>
  <SECTION title="6 Conclusions and future work" number="6">
    <S sid="135" ssid="1">We studied several proposals for improving a baseline system for parsing the Basque Treebank.</S>
    <S sid="136" ssid="2">All the results were evaluated on the new version, BDT II, three times larger than the previous one.</S>
    <S sid="137" ssid="3">We have obtained the following main results: &#8226; Using rich morphological features.</S>
    <S sid="138" ssid="4">We have extended previous works, giving a finer grained description of morphosyntactic features on the learner&#8217;s configuration, (FO: feature optimization; TP TC TS: Pseudo-projective, Coordination and Subordinated sentence transformations; SVG, SNP, SC: Stacking (feature passing) on Verb Groups, NPs and Coordination; SP, SCH, SGP: Stacking (category, features and dependency) on Parent, CHildren and GrandParent; *: statistically significant in McNemar's test, p &lt; 0.005; **: statistically significant, p &lt; 0.001) showing that it can significantly improve the results.</S>
    <S sid="139" ssid="5">In particular, differentiating case and the type of subordinated sentence gives the best LAS increase (+0.82%). the effect of a stacked learning scheme.</S>
    <S sid="140" ssid="6">Some of the stacked features were languageindependent, as in (Nivre and McDonald.</S>
    <S sid="141" ssid="7">2008), but we have also applied a generalization of the stacking mechanism to a morphologically rich language, as some of the stacked features are morphosyntactic features (such as case and number) which were propagated through a first stage dependency tree by the application of linguistic principles (noun phrases, verb groups and coordination). with respect to the individual systems, some others do not give a improvement over the basic systems.</S>
    <S sid="142" ssid="8">A careful study must be conducted to investigate whether the approaches are exclusive or complementary.</S>
    <S sid="143" ssid="9">For example, the transformation on subordinated sentences and feature propagation on verbal groups seem to be attacking the same problem, i. e., the relations between main and subordinated sentences.</S>
    <S sid="144" ssid="10">In this respect, they can be viewed as alternative approaches to dealing with these phenomena.</S>
    <S sid="145" ssid="11">The results show that the application of these techniques can give noticeable results, getting an overall improvement of 1.90% (from 77.08% until 78.98%), which can be roughly comparable to the effect of doubling the size of the treebank (see the last two lines of Table 1).</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="146" ssid="1">This research was supported by the Department of Industry of the Basque Government (IE09-262) and the University of the Basque Country (GIU09/19).</S>
    <S sid="147" ssid="2">Thanks to Joakim Nivre and his team for their support using Maltparser and his fruitful suggestion about the use of stacked features.</S>
  </SECTION>
</PAPER>
