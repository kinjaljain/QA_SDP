<PAPER>
	<S sid="0">Better Arabic Parsing: Baselines Evaluations and Analysis</S><ABSTRACT>
		<S sid="1" ssid="1">In this paper, we offer broad insightinto the underperformance of Arabic constituency parsing by analyzing the inter play of linguistic phenomena, annotationchoices, and model design.</S>
		<S sid="2" ssid="2">First, we identify sources of syntactic ambiguity under studied in the existing parsing literature.</S>
		<S sid="3" ssid="3">Second, we show that although the PennArabic Treebank is similar to other tree banks in gross statistical terms, annotation consistency remains problematic.</S>
		<S sid="4" ssid="4">Third,we develop a human interpretable grammar that is competitive with a latent vari able PCFG.</S>
		<S sid="5" ssid="5">Fourth, we show how to build better models for three different parsers.Finally, we show that in application set tings, the absence of gold segmentation lowers parsing performance by 2?5% F1.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">It is well-known that constituency parsing mod els designed for English often do not generalizeeasily to other languages and treebanks.1 Expla nations for this phenomenon have included the relative informativeness of lexicalization (Dubeyand Keller, 2003; Arun and Keller, 2005), insensi tivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima?an, 2008), and the effect ofvariable word order (Collins et al, 1999).</S>
			<S sid="7" ssid="7">Certainly these linguistic factors increase the diffi culty of syntactic disambiguation.</S>
			<S sid="8" ssid="8">Less frequentlystudied is the interplay among language, annota tion choices, and parsing model design (Levy and Manning, 2003; Ku?bler, 2005).1The apparent difficulty of adapting constituency mod els to non-configurational languages has been one motivation for dependency representations (Hajic?</S>
			<S sid="9" ssid="9">and Zema?nek, 2004; Habash and Roth, 2009).</S>
			<S sid="10" ssid="10">To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply ?Arabic?)</S>
			<S sid="11" ssid="11">because of the unusual opportunity it presents for comparison to English parsing results.</S>
			<S sid="12" ssid="12">The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al, 2004) werepurposefully borrowed without major modifica tion from English (Marcus et al, 1993).</S>
			<S sid="13" ssid="13">Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.</S>
			<S sid="14" ssid="14">ButArabic contains a variety of linguistic phenom ena unseen in English.</S>
			<S sid="15" ssid="15">Crucially, the conventional orthographic form of MSA text is unvocalized, aproperty that results in a deficient graphical rep resentation.</S>
			<S sid="16" ssid="16">For humans, this characteristic canimpede the acquisition of literacy.</S>
			<S sid="17" ssid="17">How do addi tional ambiguities caused by devocalization affect statistical learning?</S>
			<S sid="18" ssid="18">How should the absence of vowels and syntactic markers influence annotation choices and grammar development?</S>
			<S sid="19" ssid="19">Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering.Our analysis begins with a description of syn tactic ambiguity in unvocalized MSA text (?2).Next we show that the ATB is similar to other tree banks in gross statistical terms, but that annotation consistency remains low relative to English (?3).</S>
			<S sid="20" ssid="20">We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (?4).</S>
			<S sid="21" ssid="21">To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation isassumed (?5).</S>
			<S sid="22" ssid="22">Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (?6).</S>
			<S sid="23" ssid="23">Wequantify error categories in both evaluation set tings.</S>
			<S sid="24" ssid="24">To our knowledge, ours is the first analysis of this kind for Arabic parsing.</S>
			<S sid="25" ssid="25">394</S>
	</SECTION>
	<SECTION title="Syntactic Ambiguity in Arabic. " number="2">
			<S sid="26" ssid="1">Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages.</S>
			<S sid="27" ssid="2">The basic word order is VSO, butSVO, VOS, and VO configurations are also pos sible.2 Nouns and verbs are created by selectinga consonantal root (usually triliteral or quadrilit eral), which bears the semantic core, and adding affixes and diacritics.</S>
			<S sid="28" ssid="3">Particles are uninflected.</S>
			<S sid="29" ssid="4">Diacritics can also be used to specify grammatical relations such as case and gender.</S>
			<S sid="30" ssid="5">But diacritics are not present in unvocalized text, which is the standard form of, e.g., news media documents.3 Let us consider an example of ambiguity caused by devocalization.</S>
			<S sid="31" ssid="6">Table 1 shows four wordswhose unvocalized surface forms ? an are indistinguishable.</S>
			<S sid="32" ssid="7">Whereas Arabic linguistic theory as signs (1) and (2) to the class of pseudo verbs ? Ahw ? inna and her sisters since they can beinflected, the ATB conventions treat (2) as a com plementizer, which means that it must be the head of SBAR.</S>
			<S sid="33" ssid="8">Because these two words have identicalcomplements, syntax rules are typically unhelpful for distinguishing between them.</S>
			<S sid="34" ssid="9">This is es pecially true in the case of quotations?which are common in the ATB?where (1) will follow a verb like (2) (Figure 1).Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues.</S>
			<S sid="35" ssid="10">Two common cases are the attribu tive adjective and the process nominal CdOm?maSdar, which can have a verbal reading.4 Attributive adjectives are hard because they are orthographically identical to nominals; they are in flected for gender, number, case, and definiteness.</S>
			<S sid="36" ssid="11">Moreover, they are used as substantives much 2Unlike machine translation, constituency parsing is not significantly affected by variable word order.</S>
			<S sid="37" ssid="12">However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al, 2009).In particular, the decision to represent arguments in verbinitial clauses as VP internal makes VSO and VOS configu rations difficult to distinguish.</S>
			<S sid="38" ssid="13">Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop).</S>
			<S sid="39" ssid="14">3Techniques for automatic vocalization have been studied (Zitouni et al, 2006; Habash and Rambow, 2007).</S>
			<S sid="40" ssid="15">However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance.</S>
			<S sid="41" ssid="16">4Traditional Arabic linguistic theory treats both of these types as subcategories of noun ?F?.</S>
			<S sid="42" ssid="17">Word Head Of Complement POS 1 ?? inna ?Indeed, truly?</S>
			<S sid="43" ssid="18">VP Noun VBP 2 ??</S>
			<S sid="44" ssid="19">anna ?That?</S>
			<S sid="45" ssid="20">SBAR Noun IN 3 ? in ?If?</S>
			<S sid="46" ssid="21">SBAR Verb IN 4 ? an ?to?</S>
			<S sid="47" ssid="22">SBAR Verb IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form ? an.</S>
			<S sid="48" ssid="23">The distinctions in the ATB are linguistically justified, but complicate parsing.</S>
			<S sid="49" ssid="24">Table 8a shows that the best model recovers SBAR at only 71.0% F1.</S>
			<S sid="50" ssid="25">VP VBD ?AR she added S VP PUNC ? VBP ? Indeed NP NN ?d} Saddam . . .</S>
			<S sid="51" ssid="26">(a) Reference VP VBD ?AR she added SBAR PUNC ? IN ? Indeed NP NN ?d} Saddam . . .</S>
			<S sid="52" ssid="27">(b) Stanford Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form ? an (Table 1).</S>
			<S sid="53" ssid="28">more frequently than is done in English.Process nominals name the action of the tran sitive or ditransitive verb from which they derive.</S>
			<S sid="54" ssid="29">The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case.</S>
			<S sid="55" ssid="30">When the maSdar lacksa determiner, the constituent as a whole resem bles the ubiquitous annexation construct T?AR? iDafa.</S>
			<S sid="56" ssid="31">Gabbard and Kulick (2008) show thatthere is significant attachment ambiguity associ ated with iDafa, which occurs in 84.3% of the trees in our development set.</S>
			<S sid="57" ssid="32">Figure 4 shows a constituent headed by a process nominal withan embedded adjective phrase.</S>
			<S sid="58" ssid="33">All three mod els evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly.For parsing, the most challenging form of am biguity occurs at the discourse level.</S>
			<S sid="59" ssid="34">A definingcharacteristic of MSA is the prevalence of dis course markers to connect and subordinate words and phrases (Ryding, 2005).</S>
			<S sid="60" ssid="35">Instead of offsettingnew topics with punctuation, writers of MSA in sert connectives such as ? wa and ? fa to link new elements to both preceding clauses and the text as a whole.</S>
			<S sid="61" ssid="36">As a result, Arabic sentences are usually long relative to English, especially after 395 Length English (WSJ) Arabic (ATB) ? 20 41.9% 33.7% ? 40 92.4% 73.2% ? 63 99.7% 92.6% ? 70 99.9% 94.9% Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2?23) and the ATB (p1?3).</S>
			<S sid="62" ssid="37">English parsing evaluations usually report results on sentences up to length 40.</S>
			<S sid="63" ssid="38">Arabic sentences of up to length 63 would need to be.</S>
			<S sid="64" ssid="39">evaluated to account for the same fraction of the data.</S>
			<S sid="65" ssid="40">We propose a limit of 70 words for Arabic parsing evaluations.</S>
			<S sid="66" ssid="41">Part of Speech Tag Freq.</S>
			<S sid="67" ssid="42">wa ?and?</S>
			<S sid="68" ssid="43">conjunction CC 4256 preposition IN 6 abbreviation NN 6 ? fa ?so, then?</S>
			<S sid="69" ssid="44">conjunction CC 160 connective particle RP 67 abbreviation NN 22 response conditioning particle RP 11 subordinating conjunction IN 3Table 3: Dev set frequencies for the two most significant dis course markers in Arabic are skewed toward analysis as a conjunction.</S>
			<S sid="70" ssid="45">segmentation (Table 2).</S>
			<S sid="71" ssid="46">The ATB gives severaldifferent analyses to these words to indicate dif ferent types of coordination.</S>
			<S sid="72" ssid="47">But it conflates the coordinating and discourse separator functions of wa (?W`? ??)</S>
			<S sid="73" ssid="48">into one analysis: conjunction(Table 3).</S>
			<S sid="74" ssid="49">A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (Al-Batal, 1990).</S>
			<S sid="75" ssid="50">We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).</S>
	</SECTION>
	<SECTION title="Treebank Comparison. " number="3">
			<S sid="76" ssid="1">3.1 Gross Statistics.</S>
			<S sid="77" ssid="2">Linguistic intuitions like those in the previous sec tion inform language-specific annotation choices.The resulting structural differences between treebanks can account for relative differences in parsing performance.</S>
			<S sid="78" ssid="3">We compared the ATB5 to tree banks for Chinese (CTB6), German (Negra), andEnglish (WSJ) (Table 4).</S>
			<S sid="79" ssid="4">The ATB is disadvan taged by having fewer trees with longer average 5LDC A-E catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).</S>
			<S sid="80" ssid="5">We map the ATB morphological analyses to the shortened ?Bies?</S>
			<S sid="81" ssid="6">tags for all experiments.</S>
			<S sid="82" ssid="7">ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Depth (?</S>
			<S sid="83" ssid="8">/ ?2) 3.87 / 0.74 5.01 / 1.44 3.58 / 0.89 4.18 / 0.74 Breadth (?</S>
			<S sid="84" ssid="9">/ ?2) 14.6 / 7.31 10.2 / 4.44 7.50 / 4.56 12.1 / 4.65 Length (?</S>
			<S sid="85" ssid="10">/ ?2) 31.5 / 22.0 27.7 / 18.9 17.2 / 10.9 23.8 / 11.2 Constituents (?)</S>
			<S sid="86" ssid="11">32.8 32.5 8.29 19.6 ? Const.</S>
			<S sid="87" ssid="12">/ ? Length 1.04 1.18 0.482 0.820 Table 4: Gross statistics for several different treebanks.</S>
			<S sid="88" ssid="13">Test set OOV rate is computed using the following splits: ATB(Chiang et al, 2006); CTB6 (Huang and Harper, 2009); Ne gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test).</S>
			<S sid="89" ssid="14">yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (?</S>
			<S sid="90" ssid="15">Constituents / ? Length).</S>
			<S sid="91" ssid="16">Evalb, the standard parsing metric, isbiased toward such corpora (Sampson and Babar czy, 2003).</S>
			<S sid="92" ssid="17">Also surprising is the low test set OOVrate given the possibility of morphological varia tion in Arabic.</S>
			<S sid="93" ssid="18">In general, several gross corpusstatistics favor the ATB, so other factors must con tribute to parsing underperformance.</S>
			<S sid="94" ssid="19">3.2 Inter-annotator Agreement.</S>
			<S sid="95" ssid="20">Annotation consistency is important in any super vised learning task.</S>
			<S sid="96" ssid="21">In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al, 2008).</S>
			<S sid="97" ssid="22">To improve agreement during the revision process, a dual-blind evaluation was performed in which 10% of the data was annotated by independentteams.</S>
			<S sid="98" ssid="23">Maamouri et al (2008) reported agree ment between the teams (measured with Evalb) at 93.8% F1, the level of the CTB.</S>
			<S sid="99" ssid="24">But Rehbein and van Genabith (2007) showed that Evalb should not be used as an indication of real difference?</S>
			<S sid="100" ssid="25">or similarity?between treebanks.</S>
			<S sid="101" ssid="26">Instead, we extend the variation n-grammethod of Dickinson (2005) to compare annota tion error rates in the WSJ and ATB.</S>
			<S sid="102" ssid="27">For a corpus C, let M be the set of tuples ?n, l?, where n is an n-gram with bracketing label l. If any n appears 6Generative parsing performance is known to deterioratewith sentence length.</S>
			<S sid="103" ssid="28">As a result, Habash et al (2006) devel oped a technique for splitting and chunking long sentences.</S>
			<S sid="104" ssid="29">In application settings, this may be a profitable strategy.</S>
			<S sid="105" ssid="30">396 Corpus Sample Error % Trees Nuclei n-grams Type n-gram WSJ 2?23 43948 25041 746 12.0% 2.10% ATB 23449 20292 2100 37.0% 1.76%Table 5: Evaluation of 100 randomly sampled variation nuclei types.</S>
			<S sid="106" ssid="31">The samples from each corpus were indepen dently evaluated.</S>
			<S sid="107" ssid="32">The ATB has a much higher fraction of nuclei per tree, and a higher type-level error rate.</S>
			<S sid="108" ssid="33">in a corpus position without a bracketing label, then we also add ?n,NIL?</S>
			<S sid="109" ssid="34">to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C.Bracketing variation can result from either an notation errors or linguistic ambiguity.</S>
			<S sid="110" ssid="35">Human evaluation is one way to distinguish between thetwo cases.</S>
			<S sid="111" ssid="36">Following Dickinson (2005), we ran domly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error.</S>
			<S sid="112" ssid="37">The human evaluators werea non-native, fluent Arabic speaker (the first au thor) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus.</S>
			<S sid="113" ssid="38">The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and(1400, 4610) for the WSJ.</S>
			<S sid="114" ssid="39">The results clearly in dicate increased variation in the ATB relative to the WSJ, but care should be taken in assessing the magnitude of the difference.</S>
			<S sid="115" ssid="40">On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample.</S>
			<S sid="116" ssid="41">At the same time, the n-gram error rate is sensitive to samples with extreme n-gram counts.</S>
			<S sid="117" ssid="42">For example, one of the ATB samples was the determiner???</S>
			<S sid="118" ssid="43">dhalik?that.?</S>
			<S sid="119" ssid="44">The sample occurred in 1507 corpus po sitions, and we found that the annotations were consistent.</S>
			<S sid="120" ssid="45">If we remove this sample from the evaluation, then the ATB type-level error rises to only 37.4% while the n-gram error rate increases to 6.24%.</S>
			<S sid="121" ssid="46">The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions.7Unlike Dickinson (2005), we strip traces and only con sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing (e.g., unaries, base NPs).</S>
			<S sid="122" ssid="47">Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei.</S>
			<S sid="123" ssid="48">NP NN Tm?</S>
			<S sid="124" ssid="49">summit NP NNP ?rJ Sharm DTNNP %yK? Al-Sheikh (a) NP NN Tm?</S>
			<S sid="125" ssid="50">summit NP NNP ?rJ Sharm NP DTNNP %yK? Al-Sheikh (b) Figure 2: An ATB sample from the human evaluation.</S>
			<S sid="126" ssid="51">The ATB annotation guidelines specify that proper nouns shouldbe specified with a flat NP (a).</S>
			<S sid="127" ssid="52">But the city name Sharm Al Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).</S>
	</SECTION>
	<SECTION title="Grammar Development. " number="4">
			<S sid="128" ssid="1">We can use the preceding linguistic and annotation insights to build a manually annotated Ara bic grammar in the manner of Klein and Manning(2003).</S>
			<S sid="129" ssid="2">Manual annotation results in human interpretable grammars that can inform future tree bank annotation decisions.</S>
			<S sid="130" ssid="3">A simple lexicalizedPCFG with second order Markovization gives rel atively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).In our grammar, features are realized as annota tions to basic category labels.</S>
			<S sid="131" ssid="4">We start with noun features since written Arabic contains a very highproportion of NPs.</S>
			<S sid="132" ssid="5">genitiveMark indicates recur sive NPs with a indefinite nominal left daughterand an NP right daughter.</S>
			<S sid="133" ssid="6">This is the form of re cursive levels in iDafa constructs.</S>
			<S sid="134" ssid="7">We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008).</S>
			<S sid="135" ssid="8">For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).</S>
			<S sid="136" ssid="9">Base NPs are the other significant category of nominal phrases.</S>
			<S sid="137" ssid="10">markBaseNP indicates thesenon-recursive nominal phrases.</S>
			<S sid="138" ssid="11">This feature in cludes named entities, which the ATB marks with a flat NP node dominating an arbitrary number of NNP pre-terminal daughters (Figure 2).</S>
			<S sid="139" ssid="12">For verbs we add two features.</S>
			<S sid="140" ssid="13">First we mark any node that dominates (at any level) a verb 8We use head-finding rules specified by a native speaker of Arabic.</S>
			<S sid="141" ssid="14">This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.</S>
			<S sid="142" ssid="15">397 Feature States Tags F1 Indiv.</S>
			<S sid="143" ssid="16">?F1 ? 3208 33 76.86 ? recursiveNPHead 3287 38 77.46 +0.60 genitiveMark 3471 38 77.88 +0.42 splitPUNC 4221 47 77.98 +0.10 markContainsVerb 5766 47 79.16 +1.18 markBaseNP 6586 47 79.5 +0.34 markOneLevelIdafa 7202 47 79.83 +0.33 splitIN 7595 94 80.48 +0.65 containsSVO 9188 94 80.66 +0.18 splitCC 9492 124 80.87 +0.21 markFem 10049 141 80.95 +0.08Table 6: Incremental dev set results for the manually anno tated grammar (sentences of length ? 70).</S>
			<S sid="144" ssid="17">phrase (markContainsVerb).</S>
			<S sid="145" ssid="18">This feature has alinguistic justification.</S>
			<S sid="146" ssid="19">Historically, Arabic gram mar has identified two sentences types: those that begin with a nominal (TymF? Tlm)?), and thosethat begin with a verb (Tyl`f? Tlm)?).</S>
			<S sid="147" ssid="20">But for eign learners are often surprised by the verbless predications that are frequently used in Arabic.</S>
			<S sid="148" ssid="21">Although these are technically nominal, they havebecome known as ?equational?</S>
			<S sid="149" ssid="22">sentences.</S>
			<S sid="150" ssid="23">markContainsVerb is especially effective for distin guishing root S nodes of equational sentences.</S>
			<S sid="151" ssid="24">Wealso mark all nodes that dominate an SVO con figuration (containsSVO).</S>
			<S sid="152" ssid="25">In MSA, SVO usually appears in non-matrix clauses.Lexicalizing several POS tags improves performance.</S>
			<S sid="153" ssid="26">splitIN captures the verb/preposition id ioms that are widespread in Arabic.</S>
			<S sid="154" ssid="27">Although this feature helps, we encounter one consequence of variable word order.</S>
			<S sid="155" ssid="28">Unlike the WSJ corpus which has a high frequency of rules like VP ?VB PP, Arabic verb phrases usually have lexi calized intervening nodes (e.g., NP subjects and direct objects).</S>
			<S sid="156" ssid="29">For example, we might have VP?VB NP PP, where the NP is the subject.</S>
			<S sid="157" ssid="30">This annotation choice weakens splitIN.</S>
			<S sid="158" ssid="31">The ATB gives all punctuation a single tag.</S>
			<S sid="159" ssid="32">For parsing, this is a mistake, especially in the caseof interrogatives.</S>
			<S sid="160" ssid="33">splitPUNC restores the convention of the WSJ.</S>
			<S sid="161" ssid="34">We also mark all tags that dominate a word with the feminine ending ? taa mar buuTa (markFeminine).</S>
			<S sid="162" ssid="35">To differentiate between the coordinating anddiscourse separator functions of conjunctions (Ta ble 3), we mark each CC with the label of its right sister (splitCC).</S>
			<S sid="163" ssid="36">The intuition here is thatthe role of a discourse marker can usually be de termined by the category of the word that follows it.</S>
			<S sid="164" ssid="37">Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word.</S>
			<S sid="165" ssid="38">We create equivalence classes for verb, noun, and adjective POS categories.</S>
	</SECTION>
	<SECTION title="Standard Parsing Experiments. " number="5">
			<S sid="166" ssid="1">We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al, 2006) and Bikel (Bikel, 2004) parsers.</S>
			<S sid="167" ssid="2">All experiments use ATB parts 1?3 divided according to the canonical split suggested by Chiang et al (2006).</S>
			<S sid="168" ssid="3">Preprocessingthe raw trees improves parsing performance con siderably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.</S>
			<S sid="169" ssid="4">At the phrasal level, we remove all function tags and traces.</S>
			<S sid="170" ssid="5">We also collapse unary chains withidentical basic categories like NP ? NP.</S>
			<S sid="171" ssid="6">The pre terminal morphological analyses are mapped tothe shortened ?Bies?</S>
			<S sid="172" ssid="7">tags provided with the tree bank.</S>
			<S sid="173" ssid="8">Finally, we add ?DT?</S>
			<S sid="174" ssid="9">to the tags for definite nouns and adjectives (Kulick et al, 2006).</S>
			<S sid="175" ssid="10">The orthographic normalization strategy we useis simple.10 In addition to removing all diacritics, we strip instances of taTweel ??wW, collapse variants of alif  to bare alif,11 and map Arabic punctuation characters to their Latin equiva lents.</S>
			<S sid="176" ssid="11">We retain segmentation markers?which are consistent only in the vocalized section of the treebank?to differentiate between e.g. ??</S>
			<S sid="177" ssid="12">?they?</S>
			<S sid="178" ssid="13">and ??+ ?their.?</S>
			<S sid="179" ssid="14">Because we use the vocalized section, we must remove null pronoun markers.In Table 7 we give results for several evalua tion metrics.</S>
			<S sid="180" ssid="15">Evalb is a Java re-implementation of the standard labeled precision/recall metric.129Both the corpus split and pre-processing code are avail able at http://nlp.stanford.edu/projects/arabic.shtml.</S>
			<S sid="181" ssid="16">10Other orthographic normalization schemes have beensuggested for Arabic (Habash and Sadat, 2006), but we ob serve negligible parsing performance differences between these and the simple scheme used in this evaluation.</S>
			<S sid="182" ssid="17">11taTweel (?)</S>
			<S sid="183" ssid="18">is an elongation character used in Arabic script to justify text.</S>
			<S sid="184" ssid="19">It has no syntactic function.</S>
			<S sid="185" ssid="20">Variants of alif are inconsistently used in Arabic texts.</S>
			<S sid="186" ssid="21">For alif withhamza, normalization can be seen as another level of devo calization.</S>
			<S sid="187" ssid="22">12For English, our Evalb implementation is identical to the most recent reference (EVALB20080701).</S>
			<S sid="188" ssid="23">For Arabic we 398 Leaf Ancestor Evalb Tag Model System Length Corpus Sent Exact LP LR F1 % Stanford (v1.6.3) Baseline 70 0.791 0.825 358 80.37 79.36 79.86 95.58 all 0.773 0.818 358 78.92 77.72 78.32 95.49 GoldPOS 70 0.802 0.836 452 81.07 80.27 80.67 99.95 Bikel (v1.2) Baseline (Self-tag) 70 0.770 0.801 278 77.92 76.00 76.95 94.64 all 0.752 0.794 278 76.96 75.01 75.97 94.63 Baseline (Pre-tag) 70 0.771 0.804 295 78.35 76.72 77.52 95.68 all 0.752 0.796 295 77.31 75.64 76.47 95.68 GoldPOS 70 0.775 0.808 309 78.83 77.18 77.99 96.60 Berkeley (Sep. 09) (Petrov, 2009) all ? ?</S>
			<S sid="189" ssid="24">76.40 75.30 75.85 ? Baseline 70 0.809 0.839 335 82.32 81.63 81.97 95.07 all 0.796 0.834 336 81.43 80.73 81.08 95.02 GoldPOS 70 0.831 0.859 496 84.37 84.21 84.29 99.87 Table 7: Test set results.</S>
			<S sid="190" ssid="25">Maamouri et al (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length ? 40.</S>
			<S sid="191" ssid="26">The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them.</S>
			<S sid="192" ssid="27">We are unaware of prior results for the Stanford parser.</S>
			<S sid="193" ssid="28">75 80 85 5000 10000 15000 Berkeley Stanford Bikel training trees F1 Figure 3: Dev set learning curves for sentence lengths ? 70.</S>
			<S sid="194" ssid="29">All three curves remain steep at the maximum training set size of 18818 trees.</S>
			<S sid="195" ssid="30">The Leaf Ancestor metric measures the cost oftransforming guess trees to the reference (Sampson and Babarczy, 2003).</S>
			<S sid="196" ssid="31">It was developed in re sponse to the non-terminal/terminal bias of Evalb, but Clegg and Shepherd (2005) showed that it isalso a valuable diagnostic tool for trees with com plex deep structures such as those found in the ATB.</S>
			<S sid="197" ssid="32">For each terminal, the Leaf Ancestor metricextracts the shortest path to the root.</S>
			<S sid="198" ssid="33">It then computes a normalized Levenshtein edit distance be tween the extracted chain and the reference.</S>
			<S sid="199" ssid="34">The range of the score is between 0 and 1 (higher is better).</S>
			<S sid="200" ssid="35">We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB.</S>
			<S sid="201" ssid="36">Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation.</S>
			<S sid="202" ssid="37">with the number of exactly matching guess trees.</S>
			<S sid="203" ssid="38">5.1 Parsing Models.</S>
			<S sid="204" ssid="39">The Stanford parser includes both the manually annotated grammar (?4) and an Arabic unknown word model with the following lexical features: 1.</S>
			<S sid="205" ssid="40">Presence of the determiner ? Al. 2.</S>
			<S sid="206" ssid="41">Contains digits.</S>
			<S sid="207" ssid="42">3.</S>
			<S sid="208" ssid="43">Ends with the feminine affix ? p. 4.</S>
			<S sid="209" ssid="44">Various verbal (e.g., ?, 1) and adjectival.</S>
			<S sid="210" ssid="45">suffixes (e.g., T?)</S>
			<S sid="211" ssid="46">Other notable parameters are second order vertical Markovization and marking of unary rules.</S>
			<S sid="212" ssid="47">Modifying the Berkeley parser for Arabic is straightforward.</S>
			<S sid="213" ssid="48">After adding a ROOT node toall trees, we train a grammar using six split-and merge cycles and no Markovization.</S>
			<S sid="214" ssid="49">We use the default inference parameters.Because the Bikel parser has been parameter ized for Arabic by the LDC, we do not change thedefault model settings.</S>
			<S sid="215" ssid="50">However, when we pre tag the input?as is recommended for English?</S>
			<S sid="216" ssid="51">we notice a 0.57% F1 improvement.</S>
			<S sid="217" ssid="52">We use the log-linear tagger of Toutanova et al (2003), which gives 96.8% accuracy on the test set.</S>
			<S sid="218" ssid="53">5.2 Discussion.</S>
			<S sid="219" ssid="54">The Berkeley parser gives state-of-the-art performance for all metrics.</S>
			<S sid="220" ssid="55">Our baseline for all sentence lengths is 5.23% F1 higher than the best pre vious result.</S>
			<S sid="221" ssid="56">The difference is due to more careful 399 S-NOM VP VBG ?2A`tF restoring NP NP NN C?2 role NP PRP ? its ADJP DTJJ ?Anb? constructive DTJJ ??Af? effective (a) Reference NP NN ?2A`tF NP NP NN C?2 NP PRP ? ADJP DTJJ ?Anb? ADJP DTJJ ??Af? (b) Stanford NP NP NN ?2A`tF NP NP NN C?2 NP PRP ? ADJP DTJJ ?Anb? ADJP DTJJ ??Af? (c) Berkeley NP NN ?2A`tF NP NP NP NN C?2 NP PRP ? ADJP DTJJ ?Anb? ADJP DTJJ ??Af? (d) BikelFigure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmen tation).</S>
			<S sid="222" ssid="57">The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.</S>
			<S sid="223" ssid="58">Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993).</S>
			<S sid="224" ssid="59">In the ATB, ?2A`tF asta?adah is tagged 48 times as a noun and 9 times as verbal noun.</S>
			<S sid="225" ssid="60">Consequently, all three parsers prefer the nominal reading.</S>
			<S sid="226" ssid="61">Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.</S>
			<S sid="227" ssid="62">None of the models attach the attributive adjectives correctly.</S>
			<S sid="228" ssid="63">pre-processing.</S>
			<S sid="229" ssid="64">However, the learning curves in Figure 3 show that the Berkeley parser does notexceed our manual grammar by as wide a mar gin as has been shown for other languages (Petrov, 2009).</S>
			<S sid="230" ssid="65">Moreover, the Stanford parser achieves themost exact Leaf Ancestor matches and tagging ac curacy that is only 0.1% below the Bikel model, which uses pre-tagged input.</S>
			<S sid="231" ssid="66">In Figure 4 we show an example of variation between the parsing models.</S>
			<S sid="232" ssid="67">We include a list of per-category results for selected phrasal labels,POS tags, and dependencies in Table 8.</S>
			<S sid="233" ssid="68">The er rors shown are from the Berkeley parser output,but they are representative of the other two pars ing models.</S>
	</SECTION>
	<SECTION title="Joint Segmentation and Parsing. " number="6">
			<S sid="234" ssid="1">Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Ara bic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words.</S>
			<S sid="235" ssid="2">Since these are distinct syntactic units,they are typically segmented.</S>
			<S sid="236" ssid="3">The ATB segmen tation scheme is one of many alternatives.</S>
			<S sid="237" ssid="4">Until now, all evaluations of Arabic parsing?includingthe experiments in the previous section?have as sumed gold segmentation.</S>
			<S sid="238" ssid="5">But gold segmentationis not available in application settings, so a segmenter and parser are arranged in a pipeline.</S>
			<S sid="239" ssid="6">Seg mentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.</S>
			<S sid="240" ssid="7">Lattice parsing (Chappelier et al, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.</S>
			<S sid="241" ssid="8">Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty,2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.</S>
			<S sid="242" ssid="9">We ex tend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.</S>
			<S sid="243" ssid="10">To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al, 2009a).</S>
			<S sid="244" ssid="11">Formally, for a lexicon L and segments I ? L, O /?</S>
			<S sid="245" ssid="12">L, each word automaton accepts the language I?(O+ I)I?.</S>
			<S sid="246" ssid="13">Aside from adding a simplerule to correct alif deletion caused by the prepo sition ?, no other language-specific processing is performed.Our evaluation includes both weighted and un weighted lattices.</S>
			<S sid="247" ssid="14">We weight edges using aunigram language model estimated with GoodTuring smoothing.</S>
			<S sid="248" ssid="15">Despite their simplicity, unigram weights have been shown as an effective fea ture in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabicsegmenter, configured to replicate ATB segmen tation (Habash and Rambow, 2005).</S>
			<S sid="249" ssid="16">MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer.</S>
			<S sid="250" ssid="17">For each 13Of course, this weighting makes the PCFG an improper distribution.</S>
			<S sid="251" ssid="18">However, in practice, unknown word models also make the distribution improper.</S>
			<S sid="252" ssid="19">400 Label # gold F1 ADJP 1216 59.45 SBAR 2918 69.81 FRAG 254 72.87 VP 5507 78.83 S 6579 78.91 PP 7516 80.93 NP 34025 84.95 ADVP 1093 90.64 WHNP 787 96.00 (a) Major phrasal categories Tag # gold % Tag # gold % VBG 182 48.84 JJR 134 92.83 VN 163 60.37 DTNNS 1069 94.29 VBN 352 72.42 DTJJ 3361 95.07 DTNNP 932 83.48 NNP 4152 95.09 JJ 1516 86.09 NN 10336 95.23 ADJ NUM 277 88.93 DTNN 6736 95.78 VBP 2139 89.94 NOUN QUANT 352 98.16 RP 818 91.23 PRP 1366 98.24 NNS 907 91.75 CC 4076 98.92 DTJJR 78 92.41 IN 8676 99.07 VBD 2580 92.42 DT 525 99.81 (b) Major POS categories Parent Head Modifer Dir # gold F1 NP NP TAG R 946 0.54 S S S R 708 0.57 NP NP ADJP R 803 0.64 NP NP NP R 2907 0.66 NP NP SBAR R 1035 0.67 NP NP PP R 2713 0.67 VP TAG PP R 3230 0.80 NP NP TAG L 805 0.85 VP TAG SBAR R 772 0.86 S VP NP L 961 0.87 (c) Ten lowest scoring (Collins,2003)-style dependencies occur ring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths ? 70 (dev set, gold segmentation).</S>
			<S sid="253" ssid="20">(a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse.</S>
			<S sid="254" ssid="21">We showed in ?2 that lexical ambiguity explains the underperformance of these categories.</S>
			<S sid="255" ssid="22">(b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ).</S>
			<S sid="256" ssid="23">Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.</S>
			<S sid="257" ssid="24">(c) Coordination ambiguity is shown in dependency scores by e.g., ?S S S R?</S>
			<S sid="258" ssid="25">and ?NP NP NP R?.</S>
			<S sid="259" ssid="26">?NP NP PP R?</S>
			<S sid="260" ssid="27">and ?NP NP ADJP R?</S>
			<S sid="261" ssid="28">are both iDafa attachment.</S>
			<S sid="262" ssid="29">input token, the segmentation is then performed deterministically given the 1-best analysis.Since guess and gold trees may now have different yields, the question of evaluation is com plex.</S>
			<S sid="263" ssid="30">Cohen and Smith (2007) chose a metric like SParseval (Roark et al, 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric.</S>
			<S sid="264" ssid="31">But we follow the more direct adaptation of Evalb suggested by Tsarfaty(2006), who viewed exact segmentation as the ul timate goal.</S>
			<S sid="265" ssid="32">Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.</S>
			<S sid="266" ssid="33">Table 9 shows that MADA produces a highquality segmentation, and that the effect of cas cading segmentation errors on parsing is only 1.92% F1.</S>
			<S sid="267" ssid="34">However, MADA is language-specific and relies on manually constructed dictionaries.Conversely, the lattice parser requires no linguistic resources and produces segmentations of com parable quality.</S>
			<S sid="268" ssid="35">Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence.</S>
			<S sid="269" ssid="36">A cell in the bottomrow of the parse chart is required for each poten tial whitespace boundary.</S>
			<S sid="270" ssid="37">As we have said, parse quality decreases with sentence length.</S>
			<S sid="271" ssid="38">Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew.</S>
			<S sid="272" ssid="39">LP LR F1 Seg F1 Tag F1 Coverage STANFORD (Gold) 81.64 80.55 81.09 100.0 95.81 100.0% MADA ? ?</S>
			<S sid="273" ssid="40">97.67 ? 96.42% MADA+STANFORD 79.44 78.90 79.17 97.67 94.27 96.42% STANFORDJOINT 76.13 72.61 74.33 94.12 90.13 94.73% STANFORDJOINT+UNI 77.09 74.97 76.01 96.26 92.23 95.87%Table 9: Dev set results for sentences of length ? 70.</S>
			<S sid="274" ssid="41">Coverage indicates the fraction of hypotheses in which the char acter yield exactly matched the reference.</S>
			<S sid="275" ssid="42">Each model was able to produce hypotheses for all input sentences.</S>
			<S sid="276" ssid="43">In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="7">
			<S sid="277" ssid="1">By establishing significantly higher parsing baselines, we have shown that Arabic parsing perfor mance is not as poor as previously thought, butremains much lower than English.</S>
			<S sid="278" ssid="2">We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation er rors.</S>
			<S sid="279" ssid="3">With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.</S>
			<S sid="280" ssid="4">Our results suggest that current parsing models would benefit frombetter annotation consistency and enriched anno tation in certain syntactic configurations.</S>
			<S sid="281" ssid="5">Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work.</S>
			<S sid="282" ssid="6">We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions.</S>
			<S sid="283" ssid="7">The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship.</S>
			<S sid="284" ssid="8">This paper is based on work supported in part by DARPA through IBM.</S>
			<S sid="285" ssid="9">The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.</S>
			<S sid="286" ssid="10">401</S>
	</SECTION>
</PAPER>
