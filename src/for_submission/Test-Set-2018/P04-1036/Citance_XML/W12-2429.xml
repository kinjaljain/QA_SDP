<PAPER>
  <S sid="0">Scaling up WSD with Automatically Generated Examples</S>
  <ABSTRACT>
    <S sid="1" ssid="1">The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning.</S>
    <S sid="2" ssid="2">However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms.</S>
    <S sid="3" ssid="3">An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones.</S>
    <S sid="4" ssid="4">This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus.</S>
    <S sid="5" ssid="5">The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches).</S>
    <S sid="6" ssid="6">The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">The information contained in the biomedical literature that is available in electronic formats is useful for health professionals and researchers (Westbrook et al., 2005).</S>
    <S sid="8" ssid="2">The amount is so vast that it is difficult for researchers to identify information of interest without the assistance of automated tools (Krallinger and Valencia, 2005).</S>
    <S sid="9" ssid="3">However, processing these documents automatically is made difficult by the fact that they contain terms that are ambiguous.</S>
    <S sid="10" ssid="4">For example, &#8220;culture&#8221; can mean &#8220;laboratory procedure&#8221; (e.g.</S>
    <S sid="11" ssid="5">&#8220;In peripheral blood mononuclear cell culture&#8221;) or &#8220;anthropological culture&#8221; (e.g.</S>
    <S sid="12" ssid="6">&#8220;main accomplishments of introducing a quality management culture&#8221;).</S>
    <S sid="13" ssid="7">These lexical ambiguities are problematic for language understanding systems.</S>
    <S sid="14" ssid="8">Word sense disambiguation (WSD) is the process of automatically identifying the meanings of ambiguous terms.</S>
    <S sid="15" ssid="9">Some WSD systems for the biomedical domain are only able to disambiguate a small number of ambiguous terms (see Section 2).</S>
    <S sid="16" ssid="10">However, for WSD systems to be useful in applications they should be able to disambiguate all ambiguous terms.</S>
    <S sid="17" ssid="11">One way to create such a WSD system is to automatically create the labeled data that is used to train supervised WSD systems.</S>
    <S sid="18" ssid="12">Several approaches (Liu et al., 2002; Stevenson and Guo, 2010; JimenoYepes and Aronson, 2010) have used information from the UMLS Metathesaurus1 to create labeled training data that have successfully been used to create WSD systems.</S>
    <S sid="19" ssid="13">A key decision for any system that automatically generates labeled examples is the number of examples of each sense to create, known as the bias of the data set.</S>
    <S sid="20" ssid="14">It has been shown that the bias of a set of labeled examples affects the performance of the WSD system it is used to train (Mooney, 1996; Agirre and Mart&#180;&#305;nez, 2004b).</S>
    <S sid="21" ssid="15">Some of the previous approaches to generating labeled data relied on manually annotated examples to determine the bias of the data sets and were therefore not completely unsupervised.</S>
    <S sid="22" ssid="16">This paper describes the development of a large scale WSD system that is able to disambiguate all terms that are ambiguous in the UMLS Metathe- 2.1 Unsupervised WSD saurus.</S>
    <S sid="23" ssid="17">The system relies on labeled examples that Unsupervised WSD algorithms make use of inforare created using information from UMLS.</S>
    <S sid="24" ssid="18">Various mation from some knowledge source, rather than rebias options are explored, including ones that do not lying on training data.</S>
    <S sid="25" ssid="19">make use of information from manually labeled ex- Humphrey et al. (2006) describe an unsupervised amples, and thus we can create a completely unsu- system which uses semantic types in UMLS to dispervised system.</S>
    <S sid="26" ssid="20">Evaluation is carried out on two tinguish between the possible meanings of ambigustandard datasets (the NLM-WSD and MSH-WSD ous words.</S>
    <S sid="27" ssid="21">However, it cannot disambiguate becorpora).</S>
    <S sid="28" ssid="22">We find that WSD systems can be cre- tween senses with the same semantic type, i.e., it ated without using any information from manually is not possible for the system to recognise all sense labeled examples and that their performance is bet- distinctions. ter than a state-of-the-art unsupervised approach.</S>
    <S sid="29" ssid="23">The Personalised Page Rank (PPR) system The remainder of this paper is organized as fol- (Agirre et al., 2010; Jimeno-Yepes and Aronson, lows.</S>
    <S sid="30" ssid="24">Previous approaches to WSD in biomedical 2010) relies on a a graph-based algorithm similar documents are described in the next Section.</S>
    <S sid="31" ssid="25">Section to the Page Rank algorithm originally developed for 3 presents the methods used to identify bias in the use in search engines (Brin, 1998).</S>
    <S sid="32" ssid="26">It performs labeled examples and WSD system.</S>
    <S sid="33" ssid="27">Experiments in WSD by converting the UMLS Metathesaurus into which these approaches are compared are described a graph in which the possible meanings of ambiguin Section 4 and their results in Section 5. ous words are nodes and relations between them are 2 Background edges.</S>
    <S sid="34" ssid="28">Disambiguation is carried out by providing Many WSD systems for the biomedical domain are the algorithm with a list of senses that appear in the based on supervised learning (McInnes et al., 2007; text that is being disambiguated.</S>
    <S sid="35" ssid="29">This information is Xu et al., 2007; Stevenson et al., 2008; Yepes and then combined with the graph and a ranked list of the Aronson, 2011).</S>
    <S sid="36" ssid="30">These systems require labeled possible senses for each ambiguous word generated. training data, examples of an ambiguous term la- Unsupervised systems have the advantage of bebeled with the correct meaning.</S>
    <S sid="37" ssid="31">Some sets of labeled ing able to disambiguate all ambiguous terms.</S>
    <S sid="38" ssid="32">Howdata have been developed for the biomedical domain ever, the performance of unsupervised systems that (Weeber et al., 2001; Savova et al., 2008; Jimeno- have been developed for biomedical documents is Yepes et al., 2011).</S>
    <S sid="39" ssid="33">However, these data sets only lower than that of supervised ones. contain examples for a few hundred terms and can 2.2 Automatic Generation of Labeled Data only be used to develop WSD systems to identify Automatic generation of labeled data for WSD comthe meanings of those terms.</S>
    <S sid="40" ssid="34">The process of creat- bines the accuracy of supervised approaches with ing labeled examples is extremely time-consuming the ability of unsupervised approaches to disamand difficult (Artstein and Poesio, 2008), making it biguate all ambiguous terms.</S>
    <S sid="41" ssid="35">It was first suggested impractical to create labeled examples of all possible by Leacock et al. (1998).</S>
    <S sid="42" ssid="36">Their approach is based ambiguous terms found in biomedical documents. on the observation that some terms in a lexicon ocTwo alternative approaches have been explored to cur only once and, consequently, there is no doubt develop systems which are able to disambiguate all about their meaning.</S>
    <S sid="43" ssid="37">These are referred to as being ambiguous terms in biomedical documents.</S>
    <S sid="44" ssid="38">The first monosemous.</S>
    <S sid="45" ssid="39">Examples for each possible meaning makes use of unsupervised WSD algorithms (see of an ambiguous term are generated by identifying Section 2.1) and the second creates labeled data au- the closest monosemous term (the monosemous reltomatically and uses it to train a supervised WSD ative) in the lexicon and using examples of that term. system (see Section 2.2).</S>
    <S sid="46" ssid="40">Variants of the approach have been applied to the 232 biomedical domain using the UMLS Metathesaurus as the sense inventory.</S>
    <S sid="47" ssid="41">Liu et al. (2002) were the first to apply the monosemous relatives approach to biomedical WSD and use it to disambiguate a set of 35 abbreviations.</S>
    <S sid="48" ssid="42">They reported high precision but low recall, indicating that labeled examples could not be created for many of the abbreviations.</S>
    <S sid="49" ssid="43">Jimeno-Yepes and Aronson (2010) applied a similar approach and found that it performed better than a number of alternative approaches on a standard evaluation resource (the NLM-WSD corpus) but did not perform as well as supervised WSD.</S>
    <S sid="50" ssid="44">Stevenson and Guo (2010) compared two techniques for automatically creating labeled data, including the monosemous relatives approach.</S>
    <S sid="51" ssid="45">They found that the examples which were generated were as good as manually labeled examples when used to train a supervised WSD system.</S>
    <S sid="52" ssid="46">However, Stevenson and Guo (2010) relied on labeled data to determine the number of examples of each sense to create, and therefore the bias of the data set.</S>
    <S sid="53" ssid="47">Consequently their approach is not completely unsupervised since it could not be applied to ambiguous terms that do not have labeled training data available.</S>
  </SECTION>
  <SECTION title="3 Approach" number="2">
    <S sid="54" ssid="1">The WSD system is based on a supervised approach that has been adapted for the biomedical domain (Stevenson et al., 2008).</S>
    <S sid="55" ssid="2">The system was tested on the NLM-WSD corpus (see Section 4.1) and found to outperform alternative approaches.</S>
    <S sid="56" ssid="3">The system can exploit a wide range of features, including several types of linguistic information from the context of an ambiguous term, MeSH codes and Concept Unique Identifiers (CUIs) from the UMLS Metathesaurus.</S>
    <S sid="57" ssid="4">However, computing these features for every example is a time consuming process and to make the system suitable for large scale WSD it was restricted to using a smaller set of features.</S>
    <S sid="58" ssid="5">Previous experiments (Stevenson et al., 2008) showed that this only leads to a small drop in disambiguation accuracy while significantly reducing the computational cost of generating features.</S>
    <S sid="59" ssid="6">Two types of context words are used as features: the lemmas of all content words in the same sentence as the ambiguous word and the lemmas of all content words in a &#177;4-word window around the ambiguous term.</S>
    <S sid="60" ssid="7">A list of corpus-specific stopwords was created containing terms that appear frequently in Medline abstracts but which are not useful for disambiguation (e.g.</S>
    <S sid="61" ssid="8">&#8220;abstract&#8221;, &#8220;conclusion&#8221;).</S>
    <S sid="62" ssid="9">Any lemmas found in this list were not used as features.</S>
    <S sid="63" ssid="10">Disambiguation is carried out using the Vector Space Model, a memory-based learning algorithm in which each occurrence of an ambiguous word is represented as a vector created using the features extracted to represent it (Agirre and Martinez, 2004a).</S>
    <S sid="64" ssid="11">The Vector Space Model was found to outperform other learning algorithms when evaluated using the NLM-WSD corpus (Stevenson et al., 2008).</S>
    <S sid="65" ssid="12">During the algorithm&#8217;s training phase a single centroid vector, ~Csj, is generated for each possible sense, sj.</S>
    <S sid="66" ssid="13">This is shown in equation 1 where T is the set of training examples for a particular term and sense(~t) is the sense associated with the vector ~t.</S>
    <S sid="67" ssid="14">Disambiguation is carried out by comparing the vector representing the ambiguous word, ~a, against the centroid of each sense using the cosine metric, shown in equation 2, and choosing the one with the highest score.</S>
    <S sid="68" ssid="15">Note that the learning algorithm does not explicitly model the prior probability of each possible sense, unlike alternative approaches (e.g.</S>
    <S sid="69" ssid="16">Naive Bayes), since it was found that including this information did not improve performance.</S>
    <S sid="70" ssid="17">The approaches used for generating training examples used here are based on the work of Stevenson and Guo (2010), who describe two approaches: Both approaches are provided with a set of ambiguous CUIs from the UMLS Metathesaurus, which represent the possible meanings of an ambiguous term, and a target number of training examples to be generated for each CUI.</S>
    <S sid="71" ssid="18">Each CUI is associated with at least one term and each term is labeled with a lexical unique identifier (LUI) which represents a range of lexical variants for a particular term.</S>
    <S sid="72" ssid="19">The UMLS Metathesaurus contains a number of data files which are exploited within these techniques, including: AMBIGLUI: a list of cases where a LUI is linked to multiple CUIs.</S>
    <S sid="73" ssid="20">MRCON: every string or concept name in the Metathesaurus appears in this file.</S>
    <S sid="74" ssid="21">MRCOC: co-occuring concepts.</S>
    <S sid="75" ssid="22">For the monosemous relatives approach, the strings of monosemous LUIs of the target CUI and its relatives are used to search Medline to retrieve training examples.</S>
    <S sid="76" ssid="23">The monosemous LUIs related to a CUI are defined as any LUIs associated with the CUI in the MRCON table and not listed in AMBIGLUI table.</S>
    <S sid="77" ssid="24">The co-occurring concept approach works differently.</S>
    <S sid="78" ssid="25">Instead of using strings of monosemous LUIs of the target CUI and its relatives, the strings associated with LUIs of a number of co-occurring CUIs of the target CUI and its relatives found in MRCOC table are used.</S>
    <S sid="79" ssid="26">The process starts by finding the LUIs of the top n co-occurring CUIs of the target CUI.</S>
    <S sid="80" ssid="27">These LUIs are then used to form search queries.</S>
    <S sid="81" ssid="28">The query is quite restrictive in the beginning and requires all terms appear in the Medline citations files.</S>
    <S sid="82" ssid="29">Subsequently queries are made less restrictive by reducing the number of required terms in the query.</S>
    <S sid="83" ssid="30">These techniques were used to generate labeled examples for all terms that are ambiguous in the 2010 AB version of the UMLS Metathesaurus.2 The set of all ambiguous terms was created by analysing the AMBIGLUI table, to identify CUIs that are associated with multiple LUIs.</S>
    <S sid="84" ssid="31">The Medline Baseline Repository (MBR)3 was also analysed and it was found that some terms were ambiguous in this resource, in the sense that more than one CUI had been assigned to an instance of a term, but could not be identified from the AMBIGLUI table.</S>
    <S sid="85" ssid="32">The final list of ambiguous CUIs was created by combining those identified from the AMBIGLUI table and those find in the MBR.</S>
    <S sid="86" ssid="33">This list contained a total of 103,929 CUIs.</S>
    <S sid="87" ssid="34">Both techniques require large number of searches over the Medline database and to carry this out efficiently the MBR was indexed using the Lucene Information Retrieval system4 and all searches executed locally.</S>
    <S sid="88" ssid="35">Examples were generated using both approaches.</S>
    <S sid="89" ssid="36">The monosemous relatives approach generated examples for 98,462 CUIs and the co-occurring concepts for 98,540.</S>
    <S sid="90" ssid="37">(Examples generated using the monosemous relatives approach were preferred for the experiments reported later.)</S>
    <S sid="91" ssid="38">However, neither technique was able to generate examples for 5,497 CUIs, around 5% of the total.</S>
    <S sid="92" ssid="39">This happened when none of the terms associated with a CUI returned any documents when queried against the MBR and that CUI does not have any monosemous relatives.</S>
    <S sid="93" ssid="40">An example is C1281723 &#8220;Entire nucleus pulposus of intervertebral disc of third lumbar vertebra&#8221;.</S>
    <S sid="94" ssid="41">The lengthy terms associated with this CUI do not return any documents when used as search terms and, in addition, it is only related to one other CUI (C0223534 &#8220;Structure of nucleus pulposus of intervertebral disc of third lumbar vertebra&#8221;) which is itself only connected to C1281723.</S>
    <S sid="95" ssid="42">Fortunately there are relatively few CUIs for which no examples could be generated and none of them appear in the MBR, suggesting they refer to UMLS concepts that do not tend to be mentioned in documents.</S>
    <S sid="96" ssid="43">Three different techniques for deciding the number of training examples to be generated for each CUI (i.e. the bias) were explored.</S>
    <S sid="97" ssid="44">Uniform Bias (UB) uses an equal number of training examples to generate centroid vectors for each of the possible senses of the ambiguous term.</S>
    <S sid="98" ssid="45">Gold standard bias (GSB) is similar to the uniform bias but instead of being the same for all possible CUIs the number of training examples for each CUI is determined by the number of times it appears in a manually labeled gold standard corpus.</S>
    <S sid="99" ssid="46">Assume t is an ambiguous term and Ct is the set of possible meanings (CUIs).</S>
    <S sid="100" ssid="47">The number of training examples used to generate the centroid for that CUI, Ec, is computed according to equation 3 where Gc is the number of instances in the gold standard corpus annotated with CUI c and n is a constant which is set to 100 for these experiments.5 The final technique, Metamap Baseline Repository Bias (MBB), is based on the distribution of CUIs in the MBR.</S>
    <S sid="101" ssid="48">The number of training examples are generated in a similar way to the gold standard bias with MBR being used instead of a manually labeled corpus and is shown in equation 4 where Mc is the number of times the CUI c appears in the MBR.</S>
    <S sid="102" ssid="49">For example, consider the three possible CUIs associated with term &#8220;adjustment&#8221; in the NLM-WSD corpus: C0376209, C0456081 and C06832696.</S>
    <S sid="103" ssid="50">The corpus contains 18 examples of C0376209, 62 examples of C0456081 and 13 of C0683269.</S>
    <S sid="104" ssid="51">Using equation 3, the number of training examples when GSB is applied for C0376209 is 20, 67 for C0456081 and 14 for C0683269.</S>
    <S sid="105" ssid="52">In the Metamap Baseline Repository files, C0376209 has a frequency count of 98046, C0456081 a count of 292809 and C0683269 a count of 83530.</S>
    <S sid="106" ssid="53">Therefore the number of training examples used for the three senses when applying MBB is: 21 for C0376209, 62 for C0456081 and 18 for C0683269.</S>
  </SECTION>
  <SECTION title="4 Evaluation" number="3">
    <S sid="107" ssid="1">We evaluate our system on two datasets: the NLMWSD and MSH-WSD corpora.</S>
    <S sid="108" ssid="2">The NLM-WSD corpus7 (Weeber et al., 2001) has been widely used for experiments on WSD in the biomedical domain, for example (Joshi et al., 2005; Leroy and Rindflesch, 2005; McInnes et al., 2007; Savova et al., 2008).</S>
    <S sid="109" ssid="3">It contains 50 ambiguous terms found in Medline with 100 examples of each.</S>
    <S sid="110" ssid="4">These examples were manually disambiguated by 11 annotators.</S>
    <S sid="111" ssid="5">The guidelines provided to the annotators allowed them to label a senses as &#8220;None&#8221; if none of the concepts in the UMLS Metathesaurus seemed appropriate.</S>
    <S sid="112" ssid="6">These instances could not be mapped onto UMLS Metathesaurus and were ignored for our experiments.</S>
    <S sid="113" ssid="7">The larger MSH-WSD corpus (Jimeno-Yepes et al., 2011) contains 203 strings that are associated with more than one possible MeSH code in the UMLS Metathesaurus.</S>
    <S sid="114" ssid="8">106 of these are ambiguous abbreviations, 88 ambiguous terms and 9 a combination of both.</S>
    <S sid="115" ssid="9">The corpus contains up to 100 examples for each possible sense and a total of 37,888 examples of ambiguous strings taken from Medline.</S>
    <S sid="116" ssid="10">Unlike the NLM-WSD corpus, all of the instances can be mapped to the UMLS Metathesaurus and none was removed from the dataset for our experiments.</S>
    <S sid="117" ssid="11">The two data sets differ in the way the number of instances of each sense was determined.</S>
    <S sid="118" ssid="12">For the NLM-WSD corpus manual annotation is used to decide the number of instances that are annotated with each sense of an ambiguous term.</S>
    <S sid="119" ssid="13">However, the NLM-MSH corpus was constructed automatically and each ambiguous term has roughly the same number of examples of each possible sense.</S>
    <S sid="120" ssid="14">The WSD system described in Section 3 was tested using each of the three techniques for determining the bias, i.e. number of examples generated for each CUI.</S>
    <S sid="121" ssid="15">Performance is compared against various alternative approaches.</S>
    <S sid="122" ssid="16">Two supervised approaches are included.</S>
    <S sid="123" ssid="17">The first, most frequent sense (MFS) (McCarthy et al., 2004), is widely used baseline for supervised WSD systems.</S>
    <S sid="124" ssid="18">It consists of assigning each ambiguous term the meaning that is more frequently observed in the training data.</S>
    <S sid="125" ssid="19">The second supervised approach is to train the WSD system using manually labeled examples from the NLM-WSD and MSH-WSD corpora.</S>
    <S sid="126" ssid="20">10-fold cross validation is applied to evaluate this approach.</S>
    <S sid="127" ssid="21">Performance of the Personalised Page Rank approach described in Section 2.1 is also provided to allow comparison with an unsupervised algorithm.</S>
    <S sid="128" ssid="22">Both Personalised Page Rank and the techniques we employ to generate labeled data, base disambiguation decisions on information from the UMLS Metathesaurus.</S>
    <S sid="129" ssid="23">The performance of all approaches is measured in terms of the percentage of instances which are correctly disambiguated for each term with the average across all terms reported.</S>
    <S sid="130" ssid="24">Confidence intervals (95%) computed using bootstrap resampling (Noreen, 1989) are also shown.</S>
  </SECTION>
  <SECTION title="5 Results" number="4">
    <S sid="131" ssid="1">Results of the experiments are shown in Table 1 where the first three rows show performance of the approach described in Section 3 using the three methods for computing the bias (UB, MMB and GSB).</S>
    <S sid="132" ssid="2">MFS and Sup refer to the Most Frequent Sense supervised baseline and using manually labeled examples, respectively, and PPR to the Personalised PageRank approach.</S>
    <S sid="133" ssid="3">When the performance of the approaches using automatically labeled examples (UB, MMB and GSB) is compared it is not surprising that the best results are obtained using the gold standard bias since this is obtained from manually labeled data.</S>
    <S sid="134" ssid="4">Results using this technique for computing bias always outperform the other two, which are completely unsupervised and do not make use of any information from manually labeled data.</S>
    <S sid="135" ssid="5">However, the improvement in performance varies according to the corpus, for the NLM-WSD corpus there is an improvement of over 10% in comparison to UB while the corresponding improvement for the MSH-WSD corpus is less than 0.5%.</S>
    <S sid="136" ssid="6">A surprising result is that performance obtained using the uniform bias (UB) is consistently better than using the bias obtained by analysis of the MBR (MMB).</S>
    <S sid="137" ssid="7">It would be reasonable to expect that information about the distribution of CUIs in this corpus would be helpful for WSD but it turns out that making no assumptions whatsoever about their relative frequency, i.e., assigning a uniform baseline, produces better results.</S>
    <S sid="138" ssid="8">The relative performance of the supervised (MFS, Sup and GSB) and unsupervised approaches (UB, MMB and PPR) varies according to the corpus.</S>
    <S sid="139" ssid="9">Unsurprisingly using manually labeled data (Sup) outperforms all other approaches on both corpora.</S>
    <S sid="140" ssid="10">The supervised approaches also outperform the unsupervised ones on the NLM-WSD corpus.</S>
    <S sid="141" ssid="11">However, for the MSH-WSD corpus all of the unsupervised approaches outperform the MFS baseline.</S>
    <S sid="142" ssid="12">A key reason for the differences in these results is the different distributions of senses in the two corpora, as shown by the very different performance of the MFS approach on the two corpora.</S>
    <S sid="143" ssid="13">This is discussed in more detail later (Section 5.2).</S>
    <S sid="144" ssid="14">Comparison of the relative performance of the unsupervised approaches (UB, MMB and PPR) shows that training a supervised system with the automatically labeled examples using a uniform bias (UB) always outperforms PPR.</S>
    <S sid="145" ssid="15">This demonstrates that this approach outperforms a state-of-the-art unsupervised algorithm that relies on the same information used to generate the examples (the UMLS Metathesaurus).</S>
    <S sid="146" ssid="16">The MSH-WSD corpus contains both ambiguous terms and abbreviations (see Section 4.1).</S>
    <S sid="147" ssid="17">Performance of the approaches on both types of ambiguity are shown in Table 2.</S>
    <S sid="148" ssid="18">The relative performance of the different approaches on the terms and abbreviations is similar to the entire MSH-WSD data set (see Table 1).</S>
    <S sid="149" ssid="19">In particular using automatically generated examples with a uniform bias (UB) outperforms using the bias derived from the Medline Baseline Repository (MBR) while using the gold standard baseline (GSB) improves results slightly for terms and actually reduces them for abbreviations.</S>
    <S sid="150" ssid="20">Results for all approaches are higher when disambiguating abbreviations than terms which is consistent with previous studies that have suggested that in biomedical text abbreviations are easier to disambiguate than terms.</S>
    <S sid="151" ssid="21">An explanation of the reason for some of the results can be gained by looking at the distributions of senses in the various data sets used for the experiments.</S>
    <S sid="152" ssid="22">Kullback-Leibler divergence (or KL divergence) (Kullback and Leibler, 1951) is a commonly used measure for determining the difference between two probability distributions.</S>
    <S sid="153" ssid="23">For each term t, we define 5 as the set of possible senses of t, the sense probability distributions of t as D and D'.</S>
    <S sid="154" ssid="24">Then the KL divergence between the sense probability distributions D and D' can be calculated according to equation 5.</S>
    <S sid="155" ssid="25">The three techniques for determining the bias described in Section 3.3 each generate a probability distribution over senses.</S>
    <S sid="156" ssid="26">Table 2 shows the average KL divergence when the gold standard distribution obtained from the manually labeled data (GSB) is compared with the uniform bias (UB) and bias obtained by analysing the Medline Baseline Repository (MMB).</S>
    <S sid="157" ssid="27">The average KL divergence scores in the table are roughly similar with the exception of the much lower score obtained for the gold-standard and uniform bias for the MSH-WSD corpus (0.0406).</S>
    <S sid="158" ssid="28">This is due to the fact that the MSH-WSD corpus was designed to have roughly the same number of examples for each sense, making the sense distribution close to uniform (Jimeno-Yepes et al., 2011).</S>
    <S sid="159" ssid="29">This is evident from the MFS scores for the MSHWSD corpus which are always close to 50%.</S>
    <S sid="160" ssid="30">This also provides as explanation of why performance using automatically generated examples on the MSHWSD corpus only improves by a small amount when the gold standard bias is used (see Table 1).</S>
    <S sid="161" ssid="31">The gold standard bias simply does not provide much additional information to the WSD system.</S>
    <S sid="162" ssid="32">The situation is different in the NLM-WSD corpus, where the MFS score is much higher.</S>
    <S sid="163" ssid="33">In this case the additional information available in the gold standard sense distribution is useful for the WSD system and leads to a large improvement in performance.</S>
    <S sid="164" ssid="34">In addition, this analysis demonstrates why performance does not improve when the bias generated from the MBR is used.</S>
    <S sid="165" ssid="35">The distributions which are obtained are different from the gold standard and are therefore mislead the WSD system rather than providing useful information.</S>
    <S sid="166" ssid="36">The difference between these distributions would be expected for the MSH-WSD corpus, since it contains roughly the same number of examples for each possible sense and does not attempt to represent the relative frequency of the different senses.</S>
    <S sid="167" ssid="37">However, it is surprising to observe a similar difference for the NLMWSD corpus, which does not have this constraint.</S>
    <S sid="168" ssid="38">The difference suggests the information about CUIs in the MBR, which is generated automatically, has some limitations.</S>
    <S sid="169" ssid="39">Table 4 shows a similar analysis for the MSHWSD corpus when abbreviations and terms are considered separately and supports this analysis.</S>
    <S sid="170" ssid="40">The figures in this table show that the gold standard and uniform distributions are very similar for both abbreviations and terms, which explains the similar results for UB and GSB in Table 2.</S>
    <S sid="171" ssid="41">However, the gold standard distribution is different from the one obtained from the MBR.</S>
    <S sid="172" ssid="42">The drop in performance of MMB compared with GBS in Table 2 is a consequence of this.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="5">
    <S sid="173" ssid="1">This paper describes the development of a large scale WSD system based on automatically labeled examples.</S>
    <S sid="174" ssid="2">We find that these examples can be generated for the majority of CUIs in the UMLS Metathesaurus.</S>
    <S sid="175" ssid="3">Evaluation on the NLM-WSD and MSHWSD data sets demonstrates that the WSD system outperforms the PPR approach without making any use of labeled data.</S>
    <S sid="176" ssid="4">Three techniques for determining the number of examples to use for training are explored.</S>
    <S sid="177" ssid="5">It is found that a supervised approach (which makes use of manually labeled data) provides the best results.</S>
    <S sid="178" ssid="6">Surprisingly it was also found that using information from the MBR did not improve performance.</S>
    <S sid="179" ssid="7">Analysis showed that the sense distributions extracted from the MBR were different from those observed in the evaluation data, providing an explanation for this result.</S>
    <S sid="180" ssid="8">Evaluation showed that accurate information about the bias of training examples is useful for WSD systems and future work will explore other unsupervised ways of obtaining this information.</S>
    <S sid="181" ssid="9">Alternative techniques for generating labeled examples will also be explored.</S>
    <S sid="182" ssid="10">In addition, further evaluation of the WSD system will be carried out, such as applying it to an all words task and within applications.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="6">
    <S sid="183" ssid="1">This research has been supported by the Engineering and Physical Sciences Research Council and a Google Research Award.</S>
  </SECTION>
</PAPER>
