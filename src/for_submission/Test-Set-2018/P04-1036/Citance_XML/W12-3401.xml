<PAPER>
  <S sid="0">Probabilistic Lexical Generalization for French Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis.</S>
    <S sid="2" ssid="2">A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking.</S>
    <S sid="3" ssid="3">The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class.</S>
    <S sid="4" ssid="4">We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets.</S>
    <S sid="5" ssid="5">Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features.</S>
    <S sid="6" ssid="6">We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">In statistical, data-driven approaches to natural language syntactic parsing, a central problem is that of accurately modeling lexical relationships from potentially sparse counts within a training corpus.</S>
    <S sid="8" ssid="2">Our particular interests are centered on reducing lexical data sparseness for linear classification approaches for dependency parsing.</S>
    <S sid="9" ssid="3">In these approaches, linear models operate over feature vectors that generally represent syntactic structure within a sentence, and feature templates are defined in part over the word forms of one or more tokens in a sentence.</S>
    <S sid="10" ssid="4">Because treebanks used for training are often small, lexical features may appear relatively infrequently during training, especially for languages with richer morphology than English.</S>
    <S sid="11" ssid="5">This may, in turn, impede the parsing model&#8217;s ability to generalize well outside of its training set with respect to lexical features.</S>
    <S sid="12" ssid="6">Past approaches for achieving lexical generalization in dependency parsing have used WordNet semantic senses in parsing experiments for English (Agirre et al., 2011), and word clustering over large corpora in parsing experiments for English (Koo et al., 2008) as well as for French (Candito et al., 2010b).</S>
    <S sid="13" ssid="7">These approaches map each word to a single corresponding generalized class (synset or cluster), and integrate generalized classes into parsing models in one of two ways: (i) the replacement strategy, where each word form is simply replaced with a corresponding generalized class; (ii) a strategy where an additional feature is created for the corresponding generalized class.</S>
    <S sid="14" ssid="8">Our contribution in this paper is applying probabilistic lexical generalization, a richer framework for lexical generalization, to dependency parsing.</S>
    <S sid="15" ssid="9">Each word form is represented as a categorical distribution over a lexical target space of generalized classes, for which we consider the spaces of lemmas, synsets, and clusters.</S>
    <S sid="16" ssid="10">The standard single-mapping approach from previous work can be seen as a subcase: each categorical distribution assigns a probability of 1 to a single generalized class.</S>
    <S sid="17" ssid="11">The method we use for introducing probabilistic information into a feature vector is based on that used by Bunescu (2008), who tested the use of probabilistic part-ofspeech (POS) tags through an NLP pipeline.</S>
    <S sid="18" ssid="12">In this paper, we perform experiments for French that use the replacement strategy for integrating generalized classes into parsing models, comparing the single-mapping approach for lexical generalization with our probabilistic lexical generalization approach.</S>
    <S sid="19" ssid="13">In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al. (2004).</S>
    <S sid="20" ssid="14">For clustering we deviate from most previous work, which has integrated Brown clusters (Brown et al., 1992) into parsing models, and instead use distributional lexical semantics to create both a distributional thesaurus - for probabilistic generalization in the lemma space and ASR calculation and to perform hierarchical agglomerative clustering (HAC).</S>
    <S sid="21" ssid="15">Though unlexicalized syntactic HAC clustering has been used to improve English dependency parsing (Sagae and Gordon, 2009), we provide first results on using distributional lexical semantics for French parsing.</S>
    <S sid="22" ssid="16">We also include an out-of-domain evaluation on medical and parliamentary text in addition to an in-domain evaluation.</S>
    <S sid="23" ssid="17">In Section 2 we describe the lexical target spaces used in this paper, as well as the method of integrating probabilistic lexical information into a feature vector for classification.</S>
    <S sid="24" ssid="18">In Section 3 we discuss dependency structure and transition-based parsing.</S>
    <S sid="25" ssid="19">In Section 4 we present the experimental setup, which includes our parser implementation, the construction of our probabilistic lexical resources, and evaluation settings.</S>
    <S sid="26" ssid="20">We report parsing results both in-domain and out-of-domain in Section 5, we provide a summary of related work in Section 6, and we conclude in Section 7.</S>
  </SECTION>
  <SECTION title="2 Probabilistic Lexical Target Spaces" number="2">
    <S sid="27" ssid="1">Using terms from probability theory, we define a lexical target space as a sample space Q over which a categorical distribution is defined for each lexical item in a given source vocabulary.</S>
    <S sid="28" ssid="2">Because we are working with French, a language with relatively rich morphology, we use lemmas as the base lexical items in our source vocabulary.</S>
    <S sid="29" ssid="3">The outcomes contained in a sample space represent generalized classes in a target vocabulary.</S>
    <S sid="30" ssid="4">In this paper we consider three possible target vocabularies, with corresponding sample spaces: Ql for lemmas, Q, for synsets, and Q, for clusters.</S>
    <S sid="31" ssid="5">In the case of the lemma space, the source and target vocabularies are the same.</S>
    <S sid="32" ssid="6">To define an appropriate categorical distribution for each lemma, one where the possible outcomes also correspond to lemmas, we use a distributional thesaurus that provides similarity scores for pairs of lemmas.</S>
    <S sid="33" ssid="7">Such a thesaurus can be viewed as a similarity function D(x, y), where x, y E V and V is the vocabulary for both the source and target spaces.</S>
    <S sid="34" ssid="8">The simplest way to define a categorical distribution over Ql, for a lemma x E V , would be to use the following probability mass function px: One complication is the identity similarity D(x, x): although it can be set equal to 1 (or the similarity given by the thesaurus, if one is provided), we choose to assign a pre-specified probability mass m to the identity lemma, with the remaining mass used for generalization across other lemmas.</S>
    <S sid="35" ssid="9">Additionally, in order to account for noise in the thesaurus, we restrict each categorical distribution to a lemma&#8217;s k-nearest neighbors.</S>
    <S sid="36" ssid="10">The probability mass function px over the space Ql that we use in this paper is finally as follows:</S>
  </SECTION>
  <SECTION title="0, otherwise" number="3">
    <S sid="37" ssid="1">In the case of the synset space, the target vacabulary contains synsets from the Princeton WordNet sense hierarchy (Fellbaum, 1998).</S>
    <S sid="38" ssid="2">To define an appropriate categorical distribution over synsets for each lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x.</S>
    <S sid="39" ssid="3">We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s E Sx, following the approach of McCarthy et al. (2004).</S>
    <S sid="40" ssid="4">Representing the thesaurus as a similarity function D(x, y), letting Nx(k) be the set of k-nearest neighbors for x, and letting W(s1, s2) be a similarity function over synsets in WordNet, we define a prevalence function This function essentially weights the semantic contribution that each distributionally-similar neighbor adds to a given sense for x.</S>
    <S sid="41" ssid="5">With the prevalence scores of each sense for x having been calculated, we use the following probability mass function px over the space Q, where Sx(k) is the set of k-most prevalent senses for x: Note that the first-sense ASR approach to using WordNet synsets for parsing, which has been previously explored in the literature (Agirre et al., 2011), corresponds to setting k=1 in Equation 4.</S>
    <S sid="42" ssid="6">In the case of the cluster space, any approach for word clustering may be used to create a reduced target vocabulary of clusters.</S>
    <S sid="43" ssid="7">Defining a categorical distribution over clusters would be interesting in the case of soft clustering of lemmas, in which a lemma can participate in more than one cluster, but we have not yet explored this clustering approach.</S>
    <S sid="44" ssid="8">In this paper we limit ourselves to the simpler hard clustering HAC method, which uses a distributional thesaurus and iteratively joins two clusters together based on the similarities between lemmas in each cluster.</S>
    <S sid="45" ssid="9">We end up with a simple probability mass function px over the space Q, for a lemma x with corresponding cluster cx: In a typical classifier-based machine learning setting in NLP, feature vectors are constructed using indicator functions that encode categorical information, such as POS tags, word forms or lemmas.</S>
    <S sid="46" ssid="10">In this section we will use a running example where a and b are token positions of interest to a classifier, and for which feature vectors are created.</S>
    <S sid="47" ssid="11">If we let t stand for POS tag and l stand for lemma, a feature template for this pair of tokens might then be [talb].</S>
    <S sid="48" ssid="12">Feature templates are instantiated as actual features in a vector space depending on the categorical values they can take on.</S>
    <S sid="49" ssid="13">One possible instantiation of the template [talb] would then be the feature [ta=verbnlb=avocat], which indicates that a is a verb and b is the lemma avocat (&#8220;avocado&#8221; or &#8220;lawyer&#8221;), with the following indicator function: To perform probabilistic feature generalization, we replace the indicator function, which represents a single original feature, with a collection of weighted functions representing a set of derived features.</S>
    <S sid="50" ssid="14">Suppose the French lemma avocat is in our source vocabulary and has multiple senses in Q, (s1 for the &#8220;avocado&#8221; sense, s2 for the &#8220;lawyer&#8221; sense, etc.</S>
    <S sid="51" ssid="15">), as well as a probability mass function pav.</S>
    <S sid="52" ssid="16">We discard the old feature [ta=verbnlb=avocat] and add, for each si, a derived feature of the form [ta=verbnxb=si], where x represents a target space generalized class, with the following weighted indicator function: This process extends easily to generalizing multiple categorical variables.</S>
    <S sid="53" ssid="17">Consider the bilexical feature [la=mangernlb=avocat], which indicates that a is the lemma manger (&#8220;eat&#8221;) and b is the lemma avocat.</S>
    <S sid="54" ssid="18">If both lemmas manger and avocat appear in our source vocabulary and have multiple senses in Q, with probability mass functions pma and pav, then for each pair i, j we derive a feature of the form [xa=si&#8743;xb=sj], with the following weighted indicator function:</S>
  </SECTION>
  <SECTION title="3 Dependency Parsing" number="4">
    <S sid="55" ssid="1">Dependency syntax involves the representation of syntactic information for a sentence in the form of a directed graph, whose edges encode word-to-word relationships.</S>
    <S sid="56" ssid="2">An edge from a governor to a dependent indicates, roughly, that the presence of the dependent is syntactically legitimated by the governor.</S>
    <S sid="57" ssid="3">An important property of dependency syntax is that each word, except for the root of the sentence, has exactly one governor; dependency syntax is thus represented by trees.</S>
    <S sid="58" ssid="4">Figure 1 shows an example of an unlabeled dependency tree.1 For languages like English or French, most sentences can be represented with a projective dependency tree: for any edge from word g to word d, g dominates any intervening word between g and d. Dependency trees are appealing syntactic representations, closer than constituency trees to the semantic representations useful for NLP applications.</S>
    <S sid="59" ssid="5">This is true even with the projectivity requirement, which occasionally creates syntax-semantics mismatches.</S>
    <S sid="60" ssid="6">Dependency trees have recently seen a surge of interest, particularly with the introduction of supervised models for dependency parsing using linear classifiers.</S>
    <S sid="61" ssid="7">In this paper we focus on transition-based parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003).</S>
    <S sid="62" ssid="8">The parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures.</S>
    <S sid="63" ssid="9">In the arc-eager approach introduced by Nivre et al. (2006) the possible actions are as follows, with s0 being the token on top of the stack and n0 being the next token in the buffer: The parser uses a greedy approach, where the action selected at each step is the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions.</S>
    <S sid="64" ssid="10">The major strength of this framework is its O(n) time complexity, which allows for very fast parsing when compared to more complex global optimization approaches.</S>
  </SECTION>
  <SECTION title="4 Experimental Setup" number="5">
    <S sid="65" ssid="1">We now discuss the treebanks used for training and evaluation, the parser implementation and baseline settings, the construction of the probabilistic lexical resources, and the parameter tuning and evaluation settings.</S>
    <S sid="66" ssid="2">The treebank we use for training and in-domain evaluation is the French Treebank (FTB) (Abeill&#180;e and Barrier, 2004), consisting of 12,351 sentences from the Le Monde newspaper, converted to projective2 dependency trees (Candito et al., 2010a).</S>
    <S sid="67" ssid="3">For our experiments we use the usual split of 9,881 training, 1,235 development, and 1,235 test sentences.</S>
    <S sid="68" ssid="4">2The projectivity constraint is linguistically valid for most French parses: the authors report &lt; 2% non-projective edges in a hand-corrected subset of the converted FTB.</S>
    <S sid="69" ssid="5">Moving beyond the journalistic domain, we use two additional treebank resources for out-of-domain parsing evaluations.</S>
    <S sid="70" ssid="6">These treebanks are part of the Sequoia corpus (Candito and Seddah, 2012), and consist of text from two non-journalistic domains annotated using the FTB annotation scheme: a medical domain treebank containing 574 development and 544 test sentences of public assessment reports of medicine from the European Medicines Agency (EMEA) originally collected in the OPUS project (Tiedemann, 2009), and a parliamentary domain treebank containing 561 test sentences from the Europarl3 corpus.</S>
    <S sid="71" ssid="7">We use our own Python implementation of the arceager algorithm for transition-based parsing, based on the arc-eager setting of MaltParser (Nivre et al., 2007), and we train using the standard FTB training set.</S>
    <S sid="72" ssid="8">Our baseline feature templates and general settings correspond to those obtained in a benchmarking of parsers for French (Candito et al., 2010b), under the setting which combined lemmas and morphological features.4 Automatic POS-tagging is performed using MElt (Denis and Sagot, 2009), and lemmatization and morphological analysis are performed using the Leff lexicon (Sagot, 2010).</S>
    <S sid="73" ssid="9">Table 1 lists our baseline parser&#8217;s feature templates.</S>
    <S sid="74" ssid="10">We now describe the construction of our probabilistic lexical target space resources, whose prerequisites include the automatic parsing of a large corpus, the construction of a distributional thesaurus, the use of ASR on WordNet synsets, and the use of HAC clustering.</S>
  </SECTION>
  <SECTION title="4.3.1 Automatically-Parsed Corpus" number="6">
    <S sid="75" ssid="1">The text corpus we use consists of 125 million words from the L&#8217;Est Republicain newspaper5, 125 million words of dispatches from the Agence France-Presse, and 225 million words from a French Wikipedia backup dump6.</S>
    <S sid="76" ssid="2">The corpus is POS tag, t = fine POS tag, w = inflected word form, l = lemma, d = dependency label, mi = morphological feature from set M. For tokens, ni = ith token in the buffer, si = ith token on the stack.</S>
    <S sid="77" ssid="3">The token subscripts l, r, and h denote partially-constructed syntactic left-most dependent, right-most dependent, and head, respectively. preprocessed using the Bonsai tool7, and parsed using our baseline parser.</S>
    <S sid="78" ssid="4">We build separate distributional thesauri for nouns and for verbs,8 using straightforward methods in distributional lexical semantics based primarily on work by Lin (1998) and Curran (2004).</S>
    <S sid="79" ssid="5">We use the FreDist tool (Henestroza Anguiano and Denis, 2011) for thesaurus creation.</S>
    <S sid="80" ssid="6">First, syntactic contexts for each lemma are extracted from the corpus.</S>
    <S sid="81" ssid="7">We use all syntactic dependencies in which the secondary token has an open-class POS tag, with labels included in the contexts and two-edge dependencies used in the case of prepositional-phrase attachment and coordination.</S>
    <S sid="82" ssid="8">Example contexts are shown in Figure 2.</S>
    <S sid="83" ssid="9">For verb lemmas we limit contexts to dependencies in which the verb is governor, and we add unlexicalized versions of contexts to account for subcategorization.</S>
    <S sid="84" ssid="10">For noun lemmas, we use all dependencies in which the noun participates, and all contexts are lexicalized.</S>
    <S sid="85" ssid="11">The vocabulary is limited to lemmas with at least 1,000 context occurrences, resulting in 8,171 nouns and 2,865 verbs.</S>
    <S sid="86" ssid="12">Each pair of lemma x and context c is subsequently weighted by mutual informativeness using the point-wise mutual information metric, with probabilities estimated using frequency counts: Finally, we use the cosine metric to calculate the distributional similarity between pairs of lemmas x, y: For WordNet synset experiments we use the French EuroWordNet9 (FREWN).</S>
    <S sid="87" ssid="13">A WordNet synset mapping10 allows us to convert synsets in the FREWN to Princeton WordNet version 3.0, and after discarding a small number of synsets that are not covered by the mapping we retain entries for 9,833 nouns and 2,220 verbs.</S>
    <S sid="88" ssid="14">We use NLTK, the Natural Language Toolkit (Bird et al., 2009), to calculate similarity between synsets.</S>
    <S sid="89" ssid="15">As explained in Section 2.2, ASR is performed using the method of McCarthy et al. (2004).</S>
    <S sid="90" ssid="16">We use k=8 for the distributional nearest-neighbors to consider when ranking the senses for a lemma, and we use the synset similarity function of Jiang and Conrath (1997), with default information content counts from NLTK calculated over the British National Corpus11.</S>
    <S sid="91" ssid="17">For the HAC clustering experiments in this paper, we use the CLUTO package12.</S>
    <S sid="92" ssid="18">The distributional thesauri described above are taken as input, and the UPGMA setting is used for cluster agglomeration.</S>
    <S sid="93" ssid="19">We test varying levels of clustering, with a parameter z which determines the proportion of cluster vocabulary size with respect to the original vocabulary size (8,171 for nouns and 2,865 for verbs).</S>
    <S sid="94" ssid="20">The coverage of our lexical resources over the FTB and two out-of-domain evaluation sets, at the level of token occurrences of verbs and common nouns, is described in Table 2.</S>
    <S sid="95" ssid="21">We can see that the FTB training set vocabulary provides better coverage than the FREWN for both nouns and verbs, while the coverage of the thesauri (and derived clusters) is the highest overall.</S>
    <S sid="96" ssid="22">We evaluate four lexical target space configurations against the baseline of lemmatization, tuning parameters using ten-fold cross-validation on the FTB training set.</S>
    <S sid="97" ssid="23">The feature templates are the same as those in Table 1, with the difference that features involving lemmas are modified by the probabilistic feature generalization technique described in Section 2.4, using the appropriate categorical distributions.</S>
    <S sid="98" ssid="24">In all configurations, we exclude the French auxiliary verbs &#710;etre and avoir from participation in lexical generalization, and we replace proper nouns with a special lemma13.</S>
    <S sid="99" ssid="25">Below we describe the tuned parameters for each configuration.</S>
    <S sid="100" ssid="26">&#8722; RC: Replacement with cluster in Q, For clusters and the parameter z (cf.</S>
    <S sid="101" ssid="27">Section 4.3.4), we settled on relative cluster vocabulary size z=0.6 for nouns and z=0.7 for verbs.</S>
    <S sid="102" ssid="28">We also generalized lemmas not appearing in the distributional thesaurus into a single unknown class.</S>
    <S sid="103" ssid="29">&#8722; PKNL: Probabilistic k-nearest lemmas in Q, For the parameters k and m (cf.</S>
    <S sid="104" ssid="30">Section 2.1), we settled on k=4 and m=0.5 for both nouns and verbs.</S>
    <S sid="105" ssid="31">We also use the unknown class for low-frequency lemmas, as in the RC configuration.</S>
    <S sid="106" ssid="32">&#8722; RS: Replacement with first-sense (k=1) in Q, Since the FREWN has a lower-coverage vocabulary, we did not use an unknown class for out-of-vocabulary lemmas; instead, we mapped them to unique senses.</S>
    <S sid="107" ssid="33">In addition, we did not perform lexical generalization for verbs, due to low cross-validation performance.</S>
    <S sid="108" ssid="34">&#8722; PKPS: Probabilistic k-prevalent senses in Q For this setting we decided to not place any limit on k, due to the large variation in the number of senses for different lemmas.</S>
    <S sid="109" ssid="35">As in the RS configuration, we mapped out-ofvocabulary lemmas to unique senses and did not perform lexical generalization for verbs.</S>
  </SECTION>
  <SECTION title="5 Results" number="7">
    <S sid="110" ssid="1">Table 3 shows labeled attachment score (LAS) results for our baseline parser (Lemmas) and four lexical generalization configurations.</S>
    <S sid="111" ssid="2">For comparison, we also include results for a setting that only uses word forms (Forms), which was the baseline for previous work on French dependency parsing (Candito et al., 2010b).</S>
    <S sid="112" ssid="3">Punctuation tokens are not scored, and significance is calculated using Dan Bikel&#8217;s randomized parsing evaluation comparator14, at significance level p=0.05.</S>
    <S sid="113" ssid="4">Our in-domain evaluation yields slight improvements in LAS for some lexical generalization configurations, with PKNL performing the best.</S>
    <S sid="114" ssid="5">However, the improvements are not statistically significant.</S>
    <S sid="115" ssid="6">A potential explanation for this disappointing result is that the FTB training set vocabulary covers the FTB test set at high rates for both nouns (95.25%) and verbs (96.54%), meaning that lexical data sparseness is perhaps not a big problem for in-domain dependency parsing.</S>
    <S sid="116" ssid="7">While WordNet synsets could be expected to provide the added benefit of taking word sense into account, sense ambiguity is not really treated due to ASR not providing word sense disambiguation in context.</S>
    <S sid="117" ssid="8">Our evaluation on the medical domain yields statistically significant improvements in LAS, particularly for the two probabilistic target space approaches.</S>
    <S sid="118" ssid="9">PKNL and PKPS improve parsing for both the EMEA dev and test sets, while RC improves parsing for only the EMEA test set and RS does not significantly improve parsing for either set.</S>
    <S sid="119" ssid="10">As in our in-domain evaluation, PKNL performs the best overall, though not significantly better than other lexical generalization settings.</S>
    <S sid="120" ssid="11">One explanation for the improvement in the medical domain is the substantial increase in coverage of nouns in EMEA afforded into a single class. by the distributional thesaurus (+26%) and FREWN (+16%) over the base coverage afforded by the FTB training set.</S>
    <S sid="121" ssid="12">Our evaluation on the parliamentary domain yields no improvement in LAS across the different lexical generalization configurations.</S>
    <S sid="122" ssid="13">Interestingly, Candito and Seddah (2012) note that while Europarl is rather different from FTB in its syntax, its vocabulary is surprisingly similar.</S>
    <S sid="123" ssid="14">From Table 2 we can see that the FTB training set vocabulary has about the same high level of coverage over Europarl (94.69% for nouns and 97.76% for verbs) as it does over the FTB evaluation sets (95.35% for nouns and 96.54% for verbs).</S>
    <S sid="124" ssid="15">Thus, we can use the same reasoning as in our in-domain evaluation to explain the lack of improvement for lexical generalization methods in the parliamentary domain.</S>
    <S sid="125" ssid="16">Since lexical generalization modifies the lexical feature space in different ways, we also provide an analysis of the extent to which each parsing model&#8217;s lexical features are used during in-domain and out-ofdomain parsing.</S>
    <S sid="126" ssid="17">Table 4 describes, for each configuration, the number of lexical features stored in the parsing model along with the average lexical feature use (ALFU) of classification instances (each instance represents a parse transition) during training and parsing.15 Lexical feature use naturally decreases when moving from the training set to the evaluation sets, due to holes in lexical coverage outside of a parsing model&#8217;s training set.</S>
    <S sid="127" ssid="18">The single-mapping configurations (RC, RS) do not increase the number of lexical features in a classification instance, which explains the fact that their ALFU on the FTB training set (6.0) is the same as that of the baseline.</S>
    <S sid="128" ssid="19">However, the decrease in ALFU when parsing the evaluation sets is less severe for these configurations than for the baseline: when parsing EMEA Dev with the RC configuration, where we obtain a significant LAS improvement over the baseline, the reduction in ALFU is only 13% compared to 22% for the baseline parser.</S>
    <S sid="129" ssid="20">For the probabilistic generalization configurations, we also see decreases in ALFU when parsing the evaluation sets, though their higher absolute ALFU may help explain the strong medical domain parsing performance for these configurations.</S>
    <S sid="130" ssid="21">Another factor to note when evaluating lexical generalization is the effect that it has on running time.</S>
    <S sid="131" ssid="22">Compared to the baseline, the single-mapping configurations (RC, RS) speed up feature extraction and prediction time, due to reduced dimensionality of the feature space.</S>
    <S sid="132" ssid="23">On the other hand, the probabilistic generalization configurations (PKNL, PKPS) slow down feature extraction and prediction time, due to an increased dimensionality of the feature space and a higher ALFU.</S>
    <S sid="133" ssid="24">Running time is therefore a factor that favors the single-mapping approach over our proposed probabilistic approach.</S>
    <S sid="134" ssid="25">Taking a larger view on our findings, we hypothesize that in order for lexical generalization to improve parsing, an approach needs to achieve two objectives: (i) generalize sufficiently to ensure that lemmas not appearing in the training set are nonetheless associated with lexical features in the learned parsing model; (ii) substantially increase lexical coverage over what the training set can provide.</S>
    <S sid="135" ssid="26">The first of these objectives seems to be fulfilled through our lexical generalization methods, as indicated in Table 4.</S>
    <S sid="136" ssid="27">The second objective, however, seems difficult to attain when parsing text indomain, or even out-of-domain if the domains have a high lexical overlap (as is the case for Europarl).</S>
    <S sid="137" ssid="28">Only for our parsing experiments in the medical domain do both objectives appear to be fulfilled, as evidenced by our LAS improvements when parsing EMEA with lexical generalization.</S>
  </SECTION>
  <SECTION title="6 Related Work" number="8">
    <S sid="138" ssid="1">We now discuss previous work concerning the use of lexical generalization for parsing, both in the classic in-domain setting and in the more recently popular out-of-domain setting.</S>
    <S sid="139" ssid="2">The use of word classes for parsing dates back to the first works on generative constituency-based parsing, whether using semantic classes obtained from hand-built resources or less-informed classes created automatically.</S>
    <S sid="140" ssid="3">Bikel (2000) tried incorporating WordNet-based word sense disambiguation into a parser, but failed to obtain an improvement.</S>
    <S sid="141" ssid="4">Xiong et al. (2005) generalized bilexical dependencies in a generative parsing model using Chinese semantic resources (CiLin and HowNet), obtaining improvements for Chinese parsing.</S>
    <S sid="142" ssid="5">More recently, Agirre et al. (2008) show that replacing words with WordNet semantic classes improves English generative parsing.</S>
    <S sid="143" ssid="6">Lin et al. (2009) use the HowNet resource within the split-merge PCFG framework (Petrov et al., 2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits using the HowNet hierarchy.</S>
    <S sid="144" ssid="7">Other work has used less-informed classes, notably unsupervised word clusters.</S>
    <S sid="145" ssid="8">Candito and Crabb&#180;e (2009) use Brown clusters to replace words in a generative PCFG-LA framework, obtaining substantial parsing improvements for French.</S>
    <S sid="146" ssid="9">In dependency parsing, word classes are integrated as features in underlying linear models.</S>
    <S sid="147" ssid="10">In a seminal work, Koo et al. (2008) use Brown clusters as features in a graph-based parser, improving parsing for both English and Czech.</S>
    <S sid="148" ssid="11">However, attempts to use this technique for French have lead to no improvement when compared to the use of lemmatization and morphological analysis (Candito et al., 2010b).</S>
    <S sid="149" ssid="12">Sagae and Gordon (2009) augment a transitionbased English parser with clusters using unlexicalized syntactic distributional similarity: each word is represented as a vector of counts of emanating unlexicalized syntactic paths, with counts taken from a corpus of auto-parsed phrase-structure trees, and HAC clustering is performed using cosine similarity.</S>
    <S sid="150" ssid="13">For semantic word classes, (Agirre et al., 2011) integrate WordNet senses into a transition-based parser for English, reporting small but significant improvements in LAS (+0.26% with synsets and +0.36% with semantic files) on the full Penn Treebank with first-sense information from Semcor.</S>
    <S sid="151" ssid="14">We build on previous work by attempting to reproduce, for French, past improvements for indomain English dependency parsing with generalized lexical classes.</S>
    <S sid="152" ssid="15">Unfortunately, our results for French do not replicate the improvements for English using semantic sense information (Agirre et al., 2011) or word clustering (Sagae and Gordon, 2009).</S>
    <S sid="153" ssid="16">The primary difference between our paper and previous work, though, is our evaluation of a novel probabilistic approach for lexical generalization.</S>
    <S sid="154" ssid="17">Concerning techniques for improving out-ofdomain parsing, a related approach has been to use self-training with auto-parsed out-of-domain data, as McClosky and Charniak (2008) do for English constituency parsing, though in that approach lexical generalization is not explicitly performed.</S>
    <S sid="155" ssid="18">Candito et al. (2011) use word clustering for domain adaptation of a PCFG-LA parser for French, deriving clusters from a corpus containing text from both the source and target domains, and they obtain parsing improvements in both domains.</S>
    <S sid="156" ssid="19">We are not aware of previous work on the use of lexical generalization for improving out-of-domain dependency parsing.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="9">
    <S sid="157" ssid="1">We have investigated the use of probabilistic lexical target spaces for reducing lexical data sparseness in a transition-based dependency parser for French.</S>
    <S sid="158" ssid="2">We built a distributional thesaurus from an automatically-parsed large text corpus, using it to generate word clusters and perform WordNet ASR.</S>
    <S sid="159" ssid="3">We tested a standard approach to lexical generalization for parsing that has been previously explored, where a word is mapped to a single cluster or synset.</S>
    <S sid="160" ssid="4">We also introduced a novel probabilistic lexical generalization approach, where a lemma is represented by a categorical distribution over the space of lemmas, clusters, or synsets.</S>
    <S sid="161" ssid="5">Probabilities for the lemma space were calculated using the distributional thesaurus, and probabilities for the WordNet synset space were calculated using ASR sense prevalence scores, with probabilistic clusters left for future work.</S>
    <S sid="162" ssid="6">Our experiments with an arc-eager transitionbased dependency parser resulted in modest but significant improvements in LAS over the baseline when parsing out-of-domain medical text.</S>
    <S sid="163" ssid="7">However, we did not see statistically significant improvements over the baseline when parsing in-domain text or out-of-domain parliamentary text.</S>
    <S sid="164" ssid="8">An explanation for this result is that the French Treebank training set vocabulary has a very high lexical coverage over the evaluation sets in these domains, suggesting that lexical generalization does not provide much additional benefit.</S>
    <S sid="165" ssid="9">Comparing the standard single-mapping approach to the probabilistic generalization approach, we found a slightly (though not significantly) better performance for probabilistic generalization across different parsing configurations and evaluation sets.</S>
    <S sid="166" ssid="10">However, the probabilistic approach also has the downside of a slower running time.</S>
    <S sid="167" ssid="11">Based on the findings in this paper, our focus for future work on lexical generalization for dependency parsing is to continue improving parsing performance on out-of-domain text, specifically for those domains where lexical variation is high with respect to the training set.</S>
    <S sid="168" ssid="12">One possibility is to experiment with building a distributional thesaurus that uses text from both the source and target domains, similar to what Candito et al. (2011) did with Brown clustering, which may lead to a stronger bridging effect across domains for probabilistic lexical generalization methods.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="10">
    <S sid="169" ssid="1">This work was funded in part by the ANR project Sequoia ANR-08-EMER-013.</S>
  </SECTION>
</PAPER>
