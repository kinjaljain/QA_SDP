<PAPER>
  <S sid="0">Using Automatically Acquired Predominant Senses For Word Sense Disambiguation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
    <S sid="2" ssid="2">The first (or predominant) sense heuristic assumes the availability of handtagged data.</S>
    <S sid="3" ssid="3">Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently.</S>
    <S sid="4" ssid="4">In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text.</S>
    <S sid="5" ssid="5">We evaluate on the and English alldata.</S>
    <S sid="6" ssid="6">For accurate first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough.</S>
    <S sid="7" ssid="7">In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor many systems in</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al., 2004).</S>
    <S sid="9" ssid="2">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
    <S sid="10" ssid="3">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001).</S>
    <S sid="11" ssid="4">The first sense heuristic is a powerful one.</S>
    <S sid="12" ssid="5">Using the first sense listed in SemCor on the SENSEVAL-2 English all-words data we obtained the results given in table 1, (where the PoS was given by the gold-standard data in the SENSEVAL-2 data itself).</S>
    <S sid="13" ssid="6">1 Recall is lower than precision because there are many words which do not occur in SemCor.</S>
    <S sid="14" ssid="7">Use of the first sense listed in WordNet gives 65% precision and recall for all PoS on these items.</S>
    <S sid="15" ssid="8">The fourth column on table 1 gives the random baseline which reflects the polysemy of the data.</S>
    <S sid="16" ssid="9">Table 2 shows results obtained when we use the most common sense for an item and PoS using the frequency in the SENSEVAL-2 English all-words data itself.</S>
    <S sid="17" ssid="10">Recall is lower than precision since we only use the heuristic on lemmas which have occurred more than once and where there is one sense which has a greater frequency than the others, apart from trivial monosemous cases.</S>
    <S sid="18" ssid="11">2 Precision is higher in table 2 than in table 1 reflecting the difference between an a priori first sense determined by SemCor, and an upper bound on the performance of this heuristic for this data.</S>
    <S sid="19" ssid="12">This upper bound is quite high because of the very skewed sense distributions in the test data itself.</S>
    <S sid="20" ssid="13">The upper bound for a document, or document collection, will depend on how homogenous the content of that document collection is, and the skew of the word sense distributions therein.</S>
    <S sid="21" ssid="14">Indeed, the bias towards one sense for a given word in a given document or discourse was observed by Gale et al. (1992).</S>
    <S sid="22" ssid="15">Whilst a first sense heuristic based on a sensetagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available.</S>
    <S sid="23" ssid="16">SemCor comprises a relatively small sample of 250,000 words.</S>
    <S sid="24" ssid="17">There are words where the first sense in WordNet is counter-intuitive, because this is a small sample, and because where the frequency data does not indicate a first sense, the ordering is arbitrary.</S>
    <S sid="25" ssid="18">For example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage.</S>
    <S sid="26" ssid="19">Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts.</S>
    <S sid="27" ssid="20">However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.</S>
    <S sid="28" ssid="21">We are investigating a method of automatically ranking WordNet senses from raw text, with no reliance on manually sense-tagged data such as that in SemCor.</S>
    <S sid="29" ssid="22">The paper is structured as follows.</S>
    <S sid="30" ssid="23">We discuss our method in the following section.</S>
    <S sid="31" ssid="24">Section 3 describes an experiment using predominant senses acquired from the BNC evaluated on the SENSEVAL-2 English all-words task.</S>
    <S sid="32" ssid="25">In section 4 we present our results on the SENSEVAL-3 English all-words task.</S>
    <S sid="33" ssid="26">We discuss related work in section 5 and conclude in section 6.</S>
  </SECTION>
  <SECTION title="2 Method" number="2">
    <S sid="34" ssid="1">The method is described in (McCarthy et al., 2004), which we summarise here.</S>
    <S sid="35" ssid="2">We acquire thesauruses for nouns, verbs, adjectives and adverbs based on the method proposed by Lin (1998) using grammatical relations output from the RASP parser (Briscoe and Carroll, 2002).</S>
    <S sid="36" ssid="3">The grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts for a given PoS.</S>
    <S sid="37" ssid="4">We use the thesauruses for ranking the senses of the target words.</S>
    <S sid="38" ssid="5">Each target word ( ) e.g. plant in the thesaurus is associated with a list of nearest neighbours ( ) with distributional similarity scores ( ) e.g. factory 0.28, refinery 0.17, tree 0.14 etc... 3 Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
    <S sid="39" ssid="6">The neighbours reflect the various senses of the word ( ).</S>
    <S sid="40" ssid="7">We assume that the quantity and similarity of the neighbours pertaining to different senses will reflect the relative dominance of the senses.</S>
    <S sid="41" ssid="8">This is because there will be more relational data for the more prevalent senses compared to the less frequent senses.</S>
    <S sid="42" ssid="9">We relate the neighbours to these senses by a semantic similarity measure using the WordNet similarity package (Patwardhan and Pedersen, 2003) ( ), where the sense of the neighbour ( ) that maximises the similarity to is selected.</S>
    <S sid="43" ssid="10">The measure used for ranking the senses of a word is calculated using the distributional similarity scores of the neighbours weighted by the semantic similarity between the neighbour and the sense of the target word as shown in equation 1.</S>
    <S sid="44" ssid="11">The frequency data required by the semantic similarity measure (jcn (Jiang and Conrath, 1997)) is obtained using the BNC so that no hand-tagged data is used and our method is fully unsupervised. where: For SENSEVAL-3 we obtained thesaurus entries for all nouns, verbs, adjectives and adverbs using parsed text from the 90 million words of written English from the BNC.</S>
    <S sid="45" ssid="12">We created entries for words which occurred at least 10 times in frames involving the grammatical relations listed in table 3.</S>
    <S sid="46" ssid="13">We used 50 nearest neighbours for ranking, since this threshold has given good results in other experiments.</S>
  </SECTION>
  <SECTION title="3 Performance of the automatically acquired First sense on SENSEVAL-2" number="3">
    <S sid="47" ssid="1">We acquired sense rankings for polysemous nouns in WordNet 1.7.1 that occurred with 10 frames.</S>
    <S sid="48" ssid="2">This version was used in preparation for SENSEVAL-3.</S>
    <S sid="49" ssid="3">We then applied the predominant sense heuristic from the automatically acquired rankings to the SENSEVAL-2 data.</S>
    <S sid="50" ssid="4">4 Recall and precision figures are calculated using the SENSEVAL-2 scorer; recall is therefore particularly low for any given PoS in isolation since this is calculated over the entire corpus.</S>
    <S sid="51" ssid="5">The method produces lower results for verbs than for other PoS, this is in line with the lower performance of a manually acquired first sense heuristic and also reflects the greater polysemy of verbs shown by the lower random baseline as in tables 1 and 2.</S>
  </SECTION>
  <SECTION title="4 Results from SENSEVAL-3" number="4">
    <S sid="52" ssid="1">For SENSEVAL-3 we used the predominant senses from the automatic rankings for i) all PoS (autoPS) and ii) all PoS except verbs (autoPSNVs).</S>
    <S sid="53" ssid="2">The results are given in table 5.</S>
    <S sid="54" ssid="3">The &#8220;without U&#8221; results are used since the lack of a response by our system occurred when there were no nearest neighbours and so no ranking was available for selecting a predominant sense, rather than as an indication that the sense is missing from WordNet.</S>
    <S sid="55" ssid="4">Our system performs well in comparison with the results in SENSEVAL-2 for unsupervised systems which do not use manually labelled data such as SemCor.</S>
  </SECTION>
  <SECTION title="5 Related Work" number="5">
    <S sid="56" ssid="1">There is some related work on ranking the senses of words.</S>
    <S sid="57" ssid="2">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval.</S>
    <S sid="58" ssid="3">Buitelaar and Bogdan have evaluated their method on identifying domain specific concepts, rather than for WSD.</S>
    <S sid="59" ssid="4">In recent work, Lapata and Brew (2004) obtain predominant senses of verbs occurring in subcategorization frames, where the senses of verbs are defined using Levin classes (Levin, 1993).</S>
    <S sid="60" ssid="5">They demonstrate that these priors are useful for WSD of verbs.</S>
    <S sid="61" ssid="6">Our ranking method is related to work by Pantel and Lin (2002) who use automatic thesauruses for discovering word senses from corpora, rather than for detecting predominance.</S>
    <S sid="62" ssid="7">In their work, the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
    <S sid="63" ssid="8">They evaluate using a WordNet similarity measure to determine the precision and recall of these discovered classes with respect to WordNet synsets.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="64" ssid="1">We have demonstrated that it is possible to acquire predominant senses from raw textual corpora, and that these can be used as an unsupervised first sense heuristic that does not not rely on manually produced corpora such as SemCor.</S>
    <S sid="65" ssid="2">This approach is useful for words where there is no manually-tagged data available.</S>
    <S sid="66" ssid="3">Our predominant senses have been used within a WSD system as a back-off method when data is not available from other resources (Villarejo et al., 2004).</S>
    <S sid="67" ssid="4">The method could be particularly useful when tailoring a WSD system to a particular domain.</S>
    <S sid="68" ssid="5">We intend to experiment further using a wider variety of grammatical relations, which we hope will improve performance for verbs, and with data from larger corpora, such as the Gigaword corpus and the web, which should allow us to cover a great many more words which do not occur in manually created resources such as SemCor.</S>
    <S sid="69" ssid="6">We also intend to apply our method to domain specific text.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="70" ssid="1">We would like to thank Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package publically available.</S>
    <S sid="71" ssid="2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, and UK EPSRC project Robust Accurate Statistical Parsing (RASP).</S>
  </SECTION>
</PAPER>
