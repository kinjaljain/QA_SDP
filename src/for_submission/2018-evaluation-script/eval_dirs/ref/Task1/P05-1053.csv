Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,P05-1053,C08-1088,['15'],2005,['15'],dummy,dummy,['193'],"<S sid =""193"" ssid = ""75"">Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.</S>",['resultcitation']
3,P05-1053,C08-1088,['180'],"Zhou et al., 2005",['180'],dummy,dummy,"['51','52']","<S sid =""51"" ssid = ""7"">Moreover, we only apply the simple linear kernel, although other kernels can peform better.</S><S sid =""52"" ssid = ""8"">The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.</S>",['implicationcitation']
5,P05-1053,C10-1018,['42'],"Zhou et al., 2005",['42'],dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
6,P05-1053,C10-1018,['45'],"Zhou et al., 2005",['45'],dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
8,P05-1053,D07-1076,['14'],"Zhou et al., 2005",['14'],dummy,dummy,"['17','20']","<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S><S sid =""20"" ssid = ""20"">Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.</S>",['methodcitation']
9,P05-1053,D07-1076,['14'],Zhou et al 2005,['14'],dummy,dummy,"['196','197']","<S sid =""196"" ssid = ""78"">Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins’ parser used in our system achieves the state-of-the-art performance.</S><S sid =""197"" ssid = ""79"">Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.</S>",['implicationcitation']
10,P05-1053,D07-1076,['161'],Zhou et al 2005,['161'],dummy,dummy,"['51','52']","<S sid =""51"" ssid = ""7"">Moreover, we only apply the simple linear kernel, although other kernels can peform better.</S><S sid =""52"" ssid = ""8"">The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.</S>",['implicationcitation']
11,P05-1053,D07-1076,['167'],20 05,"['166','167']",dummy,dummy,['34'],"<S sid =""34"" ssid = ""5"">Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</S>",['methodcitation']
12,P05-1053,D07-1076,['177'],2005,"['176','177']",dummy,dummy,['33'],"<S sid =""33"" ssid = ""4"">Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</S>",['resultcitation']
14,P05-1053,D09-1149,['53'],"Zhou et al., 2005",['53'],dummy,dummy,"['60','17']","<S sid =""60"" ssid = ""3"">For each pair of mentions3, we compute various lexical, syntactic and semantic features.</S><S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
15,P05-1053,D09-1149,['81'],"Zhou et al., 2005",['81'],dummy,dummy,"['121','122']","<S sid =""121"" ssid = ""3"">ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.</S><S sid =""122"" ssid = ""4"">It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes “Based-In”, “Located” and “Residence” under the type “AT”, which are difficult even for human experts to differentiate.</S>",['implicationcitation']
16,P05-1053,D12-1074,['100'],"Zhou et al., 2005",['100'],dummy,dummy,['40'],"<S sid =""34"" ssid = ""5"">Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</S>",['resultcitation']
17,P05-1053,E06-2012,['59'],"Zelenko et al, 2003, Zhou et al, 2005",['59'],dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
18,P05-1053,E12-1020,['54'],2005,"['54','55']",dummy,dummy,"['17','20']","<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S><S sid =""20"" ssid = ""20"">Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.</S>","['methodcitation', 'resultcitation']"
19,P05-1053,E12-1020,['109'],"Zhou et al., 2005",['109'],dummy,dummy,"['37','40']","<S sid =""37"" ssid = ""8"">Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</S><S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
20,P05-1053,E12-1020,['225'],2005,"['224','225']",dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
21,P05-1053,I08-1004,['10'],Zhou et al 2005,['10'],dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
22,P05-1053,N06-1037,['9'],"Zhou et al., 2005",['9'],dummy,dummy,['193'],"<S sid =""193"" ssid = ""75"">Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.</S>",['implicationcitation']
23,P05-1053,N06-1037,['32'],2005,"['31','32']",dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
24,P05-1053,N06-1037,['36'],2005,"['35','36']",dummy,dummy,"['34','40']","<S sid =""34"" ssid = ""5"">Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</S><S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['resultcitation']
25,P05-1053,N06-1037,['39'],2005,"['38','39']",dummy,dummy,['193'],"<S sid =""193"" ssid = ""75"">Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.</S>",['methodcitation']
26,P05-1053,N06-1037,['136'],2005,"['135','136']",dummy,dummy,"['34','40']","<S sid =""34"" ssid = ""5"">Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</S><S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
27,P05-1053,N06-1037,['137'],"Zhou et al., 2005",['137'],dummy,dummy,['110'],"<S sid =""110"" ssid = ""3"">In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.</S>",['methodcitation']
28,P05-1053,N07-1015,['12'],"Zhou et al., 2005",['12'],dummy,dummy,['34'],"<S sid =""34"" ssid = ""5"">Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</S>",['methodcitation']
29,P05-1053,N07-1015,['34'],2005,"['33','34']",dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
31,P05-1053,N07-1015,['124'],2005,"['122','123','124']",dummy,dummy,"['67','68']","<S sid =""67"" ssid = ""10"">This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include",['methodcitation']
32,P05-1053,N07-1015,['132'],2005,"['130','131','132']",dummy,dummy,['93'],"<S sid =""93"" ssid = ""36"">This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.</S>",['methodcitation']
33,P05-1053,N09-3012,['7'],"Zhou et al., 2005",['7'],dummy,dummy,['20'],"<S sid =""20"" ssid = ""20"">Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.</S>",['resultcitation']
34,P05-1053,N13-1093,['108'],2005,"['107','108']",dummy,dummy,['51'],"<S sid =""51"" ssid = ""7"">Moreover, we only apply the simple linear kernel, although other kernels can peform better.</S>",['methodcitation']
35,P05-1053,N13-1093,['113'],2005,"['112','113']",dummy,dummy,['53'],"<S sid =""53"" ssid = ""9"">In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).</S>",['methodcitation']
36,P05-1053,P06-1016,['13'],Zhou et al 2005,['13'],dummy,dummy,['121'],"<S sid =""121"" ssid = ""3"">ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.</S>",['implicationcitation']
37,P05-1053,P06-1016,['18'],Zhou et al 2005,['18'],dummy,dummy,"['53','121']","<S sid =""53"" ssid = ""9"">In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).</S><S sid =""121"" ssid = ""3"">ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.</S>",['methodcitation']
39,P05-1053,P06-1016,['45'],,['45'],dummy,dummy,['130'],"<S sid =""130"" ssid = ""12"">It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.</S>",['resultcitation']
40,P05-1053,P06-1016,['128'],2005,['128'],dummy,dummy,"['110','58','59']","<S sid =""110"" ssid = ""3"">In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.</S><S sid =""58"" ssid = ""1"">The semantic relation is determined between two mentions.</S><S sid =""59"" ssid = ""2"">In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.</S>",['methodcitation']
43,P05-1053,P06-1017,['207'],2005,"['206','207']",dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
45,P05-1053,P06-1104,['39'],"Zhou et al., 2005",['39'],dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
47,P05-1053,P08-2023,['29'],2005,"['29','30']",dummy,dummy,"['18','155']","<S sid =""18"" ssid = ""18"">Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.</S><S sid =""155"" ssid = ""37"">This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.</S>","['resultcitation', 'implicationcitation']"
48,P05-1053,P09-1113,['39'],"Zhou et al., 2005",['39'],dummy,dummy,['18'],"<S sid =""18"" ssid = ""18"">Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.</S>",['resultcitation']
49,P05-1053,P09-1113,['52'],2005,"['50','51','52','53']",dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
50,P05-1053,P09-1114,['25'],2005,"['23','24','25']",dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
51,P05-1053,P09-1114,['7'],"Zhou et al., 2005",['7'],dummy,dummy,"['17','20']","<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S><S sid =""20"" ssid = ""20"">Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.</S>","['methodcitation', 'resultcitation']"
52,P05-1053,P11-1053,['10'],"Zhou et al., 2005",['10'],dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
53,P05-1053,P11-1053,['81'],2005,"['80','81']",dummy,dummy,"['40','52']","<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S><S sid =""52"" ssid = ""8"">The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.</S>",['methodcitation']
54,P05-1053,P11-1053,['193'],2005,"['193','194']",dummy,dummy,['108'],"<S sid =""108"" ssid = ""1"">This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.</S>",['methodcitation']
55,P05-1053,P11-1056,['11'],"Zhou et al., 2005",['11'],dummy,dummy,['59'],"<S sid =""59"" ssid = ""2"">In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.</S>",['methodcitation']
56,P05-1053,P11-1056,['58'],"Zhou et al., 2005",['58'],dummy,dummy,['128'],"<S sid =""128"" ssid = ""10"">In this paper, we only measure the performance of relation extraction on “true” mentions with “true” chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.</S>",['methodcitation']
57,P05-1053,P11-3012,['34'],"Zhou et al., 2005",['34'],dummy,dummy,['17'],"<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S>",['methodcitation']
58,P05-1053,P11-3012,['47'],"Zhou et al., 2005",['47'],dummy,dummy,['130'],"<S sid =""130"" ssid = ""12"">It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.</S>",['resultcitation']
59,P05-1053,P13-1147,['167'],"Zhou et al., 2005",['167'],dummy,dummy,"['17','126']","<S sid =""17"" ssid = ""17"">This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</S><S sid =""126"" ssid = ""8"">In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a “NONE” class for the case where the two mentions are not related.</S>",['methodcitation']
60,P05-1053,W06-1634,['9'],"Zhou et al., 2005",['9'],dummy,dummy,['53'],"<S sid =""53"" ssid = ""9"">In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).</S>",['methodcitation']
61,P05-1053,W06-1634,['17'],"Zhou et al., 2005",['17'],dummy,dummy,['4'],"<S sid =""4"" ssid = ""4"">This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</S>",['implicationcitation']
63,P05-1053,W06-1667,['174'],"Zhou et al., 2005",['174'],dummy,dummy,['193'],"<S sid =""193"" ssid = ""75"">Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.</S>",['resultcitation']
64,P05-1053,W08-0602,['39'],2005,['39'],dummy,dummy,['30'],"<S sid =""30"" ssid = ""1"">The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</S>",['aimcitation']
65,P05-1053,W08-0602,['136'],2005,['136'],dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
66,P05-1053,W11-1101,['203'],"Zhou et al., 2005",['203'],dummy,dummy,['40'],"<S sid =""40"" ssid = ""11"">This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.</S>",['methodcitation']
68,P05-1053,W11-1815,['10'],"Zhou et al., 2005",['10'],dummy,dummy,['12'],"<S sid =""12"" ssid = ""12"">Entities can be of five types",['hypothesiscitation']
