Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1058,C16-1060,'48',2010.0,'48',dummy,dummy,"['3','4','6']","<S sid =""3"" ssid = ""3"">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid =""4"" ssid = ""4"">This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.</S><S sid =""6"" ssid = ""6"">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S>",['methodcitation']
2,D10-1058,P13-2002,'24',"Zhao and Gildea, 2010",'24',dummy,dummy,"['18','26','33','34']","<S sid =""18"" ssid = ""18"">Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.</S><S sid =""26"" ssid = ""26"">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S><S sid =""33"" ssid = ""33"">Our model is much faster than IBM Model 4.</S><S sid =""34"" ssid = ""34"">In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.</S>",['methodcitation']
3,D10-1058,P13-2002,'33',"Zhao and Gildea, 2010",'33',dummy,dummy,"['6','46']","<S sid =""6"" ssid = ""6"">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S sid =""46"" ssid = ""46"">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S>",['methodcitation']
4,D10-1058,P13-2002,'43',"Zhao and Gildea, 2010",'43',dummy,dummy,['86'],"<S sid =""86"" ssid = ""1"">Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence",['methodcitation']
5,D10-1058,P13-2002,'82',"Zhao and Gildea, 2010",'82',dummy,dummy,"['88','90']","<S sid =""88"" ssid = ""3"">Our model has only one parameter for each target word, which can be learned more reliably.</S><S sid =""90"" ssid = ""5"">i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).</S>",['methodcitation']
6,D10-1058,P13-2002,'99',"Zhao and Gildea, 2010",'99',dummy,dummy,['110'],"<S sid =""110"" ssid = ""4"">1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.</S>",['methodcitation']
7,D10-1058,P59105CA,'45',15.0,'45',dummy,dummy,"['113','114','115']","<S sid =""113"" ssid = ""7"">This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm.</S><S sid =""114"" ssid = ""8"">However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.</S><S sid =""115"" ssid = ""9"">Instead, we do batch learning",['methodcitation']
8,D10-1058,P87-94,'79',Zhao,'79',dummy,dummy,"['14','26','27']","<S sid =""14"" ssid = ""14"">Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.</S><S sid =""26"" ssid = ""26"">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S><S sid =""27"" ssid = ""27"">It is easier to understand than IBM Model 4 (see Section 3).</S>",['methodcitation']
9,D10-1058,PBULLETIN,'104',2010.0,"'103','104'",dummy,dummy,"['29','30']","<S sid =""29"" ssid = ""29"">We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.</S><S sid =""30"" ssid = ""30"">Qc 2010 Association for Computational Linguistics estimation.</S>",['methodcitation']
10,D10-1058,PBULLETIN,'131',2010.0,'131',dummy,dummy,"['29','30','107','108']","<S sid =""29"" ssid = ""29"">We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.</S><S sid =""30"" ssid = ""30"">Qc 2010 Association for Computational Linguistics estimation.</S><S sid =""107"" ssid = ""1"">Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).</S><S sid =""108"" ssid = ""2"">The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.</S>",['methodcitation']
11,D10-1058,PCOLING-D10,'232',2010.0,'232',dummy,dummy,['3'],"<S sid =""3"" ssid = ""3"">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S>",['methodcitation']
12,D10-1058,PPROC-D10,'24',"Zhao and Gildea, 2010",'24',dummy,dummy,"['3','46']","<S sid =""3"" ssid = ""3"">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid =""46"" ssid = ""46"">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S>",['methodcitation']
13,D10-1058,PPROC-D10,'38',"Zhao and Gildea, 2010",'38',dummy,dummy,['86'],"<S sid =""86"" ssid = ""1"">Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence",['methodcitation']
14,D10-1058,PPROC-D10,'39',"Zhao and Gildea, 2010",'39',dummy,dummy,"['6','46']","<S sid =""6"" ssid = ""6"">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S sid =""46"" ssid = ""46"">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S>",['methodcitation']
15,D10-1058,PPROC-D10,'86',"Zhao and Gildea, 2010",'86',dummy,dummy,"['88','90']","<S sid =""88"" ssid = ""3"">Our model has only one parameter for each target word, which can be learned more reliably.</S><S sid =""90"" ssid = ""5"">i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).</S>",['methodcitation']
16,D10-1058,PPROC-D10,'104',"Zhao and Gildea, 2010",'104',dummy,dummy,['110'],"<S sid =""110"" ssid = ""4"">1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.</S>",['methodcitation']
17,D10-1058,Q13-1024,'60',"Zhao and Gildea, 2010",'60',dummy,dummy,"['3','4','5','6']","<S sid =""3"" ssid = ""3"">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid =""4"" ssid = ""4"">This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.</S><S sid =""5"" ssid = ""5"">It is similar in some ways to IBM Model 4, but is much easier to understand.</S><S sid =""6"" ssid = ""6"">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S>",['methodcitation']
18,D10-1058,Q13-1024,'130',"Zhao and Gildea, 2010",'130',dummy,dummy,['46'],"<S sid =""46"" ssid = ""46"">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S>",['methodcitation']
