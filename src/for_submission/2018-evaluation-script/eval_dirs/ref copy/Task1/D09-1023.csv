Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
3,D09-1023,D11-1044,'1',"Gimpel and Smith, 2009",'1',dummy,dummy,['17'],"<S ssid=""1"" sid=""81"">Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt, a | s, τs).</S><S ssid=""1"" sid=""2"">The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.</S><S ssid=""1"" sid=""90"">3 in Smith and Eisner (2006) for illustrations of the rest.)</S><S ssid=""1"" sid=""0"">Feature-Rich Translation by Quasi-Synchronous Lattice Parsing</S><S ssid=""1"" sid=""1"">We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.</S>",methodcitation
4,D09-1023,D11-1044,'16',"Gimpel and Smith, 2009",'16',dummy,dummy,['17'],"<S ssid=""1"" sid=""81"">Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt, a | s, τs).</S><S ssid=""1"" sid=""2"">The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.</S><S ssid=""1"" sid=""1"">We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.</S>",methodcitation
5,D09-1023,D11-1044,'23',"Gimpel and Smith, 2009",'23',dummy,dummy,['17'],"<S ssid=""1"" sid=""81"">Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt, a | s, τs).</S><S ssid=""1"" sid=""0"">Feature-Rich Translation by Quasi-Synchronous Lattice Parsing</S><S ssid=""1"" sid=""2"">The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.</S>",methodcitation
6,D09-1023,D11-1044,'41',"Gimpel and Smith, 2009",'41',dummy,dummy,['17'],"<S ssid=""1"" sid=""81"">Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt, a | s, τs).</S><S ssid=""1"" sid=""83"">We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Each word generated by Gs,τs is annotated with a “sense,” which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in τt and nodes in τs. In principle, any portion of τt may align to any portion of τs, but in practice we often make restrictions on the alignments to simplify computation.</S><S ssid=""1"" sid=""2"">The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.</S>",methodcitation
7,D09-1023,D11-1044,'146',"Gimpel and Smith, 2009",'146',dummy,dummy,['260'],"<S ssid=""1"" sid=""99"">It equates to finding the most probable derivation under the s/τs-specific grammar Gs,τs . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.</S>",methodcitation
9,D09-1023,PJOURNAL,'79',"Gimpel and Smith, 2009",'79',dummy,dummy,['204'],"<S ssid=""1"" sid=""25"">, m} → 2{1,...,n} θ source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where τs (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where τt (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s; ∅ denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (§2.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tj−N +1 ) language model features (§2.2): N -gram probabilities gsyn (t, τt ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (§2.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I ⊆ {1, . . .</S><S ssid=""1"" sid=""178"">We recently proposed “cube summing,” an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009).</S>",methodcitation
10,D09-1023,PJOURNAL,'139',"Gimpel and Smith, 2009",'139',dummy,dummy,['1'],"<S ssid=""1"" sid=""2"">The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.</S><S ssid=""1"" sid=""1"">We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.</S><S ssid=""1"" sid=""0"">Feature-Rich Translation by Quasi-Synchronous Lattice Parsing</S><S ssid=""1"" sid=""81"">Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt, a | s, τs).</S><S ssid=""1"" sid=""130"">Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in τt (or a deliberate choice is made by the decoder to translate it to NULL).</S>",methodcitation
12,D09-1023,PPROC-D09,'205',"Gimpel and Smith, 2009",'205',dummy,dummy,['123'],"<S ssid=""1"" sid=""104"">4.1 Translation as Monolingual Parsing.</S><S ssid=""1"" sid=""123"">Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.</S>",methodcitation
16,D09-1023,W10-1730,'17',"Gimpel and Smith, 2009",'17',dummy,dummy,['11'],"<S ssid=""1"" sid=""13"">We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.</S>",methodcitation
18,D09-1023,W11-2139,'37',"Gimpel and Smith, 2009",'37',dummy,dummy,['254'],"<S ssid=""1"" sid=""178"">We recently proposed “cube summing,” an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009).</S><S ssid=""1"" sid=""236"">In particular, we compare the effects of combining phrase features and syntactic features.</S>",methodcitation
20,D09-1023,N10-1040,'6',"Gimpel and Smith, 2009",'6',dummy,dummy,['191'],"<S ssid=""1"" sid=""185"">To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007).</S>",methodcitation
