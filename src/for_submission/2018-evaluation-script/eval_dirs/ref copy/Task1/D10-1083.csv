Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
3,D10-1083,D11-1059,['11'],2010,"['10','11']",dummy,dummy,['22'],"<S ssid=""1"" sid=""22"">In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.</S><S ssid=""1"" sid=""9"">Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.</S>",methodcitation
4,D10-1083,D11-1059,['17'],2010,"['16','17']",dummy,dummy,['239'],"<S ssid=""1"" sid=""194"">(2010) and the posterior regular- ization HMM of Grac¸a et al.</S><S ssid=""1"" sid=""211"">However, our full model takes advantage of word features not present in Grac¸a et al.</S>",methodcitation
6,D10-1083,D11-1059,['32'],"Lee et al., 2010",['32'],dummy,dummy,['52'],"<S ssid=""1"" sid=""20"">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S>",methodcitation
7,D10-1083,D11-1059,['91'],2010,"['90','91']",dummy,dummy,['112'],"<S ssid=""1"" sid=""20"">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S><S ssid=""1"" sid=""55"">Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.</S><S ssid=""1"" sid=""90"">While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al.</S><S ssid=""1"" sid=""19"">In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.</S>",methodcitation
8,D10-1083,D11-1059,['130'],2010,"['129','130']",dummy,dummy,['6'],"<S ssid=""1"" sid=""194"">(2010) and the posterior regular- ization HMM of Grac¸a et al.</S><S ssid=""1"" sid=""207"">We can only compare with Grac¸a et al.</S><S ssid=""1"" sid=""201"">While Berg-Kirkpatrick et al.</S><S ssid=""1"" sid=""208"">(2009) on Portuguese (Grac¸a et al.</S><S ssid=""1"" sid=""196"">The system of Berg-Kirkpatrick et al.</S>",methodcitation
9,D10-1083,D12-1086,['74'],"Lee et al., 2010",['74'],dummy,dummy,['9'],"<S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S><S ssid=""1"" sid=""1"">Part-of-speech (POS) tag distributions are known to exhibit sparsity — a word is likely to take a single predominant tag in a corpus.</S>",methodcitation
10,D10-1083,D12-1125,['213'],"Lee et al., 2010",['213'],dummy,dummy,['52'],"<S ssid=""1"" sid=""194"">(2010) and the posterior regular- ization HMM of Grac¸a et al.</S><S ssid=""1"" sid=""52"">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S>",methodcitation
11,D10-1083,D12-1127,['13'],"Lee et al., 2010",['13'],dummy,dummy,['52'],"<S ssid=""1"" sid=""193"">Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.</S>",methodcitation
12,D10-1083,D13-1004,['27'],"Lee et al., 2010",['27'],dummy,dummy,['243'],"<S ssid=""1"" sid=""193"">Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.</S><S ssid=""1"" sid=""201"">While Berg-Kirkpatrick et al.</S>",methodcitation
13,D10-1083,N12-1045,['14'],"Lee et al., 2010",['14'],dummy,dummy,['243'],"<S ssid=""1"" sid=""193"">Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.</S><S ssid=""1"" sid=""52"">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S><S ssid=""1"" sid=""201"">While Berg-Kirkpatrick et al.</S>",methodcitation
14,D10-1083,P11-1087,['41'],2010,"['40','41','42','43']",dummy,dummy,"['27','85','97']","<S ssid=""1"" sid=""84"">Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint.</S><S ssid=""1"" sid=""20"">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S><S ssid=""1"" sid=""177"">encodes the one tag per word constraint and is uni form over type-level tag assignments.</S><S ssid=""1"" sid=""25"">This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007).</S><S ssid=""1"" sid=""55"">Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.</S>",methodcitation
15,D10-1083,P11-1087,['153'],2010,"['152','153','154']",dummy,dummy,['155'],"<S ssid=""1"" sid=""207"">We can only compare with Grac¸a et al.</S>",methodcitation
16,D10-1083,P13-1150,['55'],"Lee et al., 2010",['55'],dummy,dummy,['236'],"<S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S><S ssid=""1"" sid=""12"">Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.</S>",methodcitation
17,D10-1083,W11-0301,['102'],"Lee et al., 2010",['102'],dummy,dummy,"['20','21']","<S ssid=""1"" sid=""56"">Conditioned on T , features of word types W are drawn.</S><S ssid=""1"" sid=""76"">The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters θ.</S><S ssid=""1"" sid=""71"">Uniform Tag Prior (1TW) Our initial lexicon component will be uniform over possible tag assignments as well as word types.</S><S ssid=""1"" sid=""73"">P St = n. β T VARIABLES ψ Y W : Word types (W1 ,.</S><S ssid=""1"" sid=""60"">Once HMM parameters (θ, φ) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from φ.</S>",methodcitation
18,D10-1083,W12-1914,['8'],2010,"['7','8']",dummy,dummy,['236'],"<S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S><S ssid=""1"" sid=""205"">Our second point of comparison is with Grac¸a et al.</S><S ssid=""1"" sid=""12"">Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.</S>",methodcitation
