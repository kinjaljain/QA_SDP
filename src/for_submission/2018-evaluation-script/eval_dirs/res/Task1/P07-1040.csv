Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,P07-1040,C08-1014,'32',"Rosti et al.,2007b",'32',dummy,dummy,"'17','46','153','14','80'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""46"">Section 3 describes confusion network decoding for MT system combination.</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S><S ssid=""1"" sid=""80"">Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.</S>",methodcitation
3,P07-1040,C08-1014,'92',"Rosti et al.,2007b","'88','89','90','91','92'",dummy,dummy,"'32','94','31','89','99'","<S ssid=""1"" sid=""32"">The average TER score was computed between each systemâ€™s -best hypothesis and all other hypotheses.</S><S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S><S ssid=""1"" sid=""31"">Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007).</S><S ssid=""1"" sid=""89"">The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.</S><S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S>",methodcitation
4,P07-1040,C08-1014,'97',"Rosti et al.,2007b","'93','94','95','96','97'",dummy,dummy,"'19','86','31','17','64'","<S ssid=""1"" sid=""19"">In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.</S><S ssid=""1"" sid=""86"">The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.</S><S ssid=""1"" sid=""31"">Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007).</S><S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""64"">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S>",methodcitation
5,P07-1040,D09-1115,'7',"Rosti et al.,2007a",'7',dummy,dummy,"'17','153','14','34','44'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S><S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""44"">Tuning is fully automatic, as opposed to (Matusov et al., 2006) where global system weights were set manually.This paper is organized as follows.</S>",methodcitation
6,P07-1040,D09-1115,"'133', '134'","Rosti et al.,2007a","'133','134'",dummy,dummy,"'111','99','140','107','106'","<S ssid=""1"" sid=""111"">First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.</S><S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S><S ssid=""1"" sid=""140"">A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.</S><S ssid=""1"" sid=""107"">If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty.</S><S ssid=""1"" sid=""106"">Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.</S>",methodcitation
7,P07-1040,D09-1115,'148',"Rosti et al.,2007a","'147','148'",dummy,dummy,"'34','106','99'","<S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""106"">Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.</S><S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S>",methodcitation
8,P07-1040,N09-2003,"'24', '25'",2007.0,"'24','25'",dummy,dummy,"'17','14','153','1','46'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S><S ssid=""1"" sid=""1"">Recently, confusion network decoding has been applied in machine translation system combination.</S><S ssid=""1"" sid=""46"">Section 3 describes confusion network decoding for MT system combination.</S>",methodcitation
9,P07-1040,P08-2021,"'2', '3'",2007.0,"'2','3'",dummy,dummy,"'200','64'","<S ssid=""1"" sid=""200"">Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.</S><S ssid=""1"" sid=""64"">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S>",methodcitation
10,P07-1040,P08-2021,"'23', '24'",2007.0,"'23','24'",dummy,dummy,"'86','64'","<S ssid=""1"" sid=""86"">The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.</S><S ssid=""1"" sid=""64"">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S>",methodcitation
11,P07-1040,P08-2021,"'29', '30'",2007.0,"'28','29','30'",dummy,dummy,"'34','67'","<S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""67"">Confusion network decoding usually requires finding the path with the highest confidence in the network.</S>",methodcitation
12,P07-1040,P08-2021,"'69', '70'",2007.0,"'69','70'",dummy,dummy,"'200','37'","<S ssid=""1"" sid=""200"">Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.</S><S ssid=""1"" sid=""37"">All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.</S>",methodcitation
13,P07-1040,P08-2021,"'78', '79'",2007.0,"'78','79'",dummy,dummy,"'94','34','14','139','172'","<S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S><S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S><S ssid=""1"" sid=""139"">The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991).</S><S ssid=""1"" sid=""172"">As expected, the scores on the metric used in tuning are the best on that metric.</S>",methodcitation
14,P07-1040,P11-1125,"'47', '48'",2007b,"'46','47','48'",dummy,dummy,'99',"<S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S>",methodcitation
15,P07-1040,P11-1125,'127',2007b,'127',dummy,dummy,"'99','135','115','17','14'","<S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S><S ssid=""1"" sid=""135"">This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).</S><S ssid=""1"" sid=""115"">The number of paths through a confusion network grows exponentially with the number of nodes.</S><S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S>",methodcitation
16,P07-1040,P39-P07,"'24', '25'",2007.0,"'24','25'",dummy,dummy,"'17','14','153','1','46'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S><S ssid=""1"" sid=""1"">Recently, confusion network decoding has been applied in machine translation system combination.</S><S ssid=""1"" sid=""46"">Section 3 describes confusion network decoding for MT system combination.</S>",methodcitation
17,P07-1040,P101121-P07,"'20', '21'",2007.0,"'19' 20','21'",dummy,dummy,"'34','153'","<S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S>",methodcitation
18,P07-1040,PLING-P07,'13',2007.0,"'12','13'",dummy,dummy,"'17','14','153','1','206'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S><S ssid=""1"" sid=""1"">Recently, confusion network decoding has been applied in machine translation system combination.</S><S ssid=""1"" sid=""206"">The improved confusion network decoding approach allows arbitrary features to be used in the combination.</S>",methodcitation
19,P07-1040,PLING-P07,'41',2007.0,'41',dummy,dummy,"'94','34'","<S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S><S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S>",methodcitation
20,P07-1040,PSEM-P07,'13',Rosti et al. 2007,'13',dummy,dummy,"'92','3'","<S ssid=""1"" sid=""92"">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S><S ssid=""1"" sid=""3"">This paper describes an improved confusion network based method to combine outputs from multiple MT systems.</S>",methodcitation
21,P07-1040,PSEM-P07,'16',Rosti et al. 2007,'116',dummy,dummy,"'92','27','17','173','2'","<S ssid=""1"" sid=""92"">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S><S ssid=""1"" sid=""27"">Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).</S><S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""173"">Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning.</S><S ssid=""1"" sid=""2"">Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs.</S>",methodcitation
22,P07-1040,W08-0329,'7',Rosti et al. 2007,'7',dummy,dummy,'19',"<S ssid=""1"" sid=""19"">In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.</S>",methodcitation
23,P07-1040,W08-0329,'17',Rosti et al. 2007,'17',dummy,dummy,"'158','37'","<S ssid=""1"" sid=""158"">The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration.</S><S ssid=""1"" sid=""37"">All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.</S>",methodcitation
24,P07-1040,W08-0329,'39',Rosti et al. 2007,'39',dummy,dummy,"'65','108','94','200','34'","<S ssid=""1"" sid=""65"">Each arc represents an alternative word at that.</S><S ssid=""1"" sid=""108"">Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.</S><S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S><S ssid=""1"" sid=""200"">Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.</S><S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S>",methodcitation
25,P07-1040,W08-0329,'88',Rosti et al. 2007,"'87','88'",dummy,dummy,'189',"<S ssid=""1"" sid=""189"">The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.</S>",methodcitation
26,P07-1040,W09-0441,'36',"Rosti et al.,2007","'35','36'",dummy,dummy,'196',"<S ssid=""1"" sid=""196"">Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.</S>",methodcitation
