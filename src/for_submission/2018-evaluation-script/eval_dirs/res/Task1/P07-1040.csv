Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,P07-1040,C08-1014,'32',"Rosti et al.,2007b",'32',dummy,dummy,"'17','46'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""46"">Section 3 describes confusion network decoding for MT system combination.</S>",methodcitation
3,P07-1040,C08-1014,'92',"Rosti et al.,2007b","'88','89','90','91','92'",dummy,dummy,"'32','94'","<S ssid=""1"" sid=""32"">The average TER score was computed between each systemâ€™s -best hypothesis and all other hypotheses.</S><S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S>",methodcitation
4,P07-1040,C08-1014,'97',"Rosti et al.,2007b","'93','94','95','96','97'",dummy,dummy,"'19','86'","<S ssid=""1"" sid=""19"">In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.</S><S ssid=""1"" sid=""86"">The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.</S>",methodcitation
5,P07-1040,D09-1115,'7',"Rosti et al.,2007a",'7',dummy,dummy,"'17','153'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S>",methodcitation
6,P07-1040,D09-1115,"'133', '134'","Rosti et al.,2007a","'133','134'",dummy,dummy,"'111','99'","<S ssid=""1"" sid=""111"">First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.</S><S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S>",methodcitation
7,P07-1040,D09-1115,'148',"Rosti et al.,2007a","'147','148'",dummy,dummy,"'34','106'","<S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""106"">Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.</S>",methodcitation
8,P07-1040,N09-2003,"'24', '25'",2007.0,"'24','25'",dummy,dummy,"'17','14'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S>",methodcitation
9,P07-1040,P08-2021,"'2', '3'",2007.0,"'2','3'",dummy,dummy,"'200','64'","<S ssid=""1"" sid=""200"">Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.</S><S ssid=""1"" sid=""64"">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S>",methodcitation
10,P07-1040,P08-2021,"'23', '24'",2007.0,"'23','24'",dummy,dummy,"'86','64'","<S ssid=""1"" sid=""86"">The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.</S><S ssid=""1"" sid=""64"">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S>",methodcitation
11,P07-1040,P08-2021,"'29', '30'",2007.0,"'28','29','30'",dummy,dummy,"'34','67'","<S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""67"">Confusion network decoding usually requires finding the path with the highest confidence in the network.</S>",methodcitation
12,P07-1040,P08-2021,"'69', '70'",2007.0,"'69','70'",dummy,dummy,"'200','37'","<S ssid=""1"" sid=""200"">Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.</S><S ssid=""1"" sid=""37"">All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.</S>",methodcitation
13,P07-1040,P08-2021,"'78', '79'",2007.0,"'78','79'",dummy,dummy,"'94','34'","<S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S><S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S>",methodcitation
14,P07-1040,P11-1125,"'47', '48'",2007b,"'46','47','48'",dummy,dummy,"'99','92'","<S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S><S ssid=""1"" sid=""92"">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S>",methodcitation
15,P07-1040,P11-1125,'127',2007b,'127',dummy,dummy,"'99','135'","<S ssid=""1"" sid=""99"">In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.</S><S ssid=""1"" sid=""135"">This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).</S>",methodcitation
16,P07-1040,P39-P07,"'24', '25'",2007.0,"'24','25'",dummy,dummy,"'17','14'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S>",methodcitation
17,P07-1040,P101121-P07,"'20', '21'",2007.0,"'19' 20','21'",dummy,dummy,"'34','153'","<S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S><S ssid=""1"" sid=""153"">The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.</S>",methodcitation
18,P07-1040,PLING-P07,'13',2007.0,"'12','13'",dummy,dummy,"'17','14'","<S ssid=""1"" sid=""17"">Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).</S><S ssid=""1"" sid=""14"">In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.</S>",methodcitation
19,P07-1040,PLING-P07,'41',2007.0,'41',dummy,dummy,"'94','34'","<S ssid=""1"" sid=""94"">In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.</S><S ssid=""1"" sid=""34"">This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.</S>",methodcitation
20,P07-1040,PSEM-P07,'13',Rosti et al. 2007,'13',dummy,dummy,"'92','3'","<S ssid=""1"" sid=""92"">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S><S ssid=""1"" sid=""3"">This paper describes an improved confusion network based method to combine outputs from multiple MT systems.</S>",methodcitation
21,P07-1040,PSEM-P07,'16',Rosti et al. 2007,'116',dummy,dummy,"'92','27'","<S ssid=""1"" sid=""92"">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S><S ssid=""1"" sid=""27"">Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).</S>",methodcitation
22,P07-1040,W08-0329,'7',Rosti et al. 2007,'7',dummy,dummy,"'19','86'","<S ssid=""1"" sid=""19"">In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.</S><S ssid=""1"" sid=""86"">The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.</S>",methodcitation
23,P07-1040,W08-0329,'17',Rosti et al. 2007,'17',dummy,dummy,"'158','37'","<S ssid=""1"" sid=""158"">The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration.</S><S ssid=""1"" sid=""37"">All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.</S>",methodcitation
24,P07-1040,W08-0329,'39',Rosti et al. 2007,'39',dummy,dummy,"'65','108'","<S ssid=""1"" sid=""65"">Each arc represents an alternative word at that.</S><S ssid=""1"" sid=""108"">Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.</S>",methodcitation
25,P07-1040,W08-0329,'88',Rosti et al. 2007,"'87','88'",dummy,dummy,"'189','175'","<S ssid=""1"" sid=""189"">The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.</S><S ssid=""1"" sid=""175"">This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR Ch in es e tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 56 .5 6 55 .8 8 58 .3 5 57 .0 9 57 .6 9 56 .1 1 29 .3 9 30 .4 5 32 .8 8 36 .1 8 33 .8 5 36 .6 4 54 .5 4 54 .3 6 56 .7 2 57 .1 1 58 .2 8 58 .9 0 no we ig hts ba sel in e 53 .1 1 53 .4 0 37 .7 7 38 .5 2 59 .1 9 59 .5 6 T E R t u n e d B L E U t u n e d M T R t u n e d 52 .1 3 53 .0 3 70 .2 7 36 .8 7 39 .9 9 28 .6 0 57 .3 0 58 .9 7 63 .1 0 Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04.</S>",methodcitation
26,P07-1040,W09-0441,'36',"Rosti et al.,2007","'35','36'",dummy,dummy,"'92','160'","<S ssid=""1"" sid=""92"">For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).</S><S ssid=""1"" sid=""160"">All four reference translations available for the tuning and test sets were used.</S>",methodcitation
