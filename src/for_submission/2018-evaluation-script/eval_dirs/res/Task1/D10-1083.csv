Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
3,D10-1083,D11-1059,['11'],2010,"['10','11']",dummy,dummy,"'22','236'","<S ssid=""1"" sid=""22"">In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.</S><S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S>",methodcitation
4,D10-1083,D11-1059,['17'],2010,"['16','17']",dummy,dummy,"'194','211'","<S ssid=""1"" sid=""194"">(2010) and the posterior regular- ization HMM of Grac¸a et al.</S><S ssid=""1"" sid=""211"">However, our full model takes advantage of word features not present in Grac¸a et al.</S>",methodcitation
6,D10-1083,D11-1059,['32'],"Lee et al., 2010",['32'],dummy,dummy,"'176','20'","<S ssid=""1"" sid=""176"">We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-.</S><S ssid=""1"" sid=""20"">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S>",methodcitation
7,D10-1083,D11-1059,['91'],2010,"['90','91']",dummy,dummy,"'20','55'","<S ssid=""1"" sid=""20"">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S><S ssid=""1"" sid=""55"">Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.</S>",methodcitation
8,D10-1083,D11-1059,['130'],2010,"['129','130']",dummy,dummy,"'194','207'","<S ssid=""1"" sid=""194"">(2010) and the posterior regular- ization HMM of Grac¸a et al.</S><S ssid=""1"" sid=""207"">We can only compare with Grac¸a et al.</S>",methodcitation
9,D10-1083,D12-1086,['74'],"Lee et al., 2010",['74'],dummy,dummy,"'236','1'","<S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S><S ssid=""1"" sid=""1"">Part-of-speech (POS) tag distributions are known to exhibit sparsity — a word is likely to take a single predominant tag in a corpus.</S>",methodcitation
10,D10-1083,D12-1125,['213'],"Lee et al., 2010",['213'],dummy,dummy,"'194','52'","<S ssid=""1"" sid=""194"">(2010) and the posterior regular- ization HMM of Grac¸a et al.</S><S ssid=""1"" sid=""52"">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S>",methodcitation
11,D10-1083,D12-1127,['13'],"Lee et al., 2010",['13'],dummy,dummy,"'193','201'","<S ssid=""1"" sid=""193"">Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.</S><S ssid=""1"" sid=""201"">While Berg-Kirkpatrick et al.</S>",methodcitation
12,D10-1083,D13-1004,['27'],"Lee et al., 2010",['27'],dummy,dummy,"'193','201'","<S ssid=""1"" sid=""193"">Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.</S><S ssid=""1"" sid=""201"">While Berg-Kirkpatrick et al.</S>",methodcitation
13,D10-1083,N12-1045,['14'],"Lee et al., 2010",['14'],dummy,dummy,"'193','52'","<S ssid=""1"" sid=""193"">Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.</S><S ssid=""1"" sid=""52"">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S>",methodcitation
14,D10-1083,P11-1087,['41'],2010,"['40','41','42','43']",dummy,dummy,"'84','20'","<S ssid=""1"" sid=""84"">Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint.</S><S ssid=""1"" sid=""20"">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S>",methodcitation
15,D10-1083,P11-1087,['153'],2010,"['152','153','154']",dummy,dummy,"'207','211'","<S ssid=""1"" sid=""207"">We can only compare with Grac¸a et al.</S><S ssid=""1"" sid=""211"">However, our full model takes advantage of word features not present in Grac¸a et al.</S>",methodcitation
16,D10-1083,P13-1150,['55'],"Lee et al., 2010",['55'],dummy,dummy,"'236','12'","<S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S><S ssid=""1"" sid=""12"">Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.</S>",methodcitation
17,D10-1083,W11-0301,['102'],"Lee et al., 2010",['102'],dummy,dummy,"'56','76'","<S ssid=""1"" sid=""56"">Conditioned on T , features of word types W are drawn.</S><S ssid=""1"" sid=""76"">The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters θ.</S>",methodcitation
18,D10-1083,W12-1914,['8'],2010,"['7','8']",dummy,dummy,"'236','205'","<S ssid=""1"" sid=""236"">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S><S ssid=""1"" sid=""205"">Our second point of comparison is with Grac¸a et al.</S>",methodcitation
