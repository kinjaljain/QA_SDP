Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
3,P06-2124,D11-1084,['53'],2006,"['52','53','54']",dummy,dummy,"'2','58','61','80','70'","<S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S ssid=""1"" sid=""58"">The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair.</S><S ssid=""1"" sid=""61"">Given a specific topic-weight vector θd for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.</S><S ssid=""1"" sid=""80"">Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.</S><S ssid=""1"" sid=""70"">Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair.</S>",methodcitation
5,P06-2124,P07-1066,['28'],"Zhao and Xing, 2006","['28','29']",dummy,dummy,"'21','243','0','68','71'","<S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""243"">In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.</S><S ssid=""1"" sid=""0"">BiTAM: Bilingual Topic AdMixture Models forWord Alignment</S><S ssid=""1"" sid=""68"">(a) Sample sentence-length Jn from Poisson(δ); (b) Sample a topic zdn from a Multinomial(θd ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far.</S><S ssid=""1"" sid=""71"">The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).</S>",methodcitation
7,P06-2124,P10-2025,['84'],"Zhao and Xing, 2006",['84'],dummy,dummy,'189',"<S ssid=""1"" sid=""189"">With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3.</S>",methodcitation
8,P06-2124,P10-2025,['86'],"Zhao and Xing, 2006",['86'],dummy,dummy,'202',"<S ssid=""1"" sid=""202"">Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments.</S>",methodcitation
9,P06-2124,P11-2032,['11'],2006,['11'],dummy,dummy,'5',"<S ssid=""1"" sid=""5"">Efficient variational approximation algorithms are designed for inference and parameter estimation.</S>",methodcitation
10,P06-2124,P12-1048,['17'],"Zhao and Xing, 2006",['17'],dummy,dummy,'27',"<S ssid=""1"" sid=""27"">Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.</S>",methodcitation
11,P06-2124,P12-1048,['149'],2006,['149'],dummy,dummy,"'2','3','1','21','43'","<S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S ssid=""1"" sid=""3"">Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).</S><S ssid=""1"" sid=""1"">We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.</S><S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""43"">Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.</S>",methodcitation
12,P06-2124,P12-1048,['160'],"Zhao and Xing, 2006",['160'],dummy,dummy,"'2','25'","<S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S ssid=""1"" sid=""25"">Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent.</S>",methodcitation
13,P06-2124,P12-1079,['8'],"Zhao and Xing, 2006",['8'],dummy,dummy,"'7','245','78','8','17'","<S ssid=""1"" sid=""7"">Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</S><S ssid=""1"" sid=""245"">The proposed models significantly improve the alignment accuracy and lead to better translation qualities.</S><S ssid=""1"" sid=""78"">The word level translation lexicon probabil- r ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }.</S><S ssid=""1"" sid=""8"">Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models.</S><S ssid=""1"" sid=""17"">Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.</S>",methodcitation
14,P06-2124,P12-1079,['40'],"Zhao and Xing, 2006",['40'],dummy,dummy,"'27','97'","<S ssid=""1"" sid=""27"">Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.</S><S ssid=""1"" sid=""97"">n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics.</S>",methodcitation
15,P06-2124,P12-2023,['30'],"Zhao and Xing, 2006",['30'],dummy,dummy,"'21','0','243','1'","<S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""0"">BiTAM: Bilingual Topic AdMixture Models forWord Alignment</S><S ssid=""1"" sid=""243"">In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.</S><S ssid=""1"" sid=""1"">We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.</S>",methodcitation
16,P06-2124,P13-2122,['27'],"Zhao and Xing, 2006",['27'],dummy,dummy,"'21','60'","<S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""60"">a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic.</S>",methodcitation
17,P06-2124,W07-0722,['13'],"Zhao and Xing, 2006","['13','14','15']",dummy,dummy,"'7','21','243','1','2'","<S ssid=""1"" sid=""7"">Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</S><S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""243"">In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.</S><S ssid=""1"" sid=""1"">We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.</S><S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S>",methodcitation
18,P06-2124,W07-0722,['62'],"Zhao and Xing, 2006",['62'],dummy,dummy,"'119','118'","<S ssid=""1"" sid=""119"">Second: interpolation smoothing.</S><S ssid=""1"" sid=""118"">In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.</S>",methodcitation
