Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
3,P06-2124,D11-1084,['53'],2006,"['52','53','54']",dummy,dummy,"'2','58'","<S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S ssid=""1"" sid=""58"">The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair.</S>",methodcitation
5,P06-2124,P07-1066,['28'],"Zhao and Xing, 2006","['28','29']",dummy,dummy,"'21','243'","<S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""243"">In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.</S>",methodcitation
7,P06-2124,P10-2025,['84'],"Zhao and Xing, 2006",['84'],dummy,dummy,"'189','66'","<S ssid=""1"" sid=""189"">With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3.</S><S ssid=""1"" sid=""66"">3.</S>",methodcitation
8,P06-2124,P10-2025,['86'],"Zhao and Xing, 2006",['86'],dummy,dummy,"'202','211'","<S ssid=""1"" sid=""202"">Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments.</S><S ssid=""1"" sid=""211"">As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic.</S>",methodcitation
9,P06-2124,P11-2032,['11'],2006,['11'],dummy,dummy,"'5','174'","<S ssid=""1"" sid=""5"">Efficient variational approximation algorithms are designed for inference and parameter estimation.</S><S ssid=""1"" sid=""174"">To estimate B, β (for BiTAM2) and α, at most eight variational EM iterations are run on the training data.</S>",methodcitation
10,P06-2124,P12-1048,['17'],"Zhao and Xing, 2006",['17'],dummy,dummy,"'16','27'","<S ssid=""1"" sid=""16"">With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous.</S><S ssid=""1"" sid=""27"">Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.</S>",methodcitation
11,P06-2124,P12-1048,['149'],2006,['149'],dummy,dummy,"'2','3'","<S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S ssid=""1"" sid=""3"">Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).</S>",methodcitation
12,P06-2124,P12-1048,['160'],"Zhao and Xing, 2006",['160'],dummy,dummy,"'2','25'","<S ssid=""1"" sid=""2"">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S ssid=""1"" sid=""25"">Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent.</S>",methodcitation
13,P06-2124,P12-1079,['8'],"Zhao and Xing, 2006",['8'],dummy,dummy,"'7','245'","<S ssid=""1"" sid=""7"">Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</S><S ssid=""1"" sid=""245"">The proposed models significantly improve the alignment accuracy and lead to better translation qualities.</S>",methodcitation
14,P06-2124,P12-1079,['40'],"Zhao and Xing, 2006",['40'],dummy,dummy,"'27','97'","<S ssid=""1"" sid=""27"">Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.</S><S ssid=""1"" sid=""97"">n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics.</S>",methodcitation
15,P06-2124,P12-2023,['30'],"Zhao and Xing, 2006",['30'],dummy,dummy,"'21','0'","<S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""0"">BiTAM: Bilingual Topic AdMixture Models forWord Alignment</S>",methodcitation
16,P06-2124,P13-2122,['27'],"Zhao and Xing, 2006",['27'],dummy,dummy,"'21','60'","<S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S ssid=""1"" sid=""60"">a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic.</S>",methodcitation
17,P06-2124,W07-0722,['13'],"Zhao and Xing, 2006","['13','14','15']",dummy,dummy,"'7','21'","<S ssid=""1"" sid=""7"">Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</S><S ssid=""1"" sid=""21"">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S>",methodcitation
18,P06-2124,W07-0722,['62'],"Zhao and Xing, 2006",['62'],dummy,dummy,"'119','118'","<S ssid=""1"" sid=""119"">Second: interpolation smoothing.</S><S ssid=""1"" sid=""118"">In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.</S>",methodcitation
