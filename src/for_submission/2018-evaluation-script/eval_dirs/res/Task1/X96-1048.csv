Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
4,X96-1048,J00-4003,['20'],Sundheim 1995,"['12','20']",dummy,dummy,"'2','125'","<S ssid=""1"" sid=""2"">The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.</S><S ssid=""1"" sid=""125"">About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks.</S>",methodcitation
5,X96-1048,J00-4003,['72'],Sundheim 1995,"['71','72']",dummy,dummy,"'210','6'","<S ssid=""1"" sid=""210"">The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.</S><S ssid=""1"" sid=""6"">The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.</S>",methodcitation
6,X96-1048,W97-1307,['29'],"Sund heim, 1995","['29','30']",dummy,dummy,"'103','233'","<S ssid=""1"" sid=""103"">COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe.</S><S ssid=""1"" sid=""233"">For MUC6, text filtering scores were as high as 98% recall (with precision in the 80th percentile) or 96% precision (with recall in the 80th percentile).</S>",methodcitation
7,X96-1048,P06-1059,['10'],"Sundheim, 1995",['10'],dummy,dummy,"'49','68'","<S ssid=""1"" sid=""49"">As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.</S><S ssid=""1"" sid=""68"">As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.</S>",methodcitation
8,X96-1048,C04-1126,['49'],"Sundheim, 1995","['49','50','51']",dummy,dummy,"'244','131'","<S ssid=""1"" sid=""244"">Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the ""key"" and the other annotator's templates were treated as the ""response"".</S><S ssid=""1"" sid=""131"">The amount of agreement between the two annotators was found to be 80% recall and 82% precision.</S>",methodcitation
9,X96-1048,C04-1126,['34'],"Sundheim, 1995",['34'],dummy,dummy,"'227','27'","<S ssid=""1"" sid=""227"">Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.</S><S ssid=""1"" sid=""27"">CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.</S>",methodcitation
10,X96-1048,W99-0612,['142'],Sundheim 1995,"['141','142']",dummy,dummy,"'51','49'","<S ssid=""1"" sid=""51"">It was also unexpected that one of the systems would match human performance on the task.</S><S ssid=""1"" sid=""49"">As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.</S>",methodcitation
11,X96-1048,E99-1001,['17'],1995,['17'],dummy,dummy,"'72','193'","<S ssid=""1"" sid=""72"">Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..</S><S ssid=""1"" sid=""193"">There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.</S>",methodcitation
12,X96-1048,M98-1003,['3'],Sundheim1995,['3'],dummy,dummy,"'246','245'","<S ssid=""1"" sid=""246"">The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.</S><S ssid=""1"" sid=""245"">No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.</S>",methodcitation
