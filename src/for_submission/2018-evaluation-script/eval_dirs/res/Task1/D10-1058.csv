Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,D10-1058,P13-2002,'24',"Zhao and Gildea, 2010",'24',dummy,dummy,"'0','35'","<S ssid=""1"" sid=""0"">A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC</S><S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S>",methodcitation
3,D10-1058,P13-2002,'33',"Zhao and Gildea, 2010",'33',dummy,dummy,"'48','199'","<S ssid=""1"" sid=""48"">(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.</S><S ssid=""1"" sid=""199"">The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.</S>",methodcitation
4,D10-1058,P13-2002,'43',"Zhao and Gildea, 2010",'43',dummy,dummy,"'186','177'","<S ssid=""1"" sid=""186"">The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S>",methodcitation
5,D10-1058,P13-2002,'82',"Zhao and Gildea, 2010",'82',dummy,dummy,"'180','182'","<S ssid=""1"" sid=""180"">One may try to solve it by forcing all these words to share a same parameter λ(einfrequent).</S><S ssid=""1"" sid=""182"">We solve the problem in the following way: estimate the parameter λ(enon empty ) for all nonempty words, all infrequent words share this parameter.</S>",methodcitation
6,D10-1058,P13-2002,'99',"Zhao and Gildea, 2010",'99',dummy,dummy,"'131','175'","<S ssid=""1"" sid=""131"">Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.</S><S ssid=""1"" sid=""175"">For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.</S>",methodcitation
7,D10-1058,P59105CA,'45',15.0,'45',dummy,dummy,"'168','177'","<S ssid=""1"" sid=""168"">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S>",methodcitation
8,D10-1058,P87-94,'79',Zhao,'79',dummy,dummy,"'35','61'","<S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S><S ssid=""1"" sid=""61"">2.2 IBM Model 1 and HMM.</S>",methodcitation
9,D10-1058,PBULLETIN,'104',2010.0,"'103','104'",dummy,dummy,"'78','2'","<S ssid=""1"" sid=""78"">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S><S ssid=""1"" sid=""2"">Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.</S>",methodcitation
10,D10-1058,PBULLETIN,'131',2010.0,'131',dummy,dummy,"'177','133'","<S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""133"">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S>",methodcitation
11,D10-1058,PCOLING-D10,'232',2010.0,'232',dummy,dummy,"'177','186'","<S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""186"">The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.</S>",methodcitation
12,D10-1058,PPROC-D10,'24',"Zhao and Gildea, 2010",'24',dummy,dummy,"'0','177'","<S ssid=""1"" sid=""0"">A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S>",methodcitation
13,D10-1058,PPROC-D10,'38',"Zhao and Gildea, 2010",'38',dummy,dummy,"'177','186'","<S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""186"">The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.</S>",methodcitation
14,D10-1058,PPROC-D10,'39',"Zhao and Gildea, 2010",'39',dummy,dummy,"'105','199'","<S ssid=""1"" sid=""105"">We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.</S><S ssid=""1"" sid=""199"">The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.</S>",methodcitation
15,D10-1058,PPROC-D10,'86',"Zhao and Gildea, 2010",'86',dummy,dummy,"'180','182'","<S ssid=""1"" sid=""180"">One may try to solve it by forcing all these words to share a same parameter λ(einfrequent).</S><S ssid=""1"" sid=""182"">We solve the problem in the following way: estimate the parameter λ(enon empty ) for all nonempty words, all infrequent words share this parameter.</S>",methodcitation
16,D10-1058,PPROC-D10,'104',"Zhao and Gildea, 2010",'104',dummy,dummy,"'131','175'","<S ssid=""1"" sid=""131"">Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.</S><S ssid=""1"" sid=""175"">For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.</S>",methodcitation
17,D10-1058,Q13-1024,'60',"Zhao and Gildea, 2010",'60',dummy,dummy,"'10','20'","<S ssid=""1"" sid=""10"">Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.</S><S ssid=""1"" sid=""20"">Most models have limited ability to model fertility.</S>",methodcitation
18,D10-1058,Q13-1024,'130',"Zhao and Gildea, 2010",'130',dummy,dummy,"'113','133'","<S ssid=""1"" sid=""113"">This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm.</S><S ssid=""1"" sid=""133"">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S>",methodcitation
