Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1058,C16-1060,'48',2010.0,'48',dummy,dummy,"'167','133','118','113','35'","<S ssid=""1"" sid=""167"">AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.</S><S ssid=""1"" sid=""133"">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S><S ssid=""1"" sid=""118"">Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.</S><S ssid=""1"" sid=""113"">This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm.</S><S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S>",['methodcitation']
2,D10-1058,P13-2002,'24',"Zhao and Gildea, 2010",'24',dummy,dummy,"'0','35','177','185','117'","<S ssid=""1"" sid=""0"">A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC</S><S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""185"">We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.</S><S ssid=""1"" sid=""117"">In this case, the fertility hidden Markov model is not faster than the HMM.</S>",['methodcitation']
3,D10-1058,P13-2002,'33',"Zhao and Gildea, 2010",'33',dummy,dummy,"'48','199','0'","<S ssid=""1"" sid=""48"">(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.</S><S ssid=""1"" sid=""199"">The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.</S><S ssid=""1"" sid=""0"">A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC</S>",['methodcitation']
4,D10-1058,P13-2002,'43',"Zhao and Gildea, 2010",'43',dummy,dummy,"'186','177','150','151','185'","<S ssid=""1"" sid=""186"">The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""150"">IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.</S><S ssid=""1"" sid=""151"">We choose t = 1, 5, and 30 for the fertility HMM.</S><S ssid=""1"" sid=""185"">We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.</S>",['methodcitation']
5,D10-1058,P13-2002,'82',"Zhao and Gildea, 2010",'82',dummy,dummy,"'180','182','93'","<S ssid=""1"" sid=""180"">One may try to solve it by forcing all these words to share a same parameter λ(einfrequent).</S><S ssid=""1"" sid=""182"">We solve the problem in the following way: estimate the parameter λ(enon empty ) for all nonempty words, all infrequent words share this parameter.</S><S ssid=""1"" sid=""93"">Therefore, we assume that φǫ follows a Poisson distribution with parameter I λ(ǫ).</S>",['methodcitation']
6,D10-1058,P13-2002,'99',"Zhao and Gildea, 2010",'99',dummy,dummy,"'131','175'","<S ssid=""1"" sid=""131"">Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.</S><S ssid=""1"" sid=""175"">For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.</S>",['methodcitation']
7,D10-1058,P59105CA,'45',15.0,'45',dummy,dummy,"'168','177','185','133','61'","<S ssid=""1"" sid=""168"">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""185"">We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.</S><S ssid=""1"" sid=""133"">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S><S ssid=""1"" sid=""61"">2.2 IBM Model 1 and HMM.</S>",['methodcitation']
8,D10-1058,P87-94,'79',Zhao,'79',dummy,dummy,"'35','61','194','200','190'","<S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S><S ssid=""1"" sid=""61"">2.2 IBM Model 1 and HMM.</S><S ssid=""1"" sid=""194"">Results are shown in Table 2; we see that better word alignment results do not lead to better translations.</S><S ssid=""1"" sid=""200"">While better word alignment results do not necessarily correspond to better translation quality, our translation results are comparable in translation quality to both the HMM and IBM Model 4.</S><S ssid=""1"" sid=""190"">We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model.</S>",['methodcitation']
9,D10-1058,PBULLETIN,'104',2010.0,"'103','104'",dummy,dummy,"'78','2'","<S ssid=""1"" sid=""78"">However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.</S><S ssid=""1"" sid=""2"">Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.</S>",['methodcitation']
10,D10-1058,PBULLETIN,'131',2010.0,'131',dummy,dummy,"'177','133','132','6','80'","<S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""133"">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S><S ssid=""1"" sid=""132"">Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.</S><S ssid=""1"" sid=""6"">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S ssid=""1"" sid=""80"">Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1.</S>",['methodcitation']
11,D10-1058,PCOLING-D10,'232',2010.0,'232',dummy,dummy,"'177','186'","<S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""186"">The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.</S>",['methodcitation']
12,D10-1058,PPROC-D10,'24',"Zhao and Gildea, 2010",'24',dummy,dummy,"'0','177','185','117','35'","<S ssid=""1"" sid=""0"">A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC</S><S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""185"">We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.</S><S ssid=""1"" sid=""117"">In this case, the fertility hidden Markov model is not faster than the HMM.</S><S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S>",['methodcitation']
13,D10-1058,PPROC-D10,'38',"Zhao and Gildea, 2010",'38',dummy,dummy,"'177','186'","<S ssid=""1"" sid=""177"">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S><S ssid=""1"" sid=""186"">The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.</S>",['methodcitation']
14,D10-1058,PPROC-D10,'39',"Zhao and Gildea, 2010",'39',dummy,dummy,"'105','199'","<S ssid=""1"" sid=""105"">We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.</S><S ssid=""1"" sid=""199"">The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.</S>",['methodcitation']
15,D10-1058,PPROC-D10,'86',"Zhao and Gildea, 2010",'86',dummy,dummy,"'180','182'","<S ssid=""1"" sid=""180"">One may try to solve it by forcing all these words to share a same parameter λ(einfrequent).</S><S ssid=""1"" sid=""182"">We solve the problem in the following way: estimate the parameter λ(enon empty ) for all nonempty words, all infrequent words share this parameter.</S>",['methodcitation']
16,D10-1058,PPROC-D10,'104',"Zhao and Gildea, 2010",'104',dummy,dummy,"'131','175'","<S ssid=""1"" sid=""131"">Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.</S><S ssid=""1"" sid=""175"">For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.</S>",['methodcitation']
17,D10-1058,Q13-1024,'60',"Zhao and Gildea, 2010",'60',dummy,dummy,"'10','20','35','21','2'","<S ssid=""1"" sid=""10"">Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.</S><S ssid=""1"" sid=""20"">Most models have limited ability to model fertility.</S><S ssid=""1"" sid=""35"">Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.</S><S ssid=""1"" sid=""21"">Liang et al.</S><S ssid=""1"" sid=""2"">Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.</S>",['methodcitation']
18,D10-1058,Q13-1024,'130',"Zhao and Gildea, 2010",'130',dummy,dummy,"'113','133','31'","<S ssid=""1"" sid=""113"">This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm.</S><S ssid=""1"" sid=""133"">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S><S ssid=""1"" sid=""31"">Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993).</S>",['methodcitation']
