Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,C02-1025,C10-2104,['115'],"Chieu and Ng, 2002",['115'],dummy,dummy,"'125','61','17'","<S ssid=""1"" sid=""125"">4.2 Global Features.</S><S ssid=""1"" sid=""61"">The features we used can be divided into 2 classes: local and global.</S><S ssid=""1"" sid=""17"">Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages.</S>",['methodcitation']
2,C02-1025,C10-2167,['65'],"Chieu et al., 2002",['65'],dummy,dummy,'19',"<S ssid=""1"" sid=""19"">Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.</S>","['methodcitation', 'resultcitation']"
3,C02-1025,I05-3013,['88'],"Chieu and Ng, 2002",['88'],dummy,dummy,"'19','194'","<S ssid=""1"" sid=""19"">Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.</S><S ssid=""1"" sid=""194"">(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.</S>","['methodcitation', 'resultcitation']"
4,C02-1025,I05-3030,['33'],Chieu 2002,['33'],dummy,dummy,"'59','68','1'","<S ssid=""1"" sid=""59"">The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.</S><S ssid=""1"" sid=""68"">In the maximum entropy framework, there is no such constraint.</S><S ssid=""1"" sid=""1"">This paper presents a maximum entropy-based named entity recognizer (NER).</S>",['methodcitation']
6,C02-1025,P02-1061,['51'],"Chieu and Ng, 2002",['51'],dummy,dummy,"'43','197'","<S ssid=""1"" sid=""43"">It uses a maximum entropy framework and classifies each word given its features.</S><S ssid=""1"" sid=""197"">This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).</S>",['methodcitation']
7,C02-1025,P03-1028,['161'],"Chieu and Ng, 2002a",['161'],dummy,dummy,'54',"<S ssid=""1"" sid=""54"">We have used the Java-based opennlp maximum entropy package1.</S>",['methodcitation']
10,C02-1025,P03-1028,['52'],2002a,['52'],dummy,dummy,'2',"<S ssid=""1"" sid=""2"">It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.</S>",['methodcitation']
12,C02-1025,P05-1045,['174'],2002,['174'],dummy,dummy,"'63','155'","<S ssid=""1"" sid=""63"">Global features are extracted from other occurrences of the same token in the whole document.</S><S ssid=""1"" sid=""155"">Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document.</S>",['methodcitation']
13,C02-1025,P05-1051,['27'],"Chieu and Ng, 2002",['27'],dummy,dummy,"'65','76'","<S ssid=""1"" sid=""65"">However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).</S><S ssid=""1"" sid=""76"">This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.</S>",['methodcitation']
15,C02-1025,W03-0423,['12'],"Chieu and Ng, 2002b",['12'],dummy,dummy,"'125','61','188','63','186'","<S ssid=""1"" sid=""125"">4.2 Global Features.</S><S ssid=""1"" sid=""61"">The features we used can be divided into 2 classes: local and global.</S><S ssid=""1"" sid=""188"">Such a classification can be seen as a not-always-correct summary of global features.</S><S ssid=""1"" sid=""63"">Global features are extracted from other occurrences of the same token in the whole document.</S><S ssid=""1"" sid=""186"">The effect of a second reference resolution classifier is not entirely the same as that of global features.</S>","['methodcitation', 'resultcitation']"
16,C02-1025,W03-0423,['32'],"Chieu and Ng, 2002a",['32'],dummy,dummy,"'120','176'","<S ssid=""1"" sid=""120"">The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List.</S><S ssid=""1"" sid=""176"">(1999) did not report using any dictionaries, but mentioned in a footnote that they have added list membership features, which have helped marginally in certain domains.</S>",['methodcitation']
17,C02-1025,W03-0423,['44'],"Chieu and Ng, 2002b",['44'],dummy,dummy,"'61','65','74','71','12'","<S ssid=""1"" sid=""61"">The features we used can be divided into 2 classes: local and global.</S><S ssid=""1"" sid=""65"">However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).</S><S ssid=""1"" sid=""74"">4.1 Local Features.</S><S ssid=""1"" sid=""71"">We group the features used into feature groups.</S><S ssid=""1"" sid=""12"">As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.</S>",['methodcitation']
18,C02-1025,W03-0423,['62'],"Chieu and Ng, 2002b",['62'],dummy,dummy,"'86','91','76','62','94'","<S ssid=""1"" sid=""86"">Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.</S><S ssid=""1"" sid=""91"">This group contains a large number of features (one for each token string present in the training data).</S><S ssid=""1"" sid=""76"">This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.</S><S ssid=""1"" sid=""62"">Local features are features that are based on neighboring tokens, as well as the token itself.</S><S ssid=""1"" sid=""94"">Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.</S>",['methodcitation']
19,C02-1025,W03-0423,['46'],"Chieu and Ng, 2002b",['46'],dummy,dummy,"'71','69','61','66'","<S ssid=""1"" sid=""71"">We group the features used into feature groups.</S><S ssid=""1"" sid=""69"">Multiple features can be used for the same token.</S><S ssid=""1"" sid=""61"">The features we used can be divided into 2 classes: local and global.</S><S ssid=""1"" sid=""66"">This might be because our features are more comprehensive than those used by Borthwick.</S>",['methodcitation']
20,C02-1025,W03-0432,['44'],"Chieu and Ng, 2002a",['44'],dummy,dummy,"'27','12','2','189','200'","<S ssid=""1"" sid=""27"">(1998) did make use of information from the whole document.</S><S ssid=""1"" sid=""12"">As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.</S><S ssid=""1"" sid=""2"">It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.</S><S ssid=""1"" sid=""189"">The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document.</S><S ssid=""1"" sid=""200"">Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.</S>",['methodcitation']
21,C02-1025,W04-0705,['8'],Chieu and Ng 2002,['8'],dummy,dummy,"'175','193','26','39'","<S ssid=""1"" sid=""175"">Bikel et al.</S><S ssid=""1"" sid=""193"">Mikheev et al.</S><S ssid=""1"" sid=""26"">Mikheev et al.</S><S ssid=""1"" sid=""39"">On the MUC6 data, Bikel et al.</S>",['methodcitation']
22,C02-1025,W04-0705,['147'],Chieu and Ng 2002,['147'],dummy,dummy,"'65','69','66'","<S ssid=""1"" sid=""65"">However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).</S><S ssid=""1"" sid=""69"">Multiple features can be used for the same token.</S><S ssid=""1"" sid=""66"">This might be because our features are more comprehensive than those used by Borthwick.</S>",['methodcitation']
23,C02-1025,W06-0119,['11'],Chieu et al. 2002,['11'],dummy,dummy,'43',"<S ssid=""1"" sid=""43"">It uses a maximum entropy framework and classifies each word given its features.</S>","['methodcitation', 'resultcitation']"
