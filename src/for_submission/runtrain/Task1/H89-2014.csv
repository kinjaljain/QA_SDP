Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,H89-2014,A92-1018,['47'],"Kupiec, 1989a",['47'],dummy,dummy,"'61','4','148','77','87'","<S ssid=""1"" sid=""61"">Extending the Basic Model The basic model was used as a benchmark for successive improvements.</S><S ssid=""1"" sid=""4"">State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.</S><S ssid=""1"" sid=""148"">It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.</S><S ssid=""1"" sid=""77"">ADJECTIVE DETERMINER To all states NOUN in Basic Network ""Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.</S><S ssid=""1"" sid=""87"">An alternative to uniformly increasing the order of the conditioning is to extend it selectively.</S>",methodcitation
3,H89-2014,A92-1018,['108'],"Kupiec, 1989a",['108'],dummy,dummy,"'71','83'","<S ssid=""1"" sid=""71"">This leaves 50% of the corpus for training all the other equivalence classes.</S><S ssid=""1"" sid=""83"">Increasing the order of the conditioning requires exponentially more parameters.</S>",methodcitation
4,H89-2014,J93-2006,['41'],Kupiec 1989,"['40','41']",dummy,dummy,"'1','23','145'","<S ssid=""1"" sid=""1"">The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.</S><S ssid=""1"" sid=""23"">An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a ""hidden"" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.</S><S ssid=""1"" sid=""145"">A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.</S>",methodcitation
5,H89-2014,H91-1046,['21'],1989,['21'],dummy,dummy,"'148','34','106'","<S ssid=""1"" sid=""148"">It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.</S><S ssid=""1"" sid=""34"">In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.</S><S ssid=""1"" sid=""106"">Modeling Dependencies across Phrases Linguistic considerations can be used to correct errors made by the model.</S>",methodcitation
6,H89-2014,H91-1046,['60'],"Kupiec, 1989",['60'],dummy,dummy,"'29','18','93','34','101'","<S ssid=""1"" sid=""29"">In this regard, word equivalence classes were used (Kupiec, 1989).</S><S ssid=""1"" sid=""18"">A word sequence is considered as being generated from an underlying sequence of categories.</S><S ssid=""1"" sid=""93"">It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner.</S><S ssid=""1"" sid=""34"">In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.</S><S ssid=""1"" sid=""101"">Whenever a transition is made to a state, the state-dependent probability distribution P(Eqvi I Ci) is used to obtain the probability of the observed equivalence class.</S>",methodcitation
7,H89-2014,C00-1081,['85'],"Kupiec, 1989",['85'],dummy,dummy,"'70','69'","<S ssid=""1"" sid=""70"">The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.</S><S ssid=""1"" sid=""69"">In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.</S>",methodcitation
8,H89-2014,H92-1022,['18'],11,['18'],dummy,dummy,'112',"<S ssid=""1"" sid=""112"">The basic model tagged these sentences correctly, except for- ""range"" and ""rises"" which were tagged as noun and plural-noun respectively 1.</S>",methodcitation
9,H89-2014,H92-1022,['9'],11,['9'],dummy,dummy,"'132','108'","<S ssid=""1"" sid=""132"">For example, 9 errors are from 3 instances of ""... as well as ..."" that arise in the text.</S><S ssid=""1"" sid=""108"">These are exemplified by the following sentence fragments: 1.</S>",methodcitation
10,H89-2014,C92-1060,['124'],"Kupiec, 1989",['124'],dummy,dummy,"'34','29','3','35','30'","<S ssid=""1"" sid=""34"">In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.</S><S ssid=""1"" sid=""29"">In this regard, word equivalence classes were used (Kupiec, 1989).</S><S ssid=""1"" sid=""3"">Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.</S><S ssid=""1"" sid=""35"">In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model.</S><S ssid=""1"" sid=""30"">There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.</S>",methodcitation
