Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,W03-0410,D09-1138,['25'],"Stevenson and Joanis, 2003",['25'],dummy,dummy,"'252','17','51','261','14'","<S ssid=""1"" sid=""252"">Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.</S><S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S><S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S><S ssid=""1"" sid=""261"">We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.</S><S ssid=""1"" sid=""14"">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S>",methodcitation
3,W03-0410,D09-1138,['70'],"Stevenson and Joanis, 2003",['70'],dummy,dummy,"'252','17','51','208','2'","<S ssid=""1"" sid=""252"">Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.</S><S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S><S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S><S ssid=""1"" sid=""208"">In the next two sections, we present unsupervised and minimally supervised approaches to this problem.</S><S ssid=""1"" sid=""2"">The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.</S>",methodcitation
4,W03-0410,D11-1095,['17'],"Stevenson and Joanis, 2003",['17'],dummy,dummy,"'131','17'","<S ssid=""1"" sid=""131"">We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.</S><S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S>",methodcitation
5,W03-0410,D11-1095,['38'],"Stevenson and Joanis, 2003",['38'],dummy,dummy,"'51','181'","<S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S><S ssid=""1"" sid=""181"">The 13-way task includes all of our classes.</S>",methodcitation
6,W03-0410,D11-1095,['40'],Stevenson and Joanis (2003),['40'],dummy,dummy,"'120','117','121','222','223'","<S ssid=""1"" sid=""120"">Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).</S><S ssid=""1"" sid=""117"">We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).</S><S ssid=""1"" sid=""121"">We began with this same set of 20 verbs per class for our current work.</S><S ssid=""1"" sid=""222"">To model this kind of approach, we selected a sample of five seed verbs from each class.</S><S ssid=""1"" sid=""223"">Each set of verbs was judged (by the authors’ intuition alone) to be “representative” of the class.</S>",methodcitation
7,W03-0410,D11-1095,['54'],"Stevenson and Joanis, 2003",['54'],dummy,dummy,"'17','39','22'","<S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S><S ssid=""1"" sid=""39"">Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</S><S ssid=""1"" sid=""22"">However, a general feature space means that most features will be irrelevant to any given verb discrimination task.</S>",methodcitation
8,W03-0410,D11-1095,['82'],"Stevenson and Joanis, 2003","['82','83']",dummy,dummy,"'17','178'","<S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S><S ssid=""1"" sid=""178"">However, it gives important information about the quality of a clustering: The other measures being equal, a clustering with a higher value indicates tighter and more separated clusters, suggesting stronger inherent patterns in the data.</S>",methodcitation
9,W03-0410,D11-1095,['167'],Stevenson and Joanis (2003),['167'],dummy,dummy,'51',"<S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S>",methodcitation
10,W03-0410,D11-1095,['168'],Stevenson and Joanis (2003),['168'],dummy,dummy,"'188','119'","<S ssid=""1"" sid=""188"">The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).</S><S ssid=""1"" sid=""119"">Table 1 above shows the number of verbs in each class at the end of this process.</S>",methodcitation
11,W03-0410,D11-1095,['169'],Stevenson and Joanis (2003),['169'],dummy,dummy,"'39','187'","<S ssid=""1"" sid=""39"">Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</S><S ssid=""1"" sid=""187"">5.1 Full Feature Set.</S>",methodcitation
12,W03-0410,J06-2001,['397'],Stevenson and Joanis 2003,['397'],dummy,dummy,"'142','28'","<S ssid=""1"" sid=""142"">4.2.1 Accuracy We can assign each cluster the class label of the majority of its members.</S><S ssid=""1"" sid=""28"">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S>",methodcitation
13,W03-0410,J06-2001,['586'],Stevenson and Joanis (2003),"['586','587','588','589']",dummy,dummy,"'27','211','51','28','17'","<S ssid=""1"" sid=""27"">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S><S ssid=""1"" sid=""211"">(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.</S><S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S><S ssid=""1"" sid=""28"">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S><S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S>",methodcitation
15,W03-0410,P03-1009,['124'],"(Stevenson and Joanis, 2003)",['124'],dummy,dummy,"'144','171'","<S ssid=""1"" sid=""144"">Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.</S><S ssid=""1"" sid=""171"">We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated.</S>",methodcitation
16,W03-0410,P03-1009,['143'],Stevenson and Joanis (2003),['143'],dummy,dummy,'51',"<S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S>",methodcitation
17,W03-0410,P04-2007,['11'],"Stevenson and Joanis, 2003",['11'],dummy,dummy,"'51','4'","<S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S><S ssid=""1"" sid=""4"">We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.</S>",methodcitation
20,W03-0410,P07-3016,['39'],"(Stevenson and Joanis, 2003)",['39'],dummy,dummy,"'39','22','23','51','261'","<S ssid=""1"" sid=""39"">Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</S><S ssid=""1"" sid=""22"">However, a general feature space means that most features will be irrelevant to any given verb discrimination task.</S><S ssid=""1"" sid=""23"">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to “the curse of dimensionality”?</S><S ssid=""1"" sid=""51"">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S><S ssid=""1"" sid=""261"">We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.</S>",methodcitation
21,W03-0410,W06-2910,['8'],"Stevenson and Joanis, 2003","['7','8']",dummy,dummy,"'14','17','261'","<S ssid=""1"" sid=""14"">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S><S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S><S ssid=""1"" sid=""261"">We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.</S>",methodcitation
22,W03-0410,W06-2910,['13'],"Stevenson and Joanis, 2003",['13'],dummy,dummy,"'39','17'","<S ssid=""1"" sid=""39"">Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</S><S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S>",methodcitation
23,W03-0410,W06-2910,['118'],"Stevenson and Joanis, 2003","['118','119']",dummy,dummy,"'150','132'","<S ssid=""1"" sid=""150"">4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.</S><S ssid=""1"" sid=""132"">In performing hierarchical clustering, both a vector distance measure and a cluster distance (“linkage”) measure are specified.</S>",methodcitation
24,W03-0410,E09-1072,['80'],"Stevenson and Joanis, 2003",['80'],dummy,dummy,'17',"<S ssid=""1"" sid=""17"">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S>",methodcitation
