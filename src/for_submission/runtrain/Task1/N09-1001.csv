Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,N09-1001,N09-CSL2013,'147',2009.0,"'146', '147'",dummy,dummy,"'37','94','15','16'","<S ssid=""1"" sid=""37"">In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.</S><S ssid=""1"" sid=""94"">To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).</S><S ssid=""1"" sid=""15"">This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous.</S><S ssid=""1"" sid=""16"">Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).</S>",['methodcitation']
2,N09-1001,N09-QWN,'22',"Su and Markert, 2009",'22',dummy,dummy,"'2','21','6'","<S ssid=""1"" sid=""2"">Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.</S><S ssid=""1"" sid=""21"">In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.</S><S ssid=""1"" sid=""6"">In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.</S>",['methodcitation']
3,N09-1001,N09-QWN,'22',"Su and Markert, 2009",'22',dummy,dummy,"'2','21','6'","<S ssid=""1"" sid=""2"">Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.</S><S ssid=""1"" sid=""21"">In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.</S><S ssid=""1"" sid=""6"">In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.</S>",['methodcitation']
4,N09-1001,N09PROD,'35',5.0,'35',dummy,dummy,"'136','204','160','26','221'","<S ssid=""1"" sid=""136"">In Section 4.5, we shortly discuss results on.</S><S ssid=""1"" sid=""204"">4.5 Comparison to Prior Approaches.</S><S ssid=""1"" sid=""160"">classification vertices in the Mincut approach.</S><S ssid=""1"" sid=""26"">Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.</S><S ssid=""1"" sid=""221"">The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut.</S>",['methodcitation']
5,N09-1001,N09PROD,'121',5.0,'121',dummy,dummy,"'203','4'","<S ssid=""1"" sid=""203"">In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1).</S><S ssid=""1"" sid=""4"">We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.</S>","['aimcitation', 'methodcitation']"
6,N09-1001,N15-1071,'147',2009.0,'147',dummy,dummy,,,"['implicationcitation', 'resultcitation']"
7,N09-1001,P101167,'146',2009.0,'146',dummy,dummy,"'220','4','1','25','65'","<S ssid=""1"" sid=""220"">We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.</S><S ssid=""1"" sid=""4"">We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.</S><S ssid=""1"" sid=""1"">We supplement WordNet entries with information on the subjectivity of its word senses.</S><S ssid=""1"" sid=""25"">Section 3 describes our proposed semi-supervised minimum cut framework in detail.</S><S ssid=""1"" sid=""65"">We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.</S>",['methodcitation']
8,N09-1001,P1018,'10,2009.0,'10',dummy,dummy,'37',"<S ssid=""1"" sid=""37"">In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.</S>","['methodcitation', 'resultcitation']"
9,N09-1001,P1018,'180',2009.0,'180',dummy,dummy,"'105','156','132','4'","<S ssid=""1"" sid=""105"">Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.</S><S ssid=""1"" sid=""156"">We use Lesk as it is one of the few measures applicable across all parts-of-speech.</S><S ssid=""1"" sid=""132"">This provides a non graph 0.16 0.84 flicker Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6.</S><S ssid=""1"" sid=""4"">We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.</S>",['methodcitation']
10,N09-1001,PEAAI-N09,'70',2009.0,'70',dummy,dummy,"'61','169','226','63','189'","<S ssid=""1"" sid=""61"">3.3 Formulation of Semi-supervised Mincuts.</S><S ssid=""1"" sid=""169"">4.4 Semi-supervised Graph Mincuts.</S><S ssid=""1"" sid=""226"">We will also explore other semi- supervised algorithms.</S><S ssid=""1"" sid=""63"">3.2 Why might Semi-supervised Minimum.</S><S ssid=""1"" sid=""189"">As can be seen, the semi-supervised Mincuts is consistently better than SVM.</S>",['methodcitation']
11,N09-1001,PPROC2014-N09,'33',2009.0,'33',dummy,dummy,,,"['implicationcitation', 'resultcitation']"
12,N09-1001,PPROC2014-N09,'70',2009.0,'70',dummy,dummy,"'220','65','94','1','13'","<S ssid=""1"" sid=""220"">We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.</S><S ssid=""1"" sid=""65"">We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.</S><S ssid=""1"" sid=""94"">To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).</S><S ssid=""1"" sid=""1"">We supplement WordNet entries with information on the subjectivity of its word senses.</S><S ssid=""1"" sid=""13"">Second, different word senses of a single word can actually be of different subjectivity or polarity.</S>",['methodcitation']
13,N09-1001,W11-0311,'231',2009.0,'231',dummy,dummy,"'40','16','13','14','17'","<S ssid=""1"" sid=""40"">Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.</S><S ssid=""1"" sid=""16"">Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).</S><S ssid=""1"" sid=""13"">Second, different word senses of a single word can actually be of different subjectivity or polarity.</S><S ssid=""1"" sid=""14"">A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive—having a positive electric charge;“protons are positive” (objective) (2) plus, positive—involving advantage or good; “a plus (or positive) factor” (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet.</S><S ssid=""1"" sid=""17"">Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use.</S>",['methodcitation']
