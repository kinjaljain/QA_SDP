Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,X96-1048,A97-1028,['3'],"Sundheim, 1995b",['3'],dummy,dummy,"'4','20'","<S ssid=""1"" sid=""4"">The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.</S><S ssid=""1"" sid=""20"">Even the simplest of the tasks, Named Entity, occasionally requires in-depth processing, e.g., to determine whether ""60 pounds"" is an expression of weight or of monetary value.</S>",['methodcitation']
4,X96-1048,J00-4003,['20'],Sundheim 1995,"['12','20']",dummy,dummy,'2',"<S ssid=""1"" sid=""2"">The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.</S>",['methodcitation']
5,X96-1048,J00-4003,['72'],Sundheim 1995,"['71','72']",dummy,dummy,,,"['implicationcitation', 'resultcitation']"
6,X96-1048,W97-1307,['29'],"Sund heim, 1995","['29','30']",dummy,dummy,'103',"<S ssid=""1"" sid=""103"">COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe.</S>",['methodcitation']
7,X96-1048,P06-1059,['10'],"Sundheim, 1995",['10'],dummy,dummy,"'49','68','128','14'","<S ssid=""1"" sid=""49"">As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.</S><S ssid=""1"" sid=""68"">As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.</S><S ssid=""1"" sid=""128"">In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.</S><S ssid=""1"" sid=""14""> Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence from anywhere in the text.</S>",['methodcitation']
8,X96-1048,C04-1126,['49'],"Sundheim, 1995","['49','50','51']",dummy,dummy,'244',"<S ssid=""1"" sid=""244"">Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the ""key"" and the other annotator's templates were treated as the ""response"".</S>","['methodcitation', 'resultcitation']"
9,X96-1048,C04-1126,['34'],"Sundheim, 1995",['34'],dummy,dummy,"'227','27'","<S ssid=""1"" sid=""227"">Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.</S><S ssid=""1"" sid=""27"">CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.</S>","['methodcitation', 'resultcitation']"
10,X96-1048,W99-0612,['142'],Sundheim 1995,"['141','142']",dummy,dummy,"'51','49','93','96'","<S ssid=""1"" sid=""51"">It was also unexpected that one of the systems would match human performance on the task.</S><S ssid=""1"" sid=""49"">As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.</S><S ssid=""1"" sid=""93"">The interannotator variability test provides reference points indicating human performance on the different aspects of the NE task.</S><S ssid=""1"" sid=""96"">These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.</S>",['resultcitation']
11,X96-1048,E99-1001,['17'],1995,['17'],dummy,dummy,"'72','193','23'","<S ssid=""1"" sid=""72"">Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..</S><S ssid=""1"" sid=""193"">There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.</S><S ssid=""1"" sid=""23"">Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem.</S>",['methodcitation']
12,X96-1048,M98-1003,['3'],Sundheim1995,['3'],dummy,dummy,"'246','245'","<S ssid=""1"" sid=""246"">The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.</S><S ssid=""1"" sid=""245"">No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.</S>",['implicationcitation']
