{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "296c8cd471424fd790c481ac59eea96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3eef144a6736453b91b015fbfe120d0c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97db1405e9dc4944994b2b6edc9a4787",
              "IPY_MODEL_02e111f8614c443bb0b5058ce4fa272e"
            ]
          }
        },
        "3eef144a6736453b91b015fbfe120d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97db1405e9dc4944994b2b6edc9a4787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e4b7a7e541a843a69c534936f82d2618",
            "_dom_classes": [],
            "description": "Processing 0.273 0.417 0.330:  50%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 17135,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8501,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35f588793d004308b9d7bc846180d4e8"
          }
        },
        "02e111f8614c443bb0b5058ce4fa272e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5af089fa61e431fbd434c951cef3687",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8501/17135 [3:43:02&lt;2:49:36,  1.18s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8863fec0a5004ce4ba5db605aa2a84f4"
          }
        },
        "e4b7a7e541a843a69c534936f82d2618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35f588793d004308b9d7bc846180d4e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5af089fa61e431fbd434c951cef3687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8863fec0a5004ce4ba5db605aa2a84f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b51a77c6bd71470d9f9d96b96e55a2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af6ad25ff7004703b67967d700a1c38e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c4839fe445c541b287f119704efae273",
              "IPY_MODEL_9d7034dbdb064e33858f57f6e429a72a"
            ]
          }
        },
        "af6ad25ff7004703b67967d700a1c38e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4839fe445c541b287f119704efae273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc26605539b64a5fbf46efcbcbba24ca",
            "_dom_classes": [],
            "description": "Processing 0.056 0.182 0.085:   1%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 17135,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 89,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f1c5defe0ff4a3dafef558ff5dd42a6"
          }
        },
        "9d7034dbdb064e33858f57f6e429a72a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_378851e844d7477d8d6eeea250fb1f98",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 89/17135 [00:03&lt;12:24, 22.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a910b82313f841f19cc139b86bbf4a7b"
          }
        },
        "bc26605539b64a5fbf46efcbcbba24ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f1c5defe0ff4a3dafef558ff5dd42a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "378851e844d7477d8d6eeea250fb1f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a910b82313f841f19cc139b86bbf4a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m5ZwF785g1g",
        "colab_type": "code",
        "outputId": "e81cb82c-5165-4b49-ee10-4b724350ee42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "296c8cd471424fd790c481ac59eea96b",
            "3eef144a6736453b91b015fbfe120d0c",
            "97db1405e9dc4944994b2b6edc9a4787",
            "02e111f8614c443bb0b5058ce4fa272e",
            "e4b7a7e541a843a69c534936f82d2618",
            "35f588793d004308b9d7bc846180d4e8",
            "e5af089fa61e431fbd434c951cef3687",
            "8863fec0a5004ce4ba5db605aa2a84f4"
          ]
        }
      },
      "source": [
        "!pip install rake-nltk\n",
        "from collections import namedtuple\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "Datum = namedtuple('Datum', 'ref cite offsets author is_test facet year')\n",
        "Offsets = namedtuple('Offsets', 'marker cite ref')\n",
        "Article = namedtuple('Article', 'id xml sentences sections')\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from rake_nltk import Rake\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "rakey = Rake(max_length=3)\n",
        "\n",
        "def encode(sentence):\n",
        "  return sentence\n",
        "\n",
        "# latest tf-idf\n",
        "\n",
        "\n",
        "global tf_vectorizer\n",
        "\n",
        "\n",
        "def build_model(cite_paper, ref_paper, n=1):\n",
        "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, n), min_df=0, stop_words='english')\n",
        "    sentences = list(cite_paper.sentences.values())\n",
        "    sentences.extend(list(ref_paper.sentences.values()))\n",
        "    tf_vectorizer = tf.fit(sentences)\n",
        "    pickle.dump(tf_vectorizer, open(\"/content/drive/My Drive/qa_stuff/tf-idf_cache/tf_cite_{}_ref_{}_n_{}.pickle\".format(cite_paper.id, ref_paper.id, n), 'wb'))\n",
        "    return tf_vectorizer\n",
        "\n",
        "def encode(sentence):\n",
        "  return sentence\n",
        "\n",
        "def get_similarity_score(cite_paper, ref_paper, sentence1, sentence2, n=1, kernel=\"none\"):\n",
        "    if os.path.exists(\"/content/drive/My Drive/qa_stuff/tf-idf_cache/tf_cite_{}_ref_{}_n_{}.pickle\".format(cite_paper.id, ref_paper.id, n)):\n",
        "        tf_vectorizer = pickle.load(open(\"/content/drive/My Drive/qa_stuff/tf-idf_cache/tf_cite_{}_ref_{}_n_{}.pickle\".format(cite_paper.id, ref_paper.id, n), 'rb'))\n",
        "    else:\n",
        "        tf_vectorizer = build_model(cite_paper, ref_paper, n)\n",
        "\n",
        "    tfidf_1 = tf_vectorizer.transform([sentence1])\n",
        "    tfidf_2 = tf_vectorizer.transform([sentence2])\n",
        "    if kernel == \"none\":\n",
        "        return cosine_similarity(tfidf_1, tfidf_2).item()\n",
        "    elif kernel == \"linear\":\n",
        "        return linear_kernel(tfidf_1, tfidf_2).item()\n",
        "    elif kernel == \"poly_2\":\n",
        "        return polynomial_kernel(tfidf_1, tfidf_2, 2).item()\n",
        "    elif kernel == \"poly_3\":\n",
        "        return polynomial_kernel(tfidf_1, tfidf_2, 3).item()\n",
        "    elif kernel == \"rbf\":\n",
        "        return rbf_kernel(tfidf_1, tfidf_2).item()\n",
        "# def get_similarity_score(sentence1, sentence2):\n",
        "#     # doesn't do anything about frequency of words in a sentence\n",
        "#     tokens1 = set(re.findall(r'[\\w]+', sentence1.lower()))\n",
        "#     tokens2 = set(re.findall(r'[\\w]+', sentence2.lower()))\n",
        "#     # print(sentence1)\n",
        "#     rakey.extract_keywords_from_sentences(sentence1.replace(\"-\", \" \").lower().split())\n",
        "\n",
        "#     keys1 = rakey.get_ranked_phrases()\n",
        "#     #keys1 = [x.split() for x in keys1]\n",
        "#     # print(keys1)\n",
        "#     #keys1 = [item for sublist in keys1 for item in sublist]\n",
        "#     # print(keys1)\n",
        "\n",
        "#     rakey.extract_keywords_from_sentences(sentence2.replace(\"-\", \" \").lower().split())\n",
        "#     keys2 = rakey.get_ranked_phrases()\n",
        "#     #keys2 = [x.split() for x in keys2]\n",
        "#     #keys2 = [item for sublist in keys2 for item in sublist]\n",
        "\n",
        "#     #keys = set(keys1) and set(keys2)\n",
        "#     tokens1 = set(keys1) - stop #- set(ref_keys)\n",
        "#     tokens2 = set(keys2) - stop #- set(cite_keys)\n",
        "#     #print(tokens1)\n",
        "#     #print(tokens2)\n",
        "#     #wefwefwef\n",
        "#     # tokens1 = tokens1 - stop\n",
        "#     # tokens2 = tokens2 - stop\n",
        "  \n",
        "#     return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "accuracy = 0\n",
        "total = 0\n",
        "empty_citations = 0\n",
        "empty_references = 0\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/My Drive/qa_stuff/processed-data-2019.json\", 'rb') as f:\n",
        "  dataset = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/My Drive/qa_stuff/processed-data-2018.json\", 'rb') as f:\n",
        "  dataset2 = pickle.load(f)\n",
        "\n",
        "dataset =  dataset2 + dataset\n",
        "\n",
        "articles = [x[1] for x in dataset]\n",
        "n = 5\n",
        "\n",
        "\n",
        "\n",
        "pbar = tqdm(dataset)\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "avg_score = 0\n",
        "all_scores = []\n",
        "for data in pbar:\n",
        "    ref_article = data.ref\n",
        "    citing_article = data.cite\n",
        "    offsets = data.offsets\n",
        "    citing_sentence_ids = offsets.cite\n",
        "    true_ref_sentences = offsets.ref\n",
        "    true_ref_sentence_ids = offsets.ref\n",
        "    if tp > 0:\n",
        "      p = tp/(max(tp+fp,1))\n",
        "      r = tp/(max(tp+fn,1))\n",
        "      f1 = 2*p*r/(p+r)\n",
        "      pbar.set_description(\"Processing %.3f %.3f %.3f\" %(p, r, f1))\n",
        "    if citing_article.sentences:\n",
        "      # raku.extract_keywords_from_sentences([citing_article.sentences[x] for x in citing_article.sentences])\n",
        "      # cite_keys = raku.get_ranked_phrases()[:0]\n",
        "      # raku.extract_keywords_from_sentences([ref_article.sentences[x] for x in ref_article.sentences])\n",
        "      # ref_keys = raku.get_ranked_phrases()[:0]\n",
        "\n",
        "      new_ids = [c for c in citing_sentence_ids]\n",
        "      for c in citing_sentence_ids:\n",
        "            # If additional context is reqd\n",
        "            to_add  = 0\n",
        "            extra = range(max(1, c-to_add), c)\n",
        "            new_ids.extend(extra)\n",
        "            extra = range(c+1, min(len(citing_article.sentences), c + to_add+1))\n",
        "            new_ids.extend(extra)\n",
        "      citing_sentence_ids = new_ids\n",
        "      complete_citing_sentence = \" \".join([citing_article.sentences[c] for c in citing_sentence_ids])\n",
        "      ref_vs = [(x[0],encode(x[1])) for x in ref_article.sentences.items() if abs(len(x[1].split()) - len(complete_citing_sentence.split())) <=40]\n",
        "      similarity_score = {}\n",
        "      for i in range(1):#c in citing_sentence_ids:\n",
        "        complete_citing_sentence = ' '.join([citing_article.sentences[x] for x in citing_sentence_ids])\n",
        "        complete_citing_sentence = citing_article.sentences[c]\n",
        "        #complete_citing_sentence = re.sub('\\((.+?)\\)', '', complete_citing_sentence)\n",
        "        complete_citing_sentence = encode(complete_citing_sentence)\n",
        "        for ref_id, ref_sentence in ref_vs: #(ref_article.sentences.items()):\n",
        "            try:\n",
        "                similarity_score[ref_id] = max(similarity_score.get(ref_id, 0) , get_similarity_score(citing_article, ref_article, ref_sentence, complete_citing_sentence, 1, \"poly_2\"))\n",
        "                # similarity_score[ref_id] = max(similarity_score.get(ref_id, 0) , get_similarity_score(ref_sentence, complete_citing_sentence))\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                pass\n",
        "      if similarity_score:\n",
        "          sorted_similarity_score = sorted(similarity_score.items(), key=lambda item: -item[1])\n",
        "          top_n = [s for s in sorted_similarity_score]\n",
        "          top_n = {x[0]:x[1] for x in top_n[:3]}\n",
        "          # if len(top_n) > 1:\n",
        "          #   top_n = rerank2(top_n, ref_article, ' '.join([citing_article.sentences[x] for x in citing_sentence_ids]))\n",
        "\n",
        "          fp += len(top_n)\n",
        "          for x in true_ref_sentence_ids:\n",
        "            if x in top_n:\n",
        "              avg_score = (tp*avg_score + top_n[x])/(max(tp,1))\n",
        "              all_scores.append(top_n[x])\n",
        "              fp -= 1\n",
        "              tp += 1\n",
        "            else:\n",
        "              #print(ref_article.sentences[x])\n",
        "              #print([citing_article.sentences[x] for x in citing_sentence_ids])\n",
        "              fn += 1\n",
        "print(tp, fp, fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "296c8cd471424fd790c481ac59eea96b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=17135), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wiso7VdDdG1",
        "colab_type": "code",
        "outputId": "d6c91e7a-10ce-4a8d-99c1-23b76463a945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  1 03:48:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knqHeSco3pZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZgy82TbqIeA",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fScFT9OYiq72",
        "colab_type": "code",
        "outputId": "d4e54a15-b449-40fc-d127-11d251ca304b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install rake-nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 9.6MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450kB 26.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.12.31)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 67.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 58.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.15.31)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.3.0->sentence-transformers) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp36-none-any.whl size=67076 sha256=01e8aa36878b49f7e2a6698ee53228d939ac96d30d2bd30c32430fe7753d29d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=3b66df8f1091bae3624107bf49f0b06efdd3e306e1c1d74994afefd292b55469\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 transformers-2.3.0\n",
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=a7a91ae885b186d75dad374127440de25015b38e5e267fc1403e36865e09661d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlLrnxKBDuSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Datum = namedtuple('Datum', 'ref cite offsets author is_test facet year')\n",
        "Offsets = namedtuple('Offsets', 'marker cite ref')\n",
        "Article = namedtuple('Article', 'id xml sentences sections')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd6pGH1OcESh",
        "colab_type": "code",
        "outputId": "8157c836-98c2-4dc1-9d2d-d8a31a6f2881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "\n",
        "import os\n",
        "import copy\n",
        "import xml.etree.cElementTree as ET\n",
        "import logging\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import requests\n",
        "\n",
        "DATA_ROOT = '/content/drive/My Drive/data'\n",
        "sep = os.path.sep\n",
        "ET.XMLParser(encoding=\"utf-8\")\n",
        "\n",
        "paper_load_fail = 0\n",
        "annotation_load_fail = 0\n",
        "\n",
        "l = logging.getLogger('load_parse')\n",
        "replace_count = 0\n",
        "\n",
        "# Gets names of folders where data is present. ex : \"./data/Training-Set-2016/C90-2039_TRAIN\"\n",
        "def get_folders(data_root):\n",
        "    folders = []\n",
        "    global folder_skips\n",
        "    for year_folder in filter(lambda x: os.path.isdir(data_root + sep + x), os.listdir(data_root)):\n",
        "        year_folder = data_root + sep + year_folder\n",
        "        for data_folder in os.listdir(year_folder):\n",
        "            data_folder = year_folder + sep + data_folder\n",
        "            if not os.path.isdir(data_folder) or '2018' not in data_folder or 'Test-Set-2018' in data_folder:\n",
        "                continue\n",
        "                # readme\n",
        "            folders.append(data_folder)\n",
        "    return folders\n",
        "\n",
        "\n",
        "def load_article(filename):\n",
        "    global paper_load_fail\n",
        "    if not os.path.splitext(filename)[1] == '.xml':\n",
        "        l.info(\"Skipping Non xml : \" + filename)\n",
        "        return\n",
        "    l.info(\"parsing :\" + filename)\n",
        "    try:\n",
        "        # Ignoring non UTF characters for now\n",
        "        with codecs.open(filename, mode='r', encoding='utf-8', errors='ignore') as target_file:\n",
        "\n",
        "            xml = ET.parse(target_file)\n",
        "            parent_map = {c: p for p in xml.getroot().iter() for c in p}\n",
        "            sentence_elements = list(xml.getroot().iter('S'))\n",
        "            sentence_elements = [(x.text,\n",
        "                                  parent_map[x].attrib['title'] if len(parent_map[x].attrib) > 0 else parent_map[x].tag,\n",
        "                                  int(x.attrib['sid']) if 'sid' in x.attrib else 99)\n",
        "                                 for x in\n",
        "                                 sentence_elements]\n",
        "            # TODO: Check if this is too memory inefficient. Should mostly be okay\n",
        "            sentence_map = {x[2]: x[0] for i, x in enumerate(sentence_elements)}\n",
        "            section_map = {x[2]: x[1] for i, x in enumerate(sentence_elements)}\n",
        "            article = Article(\"\", xml, sentence_map, section_map)\n",
        "            return article\n",
        "    except Exception as e:\n",
        "        l.error(\"Error with : \" + filename + \" with ex : \" + str(e))\n",
        "        paper_load_fail += 1\n",
        "        return Article(\"\", ET.fromstring(\"<xml></xml>\"), {}, {})\n",
        "\n",
        "\n",
        "def newest_file(path):\n",
        "    files = os.listdir(path)\n",
        "    paths = [os.path.join(path, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "\n",
        "author_memo = {}\n",
        "def load_folder_data(annotation_file, articles, is_test=False):\n",
        "    global annotation_load_fail\n",
        "    global author_memo\n",
        "    data = []\n",
        "    # try:\n",
        "    #     year_matches = re.findall(\".*Set[-]([0-9]{4}).*\", annotation_file)\n",
        "    #     year = int(year_matches[0]) if len(year_matches) > 0 else 2016 # Dev set\n",
        "    # except:\n",
        "    #     print(\"kk\")\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if len(line.strip()) == 0:\n",
        "                continue\n",
        "            try:\n",
        "                parts = line.split(\" | \")\n",
        "                parts = [part.strip() for part in parts]\n",
        "                ref_article_name = parts[1].split(\":\")[1].strip().upper().replace(\".XML\", \"\").replace(\".TXT\", \"\")\n",
        "                ref_article = articles[ref_article_name]\n",
        "                cite_article = articles[\"-\".join(\n",
        "                    parts[2].split(\":\")[1].strip().upper().replace(\"_\", \"-\").replace(\".XML\", \"\").replace(\".TXT\",\n",
        "                                                                                                         \"\").split(\"-\")[\n",
        "                    :2])]\n",
        "                marker_offset = [int(x) for x in (\n",
        "                    parts[5].split(\":\")[1].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()).replace(\",\",\n",
        "                                                                                                               \" \").split()]\n",
        "                citation_offsets = [int(x) for x in (\n",
        "                    parts[5].split(\":\")[1].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()).replace(\",\",\n",
        "                                                                                                               \" \").split()]\n",
        "                ref_offsets = [int(x) for x in (\n",
        "                    parts[7].split(\":\")[1].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()).replace(\",\",\n",
        "                                                                                                               \" \").split()]\n",
        "                facet = parts[9].split(\":\")[1].strip()\n",
        "                facet = [facet] if '[' not in facet else eval(facet)\n",
        "                if ref_article_name not in author_memo:\n",
        "                    url = \"https://www.aclweb.org/anthology/{}.bib\".format(ref_article_name)\n",
        "                    info = requests.get(url=url).text\n",
        "                    author_info = info.split('{')[1].split(',')[0].split('-')\n",
        "                    author = get_formatted_author_info(author_info)\n",
        "                    author_memo[ref_article_name] = author_info\n",
        "                else:\n",
        "                    author_info = author_memo[ref_article_name]\n",
        "                    author = get_formatted_author_info(author_info)\n",
        "                year = author_info[-2]\n",
        "                # author = '' if len(parts) < 10 or \":\" not in parts[10] else parts[10].split(\":\")[1].replace(\"|\",\n",
        "                #                                                                                             \"\").strip()\n",
        "                reference, cite = get_clean_cite_and_ref(ref_article, cite_article, ref_offsets, citation_offsets,\n",
        "                                                         author, year)\n",
        "                # reference, cite = ref_article, cite_article\n",
        "                d = Datum(reference, cite, Offsets(marker_offset, citation_offsets, ref_offsets), author,\n",
        "                          is_test, facet, year)\n",
        "                data.append(d)\n",
        "            except Exception as e:\n",
        "                l.error(e)\n",
        "                annotation_load_fail += 1\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_folder(folder_root):\n",
        "    annotation_folder = folder_root + sep + \"annotation\"\n",
        "    citance_dir = folder_root + sep + \"Citance_XML\"\n",
        "    ref_name = folder_root.split(sep)[-1].replace(\"_TRAIN\", \"\")\n",
        "    ref_dir = folder_root + sep + \"Reference_XML\" + sep + ref_name + \".xml\"\n",
        "    articles = {\n",
        "        '-'.join(os.path.splitext(x)[0].upper().replace(\"_\", \"-\").split(\"-\")[:2]): load_article(citance_dir + sep + x)\n",
        "        for x in os.listdir(citance_dir)}\n",
        "\n",
        "    assert len(articles) == len(os.listdir(citance_dir))\n",
        "\n",
        "    ref_article = load_article(ref_dir)\n",
        "    articles[ref_name.upper()] = ref_article\n",
        "    # Hack\n",
        "    for key in articles:\n",
        "        if articles[key]:\n",
        "            articles[key] = articles[key]._replace(id=key)\n",
        "\n",
        "    annotation_file = newest_file(annotation_folder)\n",
        "    folder_data = load_folder_data(annotation_file, articles)\n",
        "    return folder_data\n",
        "\n",
        "\n",
        "def load_all(root):\n",
        "    print(\"Loading all folders, skipping 2019 and test set 2018 for now\")\n",
        "    folders = get_folders(root)\n",
        "    print(\"Going to load :\", len(folders))\n",
        "    dataset = [load_folder(folder) for folder in tqdm(folders)]\n",
        "    dataset = [data for datalist in dataset for data in datalist]\n",
        "    print()\n",
        "    print(\"Paper load fails due to xml errors : \", paper_load_fail)\n",
        "    print(\"Annotation load fails due to random reasons\", annotation_load_fail)\n",
        "    print(\"Overall loaded \", len(dataset), \" datapoints\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_clean_cite_and_ref(ref_article, cite_article, ref_offsets, citation_offsets, author, year):\n",
        "    global replace_count\n",
        "    ref = copy.copy(ref_article)\n",
        "    cite = copy.copy(cite_article)\n",
        "\n",
        "    # remove cites in ref\n",
        "    for key, sentence in ref.sentences.items():\n",
        "        ref.sentences[key] = get_cites(sentence)\n",
        "\n",
        "    cite_sentence = \" \".join([cite.sentences[c] for c in citation_offsets])\n",
        "    cite_sentence = re.sub(r\"\\D(\\d{4})\\D\", '', cite_sentence)  # regex for removing years\n",
        "    cite_sentence = re.sub(r\"\\[[0-9]{1,3}\\]\", '', cite_sentence)  # regex for removing citation numbers\n",
        "    translation = {ord(')'): None, ord('('): None, ord('.'): None,  ord(','): None, ord('!'): 'l'}\n",
        "    cite_sentence = cite_sentence.translate(translation)\n",
        "    author_info = author.split(\" \")\n",
        "    author_2 = None\n",
        "    if author_info[-1] in [\"et.al.\", \"etal\", \"Etal\"]:\n",
        "        author_1 = \" \".join(author_info[:-1]) + \" et al\"\n",
        "        if len(author_info) >= 2:\n",
        "            author_2 = author_info[0] + \" and \" + author_info[1]\n",
        "    elif len(author_info) >= 2 and author_info[-2] not in [\"And\", \"and\"]:\n",
        "        author_1 = author_info[0] + \" & \" + author_info[1]\n",
        "        author_2 = author_info[0] + \" and \" + author_info[1]\n",
        "    else:\n",
        "        author_1 = author_info[-1]\n",
        "\n",
        "    citing_paper_text = author_1\n",
        "    old_cite_sentence = cite_sentence\n",
        "    cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "    if cite_sentence != old_cite_sentence:\n",
        "        replace_count += 1\n",
        "    elif author_2:\n",
        "        citing_paper_text = author_2\n",
        "        cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "        if cite_sentence != old_cite_sentence:\n",
        "            replace_count += 1\n",
        "        elif len(author_info) >= 2 and author_info[-2] not in [\"And\", \"and\"]:\n",
        "            author_info.sort()\n",
        "            author_1 = author_info[0] + \" & \" + author_info[1]\n",
        "            author_2 = author_info[0] + \" and \" + author_info[1]\n",
        "            citing_paper_text = author_1\n",
        "            old_cite_sentence = cite_sentence\n",
        "            cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "            if cite_sentence != old_cite_sentence:\n",
        "                replace_count += 1\n",
        "            elif author_2:\n",
        "                citing_paper_text = author_2\n",
        "                cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "                if cite_sentence != old_cite_sentence:\n",
        "                    replace_count += 1\n",
        "                else:\n",
        "                    print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence,\n",
        "                                                                                          old_cite_sentence, author))\n",
        "            else:\n",
        "                print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence,\n",
        "                                                                                      old_cite_sentence, author))\n",
        "        else:\n",
        "            print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence, old_cite_sentence, author))\n",
        "    else:\n",
        "        print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence, old_cite_sentence, author))\n",
        "\n",
        "    # remove all other cites in cite\n",
        "    cite.sentences[citation_offsets[0]] = cite_sentence\n",
        "    for key, sentence in cite.sentences.items():\n",
        "        cite.sentences[key] = get_cites(sentence)\n",
        "\n",
        "    if len(citation_offsets) > 1:\n",
        "        for offset in citation_offsets[1:]:\n",
        "            cite.sentences[offset] = \"\"\n",
        "    print(\"replace_count: \", replace_count)\n",
        "    return ref, cite\n",
        "\n",
        "\n",
        "def get_formatted_author_info(author_info):\n",
        "    author_details = \" \".join(author_info)\n",
        "    author_info = re.sub(r\"[0-9]{4}\", '_', author_details).split('_')[0].split()\n",
        "    author = \" \".join([x.capitalize() if x is not \"and\" else x for x in author_info])\n",
        "    return author\n",
        "\n",
        "def get_cites(sentence):\n",
        "    regex = r\"\\(\\D*\\d{4}(;\\D*\\d{4})*\\)\"\n",
        "    sentence = re.sub(regex, \" \", sentence)\n",
        "    return sentence\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(filename='example.log', level=logging.ERROR, filemode='w')\n",
        "    print(\"Logging to example.log\")\n",
        "dataset = load_all(DATA_ROOT)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to example.log\n",
            "Loading all folders, skipping 2019 and test set 2018 for now\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-24646f59e22d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'example.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logging to example.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-24646f59e22d>\u001b[0m in \u001b[0;36mload_all\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading all folders, skipping 2019 and test set 2018 for now\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_folders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Going to load :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-24646f59e22d>\u001b[0m in \u001b[0;36mget_folders\u001b[0;34m(data_root)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mfolder_skips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0myear_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0myear_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0myear_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEKF5Vsycuzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRdtxJyN0Ovb",
        "colab_type": "code",
        "outputId": "a3079618-af0e-42f6-a8b4-dffc04a5af11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "pip install transformers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 61.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 61.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=6650412c6ebd99525730053f180e28016e24dc1b9d6b7e8611eccee057e55b3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu-StjCM0RaV",
        "colab_type": "code",
        "outputId": "e9162ef8-4f95-4d54-cf44-4fd43ae35324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3MRjFPniOxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "class CiteModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CiteModel, self).__init__()\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.fc1 = nn.Linear(in_features=768*4, out_features=1024)\n",
        "        # self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
        "        # self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(in_features=512, out_features=2)\n",
        "\n",
        "    def forward(self, ref_sentences, cite_sentences):\n",
        "        ref_encoded = self.tokenizer.batch_encode_plus([x.lower() for x in ref_sentences], add_special_tokens=True)['input_ids']\n",
        "        cite_encoded = self.tokenizer.batch_encode_plus([x.lower() for x in cite_sentences], add_special_tokens=True)['input_ids']\n",
        "        # print(ref_encoded)\n",
        "        # print(cite_encoded)\n",
        "        # sdfc\n",
        "        ref_encoded = pad_to_max(ref_encoded)\n",
        "        cite_encoded = pad_to_max(cite_encoded)\n",
        "        # print(ref_encoded)\n",
        "        # print(cite_encoded)\n",
        "        # sdfc\n",
        "        ref_encoded, cite_encoded = torch.LongTensor(ref_encoded), torch.LongTensor(cite_encoded)\n",
        "        x, ref_encoded = self.model(ref_encoded.to(device))\n",
        "        y, cite_encoded = self.model(cite_encoded.to(device))\n",
        "        x = torch.cat([ref_encoded, cite_encoded, ref_encoded-cite_encoded, ref_encoded*cite_encoded], dim=1).to(device)\n",
        "        x = self.act1(self.dropout1(self.fc1(x)))\n",
        "        x = self.act2(self.dropout2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxzCSUX2kNTI",
        "colab_type": "code",
        "outputId": "9dd4afb6-95fd-47f9-9d57-088874f6da80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRXx_iH2mCGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rake_nltk import Rake\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "rakey = Rake(max_length=3)\n",
        "\n",
        "def encode(sentence):\n",
        "  return sentence\n",
        "\n",
        "\n",
        "def get_similarity_score(sentence1, sentence2):\n",
        "    # doesn't do anything about frequency of words in a sentence\n",
        "    tokens1 = set(re.findall(r'[\\w]+', sentence1.lower()))\n",
        "    tokens2 = set(re.findall(r'[\\w]+', sentence2.lower()))\n",
        "    # print(sentence1)\n",
        "    rakey.extract_keywords_from_sentences(sentence1.replace(\"-\", \" \").lower().split())\n",
        "\n",
        "    keys1 = rakey.get_ranked_phrases()\n",
        "    #keys1 = [x.split() for x in keys1]\n",
        "    # print(keys1)\n",
        "    #keys1 = [item for sublist in keys1 for item in sublist]\n",
        "    # print(keys1)\n",
        "\n",
        "    rakey.extract_keywords_from_sentences(sentence2.replace(\"-\", \" \").lower().split())\n",
        "    keys2 = rakey.get_ranked_phrases()\n",
        "    #keys2 = [x.split() for x in keys2]\n",
        "    #keys2 = [item for sublist in keys2 for item in sublist]\n",
        "\n",
        "    #keys = set(keys1) and set(keys2)\n",
        "    tokens1 = set(keys1) - stop #- set(ref_keys)\n",
        "    tokens2 = set(keys2) - stop #- set(cite_keys)\n",
        "    #print(tokens1)\n",
        "    #print(tokens2)\n",
        "    #wefwefwef\n",
        "    # tokens1 = tokens1 - stop\n",
        "    # tokens2 = tokens2 - stop\n",
        "  \n",
        "    return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEUtYKQtoKBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_to_max(sentences):\n",
        "  max_len = max([len(x) for x in sentences])\n",
        "  sentences = [np.pad(x, (0, max_len - len(x)), mode='constant', constant_values=3) for x in sentences]\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M4yTaAGEdiH",
        "colab_type": "code",
        "outputId": "363303d0-90d7-4da3-9841-4fcda7b8bb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rakey = Rake(max_length=1)\n",
        "rakey.extract_keywords_from_sentences(\"We introduce a morphological tagger which are used to tag important documents\".lower().split())\n",
        "keys2 = rakey.get_ranked_phrases()\n",
        "print(set(keys2)-stop)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tag', 'introduce', 'important', 'tagger', 'used', 'documents', 'morphological'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK_hyWGPz0fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CiteModel()\n",
        "model.cuda()\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/QA/model2.pt'))\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60h6CKCvgCGl",
        "colab_type": "code",
        "outputId": "18e3809b-9343-46bd-f195-4d54f7f0580c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "b51a77c6bd71470d9f9d96b96e55a2a2",
            "af6ad25ff7004703b67967d700a1c38e",
            "c4839fe445c541b287f119704efae273",
            "9d7034dbdb064e33858f57f6e429a72a",
            "bc26605539b64a5fbf46efcbcbba24ca",
            "9f1c5defe0ff4a3dafef558ff5dd42a6",
            "378851e844d7477d8d6eeea250fb1f98",
            "a910b82313f841f19cc139b86bbf4a7b"
          ]
        }
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "accuracy = 0\n",
        "total = 0\n",
        "empty_citations = 0\n",
        "empty_references = 0\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/My Drive/processed-data-2019.json\", 'rb') as f:\n",
        "  dataset = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/My Drive/processed-data-2018.json\", 'rb') as f:\n",
        "  dataset2 = pickle.load(f)\n",
        "\n",
        "dataset =  dataset2 + dataset\n",
        "\n",
        "articles = [x[1] for x in dataset]\n",
        "n = 5\n",
        "\n",
        "\n",
        "\n",
        "pbar = tqdm(dataset)\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "avg_score = 0\n",
        "all_scores = []\n",
        "for data in pbar:\n",
        "    ref_article = data.ref\n",
        "    citing_article = data.cite\n",
        "    offsets = data.offsets\n",
        "    citing_sentence_ids = offsets.cite\n",
        "    true_ref_sentences = offsets.ref\n",
        "    true_ref_sentence_ids = offsets.ref\n",
        "    if tp > 0:\n",
        "      p = tp/(max(tp+fp,1))\n",
        "      r = tp/(max(tp+fn,1))\n",
        "      f1 = 2*p*r/(p+r)\n",
        "      pbar.set_description(\"Processing %.3f %.3f %.3f\" %(p, r, f1))\n",
        "    if citing_article.sentences:\n",
        "      # raku.extract_keywords_from_sentences([citing_article.sentences[x] for x in citing_article.sentences])\n",
        "      # cite_keys = raku.get_ranked_phrases()[:0]\n",
        "      # raku.extract_keywords_from_sentences([ref_article.sentences[x] for x in ref_article.sentences])\n",
        "      # ref_keys = raku.get_ranked_phrases()[:0]\n",
        "\n",
        "      new_ids = [c for c in citing_sentence_ids]\n",
        "      for c in citing_sentence_ids:\n",
        "            # If additional context is reqd\n",
        "            to_add  = 0\n",
        "            extra = range(max(1, c-to_add), c)\n",
        "            new_ids.extend(extra)\n",
        "            extra = range(c+1, min(len(citing_article.sentences), c + to_add+1))\n",
        "            new_ids.extend(extra)\n",
        "      citing_sentence_ids = new_ids\n",
        "      complete_citing_sentence = \" \".join([citing_article.sentences[c] for c in citing_sentence_ids])\n",
        "      ref_vs = [(x[0],encode(x[1])) for x in ref_article.sentences.items() if abs(len(x[1].split()) - len(complete_citing_sentence.split())) <=40]\n",
        "      similarity_score = {}\n",
        "      for i in range(1):#c in citing_sentence_ids:\n",
        "        complete_citing_sentence = ' '.join([citing_article.sentences[x] for x in citing_sentence_ids])\n",
        "        complete_citing_sentence = citing_article.sentences[c]\n",
        "        #complete_citing_sentence = re.sub('\\((.+?)\\)', '', complete_citing_sentence)\n",
        "        complete_citing_sentence = encode(complete_citing_sentence)\n",
        "        for ref_id, ref_sentence in ref_vs: #(ref_article.sentences.items()):\n",
        "            try:\n",
        "                similarity_score[ref_id] = max(similarity_score.get(ref_id, 0) , get_similarity_score(ref_sentence, complete_citing_sentence))\n",
        "            except Exception as e:\n",
        "                # print(e)\n",
        "                pass\n",
        "      if similarity_score:\n",
        "          sorted_similarity_score = sorted(similarity_score.items(), key=lambda item: -item[1])\n",
        "          top_n = [s for s in sorted_similarity_score]\n",
        "          top_n = {x[0]:x[1] for x in top_n[:5]}\n",
        "          # if len(top_n) > 1:\n",
        "          #   top_n = rerank2(top_n, ref_article, ' '.join([citing_article.sentences[x] for x in citing_sentence_ids]))\n",
        "\n",
        "          fp += len(top_n)\n",
        "          for x in true_ref_sentence_ids:\n",
        "            if x in top_n:\n",
        "              avg_score = (tp*avg_score + top_n[x])/(max(tp,1))\n",
        "              all_scores.append(top_n[x])\n",
        "              fp -= 1\n",
        "              tp += 1\n",
        "            else:\n",
        "              #print(ref_article.sentences[x])\n",
        "              #print([citing_article.sentences[x] for x in citing_sentence_ids])\n",
        "              fn += 1\n",
        "print(tp, fp, fn)\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b51a77c6bd71470d9f9d96b96e55a2a2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=17135), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEjC9vtCjx59",
        "colab_type": "code",
        "outputId": "a930afb6-6a78-4303-d997-249c3ea39b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-300\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.10.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.2)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.23)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.7.2)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.0.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.23)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (46.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (4.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.8)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.51.0)\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVP86bl2noms",
        "colab_type": "code",
        "outputId": "de761def-4fca-4917-be84-7f37cadde9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "s1 = 'This room is dirty'\n",
        "s2 = 'dirty and disgusting room' #corrected variable name\n",
        "print(model.wv.n_similarity(s1.lower().split(), s2.lower().split()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.80902576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLZpf-nmzuJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def rerank(top_n, ref_article, cite):\n",
        "  with torch.no_grad():\n",
        "    scores = {}\n",
        "    refs = []\n",
        "    cites = []\n",
        "    ref_article, ref_sections = ref_article.sentences, ref_article.sections\n",
        "    try:\n",
        "      ref_title = ref_article[0]\n",
        "    except:\n",
        "      ref_title = \"No title\"\n",
        "    id_map = {}\n",
        "    i = 0\n",
        "    for x in top_n:\n",
        "      x1 = (\" \".join([ref_article[x]]),\n",
        "                            cite, 1, ref_title.strip(),\n",
        "                            ref_sections[x].strip() if any(c.isalpha() for c in ref_sections[x].strip()) else \"##other##\")\n",
        "      ref = (\" \" + x1[3]+x1[4]+x1[0])[:512]\n",
        "      cite = \" \" + x1[1][:512]\n",
        "      refs.append(ref)\n",
        "      cites.append(cite)\n",
        "      id_map[x] = i\n",
        "      i += 1\n",
        "    results = model.forward(refs, cites)\n",
        "    results = F.softmax(results)\n",
        "    results = results[:,1].cpu().detach().numpy().tolist()\n",
        "    top_n_v = top_n.items()\n",
        "    # print(top_n_v)\n",
        "    top_n_v = sorted(top_n_v, key = lambda x : results[id_map[x[0]]], reverse=True)\n",
        "    top_n_v = [x for x in top_n_v if results[id_map[x[0]]] > 0.27]\n",
        "    top_n_v = top_n_v[:3]\n",
        "    top_n = {x:y for x,y in top_n_v}\n",
        "    return top_n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igog7ZvMxdKE",
        "colab_type": "code",
        "outputId": "babcc68d-e3c1-49f5-8171-1f0db6fedc93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3g5Z51QoSmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxtZPgV1-1KU",
        "colab_type": "code",
        "outputId": "ba46e4e0-f31f-4c47-f316-acb87c8241f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer, AdamW\n",
        "import json\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/model.npy\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7qmuIbj-Syi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def rerank2(top_n, ref_article, cite):\n",
        "  with torch.no_grad():\n",
        "    scores = {}\n",
        "    refs = []\n",
        "    cites = []\n",
        "    ref_article, ref_sections = ref_article.sentences, ref_article.sections\n",
        "    try:\n",
        "      ref_title = ref_article[0]\n",
        "    except:\n",
        "      ref_title = \"No title\"\n",
        "    id_map = {}\n",
        "    i = 0\n",
        "    for x in top_n:\n",
        "      x1 = (\" \".join([ref_article[x]]),\n",
        "                            cite, 1, ref_title.strip(),\n",
        "                            ref_sections[x].strip() if any(c.isalpha() for c in ref_sections[x].strip()) else \"##other##\")\n",
        "      ref = (\" \" + x1[3]+x1[4]+x1[0])[:512]\n",
        "      cite = \" \" + x1[1][:512]\n",
        "      refs.append(ref)\n",
        "      cites.append(cite)\n",
        "      id_map[x] = i\n",
        "      i += 1\n",
        "    \n",
        "    pairofstrings = list(zip([x.lower() for x in refs], [x.lower() for x in cites]))\n",
        "    encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',max_length=512,\n",
        "                                                            return_special_tokens_masks=True, pad_to_max_length=True)\n",
        "    attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "    input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(device)\n",
        "    results = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0] \n",
        "    results = F.softmax(results)\n",
        "    results = results[:,1].cpu().detach().numpy().tolist()\n",
        "    top_n_v = top_n.items()\n",
        "    # print(top_n_v)\n",
        "    top_n_v = sorted(top_n_v, key = lambda x : results[id_map[x[0]]], reverse=True)\n",
        "    top_n_v = [x for x in top_n_v if results[id_map[x[0]]] > 0.1 ]\n",
        "    top_n_v = top_n_v[:3]\n",
        "    top_n = {x:y for x,y in top_n_v}\n",
        "    return top_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u-mVec2oiUf",
        "colab_type": "code",
        "outputId": "e1ab93c6-9009-4db9-bb60-e8113576a0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sorted(all_scores)[len(all_scores)//2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7334244"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDgZgwBKoo6r",
        "colab_type": "code",
        "outputId": "84363a55-5942-46fd-d15e-e6544acb0342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Datum(ref=Article(xml=<xml.etree.ElementTree.ElementTree object at 0x7ff71e1e0d68>, sentences={0: 'OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION', 1: 'Test abstract', 2: 'The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.', 3: 'Participants were invited to enter their systems in as many as four different task-oriented evaluations.', 4: 'The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.', 5: 'The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years.', 6: 'The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.', 7: 'All except the Scenario Template task are defined independently of any particular domain.', 8: 'This paper surveys the results of the evaluation on each task and, to a more limited extent, across tasks.', 9: 'Discussion of the results for each task is organized generally under the following topics:  Results on task as whole;  Results on some aspects of task;  Performance on \"walkthrough article.\"', 10: 'The walkthrough article is an article selected from the test set.', 11: \"Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers.\", 12: 'EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.', 13: ' Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.', 14: ' Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence from anywhere in the text.', 15: ' Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.', 16: 'The two SGML-based tasks required innovations to tie system-internal data structures to the original text so that the annotations could be inserted by the system without altering the original text in any other way.', 17: 'This capability has other useful applications as well, e.g., it enables text highlighting in a browser.', 18: 'It also facilitates information extraction, since some of the information in the extraction templates is in the form of literal text strings, which some systems have in the past had difficulty reproducing in their output.', 19: 'The inclusion of four different tasks in the evaluation implicitly encouraged sites to design general-purpose architectures that allow the production of a variety of types of output from a single internal representation in order to allow use of the full range of analysis techniques for all tasks.', 20: 'Even the simplest of the tasks, Named Entity, occasionally requires in-depth processing, e.g., to determine whether \"60 pounds\" is an expression of weight or of monetary value.', 21: 'Nearly half the sites chose to participate in all four tasks, and all but one site participated in at least one SGML task and one extraction task.', 22: 'The variety of tasks designed for MUC6 reflects the interests of both participants and sponsors in assessing and furthering research that can satisfy some urgent text processing needs in the very near term and can lead to solutions to more challenging text understanding problems in the longer term.', 23: 'Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem.', 24: 'Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns.', 25: 'The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting.', 26: 'Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1].', 27: 'CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.', 28: 'The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.', 29: 'This period comprised the \"evaluation epoch.\"', 30: 'As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.', 31: 'The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.', 32: 'It can also be used to do unranked, Boolean retrievals.', 33: 'The Boolean retrieval method was used in the initial probing of the corpus to identify candidates for the Scenario Template task, because the Boolean retrieval is relatively fast, and the unranked results are easy to scan to get a feel for the variety of nonrelevant as well as relevant documents that match all or some of the query terms.', 34: 'Once the scenario had been identified, the ranked retrieval method was used, and the ranked list was sampled at different points to collect approximately 200 relevant and 200 nonrelevant articles, representing a variety of article types (feature articles, brief notices, editorials, etc.).', 35: 'From those candidate articles, the training and test sets were selected blindly, with later checks and corrections for imbalances in the relevant/nonrelevant categories and in article types.', 36: 'From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks.', 37: 'The selection was again done blindly, with later checks to ensure that the set was fairly representative in terms of article length and type.', 38: 'Note that although Named Entity, Coreference and Template Element are defined as domain-independent tasks, the articles that were used for MUC6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task.', 39: 'The manually filled templates were created with the aid of Tabula Rasa, a software tool developed for the Tipster Text Program by New Mexico State University Computing Research Laboratory.', 40: 'NAMED ENTITY The Named Entity (NE) task requires insertion of SGML tags into the text stream.', 41: 'The tag elements are ENAMEX (for entity names, comprising organizations, persons, and locations), TIMEX (for temporal expressions, namely direct mentions of dates and times), and NUMEX (for number expressions, consisting only of direct mentions of currency values and percentages).', 42: 'A TYPE attribute accompanies each tag element and identifies the subtype of each tagged string: for ENAMEX, the TYPE value can be ORGANIZATION, PERSON, or LOCATION; for TIMEX, the TYPE value can be DATE or TIME; and for NUMEX, the TYPE value can be MONEY or PERCENT.', 43: 'Text strings that are to be annotated are termed markables.', 44: 'As indicated above, markables include names of organizations, persons, and locations, and direct mentions of dates, times, currency values and percentages.', 45: 'Non-markables include names of products and other miscellaneous names (\"Macintosh,\" \"Wall Street Journal\" (in reference to the periodical as a physical object), \"Dow Jones Industrial Average\"); names of groups of people and miscellaneous usages of person names (\"Republicans,\" \"GrammRudman,\" \"Alzheimer[\\'s]\"); addresses and adjectival forms of location names (\"53140 Gatchell Rd.,\" \"American\"); indirect and vague mentions of dates and times (\"a few minutes after the hour,\" \"thirty days before the end of the year\"); and miscellaneous uses of numbers, including some that are similar to currency or percentage expressions (\"[Fees] 1 3/4,\" \"12 points,\" \"1.5 times\").', 46: 'The evaluation metrics used for NE are essentially the same as those used for the two template-filling tasks, Template Element and Scenario Template.', 47: 'The following breakdowns of overall scores on NE are computed:  by slot, i.e., for performance across tag elements, across TYPE attributes, and across tag strings;  by subcategorization, i.e., for performance on each TYPE attribute separately;  by document section, i.e., for performance on distinct subparts of the article, as identified by the SGML tags contained in the original text: <HL> (\"headline\"), <DD> (\"document date\"), <DATELINE>, and <TXT> (the body of the article).', 48: 'NE Results Overall Fifteen sites participated in the NE evaluation, including two that submitted two system configurations for testing and one that submitted four, for a total of 20 systems.', 49: 'As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.', 50: 'On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well.', 51: 'It was also unexpected that one of the systems would match human performance on the task.', 52: 'Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC.', 53: 'This test measures the amount of variability between the annotators.', 54: 'When the outputs are scored in \"key-to-response\" mode, as though one annotator\\'s output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.', 55: 'The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%.', 56: 'In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1.', 57: 'Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74..', 58: ' It represents just one style of writing \"the Chrysler division\" (currently, only \"Chrysler\" (journalistic) and has a basic basic toward financial news and a specific bias toward the topic of the Scenario Template task.', 59: ' It was very small (only 30 articles).', 60: 'There were no markable time expressions in the test set, and there were only a few markable percentage expressions.', 61: 'The results should also be qualified by saying that they reflect performance on data that makes accurate usage of upper and lower case distinctions.', 62: \"What would performance be on data where case provided no (reliable) clues and for languages where case doesn't distinguish names?\", 63: 'SRA ran an experiment on an uppercase version of the test set that showed 85% recall and 89% precision overall, with identification of organization names presenting the greatest problem.', 64: 'That result represents nearly a 10-point decrease on the F-measure from their official baseline.', 65: \"The case-insensitive results would be slightly better if the task guidelines themselves didn't depend on case distinctions in certain situations, as when identifying the right boundary for the organization name span in a string such as would be tagged).\", 66: 'NE Results on Some Aspects of Task Figures 1 and 2 show the sample size for the various tag elements and TYPE values.', 67: 'Note that nearly 80% of the tags were ENAMEX and that almost half of those were subcategofized as organization names.', 68: 'As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.', 69: 'Organization names are varied in their form, consisting of proper nouns, general vocabulary, or a mixture of the two.', 70: 'They can also be quite long and complex and can even have internal punctuation such as a commas or an ampersand.', 71: 'Sometimes it is difficult to distinguish them from names of other types, especially from person names.', 72: 'Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..', 73: 'The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA.', 74: 'The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration.', 75: 'The changes occurred only in performance on identifying organizations.', 76: 'BBN conducted a comparative test in which the experimental configuration used a larger lexicon than the baseline configuration, but the exact nature of the difference is not known and the performance differences are very small.', 77: 'As with the SRA experiment, the only differences in performance between the two BBN configurations are with the organization type.', 78: \"The University of Durham reported that they had intended to use gazetteer and company name lists, but didn't, because they found that the lists did not have much effect on their system's performance.\", 79: 'The error scores for persons, dates, and monetary expressions was less than or equal to 10% for the large majority of systems.', 80: 'Several systems posted scores under 10% error for locations, but none was able to do so for oganizations.', 81: 'For percentages, about half the systems had 0% error, which reflects the simplicity of that particular subtask.', 82: 'Note that the number of instances of percentages in the test set is so small that a single mistake could result in an error of 6%.', 83: 'Slot-level performance on ENAMEX follows a different pattern for most systems from slot-level performance on NUMEX and TIMEX.', 84: 'The general pattern is for systems to have done better on the TEXT slot than on the TYPE slot for ENAMEX tags and for systems to have done better on the TYPE slot than on the TEXT slot for NUMEX and TIMEX tags.', 85: 'Errors on the TEXT slot are errors in finding the right span for the tagged string, and this can be a problem for all three subcategories of tag.', 86: 'The TYPE slot, however, is a more difficult slot for ENAMEX than for the other subcategories.', 87: 'It involves a three-way distinction for ENAMEX and only a two-way distinction for NUMEX and TIMEX, and it offers the possibility of confusing names of one type with names of another, especially the possibility of confusing organization names with person names.', 88: 'Looking at the document section scores in table 3, we see that the error score on the body of the text was much lower than on the headline for all but a few systems.', 89: 'There was just one system that posted a higher error score on the body than on the headline, the baseline NMSU CRL configuration, and the difference in scores is largely due to the fact that the system overgenerated to a greater extent on the body than on the headline.', 90: 'Its basic strategy for 96.42 0 95.66 0 0 7 7 94.92 0 0 8 8 94.00 0 0 20 9 93.65 0 2 16 10 93.33 0 4 38 9 92.88 0 0 18 10 92.74 0 0 22 11 92.61 100 0 18 9 91.20 0 0 30 13 90.84 3 11 19 14 89.06 3 4 28 18 88.19 0 0 22 20 85.82 0 6 18 21 85.73 0 44 53 21 84.95 0 0 50 21 Table 3.', 91: 'NE document subsection scores (ERR metric), in order of decreasing overall F-measure (P&R) headlines was a conservative one: tag a string in the headline as a name only if the system had found it in the body of the text or if the system had predicted the name based on truncation of names found in the body of the text.', 92: 'Most, if not all, the systems that were evaluated on the NE task adopted the basic strategy of processing the headline after processing the body of the text.', 93: 'The interannotator variability test provides reference points indicating human performance on the different aspects of the NE task.', 94: 'The document section results show 0% error on Document Date and Dateline, 7% error on Headline, and 6% error on Text.', 95: 'The subcategory error scores were 6% on Organization, 1% on Person, and 4% on Location, 8% on Date, and 0% on Money and Percent.', 96: 'These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.', 97: 'Analysis of the results shows that some Date errors were a result of simple oversight (e.g., \"fiscal 1994\") and others were a consequence of forgetting or misinterpreting the task guidelines with respect to determining the maximal span of the date expression (e.g., tagging \"fiscal 1993\\'s second quarter\" and \"Aug. 1\" separately, rather than tagging \"fiscal 1993\\'s second quarter, ended Aug. 1\" as a single expression in accordance with the task guidelines).', 98: 'NE Results on \"Walkthrough Article\" In the answer key for the walkthrough article there are 69 ENAMEX tags (including a few optional ones), six TIMEX tags and six NUMEX tags.', 99: 'Interannotator scoring showed that one annotator missed tagging one instance of \"Coke\" as an (optional) organization, and the other annotator missed one date expression (\"September\").', 100: 'Common mistakes made by the systems included missing the date expression, \"the 21st century,\" and spuriously identifying \"60 pounds\" (which appeared in the context, \"Mr. Dooner, who recently lost 60 pounds over three-and-a-half months ....', 101: '\") as a monetary value rather than ignoring it as a weight.', 102: 'In addition, a number of errors identifying entity names were made; some of those errors also showed up as errors on the Template Element task and are described in a later section of this paper.', 103: 'COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe.', 104: 'The variety of high-frequency phenomena covered by the task is partially represented in the following hypothetical example, where all bracketed text segments are considered coreferential: 428 [Motor Vehicles International Corp.] announced a major management shakeup ....', 105: '[MVI] said the chief executive officer has resigned ....', 106: '[The Big 10 auto maker] is attempting to regain market share ....', 107: '[It] will announce significant losses for the fourth quarter ....', 108: 'A [company] spokesman said [they] are moving [their] operations to Mexico in a cost-saving effort ....', 109: '[MVI, [the first company to announce such a move since the passage of the new international trade agreement],] is facing increasing demands from unionized workers ....', 110: '[Motor Vehicles International] is [the biggest American auto exporter to Latin America].', 111: 'The example passage covers a broad spectrum of the phenomena included in the task.', 112: 'At one end of the spectrum are the proper names and aliases, which are inherently definite and whose referent may appear anywhere in the text.', 113: 'In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.', 114: 'On the periphery of the central phenomena are markables whose status as coreferring expressions is determined by syntax, such as predicate nominals (\"Motor Vehicles International is the biggest American auto exporter to Latin America\") and 100 90 80 70 60 50 40 30 20 10 0 0 10 20 30 appositives (\"MVI, the first company to announce such a move since the passage of the new international trade agreement\").', 115: 'At the far end of the spectrum are bare common nouns, such as the prenominal \"company\" in the example, whose status as a referring expression may be questionable.', 116: 'An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task.', 117: 'The algorithm compares the equivalence classes defined by the coreference links in the manually-generated answer key and the system-generated response.', 118: 'The equivalence classes are the models of the identity equivalence coreference relation.', 119: 'Using a simple counting scheme, the algorithm obtains recall and precision scores by determining the minimal perturbations required to align the equivalence classes in the key and response.', 120: 'No metrics other than recall and precision were defined for this task, and no statistical significance testing was performed on the scores.', 121: 'CO Results Overall In all, seven sites participated in the MUC6 coreference evaluation.', 122: 'Most systems achieved approximately the same levels of performance: five of the seven systems were in the 51%-63% recall O0  40 50 60 70 80 90 100 Recall Figure 3.', 123: 'Overall recall and precision on the CO task 2 2 Key to recall and precision scores: UDurham 36R/44P, UManitoba 63R/63P, UMass 44R/51P, NYU 53R/62P, UPenn 55R/63P, USheffield 51R/71P, SRI 59R/72P..', 124: 'range and 62%-72% precision range.', 125: 'About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks.', 126: 'A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision.', 127: 'The precision figure is supported by evidence from the NE evaluation.', 128: 'In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.', 129: 'In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.', 130: 'The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode.', 131: 'The amount of agreement between the two annotators was found to be 80% recall and 82% precision.', 132: 'There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..', 133: 'Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.', 134: 'CO Results on Some Aspects of Task and on \"Walkthrough Article\" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way.', 135: 'Two useful attributes for the equivalence class as a whole would be one to distinguish individual coreference from type coreference and one to identify the general semantic type of the class (organization, person, location, time, currency, etc.).', 136: 'For each NP in the equivalence class, it would be useful to identify its grammatical type (proper noun phrase, definite common noun phrase, bare singular common noun phrase, personal pronoun, etc.).', 137: 'The decision to minimize the annotation effort makes it difficult to do detailed quantitative analysis of the results.', 138: \"An analysis by the participating sites of their system's performance on the walkthrough article provides some insight into performance on aspects of the coreference task that were dominant in that article.\", 139: 'The article contains about 1000 words and approximately 130 coreference links, of which all but about a dozen are references to individual persons or individual organizations.', 140: 'Approximately 50 of the anaphors are personal pronouns, including reflexives and possessives, and 58 of the markables (anaphors and antecedents) are proper names, including aliases.', 141: 'The percentage of personal pronouns is relatively high (38%), compared to the test set overall (24%), as is the percentage of proper names (40% on this text versus an estimate of 30% overall).', 142: 'Performance on this particular article for some systems was higher than performance on the test set overall, reaching as high as 77% recall and 79% precision.', 143: 'These scores indicate that pronoun resolution techniques as well as proper noun matching techniques are good, compared to the techniques required to determine references involving common noun phrases.', 144: 'For common noun phrases, the systems were not required to include the entire NP in the response; the response could minimally contain only the head noun.', 145: 'Despite this flexibility in the expected contents of the response, the systems nonetheless had to implicitly recognize the full NP, since to be considered coreferential, the head and its modifiers all had to be consistent with another markable.', 146: 'TEMPLATE ELEMENT The Template Element (TE) task requires extraction of certain general types of information about entities and merging of the information about any given entity before presentation in the form of a template (or \"object\").', 147: 'For MUC6 the entities that were to be extracted were limited to organizations and persons) The ORGANIZATION object contains attributes (\"slots\") for the string representing the organization name (ORG NAME), for strings representing any abbreviated versions of the name (ORG_ALIAS), for a string that describes the particular organization (ORG_DESCRIPTOR), for a subcategory of the type of organization (ORG_TYPE, whose permissible values are GOVERNMENT, COMPANY, and OTHER), and for canonical forms of the specific and general location of the organization (ORG LOCALE and ORG_COUNTRY).', 148: 'The PERSON object contains 3The task documentation includes definition of an \"artifact\" entity, but that entity type was not used in MUC6 for either the dry run or the formal run.', 149: 'The entity types that were involved in the evaluation are the same as those required for the Scenario Template task.', 150: 'slots only for the string representing the person name (PER_NAME), for strings representing any abbreviated versions of the name (PERALIAS), and for strings representing a very limited range of titles (PER_TITLE).', 151: 'The task places heavy emphasis on recognizing proper noun phrases, as in the NE task, since all slots except ORG_DESCRIPTOR and PERTITLE expect proper names as slot fillers (in string or canonical form, depending on the slot.', 152: 'However, the organization portion of the TE task is not limited to recognizing the referential identity between full and shortened names; it requires the use of text analysis techniques at all levels of text structure to associate the descriptive and locative information with the appropriate entity.', 153: 'Analysis of complex NP structures, such as appositional structures and postposed modifier adjuncts, is needed in order to relate the locale and descriptor to the name in \"Creative Artists Agency, the big Hollywood talent agency\" and in \"Creative Artists Agency, a big talent agency based in Hollywood.\"', 154: 'Analysis of sentence structures to identify grammatical relations such as predicate nominals is needed in order to relate those same pieces of information in \"Creative Artists Agency is a big talent agency based in Hollywood.\"', 155: 'Analysis of discourse structure is needed in order to identify long-distance relationships.', 156: 'The answer key for the TE task contains one object for each specific organization and person mentioned in the text.', 157: 'For generation of a PERSON object, the text must provide the name of the person (full name or part of a name).', 158: 'For generation of an ORGANIZATION object, the text must provide either the name (full or part) or a descriptor of the organization.', 159: 'Since the generation of these objects is independent of the relevance criteria imposed by the Scenario Template (ST) task, there are many more ORGANIZATION and PERSON objects in the TE key than in the ST key.', 160: 'For the formal evaluation, there were 606 ORGANIZATION and 496 PERSON objects in the TE key, versus 120 ORGANIZATION and 137 PERSON objects in the ST key.', 161: 'The same set of articles was used for TE as for ST; therefore, the content of the articles is oriented toward the terms and subject matter covered by the ST task, which concerns changes in corporate management.', 162: '4 One effect of this bias is simply the number of entities mentioned in the articles: for the 4 The method used for selecting the articles for the test set is described at the beginning of this article..', 163: 'test set used for the MUC6 dry run, which was based on a scenario concerning labor union contract negotiations, there were only about half as many organizations and persons mentioned as there were in the test set used for the formal run.', 164: 'TE Results Overall Twelve systems -- from eleven sites, including one that submitted two system configurations for testing--were tested on the TE task.', 165: 'All but two of the systems posted F-measure scores in the 7080% range, and four of the systems were able to achieve recall in the 7080% range while maintaining precision in the 8090% range, as shown in the figure 4.', 166: 'Human performance was measured in terms of variability between the outputs produced by the two NRaD and SAIC evaluators for 30 of the articles in the test set (the same 30 articles that were used for NE and CO testing).', 167: 'Using the scoring method in which one annotator\\'s draft key serves as the \"key\" and the other annotator\\'s draft key serves as the \"response,\" the overall consistency score was 93.14 on the F-measure, with 93% recall and 93% precision.', 168: 'TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5.', 169: 'Figure 6 indicates the relative amount of error contributed by each of the slots in the ORGANIZATION object.', 170: 'It is evident that the more linguistic processing necessary to fill a slot, the harder the slot is to fill correctly.', 171: 'The ORG_COUNTRY slot is a special case in a way, since it is required to be filled when the ORG_LOCALE slot is filled.', 172: '(The reverse is not the case, i.e., ORG_COUNTRY may be filled even if ORG_LOCALE is not, but this situation is relatively rare.)', 173: 'Since a missing or spurious ORG_LOCALE is likely to incur the same error in ORG_COUNTRY, the error scores for the two slots are understandably similar.', 174: '5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..', 175: '((XI 90  ,,v 80   04 ~) 5O 4O 20 10 0 ..', 176: '0 10 220 30 41) 50 60 7(1 80 91) Recall I(X) Figure 4.', 177: 'Overall recall and precision on the TE task 6 10090 80  \"7\" o qb l  70 60 50 40 30 20 10 0 0 10 20 30 40 50 60 70 80 90 100 Recall Figure 5.', 178: 'Organization and Person object recall and precision on the TE task 6Key to recall and precision scores: BBN 66R/79P, UDurham 49R/60P, Lockheed-Martin 76R/77P, UManitoba 71R/78P, UMass 53R/72P, MITRE 71R/85P, NYU 62R/83P, USheffield 66R/74P, SRA baseline configuration 75R/86P, SRA \"noref\" configuration 74R/87P, SRI 74R/76P, Sterling Software 72R/83P.', 179: '432 8o I ,o l II ii 40 3o 2o ,~ 10- type name alias country locale descriptor ORGANIZATION Slot= Figure 6.', 180: 'Best and average error per response fill Organization object slot scores for TE task With respect to performance on ORG_DESCRIPTOR, note that there may be multiple descriptors (or none) in the text.', 181: 'However, the task does not require the system to extract all descriptors of an entity that are contained in the text; it requires only that the system extract one (or none).', 182: 'Frequently, at least one can be found in close proximity to an organization\\'s name, e.g., as an appositive (\"Creative Artists Agency, the big Hollywood talent agency\").', 183: 'Nonetheless, performance is much lower on this slot than on others.', 184: 'Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot?', 185: 'One reason for low performance is that an organization may be identified in a text solely by a descriptor, i.e., without a fill for the ORG_NAME slot and therefore without the usual local clues that the NP is in fact a relevant descriptor.', 186: 'It is, of course, also possible that a text may identify an organization solely by name.', 187: 'Both possibilities present increased opportunities for systems to undergenerate or overgenerate.', 188: 'Also, the descriptor is not always close to the name, and some discourse processing may be requ~ed in order to identify it --this is likely to increase the opportunity for systems to miss the information.', 189: 'A third significant reason is that the response fill had to match the key fill exactly in order to be counted correct; there was no allowance made in the scoring software for assigning full or partial credit if the response fill only partially matched the key fill.', 190: 'It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys.', 191: 'TE Results on \"Walkthrough Article\" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems.', 192: 'Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing.', 193: 'There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.', 194: 'Errors of these kinds result in a penalty at the object level, since the extracted information is contained in the wrong type of object.', 195: 'Examples of each of these types of error appear below, along with the number of systems that committed the error.', 196: '(An experimental configuration of the SRA system produced the same output as the baseline configuration and has been disregarded in the tallies; thus, the total number of systems tallied is eleven.)', 197: '1.', 198: 'Miscategorizations of entities as person (PER_NAME or PER_ALIAS) instead of organization (ORG_NAME or ORG_ALIAS).', 199: '433  Six systems: McCannErickson (also extracted with the name of \"McCann,\" \"One McCann,\" \"While McCann\"; organization category is indicated clearly by context in which full name appears, \"John Dooner Will Succeed James At Helm of McCannErickson\" in headline and \"Robert L. James, chairman and chief executive officer of McCannErickson, and John J. Dooner Jr., the agency\\'s president and chief operating officer\" in the body of the article) eSix systems: J. Walter Thompson (also extracted with the name of \"Walter Thompson\"; organization category is indicated by context, \"Peter Kim was hired from WPP Group\\'s J. Walter Thompson last September...\") eFour systems: Fallon McElligott (organization category is indicated by context, \"...other ad agencies, such as Fallon McElligott\") eOne system: Ammirati & Puris (the presence of the ampersand is a clue, as is the context, \"...president and chief executive officer of Ammirati & Puris\"; but note that the article also mentions the name of one of the company\\'s founders, Martin Puris)', 200: 'organization (ORG NAME) instead of location (ORG_LOCALE) eTwo systems: Hollywood (location category is indicated by context, \"Creative Artists Agency, the big Hollywood talent agency\") . Miscategorization of nonrelevant entities as organization name, alias or descriptor (ORG NAME, ORG_ALIAS, ORG_DESCRIPTOR).', 201: 'oSix systems: New York Times (publication name in phrase, \"a framed page from the New York Times\"; without sufficient context, the name can be ambiguous in its reference to a physical object versus an organization) eThree systems: Coca-Cola Classic (product name deriving from \"Coca-Cola,\" which appears separately in several places in the article and is occasionally ambiguous even in context between product name and organization name) eOne system: Not Butter (part of product name, \"I Can\\'t Believe It\\'s Not Butter\") eOne system: Taster (part of product name, \"Taster\\'s Choice\")  One system: Choice (part of product name, \"Taster\\'s Choice\") eFive systems: a hot agency (nonspecific use of indefinite in phrase \"...is interested in acquiring a hot agency\") Given the variety of contextual clues that must be taken into account in order to analyze the above entities correctly, it is understandable that just about any given system would commit at least one of them.', 202: 'But the problems are certainly tractable; none of the fifteen TE entities in the key (ten ORGANIZATION entities and five PERSON entities) was miscategofized by all of the systems.', 203: 'In addition to miscategorization errors, the walkthrough text provides other interesting examples of system errors at the object level and the slot level, plus a number of examples of system successes.', 204: 'One success for the systems as a group is that each of the six smaller ORGANIZATION objects and four smaller PERSON objects (those with just one or two filled slots in the key) was matched perfectly by at least one system; in addition, one larger ORGANIZATION object and two larger PERSON objects were perfectly matched by at least one system.', 205: 'Thus, each of the five PERSON objects in the key and seven of the ten ORGANIZATION objects in the key were matched perfectly by at least one system.', 206: 'The three larger ORGANIZATION objects that none of the systems got perfectly correct are for the McCannErickson, Creative Artists Agency, and Coca-Cola companies.', 207: \"Common errors in these three ORGANIZATION objects included missing the locale/country or failing to organization's alias with its name.\", 208: 'descriptor identify or the SCENARIO TEMPLATE A Scenario Template (ST) task captures domain-and task-specific information.', 209: 'Three scenarios were defined in the course of MUC6: (1) a scenario concerning the event of organizations placing orders to buy aircraft with aircraft manufacturers (the \"aircraft order\" scenario); (2) a scenario concerning the event of contract negotiations between labor unions and companies (the \"labor negotiations\" scenario); (3) a scenario concerning changes in corporate managers occupying executive posts (the \"management succession\" scenario).', 210: 'The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.', 211: 'One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure.', 212: 'In this article, the management succession scenario will be used as the basis for discussion.', 213: 'The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.', 214: 'At the top level is the TEMPLATE object, of which there is one instantiated for every document.', 215: 'This object points down to one or more SUCCESSION_EVENT objects if the document meets the event relevance criteria given in the task documentation.', 216: 'Each event object captures the changes occurring within a company with respect to one management post.', 217: 'The SUCCESSION_EVENT object points down to the Ib~AND_OUT object, which in turn points down to PERSON Template Element objects that represent the persons involved in the succession event.', 218: 'The IN_AND_OUT object contains ST-specific information that relates the event with the persons.', 219: 'The ORGANIZATION Template Element objects are present at the lowest level along with the PERSON objects, and they are pointed to not only by the IN_AND_OUT object but also by the SUCCESSION_EVENT object.', 220: 'The organization pointed to by the event object is the organization where the relevant management post exists; the organization pointed to by the relational object is the organization that the person who is moving in or out of the post is coming from or going to.', 221: 'The scenario is designed around the management post rather than around the succession act itself.', 222: 'Although the management post and information associated with it are represented in the SUCCESSION_EVENT object, that object does not actually represent an event, but rather a state, i.e., the vacancy of some management post.', 223: 'The relational-level Iih~AND_OUT objects represent the personnel changes pertaining to that state.', 224: 'ST Results Overall Nine sites submitted a total of eleven systems for evaluation on the ST task.', 225: 'All the participating sites also submitted systems for evaluation on the TE and NE tasks.', 226: 'All but one of the development teams (UDurham) had members who were veterans of MUC5.', 227: 'Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.', 228: 'Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object.', 229: 'The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT \" Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7.', 230: 'Management Succession Template Structure intentional and is comparable to the richness of the MUC3 \"TST2\" test set and the MUC4 \"TST4\" test set.', 231: '(The test sets used for MUC5 had a much higher proportion of relevant texts.)', 232: 'Systems are measured for their performance on distinguishing relevant from nonrelevant texts via the text filtering metric, which uses the classic information retrieval definitions of recall and precision.', 233: 'For MUC6, text filtering scores were as high as 98% recall (with precision in the 80th percentile) or 96% precision (with recall in the 80th percentile).', 234: 'Similar tradeoffs and upper bounds on performance can be seen in the TST2 and TST4 results (see score reports in sections 2 and 4 of appendix G in [2]).', 235: 'However, performance of the systems as a group is better on the MUC6 test set.', 236: 'The text filtering results for MUC6, MUC4 (TST4) and MUC3 (TST2) are shown in figure 8.', 237: \"Whereas the Text Filter row in the score report shows the system's ability to do text filtering (document detection), the All Objects row and the individual Slot rows show the system's ability to do information extraction.\", 238: 'The measures used for information extraction include two overall ones, the F-measure and error per response fill, and several other, more diagnostic ones (recall, precision, undergeneration, overgeneration, and substitution).', 239: 'The text filtering definition of precision is different from the information extraction definition of precision; the latter definition includes an element in the formula that accounts for the number of spurious template fills generated.', 240: 'The All Objects recall and precision scores are shown in figure 9.', 241: 'The highest ST F-measure score was 56.40 (47% recall, 70% precision).', 242: 'Statistically, large differences of up to 15 points may not be reflected as a difference in the ranking of the systems.', 243: 'Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]).', 244: 'Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator\\'s templates were treated as the \"key\" and the other annotator\\'s templates were treated as the \"response\".', 245: 'No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.', 246: 'The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.', 247: 'Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations.', 248: 'MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4.', 249: 'Highest P&R F-Measure scores posted for MUC6 and MUC5 ST tasks Note that table 4 shows four top scores for MUC5, one for each language-domain pair: English Joint Ventures (EJV), Japanese Joint Ventures (JJV), English Microelectronics (EME), and Japanese Microelectronics (JME).', 250: 'From this table, it may be reasonable to conclude that progress has been made, since the MUC6 performance level is at least as high as for three of the four MUC5 tasks and since that performance level was reached after a much shorter time.', 251: 'ST Results on Some Aspects of Task and on \"Walkthrough Article\" Three succession events are reported in the walkthrough article.', 252: 'Successful interpretation of three sentences from the walkthrough article is necessary for high performance on these events.', 253: 'The tipoff on the first two events comes at the end of the second paragraph: Yesterday, McCann made official what had been widely anticipated: Mr. James, 57 years old, is stepping down as chief executive officer on July 1 and will retire as chairman at the end of the year.', 254: 'He will be succeeded by Mr. Dooner, 45.', 255: \"The basis of the third event comes halfway through the two-page article: In addition, Peter Kim was hired from WPP Group's J. Walter Thompson last September as vice chairman, chief strategy officer, worldwide.\", 256: '7Key to recall and precision scores: BBN 50R/59P, UDurham 33R/34P, Lockheed-Martin 43R/64P, UManitoba 39R/62P, UMass 36R/46P, NYU 47R/70P, USheffield 37R/73P, SRA baseline configuration 47R/62P, SRA \"precision\" configuration 32R/66P, SRA \"recall\" configuration 58R/46P, SRI 44R/61P.', 257: '437 Answer Key System Output Event James out, Dooner in as CEO of McCannJames out, Dooner in as CEO of McCann- #1 Erickson as a result of James departing the Erickson as a result of a reassignment of James; workforce; James is still on the job as CEO; James is no__!', 258: 'on the job as CEO any more, Dooner is not on the job as CEO yet, and his old his new job is at the same as his old job; Dooner job was with the same org as his new job.', 259: 'may or may not be on the job as CEO yet, and his old job was with the same org as his new job.', 260: '(SRA satie_base system) Event James out, Dooner in as chairman of James out, Dooner in as chairman of #2 McCannErickson as a result of James departing McCannErickson as a result of James departing the workforce; James is still on the job as the workforce; James is no_4 on the job as chairman; Dooner is not on the job as chairman chairman any more; Dooner is already on the job yet, and his old job was with the same org as his as chairman, and his old job was with Ammirati new job.', 261: '& Puris.', 262: '(NYU system) Event Kim in as \"vice chairman, chief strategy Kim in as vice chairman of WPP Group, #3 officer, worldwide\" of McCannErickson, where where the vacancy existed for other/unknown the vacancy existed for other/unknown reasons; reasons; he may or may not be on the job in that he is already on the job in the post, and his old post yet, and the article doesn\\'t say where his old job was with J. Walter Thompson job was.', 263: '(BBN system) Table 5.', 264: 'Paraphrased summary of ST outputs for walkthrough article The article was relatively straightforward for the annotators who prepared the answer key, and there were no substantive differences in the output produced by each of the two annotators.', 265: 'Table 5 contains a paraphrased summary of the output that was to be generated for each of these events, along with a summary of the output that was actually generated by systems evaluated for MUC6.', 266: 'The system-generated outputs are from three different systems, since no one system did better than all other systems on all three events.', 267: 'The substantive differences between the system-generated output and the answer key are indicated by underlining in the system output.', 268: \"Recurring problems in the system outputs include the information about whether the person is currently on the job or not and the information on where the outgoing person's next job would be and where the incoming person's previous job was.\", 269: 'Note also that even the best system on the third event was unable to determine that the succession event was occurring at McCannEfickson; in addition, it only partially captured the full title of the post.', 270: 'To its credit, however, it did recognize that the event was relevant; only two systems produced output that is recognizable as pertaining to this event.', 271: 'One common problem was the simple failure to recognize \"hire\" as an indicator of a succession.', 272: 'Two systems never filled the OTHER_ORG slot or its dependent slot, REL OTHER_ORG, despite the fact that data to fill those slots was often present; over half the IN_AND_OUT objects in the answer key contain data for those two slots.', 273: 'Almost without exception, systems did more poorly on those two slots than on any others in the SUCCESSION_EVENT and IN_AND_OUT objects; the best scores posted were 70% error on OTHER_ORG (median score of 79%) and 72% error on REL_OTHER ORG (median of 86%).', 274: 'Performance on the VACANCY_REASON and ON_THE JOB slots was better for nearly all systems.', 275: \"The lowest error scores were 56% on VACANCY_REASON (median of 70%) and 62% on ONZI'HE_JOB (median of 71%).\", 276: 'The slot that most systems performed best on is NEWSTATUS; the lowest error score posted on that slot is 47% (median of 55%).', 277: 'This slot has a limited number of fill options, and the right answer is almost always either IN or OUT, depending on whether the person involved is assuming a post (IN) or vacating a post (OUT).', 278: 'Performance on the POST slot was not quite as good; the lowest error was 52% (median of 65%).', 279: 'The POST slot requires a text string as fill, and there is no finite list of possible fills for the slot.', 280: 'As seen in the third event of the walkthrough article, the fill can be an extended title such as \"vice chairman, chief strategy officer, worldwide.\"', 281: 'For most events, however, the fill is one of a large handful of possibilities, including \"chairman,\" \"president,\" \"chief executive [officer],\" \"CEO,\" \"chief operating officer,\" \"chief financial officer,\" etc. 438 DISCUSSION: CRITIQUE OF TASKS Named Entity The primary subject for review in the NE evaluation is its limited scope.', 282: 'A variety of proper name types were excluded, e.g. product names.', 283: 'The range of numerical and temporal expressions covered by the task was also limited; one notable example is the restriction of temporal expressions to exclude \"relative\" time expressions such as \"last week\".', 284: 'Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation.', 285: 'Some work on expanding the scope of the NE task has been carried out in the context of a foreign- language NE evaluation conducted in the spring of 1996.', 286: 'This evaluation is called the MET (Multilingual Named Entity) and, like MUC6, was carried out under the auspices of the Tipster Text program.', 287: 'The experience gained from that evaluation will serve as critical input to revising the Engish version of the task.', 288: 'Coreference Many aspects of the CO task are in definite need of review for reasons of either theory or practice.', 289: 'One set of issues concerns the range of syntactically governed correference phenomena that are considered markable.', 290: 'For example, apposition as a markable phenomenon was restrictively defined to exclude constructs that could rather be analyzed as left modification, such as \"chief executive Scott McNealy,\" which lacks the comma punctuation that would clearly identify \"executive\" as the head of an appositive construction.', 291: 'Another set of issues is semantic in nature and includes fimdamental questions such as the validity of including type coreferrence in the task and the legitimacy of the implied definition of coteference versus reference.', 292: 'If an antecedent expression is nonreferential, can it nonetheless be considered coreferential with subsequent anaphoric expressions?', 293: 'Or can only referring expressions corefer?', 294: 'Finally, the current notation presents a set of issues, such as its inability to represent multiple antecedents, as in conjoined NPs, or alternate antecedents, as in the case of referential ambiguity.', 295: 'In short, the preliminary nature of the task design is reflected in the somewhat unmotivated boundaries between markables and nonmarkables and in weaknesses in the notation.', 296: 'One indication of immaturity of the task definition (as well as an indication of the amount of genuine textual ambiguity) is the fact that over ten percent of the linkages in the answer key were marked as \"optional.\"', 297: '(Systems were not penalized if they failed to include such linkages in their output.)', 298: 'The task definition is now under review by a discourse working group formed in 1996 with representatives from both inside and outside the MUC commuity, including representatives from the spoken-language community.', 299: 'Template Element There are miscellaneous outstanding problems with the TE task.', 300: 'With respect to the ORGANIZATION and PERSON objects, there are issues such as rather fuzzy distinctions among the three organization subtypes and between the organization name and alias, the extremely limited scope of the person title slot, and the lack of a person descriptor slot.', 301: 'The ARTIFACT object, which was not used for either the dry run or the formal evaluation, needs to be reviewed with respect to its general utility, since its definition reflects primarily the requirements of the MUC5 microelectronics task domain.', 302: 'There is a task-neutral DATE slot that is defined as a template element; it was used in the MUC6 dry run as part of the labor negotiation scenario, but as currently defined, it fails to capture meaningfully some of the recurring kinds of date information.', 303: 'In particular, problems remain with normalizing various types of date expressions, including ones that are vague and/or require extensive use of calendar information.', 304: 'Scenario Template The issues with respect to the ST task relate primarily to the ambitiousness of the scenario templates defined for MUC6.', 305: 'Although the management scenario contained only five domain- specific slots (disregarding slots containing pointers to other objects), it nonetheless reflected an interest in capturing as complete a representation of the basic event as possible.', 306: 'As a result, a few \"peripheral\" facts about the event were included that were difficult to define in the task documentation and/or were not reported clearly in many of the articles.', 307: 'Two of the slots, VACANCY_REASON and ON_THE_JOB, had to be filled on the basis of inference from subtle linguistic cues in many cases.', 308: 'An entire appendix to the scenario definition is devoted to heuristics for filling the ON_THE JOB slot.', 309: 'These two slots caused problems for the annotators as well as for the systems.', 310: \"The annotators' problems with VACANCY_REASON may have had more to do with understanding what the scenario definition was saying than with understanding what the news articles were saying.\", 311: \"The annotators' problems with ONZI'HE_JOB were probably more substantive, since the heuristics documented in the appendix were complex and sometimes hard to map onto the expressions found in the news articles.\", 312: 'A third slot, REL_OTHER_ORG, required special inferencing on the basis of both linguistics and world knowledge in order to determine the corporate relationship between the organization a manager is leaving and the one the manager is going to.', 313: 'There may, in fact, be just one organization involved --the person could be leaving a post at a company in order to take a different (or an additional) post at the same company.', 314: 'Defining a generalized template structure and using Template Element objects as one layer in the structure reduced the amount of effort required for participants to move their system from one scenario to another.', 315: 'Further simplification may be advisable in order to focus on core information elements and exclude somewhat idiosyncratic ones such as the three slots described above.', 316: 'In the case of the management succession scenario, a proposal was made to eliminate the three slots discussed above and more, including the relational object itself, and to put the personnel information in the event object.', 317: 'Much less information about the event would be captured, but there would be a much stronger focus on the most essential information elements.', 318: 'This would possibly lead to significant improvements in performance on the basic event-related elements and to development of good end-user tools for incorporating some of the domain-specific patterns into a generic extraction system.', 319: 'CONCLUSIONS The results of the evaluation give clear evidence of the challenges that have been overcome and the ones that remain along dimensions of both breadth and depth in automated text analysis.', 320: 'The NE evaluation results serve mainly to document in the MUC context what was already strongly suspected: 1.', 321: 'Automated identification is extremely accurate when identification of lexical pattern types depends only on \"shallow\" information, such as the form of the string that satisfies the pattern and/or immediate context; 2.', 322: 'Automated identification is significantly less accurate when identification is clouded by uncertainty or ambiguity (as when case distinctions are not made, when organizations are named after persons, etc.) and must depend on one or more \"deep\" pieces of information (such as world knowledge, pragmatics, or inferences drawn from structural analysis at the sentential and suprasentential levels).', 323: 'The vast majority of cases are simple ones; thus, some systems score extremely well --well enough, in fact, to compete overall with human performance.', 324: 'Commercial systems are available already that include identification of those defined for this MUC6 task, and since a number of systems performed very well for MUC6, it is evident that high performance is probably within reach of any development site that devotes enough effort to the task.', 325: 'Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one.', 326: 'The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks.', 327: 'The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task.', 328: 'The association of shortened forms of the name with the full name depends on techniques that could be used for NE and CO as well as for TE.', 329: 'The real challenge of TE comes from associating other bits of information with the entity.', 330: 'For PERSON objects, this challenge is small, since the only additional bit of information required is the person\\'s title (\"Mr.,\" \"Ms.,\" \"Dr.,\" etc.), which appears immediately before the name/alias in the text.', 331: 'For ORGANIZATION objects, the challenge is greater, requiring extraction of location, description, and identification of the type of organization.', 332: 'Performance on TE overall is as high as 80% on the F-measure, with performance on ORGANIZATION objects significantly lower (70th percentile) than on PERSON objects (90th percentile).', 333: 'Top performance on PERSON objects came close to human performance, while performance on ORGANIZATION objects fell significantly short of human performance, with the caveat that human performance was measured on only a portion of the test set.', 334: 'Some of the shortfall in performance on the ORGANIZATION object is due to inadequate discourse processing, which is needed in order to get some of the non-local instances of the ORG_DESCRIPTOR, ORG LOCALE and ORG_COUNTRY slot fills.', 335: 'In the case of ORG_DESCRIPTOR, the results of the CO evaluation seem to provide further evidence for the relative inadequacy of current techniques for relating entity descriptions with entity names.', 336: 'Systems scored approximately 1525 points lower (F-measure) on ST than on TE.', 337: 'As defined for MUC6, the ST task presents a significant challenge in terms of system portability, in that the test procedure requ~ed that all domain-specific development be done in a period of one month.', 338: 'For past MUC evaluations, the formal run had been conducted using the same scenario as the dry run, and the task definition was released well before the dry run.', 339: 'Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels.', 340: 'However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1.', 341: 'The standardized template structure minimizes the amount of idiosyncratic programming required to produce the expected types of objects, links, and slot fills.', 342: '2.', 343: 'The fact that the domain-neutral Template Element evaluation was being conducted led to increased focus on getting the low- level information correct, which would carry over to the ST task, since approximately 25% of the expected information in the ST test set was contained in the low-level objects.', 344: '3.', 345: 'Many of the veteran participating sites had gotten to the point in their ongoing development where they had fast and efficient methods for updating their systems and monitoring their progress.', 346: 'It appears that there is a wide variety of sources of error that impose limits on system effectiveness, whatever the techniques employed by the system.', 347: 'In addition, the short time frame allocated for domain-specific development naturally makes it very difficult for developers to do sufficient development to fill complex slots that either are not always expected to be filled or are not crucial elements in the template structure.', 348: 'Sites have developed architectures that are at least as general-purpose techniques as ever, perhaps as a result of having to produce outputs for as many as four different tasks.', 349: 'Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems.', 350: 'However, we still have full-sentence parsing (e.g. USheffield, UDurham, UManitoba); we sometimes have expectations of \"deep understanding\" (cf.', 351: \"UDurham's use of a world model) and sometimes not (cf.\", 352: \"UManitoba's production of ST output directly from dependency trees, with no semantic representation per se).\", 353: 'Some systems completed all stages of analysis before producing outputs for any of the tasks, including NE.', 354: 'Six of the seven sites that participated in the coreference evaluation also participated in the MUC6 information extraction evaluation, and five of the six made use of the results of the processing that produced their coreference output in the processing that produced their information extraction output.', 355: 'The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations.', 356: 'Other sources of excitement are the spinoff efforts that the NE and CO tasks have inspired that bring these tasks and their potential applications to the attention of new research groups and new customer groups.', 357: 'In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.', 358: 'Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas.', 359: 'The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well.', 360: 'The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors.', 361: 'Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies.', 362: 'The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.'}, sections={0: 'PAPER', 1: 'ABSTRACT', 2: 'INTRODUCTION', 3: 'INTRODUCTION', 4: 'INTRODUCTION', 5: 'INTRODUCTION', 6: 'INTRODUCTION', 7: 'INTRODUCTION', 8: 'INTRODUCTION', 9: 'INTRODUCTION', 10: 'INTRODUCTION', 11: 'INTRODUCTION', 12: 'INTRODUCTION', 13: 'INTRODUCTION', 14: 'INTRODUCTION', 15: 'INTRODUCTION', 16: 'INTRODUCTION', 17: 'INTRODUCTION', 18: 'INTRODUCTION', 19: 'INTRODUCTION', 20: 'INTRODUCTION', 21: 'INTRODUCTION', 22: 'INTRODUCTION', 23: 'INTRODUCTION', 24: 'INTRODUCTION', 25: 'INTRODUCTION', 26: 'INTRODUCTION', 27: 'INTRODUCTION', 28: 'INTRODUCTION', 29: 'INTRODUCTION', 30: 'INTRODUCTION', 31: 'INTRODUCTION', 32: 'INTRODUCTION', 33: 'INTRODUCTION', 34: 'INTRODUCTION', 35: 'INTRODUCTION', 36: 'INTRODUCTION', 37: 'INTRODUCTION', 38: 'INTRODUCTION', 39: 'INTRODUCTION', 40: 'INTRODUCTION', 41: 'INTRODUCTION', 42: 'INTRODUCTION', 43: 'INTRODUCTION', 44: 'INTRODUCTION', 45: 'INTRODUCTION', 46: 'INTRODUCTION', 47: 'INTRODUCTION', 48: 'INTRODUCTION', 49: 'INTRODUCTION', 50: 'INTRODUCTION', 51: 'INTRODUCTION', 52: 'INTRODUCTION', 53: 'INTRODUCTION', 54: 'INTRODUCTION', 55: 'INTRODUCTION', 56: 'INTRODUCTION', 57: 'INTRODUCTION', 58: 'INTRODUCTION', 59: 'INTRODUCTION', 60: 'INTRODUCTION', 61: 'INTRODUCTION', 62: 'INTRODUCTION', 63: 'INTRODUCTION', 64: 'INTRODUCTION', 65: 'INTRODUCTION', 66: 'INTRODUCTION', 67: 'INTRODUCTION', 68: 'INTRODUCTION', 69: 'INTRODUCTION', 70: 'INTRODUCTION', 71: 'INTRODUCTION', 72: 'INTRODUCTION', 73: 'INTRODUCTION', 74: 'INTRODUCTION', 75: 'INTRODUCTION', 76: 'INTRODUCTION', 77: 'INTRODUCTION', 78: 'INTRODUCTION', 79: 'INTRODUCTION', 80: 'INTRODUCTION', 81: 'INTRODUCTION', 82: 'INTRODUCTION', 83: 'INTRODUCTION', 84: 'INTRODUCTION', 85: 'INTRODUCTION', 86: 'INTRODUCTION', 87: 'INTRODUCTION', 88: 'INTRODUCTION', 89: 'INTRODUCTION', 90: 'INTRODUCTION', 91: 'INTRODUCTION', 92: 'INTRODUCTION', 93: 'INTRODUCTION', 94: 'INTRODUCTION', 95: 'INTRODUCTION', 96: 'INTRODUCTION', 97: 'INTRODUCTION', 98: 'INTRODUCTION', 99: 'INTRODUCTION', 100: 'INTRODUCTION', 101: 'INTRODUCTION', 102: 'INTRODUCTION', 103: 'INTRODUCTION', 104: 'INTRODUCTION', 105: 'INTRODUCTION', 106: 'INTRODUCTION', 107: 'INTRODUCTION', 108: 'INTRODUCTION', 109: 'INTRODUCTION', 110: 'INTRODUCTION', 111: 'INTRODUCTION', 112: 'INTRODUCTION', 113: 'INTRODUCTION', 114: 'INTRODUCTION', 115: 'INTRODUCTION', 116: 'INTRODUCTION', 117: 'INTRODUCTION', 118: 'INTRODUCTION', 119: 'INTRODUCTION', 120: 'INTRODUCTION', 121: 'INTRODUCTION', 122: 'INTRODUCTION', 123: 'INTRODUCTION', 124: 'INTRODUCTION', 125: 'INTRODUCTION', 126: 'INTRODUCTION', 127: 'INTRODUCTION', 128: 'INTRODUCTION', 129: 'INTRODUCTION', 130: 'INTRODUCTION', 131: 'INTRODUCTION', 132: 'INTRODUCTION', 133: 'INTRODUCTION', 134: 'INTRODUCTION', 135: 'INTRODUCTION', 136: 'INTRODUCTION', 137: 'INTRODUCTION', 138: 'INTRODUCTION', 139: 'INTRODUCTION', 140: 'INTRODUCTION', 141: 'INTRODUCTION', 142: 'INTRODUCTION', 143: 'INTRODUCTION', 144: 'INTRODUCTION', 145: 'INTRODUCTION', 146: 'INTRODUCTION', 147: 'INTRODUCTION', 148: 'INTRODUCTION', 149: 'INTRODUCTION', 150: 'INTRODUCTION', 151: 'INTRODUCTION', 152: 'INTRODUCTION', 153: 'INTRODUCTION', 154: 'INTRODUCTION', 155: 'INTRODUCTION', 156: 'INTRODUCTION', 157: 'INTRODUCTION', 158: 'INTRODUCTION', 159: 'INTRODUCTION', 160: 'INTRODUCTION', 161: 'INTRODUCTION', 162: 'INTRODUCTION', 163: 'INTRODUCTION', 164: 'INTRODUCTION', 165: 'INTRODUCTION', 166: 'INTRODUCTION', 167: 'INTRODUCTION', 168: 'INTRODUCTION', 169: 'INTRODUCTION', 170: 'INTRODUCTION', 171: 'INTRODUCTION', 172: 'INTRODUCTION', 173: 'INTRODUCTION', 174: 'INTRODUCTION', 175: 'INTRODUCTION', 176: 'INTRODUCTION', 177: 'INTRODUCTION', 178: 'INTRODUCTION', 179: 'INTRODUCTION', 180: 'INTRODUCTION', 181: 'INTRODUCTION', 182: 'INTRODUCTION', 183: 'INTRODUCTION', 184: 'INTRODUCTION', 185: 'INTRODUCTION', 186: 'INTRODUCTION', 187: 'INTRODUCTION', 188: 'INTRODUCTION', 189: 'INTRODUCTION', 190: 'INTRODUCTION', 191: 'INTRODUCTION', 192: 'INTRODUCTION', 193: 'INTRODUCTION', 194: 'INTRODUCTION', 195: 'INTRODUCTION', 196: 'INTRODUCTION', 197: 'INTRODUCTION', 198: 'INTRODUCTION', 199: 'INTRODUCTION', 200: 'Miscategorization  of  entity  as. ', 201: 'Miscategorization  of  entity  as. ', 202: 'Miscategorization  of  entity  as. ', 203: 'Miscategorization  of  entity  as. ', 204: 'Miscategorization  of  entity  as. ', 205: 'Miscategorization  of  entity  as. ', 206: 'Miscategorization  of  entity  as. ', 207: 'Miscategorization  of  entity  as. ', 208: 'Miscategorization  of  entity  as. ', 209: 'Miscategorization  of  entity  as. ', 210: 'Miscategorization  of  entity  as. ', 211: 'Miscategorization  of  entity  as. ', 212: 'Miscategorization  of  entity  as. ', 213: 'Miscategorization  of  entity  as. ', 214: 'Miscategorization  of  entity  as. ', 215: 'Miscategorization  of  entity  as. ', 216: 'Miscategorization  of  entity  as. ', 217: 'Miscategorization  of  entity  as. ', 218: 'Miscategorization  of  entity  as. ', 219: 'Miscategorization  of  entity  as. ', 220: 'Miscategorization  of  entity  as. ', 221: 'Miscategorization  of  entity  as. ', 222: 'Miscategorization  of  entity  as. ', 223: 'Miscategorization  of  entity  as. ', 224: 'Miscategorization  of  entity  as. ', 225: 'Miscategorization  of  entity  as. ', 226: 'Miscategorization  of  entity  as. ', 227: 'Miscategorization  of  entity  as. ', 228: 'Miscategorization  of  entity  as. ', 229: 'Miscategorization  of  entity  as. ', 230: 'Miscategorization  of  entity  as. ', 231: 'Miscategorization  of  entity  as. ', 232: 'Miscategorization  of  entity  as. ', 233: 'Miscategorization  of  entity  as. ', 234: 'Miscategorization  of  entity  as. ', 235: 'Miscategorization  of  entity  as. ', 236: 'Miscategorization  of  entity  as. ', 237: 'Miscategorization  of  entity  as. ', 238: 'Miscategorization  of  entity  as. ', 239: 'Miscategorization  of  entity  as. ', 240: 'Miscategorization  of  entity  as. ', 241: 'Miscategorization  of  entity  as. ', 242: 'Miscategorization  of  entity  as. ', 243: 'Miscategorization  of  entity  as. ', 244: 'Miscategorization  of  entity  as. ', 245: 'Miscategorization  of  entity  as. ', 246: 'Miscategorization  of  entity  as. ', 247: 'Miscategorization  of  entity  as. ', 248: 'Miscategorization  of  entity  as. ', 249: 'Miscategorization  of  entity  as. ', 250: 'Miscategorization  of  entity  as. ', 251: 'Miscategorization  of  entity  as. ', 252: 'Miscategorization  of  entity  as. ', 253: 'Miscategorization  of  entity  as. ', 254: 'Miscategorization  of  entity  as. ', 255: 'Miscategorization  of  entity  as. ', 256: 'Miscategorization  of  entity  as. ', 257: 'Miscategorization  of  entity  as. ', 258: 'Miscategorization  of  entity  as. ', 259: 'Miscategorization  of  entity  as. ', 260: 'Miscategorization  of  entity  as. ', 261: 'Miscategorization  of  entity  as. ', 262: 'Miscategorization  of  entity  as. ', 263: 'Miscategorization  of  entity  as. ', 264: 'Miscategorization  of  entity  as. ', 265: 'Miscategorization  of  entity  as. ', 266: 'Miscategorization  of  entity  as. ', 267: 'Miscategorization  of  entity  as. ', 268: 'Miscategorization  of  entity  as. ', 269: 'Miscategorization  of  entity  as. ', 270: 'Miscategorization  of  entity  as. ', 271: 'Miscategorization  of  entity  as. ', 272: 'Miscategorization  of  entity  as. ', 273: 'Miscategorization  of  entity  as. ', 274: 'Miscategorization  of  entity  as. ', 275: 'Miscategorization  of  entity  as. ', 276: 'Miscategorization  of  entity  as. ', 277: 'Miscategorization  of  entity  as. ', 278: 'Miscategorization  of  entity  as. ', 279: 'Miscategorization  of  entity  as. ', 280: 'Miscategorization  of  entity  as. ', 281: 'Miscategorization  of  entity  as. ', 282: 'Miscategorization  of  entity  as. ', 283: 'Miscategorization  of  entity  as. ', 284: 'Miscategorization  of  entity  as. ', 285: 'Miscategorization  of  entity  as. ', 286: 'Miscategorization  of  entity  as. ', 287: 'Miscategorization  of  entity  as. ', 288: 'Miscategorization  of  entity  as. ', 289: 'Miscategorization  of  entity  as. ', 290: 'Miscategorization  of  entity  as. ', 291: 'Miscategorization  of  entity  as. ', 292: 'Miscategorization  of  entity  as. ', 293: 'Miscategorization  of  entity  as. ', 294: 'Miscategorization  of  entity  as. ', 295: 'Miscategorization  of  entity  as. ', 296: 'Miscategorization  of  entity  as. ', 297: 'Miscategorization  of  entity  as. ', 298: 'Miscategorization  of  entity  as. ', 299: 'Miscategorization  of  entity  as. ', 300: 'Miscategorization  of  entity  as. ', 301: 'Miscategorization  of  entity  as. ', 302: 'Miscategorization  of  entity  as. ', 303: 'Miscategorization  of  entity  as. ', 304: 'Miscategorization  of  entity  as. ', 305: 'Miscategorization  of  entity  as. ', 306: 'Miscategorization  of  entity  as. ', 307: 'Miscategorization  of  entity  as. ', 308: 'Miscategorization  of  entity  as. ', 309: 'Miscategorization  of  entity  as. ', 310: 'Miscategorization  of  entity  as. ', 311: 'Miscategorization  of  entity  as. ', 312: 'Miscategorization  of  entity  as. ', 313: 'Miscategorization  of  entity  as. ', 314: 'Miscategorization  of  entity  as. ', 315: 'Miscategorization  of  entity  as. ', 316: 'Miscategorization  of  entity  as. ', 317: 'Miscategorization  of  entity  as. ', 318: 'Miscategorization  of  entity  as. ', 319: 'Miscategorization  of  entity  as. ', 320: 'Miscategorization  of  entity  as. ', 321: 'Miscategorization  of  entity  as. ', 322: 'Miscategorization  of  entity  as. ', 323: 'Miscategorization  of  entity  as. ', 324: 'Miscategorization  of  entity  as. ', 325: 'Miscategorization  of  entity  as. ', 326: 'Miscategorization  of  entity  as. ', 327: 'Miscategorization  of  entity  as. ', 328: 'Miscategorization  of  entity  as. ', 329: 'Miscategorization  of  entity  as. ', 330: 'Miscategorization  of  entity  as. ', 331: 'Miscategorization  of  entity  as. ', 332: 'Miscategorization  of  entity  as. ', 333: 'Miscategorization  of  entity  as. ', 334: 'Miscategorization  of  entity  as. ', 335: 'Miscategorization  of  entity  as. ', 336: 'Miscategorization  of  entity  as. ', 337: 'Miscategorization  of  entity  as. ', 338: 'Miscategorization  of  entity  as. ', 339: 'Miscategorization  of  entity  as. ', 340: 'Miscategorization  of  entity  as. ', 341: 'Miscategorization  of  entity  as. ', 342: 'Miscategorization  of  entity  as. ', 343: 'Miscategorization  of  entity  as. ', 344: 'Miscategorization  of  entity  as. ', 345: 'Miscategorization  of  entity  as. ', 346: 'Miscategorization  of  entity  as. ', 347: 'Miscategorization  of  entity  as. ', 348: 'Miscategorization  of  entity  as. ', 349: 'Miscategorization  of  entity  as. ', 350: 'Miscategorization  of  entity  as. ', 351: 'Miscategorization  of  entity  as. ', 352: 'Miscategorization  of  entity  as. ', 353: 'Miscategorization  of  entity  as. ', 354: 'Miscategorization  of  entity  as. ', 355: 'Miscategorization  of  entity  as. ', 356: 'Miscategorization  of  entity  as. ', 357: 'Miscategorization  of  entity  as. ', 358: 'Miscategorization  of  entity  as. ', 359: 'Miscategorization  of  entity  as. ', 360: 'ACKNOWLEDGEMENTS', 361: 'ACKNOWLEDGEMENTS', 362: 'ACKNOWLEDGEMENTS'}), cite=Article(xml=<xml.etree.ElementTree.ElementTree object at 0x7ff71e1e0f98>, sentences={1: 'There is currently much interest, in both research and com\\xad mercial arenas, in natural language processing systems which can perform multilingual information extraction (IE), the task of automatically identifying the various aspects of a text that are of interest to specific users.', 2: 'An example of IE is the Named Entity (NE) task, which has become established as the important first step in many other IE tasks, provid\\xad ing information useful for coreference and template filling.', 3: 'Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).', 4: 'Several organized evaluations have been held to determine the state-of-the-art in NE systems, and there are commercial systems available.', 5: 'The goal of the NE task is to automatically identify the boundaries of a variety of phrases in a raw text, and then to categorize the phrases identified.', 6: 'There are three categories of named-entities defined by the guidelines: TIMEX, NUMEX, and ENAMEX.', 7: 'TIMEX phrases are temporal expressions, which are subdivided into date expressions ( April 7) and time expressions ( noon EST).', 8: 'NUMEX phrases are numeric expres\\xad sions, which are subdivided into percent expressions (3.2%) and money expressions ( $180 million).', 9: 'ENAMEX phrases are proper names, representing references in a text to persons ( Jeffre y H. Birnbaum), locations ( New York), and organiza\\xad tions ( Northwest Airlines ).', 10: 'Evaluation of system performance for the NE task is done using an automatic scoring program (Chinchor, 1995), with the scores based on two measures - recall and precision.', 11: 'Recall is the percent of the \"correct\" named-entities that the system identifies; precision is the percent of the phrases that the system identifies that are actually correct NE phrases.', 12: 'The component recall and precision scores are then used to calculate a balanced F-measure (Rijsbergen, 1979), where F = 2PR/ ( P + R).', 13: 'Human performance on the NE task has been determined to be quite high, with F-measures better than 96% (Sund\\xad heim, 1995b).', 14: 'Despite the fact that some systems in recent evaluations have performance approaching this human perfor\\xad mance, it is important to note that named-entity recognition is by no means a \"solved problem.\"', 15: 'The fact that existing sys\\xad tems perform extremely well on mixed case English newswire corpora is certainly related to the years of research (and or\\xad ganized evaluations) on this specific task in this language.', 16: 'Although performance by MUC6 and MET systems is en\\xad couraging, it is not clear what resources are required to adapt systems to new languages.', 17: 'It is also unknown how the exist\\xad ing high scoring systems would perform on less well-behaved texts, such as single-case texts, non-newswire texts, or texts obtained via optical character recognition (OCR).', 18: 'There has been little discussion of the linguistic signifi\\xad cance of performing NE recognition, or of how much linguistic knowledge is required to perform well on such an evaluation.', 19: 'However, any given language task should be examined care\\xad fully to establish a baseline of performance which should be attainable by any system; only then can we adequately de\\xad termine the significance of the results reported on that task.', 20: 'In this paper we give the results of an analysis of NE cor\\xad pora in six languages from the point of view of a system with no knowledge of the languages; that is, we performed an analysis based purely on the strings of characters composing the texts and the named-entity phrases.', 21: 'The performance of such a straw-man system, which did not use language-specific lexicons or word lists or even information about tokeniza\\xad tion/segmentation or part-of speech, can serve as a baseline score for comparison of more sophisticated systems.', 22: 'The definition of the NE task we discuss in this paper was taken from the guidelines for the Sixth Message Understand\\xad ing Conferences (MUC6) (Sundheim, 1995a) and the recent Multilingual Entity Task (MET, May 1996), both sponsored by the TIPSTER program.', 23: 'MUC6 evaluated English NE systems, and MET evaluated Spanish, Japanese, and Chinese NE systems.', 24: 'The Spanish, Japanese, and Chinese corpora we analyzed each consisted of the MET training documents; sim\\xad ilarly, the English corpus contains 60 Wall Street Journal ar\\xad ticles prepared for the MUC6 dry-run and official evaluation.', 25: 'In addition to the four corpora available from the recent orga\\xad nized NE evaluations, we analyzed similar-sized French and 190 Portuguese corpora1 which were prepared according to the MET guidelines.', 26: 'Table 1 shows the sources for the corpora.', 27: 'I language, as well as a breakdown of total phrases into the three individual categories.', 28: 'Table 1: Corpora sources.', 29: 'All six corpora consisted of a collection of newswire articles, and none of the articles in any language was a translation of an article in another language.', 30: 'There were important differ\\xad ences in the makeup of these individual corpora that affected this analysis.', 31: 'The French corpus, for example, contained a wide range of articles from a single issue of Le Monde, so the topics of the articles ranged from world politics to the Paris fashion scene.', 32: 'The articles in the English and Spanish corpora were specifically selected (by the MUC6 and MET evaluation organizers) because they contained references to press conferences.', 33: 'While the content was more homogeneous in the English corpus, the articles were nevertheless drawn from a range of several months of the Wall Street Journal, so the specific topics (and constituent Named Entities) were very diverse.', 34: 'The Chinese Xinhua corpus was, in contrast, ex\\xad tremely homogeneous.', 35: 'These differences demonstrate a num\\xad ber of difficulties presented by corpora in different languages.', 36: 'In order to estimate the complexity of the NE task, we first determined the vocabulary size of the corpora involved (i.e. \"count the words\"), in terms of individual lexemes of the lan\\xad guage.', 37: 'For our analysis of the European-language corpora, we considered a token to be any sequence of characters de\\xad limited by white space, and we ignored the case of all letters.', 38: 'The Japanese corpus was segmented using NEWJUMAN, the Chinese corpus with a segmenter made available by New Mex\\xad ico State University.', 39: 'This segmentation information was used only to estimate the corpora sizes and was not used in any of the other portions of our analysis.', 40: 'Language Lexeme Tokens Lexeme Types Token/ Type Chinese 34782 4584 7.6 English 24797 5764 4.3 French 35997 8691 4.1 Japanese 21484 3655 5.9 Portuguese 42621 7756 5.5 Spanish 31991 7850 4.1 Table 2: Corpora size by lexeme.', 41: 'Table 3 shows the total number of NE phrases for each 1The French corpus was prepared by Marc Vilain; the Por\\xad tuguese corpus was prepared by Sasha Caskey.', 42: 'Table 3: NE phrases, by subcategory.', 43: '2.1 NUMEX and TIMEX phrases.', 44: 'From Table 3 we see that TIMEX and NUMEX phrases to\\xad gether composed only 2030% of all NE phrases in each lan\\xad guage.', 45: 'Furthermore, these phrases were the easiest to recog\\xad nize, because they could be represented by very few simple patterns.', 46: 'Upon inspection of the corpora, for example, we were able to represent nearly all NUMEX phrases in each of the six corpora with just 5 patterns.2 Similarly, given a simple list of the basic temporal phrase words for a lan\\xad guage (months, days of the week, seasons, etc.), it was possi\\xad ble to construct a series of patterns to represent most of the TIMEX phrases.', 47: '3 We were able to represent at least 95% of all TIMEX in each language in similar ways with just a few patterns (less than 30 per language), constructed in a few hours.', 48: 'Since we found most NUMEX and TIMEX phrases to be easy to recognize, we therefore restricted our further analysis of the corpora to ENAMEX phrases, which proved to be significantly more complex.', 49: '2.2 ENAMEX phrases.', 50: 'Table 4 shows the numbers of ENAMEX phrases tokens con\\xad tained by the six corpora.', 51: 'The average occurrence of each token in each language was quite low (much lower than the av\\xad erage occurrence of each lexeme), which indicated that many phrases occurred very infrequently in the corpus.', 52: 'ou ld ypes . OC A\\xad d be 2 An example of a NUMEX pattern representing a Spanish PERCENT would be a sequence of digits followed by either the percent sign (%) or the words \"por ciento\".', 53: '3An example of a NUMEX pattern representing a Spanish DATE would be the name of a month (or its abbreviation) followed by a sequence of digits (the day), optionally followed by a comma and another sequence of digits (the year).', 54: 'accounted for by the three common Chinese words for China.', 55: 'Figure 1 shows a graph of the cumulative percentage of all phrases of the corresponding category represented by the x most frequently-occurring phrases of that type in the given language.', 56: 'Figure 1: Graph of the cumulative % of phrase tokens provided by % of phrase types.', 57: 'The graph shows a similar shape for all subcategories of ENAMEX phrases in all the languages investigated, although the rate of increase varies slightly.', 58: 'It is clear from the classic Zipfian distribution (d.', 59: '(Zipf, 1932; Zipf, 1949}) shown by the graph that a significant percentage of the ENAMEX phrase tokens could be represented by a small amount of frequently\\xad occurring phrase types.', 60: \"However, Zipf 's law also tells us that a nontrivial percentage of the phrases (those in the tail of the graph) are very infrequent, most likely never occurring in any amount of training data.\", 61: 'Unlike the distribution of the overall NE phrases, the rel\\xad ative proportion of constituent ENAMEX phrase subcate\\xad gories (PERSON, LOCATION, and ORGANIZATION) var\\xad ied greatly by language.', 62: 'The breakdown by ENAMEX phrase subcategory is shown in Table 5.', 63: 'Since high performance on training texts is meaningless if a system performs poorly on new, unseen texts, we estimated the performance of a simple memorization algorithm on un\\xad seen data.', 64: 'For our simple system, the answer to the question depended on the vocabulary transfer rate of the corpus, the percentage of phrases occurring in the training corpus which also occurred in the test corpus.', 65: 'To measure the vocab\\xad ulary transfer rate for the six corpora, we randomly divided each corpus into a training set and a test set, with each test set containing about 450 ENAMEX phrases, and each train\\xad ing set containing all remaining phrases.', 66: 'We then examined the ENAMEX phrases in the training set to determine how many also occurred in the test set.', 67: 'The results of this experiment showed that, to a certain extent, a word list built from the training set provided rea\\xad sonable performance.', 68: 'Just as some frequent phrase types comprised a large percentage of the phrase tokens within a corpus, a small number of phrase types from the training set accounted for many tokens in the test set.', 69: 'As shown by the transfer curve for the six languages in Figure 2, the transfer rate varied dramatically depending on the language, but the graph has the same shape for each, even though the six cor\\xad pora contained different amounts of training data (thus the lines of different length).', 70: 'Figure 2: Graph of the cumulative test phrase tokens (%) covered by training phrase types.', 71: 'Table 5: ENAMEX phrases by subcategory.', 72: 'The significance of this result is that each ENAMEX phrase subcategory had to be treated as equivalent.', 73: 'It was not pos\\xad sible to focus on a particular subcategory to obtain a con\\xad sistently high score.', 74: 'In other words, a strategy that focuses on locations would do well on the Chinese corpus where loca\\xad tions comprise 59.8% of the ENAMEX phrases, but would do poorly on the English corpus, where locations are only 14.5% of the ENAMEX.', 75: 'A logical question to pose is, \"How well can our system per\\xad form if it simply memorizes the phrases in the training texts?\"', 76: 'In each language, the transfer rate for the most frequent phrase types (the steep part of the graph) was quite high; however, the graph rapidly peaks and leaves a large per\\xad centage of the phrases uncovered by the training phrases.', 77: 'The remaining \"uncovered\" phrases can only be recognized by means other than \"memorization,\" such as by examining contextual clues.', 78: 'Table 6 shows the transfer rates of phrase tokens.', 79: 'The accuracy of the pure memorization can be reduced by two forms of ambiguity.', 80: 'Phrases or parts of phrases can oc\\xad cur within two or more named-entity categories, such as the string Boston, which by itself is a location but within Boston Red Sox is an organization.', 81: 'In most cases this ambiguity can be resolved using a simple longest-match heuristic.', 82: \"Another source of ambiguity occurs when a string can occur both as a Language Overall ENAMEX Org Loe Pers Chinese 73.2% 46.9'7o 87.1% 42.6% English 21.2% 17.7% 42.7% 13.3% French 23.6% 13.4% 45.9% 11.2% Japanese 59.2% 56.2% 72.7% 37.5% Portuguese 61.3% 56.4% 57.4% 47.9% Spanish 48.1% 49.8% 71.4% 13.7% Table 6: Vocabulary transfer (tokens).\", 83: 'NE phrase and as a non-phrase, such as Apple, which would sometimes refer to the computer company (and thus be tagged an organization) and sometimes refer to the fruit (and thus not be tagged at all).', 84: 'Such cases, although infrequent, would result in precision errors which we do not factor into the fol\\xad lowing estimation of a recall lower bound.', 85: 'Given the above statistical analysis, we estimated a baseline score for our straw-man algorithm on the NE task, a score which should easily be attainable by any system attempting to perform the task.', 86: 'First, we estimated that any system should be able to recognize a large percentage of NUMEX and TIMEX phrases; our experience indicates that 95% is possible due to the small number of patterns which compose most of these phrases.', 87: 'In order to estimate a lower bound for ENAMEX recogni\\xad tion, we relied on the transfer graph in Figure 2.', 88: 'It is clear from the graph that the contribution of the training data has leveled off in ea.ch language by the time the number of training types is roughly equal to the size of the test data (450 in this case).', 89: 'Selecting this point on the graph allowed us to directly compare memorization performance for the six languages.', 90: 'An ideal memorization-based algorithm would be able to recog\\xad nize phrases according to the transfer rate corresponding to this amount of training data.', 91: 'Our lower bound formula.', 92: 'would thus be ( ( N N uM E X + NT I M E x ) * a) + ( N EN AM E X * TEN AM E X ) where a = 0.95 (in our experience) Neat = Percentage of NE phrases represented by category (from Table 3) TEN AM E X = ENAMEX transfer rate (from Figure 2) The resulting lower bound scores, shown in Table 7, were surprisingly high, indicating that a very simple NE system could easily achieve a recall above 70 for some languages.', 93: 'The range of lower bound scores can partly be attributed to the differences in corpus makeup discussed in Section 3, but the range also illustrates the large score differences which are possible from one corpus to the next.', 94: 'The upper bounds of memorization algorithms implied by the preceding analysis do not require that a. deeper under\\xad standing of the linguistic phenomena of a target language is necessary to generalize NE recognition in unseen test data..', 95: 'Contextual clues can improve the expected score of a base\\xad line system without requiring extensive linguistic knowledge.', 96: 'Just as most of the TIMEX and NUMEX phrases in any lan\\xad guage can be recognized upon inspection using simple pattern Table 7: Estimated lower bounds.', 97: 'matching, a large percentage of the ENAMEX phrases could be codified given an adequate analysis of the phrasal contexts in the training documents.', 98: 'Furthermore, lists of titles, ge\\xad ographic units, and corporate designators would assist this contextual analysis and improve the expected baseline.', 99: 'In\\xad deed, such simple strategies drive most current NE systems.', 100: 'The results of this analysis indicate that it is possible to per\\xad form much of the task of named-entity recognition with a. very simple analysis of the strings composing the NE phrases; even more is possible with an additional inspection of the common phrasal contexts.', 101: \"The underlying principle is Zipf 's Law; due to the prevalence of very frequent phenomena, a little effort goes a long way and very high scores can be achieved directly from the training data.\", 102: 'Yet according to the same Law that gives us that initial high score, incremental advances above the baseline can be arduous and very language specific.', 103: 'Such improvement can most certainly only be achieved with a cer\\xad tain amount of well-placed linguistic intuition.', 104: 'The analysis also demonstrated the large differences in lan\\xad guages for the NE task, suggesting that we need to not only examine the overall score but also the ability to surpass the limitations of word lists, especially since extensive lists are available in very few languages.', 105: 'It is particularly important to evaluate system performance beyond a lower bound, such as that proposed in Section 4.', 106: 'Since the baseline scores will differ for different languages and corpora, scores for different corpora that appear equal may not necessarily be comparable.'}, sections={1: 'The Named Entity task. ', 2: 'The Named Entity task. ', 3: 'The Named Entity task. ', 4: 'The Named Entity task. ', 5: 'The Named Entity task. ', 6: 'The Named Entity task. ', 7: 'The Named Entity task. ', 8: 'The Named Entity task. ', 9: 'The Named Entity task. ', 10: 'The Named Entity task. ', 11: 'The Named Entity task. ', 12: 'The Named Entity task. ', 13: 'The Named Entity task. ', 14: 'The Named Entity task. ', 15: 'The Named Entity task. ', 16: 'The Named Entity task. ', 17: 'The Named Entity task. ', 18: 'The Named Entity task. ', 19: 'The Named Entity task. ', 20: 'The Named Entity task. ', 21: 'The Named Entity task. ', 22: 'The Corpora. ', 23: 'The Corpora. ', 24: 'The Corpora. ', 25: 'The Corpora. ', 26: 'The Corpora. ', 27: 'The Corpora. ', 28: 'The Corpora. ', 29: 'The Corpora. ', 30: 'The Corpora. ', 31: 'The Corpora. ', 32: 'The Corpora. ', 33: 'The Corpora. ', 34: 'The Corpora. ', 35: 'The Corpora. ', 36: 'The Corpora. ', 37: 'The Corpora. ', 38: 'The Corpora. ', 39: 'The Corpora. ', 40: 'The Corpora. ', 41: 'The Corpora. ', 42: 'The Corpora. ', 43: 'The Corpora. ', 44: 'The Corpora. ', 45: 'The Corpora. ', 46: 'The Corpora. ', 47: 'The Corpora. ', 48: 'The Corpora. ', 49: 'The Corpora. ', 50: 'The Corpora. ', 51: 'The Corpora. ', 52: 'The Corpora. ', 53: 'The Corpora. ', 54: 'The Corpora. ', 55: 'The Corpora. ', 56: 'The Corpora. ', 57: 'The Corpora. ', 58: 'The Corpora. ', 59: 'The Corpora. ', 60: 'The Corpora. ', 61: 'The Corpora. ', 62: 'The Corpora. ', 63: 'The Corpora. ', 64: 'The Corpora. ', 65: 'The Corpora. ', 66: 'The Corpora. ', 67: 'The Corpora. ', 68: 'The Corpora. ', 69: 'The Corpora. ', 70: 'The Corpora. ', 71: 'The Corpora. ', 72: 'The Corpora. ', 73: 'The Corpora. ', 74: 'The Corpora. ', 75: 'Training and ambiguity. ', 76: 'Training and ambiguity. ', 77: 'Training and ambiguity. ', 78: 'Training and ambiguity. ', 79: 'Training and ambiguity. ', 80: 'Training and ambiguity. ', 81: 'Training and ambiguity. ', 82: 'Training and ambiguity. ', 83: 'Training and ambiguity. ', 84: 'Training and ambiguity. ', 85: 'Estimating  a lower bound. ', 86: 'Estimating  a lower bound. ', 87: 'Estimating  a lower bound. ', 88: 'Estimating  a lower bound. ', 89: 'Estimating  a lower bound. ', 90: 'Estimating  a lower bound. ', 91: 'Estimating  a lower bound. ', 92: 'Estimating  a lower bound. ', 93: 'Estimating  a lower bound. ', 94: 'Estimating  a lower bound. ', 95: 'Estimating  a lower bound. ', 96: 'Estimating  a lower bound. ', 97: 'Estimating  a lower bound. ', 98: 'Estimating  a lower bound. ', 99: 'Estimating  a lower bound. ', 100: 'Discussion. ', 101: 'Discussion. ', 102: 'Discussion. ', 103: 'Discussion. ', 104: 'Discussion. ', 105: 'Discussion. ', 106: 'Discussion. '}), offsets=Offsets(marker=[3], cite=[3], ref=[357]), author='Muthu Kumar Chandrasekaran, NUS', is_test=False, facet=['Implication_Citation'], year=2018)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIZd2qulIwWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xy = {}\n",
        "for data in dataset:\n",
        "  if not data.cite.sentences or not data.ref.sentences:\n",
        "        continue\n",
        "  y = data.facet\n",
        "  ref_sents = [''.join([data.ref.sentences[x] for x in data.offsets.ref])]\n",
        "  cite_sents = [''.join([data.cite.sentences[x] for x in data.offsets.cite])]\n",
        "  for sent in ref_sents + cite_sents:\n",
        "      xy[sent] = y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isSunsoKJfek",
        "colab_type": "code",
        "outputId": "1297f95a-e272-48c2-fc7f-64117d1f3ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(xy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1123"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2THDS1zNSkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = {}\n",
        "for x in xy:\n",
        "  y = xy[x]\n",
        "  y_ = \"_\".join(sorted([x.lower().replace(\"_\", \"\").replace(\"s\", \"\").replace(\" \", \"\") for x in y]))\n",
        "  #print(y)\n",
        "  if \"aim\" in \"\".join(y).lower():\n",
        "    print(x)\n",
        "    print(\"----\")\n",
        "  f[y_] = f.get(y_, 0) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2EQFZYQNcz3",
        "colab_type": "code",
        "outputId": "112836b8-08ca-4214-a48c-ffefdca5c36e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "f\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aimcitation': 59,\n",
              " 'aimcitation_hypotheicitation': 5,\n",
              " 'aimcitation_methodcitation': 25,\n",
              " 'aimcitation_reultcitation': 4,\n",
              " 'hypotheicitation': 19,\n",
              " 'hypotheicitation_methodcitation': 6,\n",
              " 'implicationcitation': 80,\n",
              " 'implicationcitation_methodcitation': 14,\n",
              " 'implicationcitation_reultcitation': 4,\n",
              " 'methodcitation': 754,\n",
              " 'methodcitation_reultcitation': 35,\n",
              " 'reultcitation': 118}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU9JLMX-NkEM",
        "colab_type": "code",
        "outputId": "0dca2670-836a-4965-e94c-13f998d10f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled2.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1OVxFNbLNcEcJnoNn80hhp2SC_Rk_QzD8\n",
        "\"\"\"\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import requests\n",
        "\n",
        "Datum = namedtuple('Datum', 'ref cite offsets author is_test facet year')\n",
        "Offsets = namedtuple('Offsets', 'marker cite ref')\n",
        "Article = namedtuple('Article', 'xml sentences sections')\n",
        "\n",
        "# Currently only works for 2016 - 2018\n",
        "\n",
        "import os\n",
        "import xml.etree.cElementTree as ET\n",
        "import logging\n",
        "import codecs\n",
        "from tqdm import tqdm as tqdm\n",
        "import re\n",
        "\n",
        "DATA_ROOT = '/content/drive/My Drive/data'\n",
        "\n",
        "sep = os.path.sep\n",
        "ET.XMLParser(encoding=\"utf-8\")\n",
        "\n",
        "paper_load_fail = 0\n",
        "annotation_load_fail = 0\n",
        "\n",
        "l = logging.getLogger('load_parse')\n",
        "\n",
        "\n",
        "# Gets names of folders where data is present. ex : \"./data/Training-Set-2016/C90-2039_TRAIN\"\n",
        "def get_folders(data_root):\n",
        "    folders = []\n",
        "    global folder_skips\n",
        "    for year_folder in filter(lambda x: os.path.isdir(data_root + sep + x), os.listdir(data_root)):\n",
        "        year_folder = data_root + sep + year_folder\n",
        "        for data_folder in os.listdir(year_folder):\n",
        "            data_folder = year_folder + sep + data_folder\n",
        "            if not os.path.isdir(\n",
        "                    data_folder) or '2019' in data_folder or 'Test-Set-2018' in data_folder or '2018' not in data_folder:\n",
        "                continue\n",
        "                # readme\n",
        "            folders.append(data_folder)\n",
        "    return folders\n",
        "\n",
        "\n",
        "def load_article(filename):\n",
        "    global paper_load_fail\n",
        "    if not os.path.splitext(filename)[1] == '.xml':\n",
        "        l.info(\"Skipping Non xml : \" + filename)\n",
        "        return\n",
        "    l.info(\"parsing :\" + filename)\n",
        "    try:\n",
        "        # Ignoring non UTF characters for now\n",
        "        with codecs.open(filename, mode='r', encoding='utf-8', errors='ignore') as target_file:\n",
        "\n",
        "            xml = ET.parse(target_file)\n",
        "            parent_map = {c: p for p in xml.getroot().iter() for c in p}\n",
        "            sentence_elements = list(xml.getroot().iter('S'))\n",
        "            sentence_elements = [(x.text,\n",
        "                                  parent_map[x].attrib['title'] if len(parent_map[x].attrib) > 0 else parent_map[x].tag,\n",
        "                                  int(x.attrib['sid']) if 'sid' in x.attrib else 99)\n",
        "                                 for x in\n",
        "                                 sentence_elements]\n",
        "            # TODO: Check if this is too memory inefficient. Should mostly be okay\n",
        "            sentence_map = {x[2]: x[0] for i, x in enumerate(sentence_elements)}\n",
        "            section_map = {x[2]: x[1] for i, x in enumerate(sentence_elements)}\n",
        "            article = Article(xml, sentence_map, section_map)\n",
        "            return article\n",
        "    except Exception as e:\n",
        "        l.error(\"Error with : \" + filename + \" with ex : \" + str(e))\n",
        "        paper_load_fail += 1\n",
        "        return Article(ET.fromstring(\"<xml></xml>\"), {}, {})\n",
        "\n",
        "\n",
        "def newest_file(path):\n",
        "    files = os.listdir(path)\n",
        "    paths = [os.path.join(path, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "author_memo = {}\n",
        "\n",
        "def load_folder_data(annotation_file, articles, is_test=False):\n",
        "    global annotation_load_fail\n",
        "    data = []\n",
        "    global author_memo\n",
        "\n",
        "    year_matches = re.findall(\".*Set[-]([0-9]{4}).*\", annotation_file)\n",
        "    year = int(year_matches[0]) if len(year_matches) > 0 else 2016  # Dev set\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if len(line.strip()) == 0:\n",
        "                continue\n",
        "            try:\n",
        "                parts = line.split(\" | \")\n",
        "                parts = [part.strip() for part in parts]\n",
        "                ref_article_name = parts[1].split(\":\")[1].strip().upper().replace(\".XML\", \"\").replace(\".TXT\", \"\")\n",
        "                if ref_article_name == \"P07-1040\":\n",
        "                    print(\"gg\")\n",
        "                ref_article = articles[parts[1].split(\":\")[1].strip().upper().replace(\".XML\", \"\").replace(\".TXT\", \"\")]\n",
        "                cite_article = articles[\"-\".join(\n",
        "                    parts[2].split(\":\")[1].strip().upper().replace(\"_\", \"-\").replace(\".XML\", \"\").replace(\".TXT\",\n",
        "                                                                                                         \"\").split(\"-\")[\n",
        "                    :2])]\n",
        "                marker_offset = [int(x) for x in (\n",
        "                    parts[5].split(\":\")[1].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()).replace(\",\",\n",
        "                                                                                                               \" \").split()]\n",
        "                citation_offsets = [int(x) for x in (\n",
        "                    parts[5].split(\":\")[1].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()).replace(\",\",\n",
        "                                                                                                               \" \").split()]\n",
        "                ref_offsets = [int(x) for x in (\n",
        "                    parts[7].split(\":\")[1].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").strip()).replace(\",\",\n",
        "                                                                                                               \" \").split()]\n",
        "                facet = parts[9].split(\":\")[1].strip()\n",
        "                facet = [facet] if '[' not in facet else eval(facet)\n",
        "                author = '' if len(parts) < 10 or \":\" not in parts[10] else parts[10].split(\":\")[1].replace(\"|\",\n",
        "                                                                                                            \"\").strip()\n",
        "                if ref_article_name not in author_memo:\n",
        "                    url = \"https://www.aclweb.org/anthology/{}.bib\".format(ref_article_name)\n",
        "                    info = requests.get(url=url).text\n",
        "                    author_info = info.split('{')[1].split(',')[0].split('-')\n",
        "                    author = get_formatted_author_info(author_info)\n",
        "                    author_memo[ref_article_name] = author_info\n",
        "                else:\n",
        "                    print(\"Hit\")\n",
        "                    author_info = author_memo[ref_article_name]\n",
        "                    author = get_formatted_author_info(author_info)\n",
        "                year = author_info[-2]\n",
        "                ref_article, cite_article = get_clean_cite_and_ref(ref_article, cite_article, ref_offsets, citation_offsets,\n",
        "                                                         author, year)\n",
        "\n",
        "                d = Datum(ref_article, cite_article, Offsets(marker_offset, citation_offsets, ref_offsets), author,\n",
        "                          is_test, facet, year)\n",
        "                data.append(d)\n",
        "            except Exception as e:\n",
        "                l.error(e)\n",
        "                annotation_load_fail += 1\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_folder(folder_root):\n",
        "    annotation_folder = folder_root + sep + \"annotation\"\n",
        "    citance_dir = folder_root + sep + \"Citance_XML\"\n",
        "    ref_name = folder_root.split(sep)[-1].replace(\"_TRAIN\", \"\")\n",
        "    ref_dir = folder_root + sep + \"Reference_XML\" + sep + ref_name + \".xml\"\n",
        "    articles = {\n",
        "        '-'.join(os.path.splitext(x)[0].upper().replace(\"_\", \"-\").split(\"-\")[:2]): load_article(citance_dir + sep + x)\n",
        "        for x in os.listdir(citance_dir)}\n",
        "\n",
        "    assert len(articles) == len(os.listdir(citance_dir))\n",
        "\n",
        "    ref_article = load_article(ref_dir)\n",
        "    articles[ref_name.upper()] = ref_article\n",
        "    annotation_file = newest_file(annotation_folder)\n",
        "    folder_data = load_folder_data(annotation_file, articles)\n",
        "    return folder_data\n",
        "\n",
        "import copy\n",
        "\n",
        "replace_count = 0\n",
        "\n",
        "def get_clean_cite_and_ref(ref_article, cite_article, ref_offsets, citation_offsets, author, year):\n",
        "    global replace_count\n",
        "    ref = copy.copy(ref_article)\n",
        "    cite = copy.copy(cite_article)\n",
        "    cite_sentence = \" \".join([cite.sentences[c] for c in citation_offsets])\n",
        "    cite_sentence = re.sub(r\"\\D(\\d{4})\\D\", '', cite_sentence)  # regex for removing years\n",
        "    cite_sentence = re.sub(r\"\\[[0-9]{1,3}\\]\", '', cite_sentence)  # regex for removing citation numbers\n",
        "    translation = {ord(')'): None, ord('('): None, ord('.'): None,  ord(','): None, ord('!'): 'l'}\n",
        "    cite_sentence = cite_sentence.translate(translation)\n",
        "    author_info = author.split(\" \")\n",
        "    author_2 = None\n",
        "    if author_info[-1] in [\"et.al.\", \"etal\", \"Etal\"]:\n",
        "        author_1 = \" \".join(author_info[:-1]) + \" et al\"\n",
        "        if len(author_info) >= 2:\n",
        "            author_2 = author_info[0] + \" and \" + author_info[1]\n",
        "    elif len(author_info) >= 2 and author_info[-2] not in [\"And\", \"and\"]:\n",
        "        author_1 = author_info[0] + \" & \" + author_info[1]\n",
        "        author_2 = author_info[0] + \" and \" + author_info[1]\n",
        "    else:\n",
        "        author_1 = author_info[-1]\n",
        "\n",
        "    citing_paper_text = author_1\n",
        "    old_cite_sentence = cite_sentence\n",
        "    cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "    if cite_sentence != old_cite_sentence:\n",
        "        replace_count += 1\n",
        "    elif author_2:\n",
        "        citing_paper_text = author_2\n",
        "        cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "        if cite_sentence != old_cite_sentence:\n",
        "            replace_count += 1\n",
        "        elif len(author_info) >= 2 and author_info[-2] not in [\"And\", \"and\"]:\n",
        "            author_info.sort()\n",
        "            author_1 = author_info[0] + \" & \" + author_info[1]\n",
        "            author_2 = author_info[0] + \" and \" + author_info[1]\n",
        "            citing_paper_text = author_1\n",
        "            old_cite_sentence = cite_sentence\n",
        "            cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "            if cite_sentence != old_cite_sentence:\n",
        "                replace_count += 1\n",
        "            elif author_2:\n",
        "                citing_paper_text = author_2\n",
        "                cite_sentence = cite_sentence.replace(citing_paper_text, \"##CITATION##\")\n",
        "                if cite_sentence != old_cite_sentence:\n",
        "                    replace_count += 1\n",
        "                else:\n",
        "                    print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence,\n",
        "                                                                                          old_cite_sentence, author))\n",
        "            else:\n",
        "                print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence,\n",
        "                                                                                      old_cite_sentence, author))\n",
        "        else:\n",
        "            print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence, old_cite_sentence, author))\n",
        "    else:\n",
        "        print(\"cite_sentence: {}\\n old_cite_sentence: {}\\n author: {}\".format(cite_sentence, old_cite_sentence, author))\n",
        "\n",
        "    cite.sentences[citation_offsets[0]] = cite_sentence\n",
        "    if len(citation_offsets) > 1:\n",
        "        for offset in citation_offsets[1:]:\n",
        "            cite.sentences[offset] = \"\"\n",
        "    print(\"replace_count: \", replace_count)\n",
        "    return ref, cite\n",
        "\n",
        "\n",
        "def get_formatted_author_info(author_info):\n",
        "    author_details = \" \".join(author_info)\n",
        "    author_info = re.sub(r\"[0-9]{4}\", '_', author_details).split('_')[0].split()\n",
        "    author = \" \".join([x.capitalize() if x is not \"and\" else x for x in author_info])\n",
        "    return author\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_all(root):\n",
        "    print(\"Loading all folders, skipping 2019 and test set 2018 for now\")\n",
        "    folders = get_folders(root)\n",
        "    print(\"Going to load :\", len(folders))\n",
        "    dataset = [load_folder(folder) for folder in tqdm(folders)]\n",
        "    dataset = [data for datalist in dataset for data in datalist]\n",
        "    print()\n",
        "    print(\"Paper load fails due to xml errors : \", paper_load_fail)\n",
        "    print(\"Annotation load fails due to random reasons\", annotation_load_fail)\n",
        "    print(\"Overall loaded \", len(dataset), \" datapoints\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(filename='example.log', level=logging.ERROR, filemode='w')\n",
        "    print(\"Logging to example.log\")\n",
        "dataset = load_all(DATA_ROOT)\n",
        "\n",
        "dataset[0]\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine as cosine_similarity\n",
        "\n",
        "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "\n",
        "def encode(text):\n",
        "    text = text.lower()\n",
        "    v = embedder.encode([text])[0]\n",
        "    return v\n",
        "\n",
        "\n",
        "def get_similarity_score(sentence1, sentence2):\n",
        "    if type(sentence1) == type(''):\n",
        "        sentence1 = encode(sentence1)\n",
        "    if type(sentence2) == type(''):\n",
        "        sentence2 = encode(sentence2)\n",
        "    return 1 - cosine_similarity(sentence1, sentence2)\n",
        "\n",
        "#\n",
        "# v1 = encode(\"Test sentence\")\n",
        "#\n",
        "# v2 = encode(\"Test sentence\")\n",
        "#\n",
        "# v3 = encode(\"Random sentence\")\n",
        "#\n",
        "# print(get_similarity_score(v1, v3))\n",
        "# print(get_similarity_score(v1, v2))\n",
        "\n",
        "from rake_nltk import Rake\n",
        "\n",
        "rakey = Rake(max_length=1)\n",
        "\n",
        "\n",
        "# def encode(sentence):\n",
        "#     return sentence\n",
        "#\n",
        "#\n",
        "# def get_similarity_score(sentence1, sentence2):\n",
        "#     # doesn't do anything about frequency of words in a sentence\n",
        "#     tokens1 = set(re.findall(r'[\\w]+', sentence1.lower()))\n",
        "#     tokens2 = set(re.findall(r'[\\w]+', sentence2.lower()))\n",
        "#     # print(sentence1)\n",
        "#     rakey.extract_keywords_from_sentences(sentence1.replace(\"-\", \" \").lower().split())\n",
        "#\n",
        "#     keys1 = rakey.get_ranked_phrases()\n",
        "#     # print(keys1)\n",
        "#\n",
        "#     rakey.extract_keywords_from_sentences(sentence2.replace(\"-\", \" \").lower().split())\n",
        "#     keys2 = rakey.get_ranked_phrases()\n",
        "#\n",
        "#     # keys = set(keys1) and set(keys2)\n",
        "#     tokens1 = set(keys1)\n",
        "#     tokens2 = set(keys2)\n",
        "#     # print(tokens1)\n",
        "#     # print(tokens2)\n",
        "#     # wefwefwef\n",
        "#\n",
        "#     return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "#\n",
        "\n",
        "import re\n",
        "\n",
        "# 106 115 123\n",
        "\n",
        "# def encode(sentence):\n",
        "#     return sentence\n",
        "#\n",
        "#\n",
        "# def get_similarity_score(sentence1, sentence2):\n",
        "#     sentence1 = re.findall(r'[\\w]+', sentence1.lower())\n",
        "#     sentence2 = re.findall(r'[\\w]+', sentence2.lower())\n",
        "#     len_sentence1 = len(sentence1)\n",
        "#     len_sentence2 = len(sentence2)\n",
        "#\n",
        "#     if not len_sentence1 or not len_sentence2:\n",
        "#         return 0.0\n",
        "#\n",
        "#     if sentence1 == sentence2:\n",
        "#         return 1.0\n",
        "#\n",
        "#     if len_sentence1 == 1 or len_sentence2 == 1:\n",
        "#         return 0.0\n",
        "#\n",
        "#     sentence1_bigrams = [sentence1[i:i + 2] for i in range(len_sentence1 - 1)]\n",
        "#     sentence2_bigrams = [sentence2[i:i + 2] for i in range(len_sentence2 - 1)]\n",
        "#\n",
        "#     sentence1_bigrams.sort()\n",
        "#     sentence2_bigrams.sort()\n",
        "#\n",
        "#     len_sentence1 = len(sentence1_bigrams)\n",
        "#     len_sentence2 = len(sentence2_bigrams)\n",
        "#\n",
        "#     matches = i = j = 0\n",
        "#     while i < len_sentence1 and j < len_sentence2:\n",
        "#         if sentence1_bigrams[i] == sentence2_bigrams[j]:\n",
        "#             matches += 2\n",
        "#             i += 1\n",
        "#             j += 1\n",
        "#         elif sentence1_bigrams[i] < sentence2_bigrams[j]:\n",
        "#             i += 1\n",
        "#         else:\n",
        "#             j += 1\n",
        "#\n",
        "#     return (2 * matches) / (len_sentence1 + len_sentence2)\n",
        "\n",
        "\n",
        "accuracy = 0\n",
        "total = 0\n",
        "empty_citations = 0\n",
        "empty_references = 0\n",
        "\n",
        "articles = [x[1] for x in dataset]\n",
        "n = 5\n",
        "\n",
        "pbar = tqdm(dataset)\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "avg_score = 0\n",
        "all_scores = []\n",
        "for data in pbar:\n",
        "    ref_article = data.ref\n",
        "    citing_article = data.cite\n",
        "    offsets = data.offsets\n",
        "    citing_sentence_ids = offsets.cite\n",
        "    true_ref_sentences = offsets.ref\n",
        "    true_ref_sentence_ids = offsets.ref\n",
        "    if tp > 0:\n",
        "        p = tp / (max(tp + fp, 1))\n",
        "        r = tp / (max(tp + fn, 1))\n",
        "        f1 = 2 * p * r / (p + r)\n",
        "        pbar.set_description(\"Processing %.3f %.3f %.3f\" % (p, r, f1))\n",
        "\n",
        "    if citing_article.sentences:\n",
        "        new_ids = [c for c in citing_sentence_ids]\n",
        "        for c in citing_sentence_ids:\n",
        "            # If additional context is reqd\n",
        "            to_add = 0\n",
        "            extra = range(max(1, c - to_add), c)\n",
        "            new_ids.extend(extra)\n",
        "            extra = range(c + 1, min(len(citing_article.sentences), c + to_add + 1))\n",
        "            new_ids.extend(extra)\n",
        "        citing_sentence_ids = new_ids\n",
        "        # complete_citing_sentence = \" \".join([citing_article.sentences[c] for c in citing_sentence_ids])\n",
        "        ref_vs = [(x[0], encode(x[1])) for x in ref_article.sentences.items()]\n",
        "        similarity_score = {}\n",
        "        for c in citing_sentence_ids:\n",
        "            complete_citing_sentence = citing_article.sentences[c]\n",
        "            complete_citing_sentence = encode(complete_citing_sentence)\n",
        "            for ref_id, ref_sentence in ref_vs:  # (ref_article.sentences.items()):\n",
        "                try:\n",
        "                    similarity_score[ref_id] = max(similarity_score.get(ref_id, 0),\n",
        "                                                   get_similarity_score(ref_sentence, complete_citing_sentence))\n",
        "                except Exception as e:\n",
        "                    # print(e)\n",
        "                    pass\n",
        "        if similarity_score:\n",
        "            sorted_similarity_score = sorted(similarity_score.items(), key=lambda item: -item[1])\n",
        "            top_n = [s for s in sorted_similarity_score]\n",
        "            top_n = {x[0]: x[1] for x in top_n[:3]}\n",
        "            fp += len(top_n)\n",
        "            for x in true_ref_sentence_ids:\n",
        "                if x in top_n:\n",
        "                    avg_score = (tp * avg_score + top_n[x]) / (max(tp, 1))\n",
        "                    all_scores.append(top_n[x])\n",
        "                    fp -= 1\n",
        "                    tp += 1\n",
        "                else:\n",
        "                    # print(ref_article.sentences[x])\n",
        "                    # print([citing_article.sentences[x] for x in citing_sentence_ids])\n",
        "                    fn += 1\n",
        "print(tp, fp, fn)\n",
        "\n",
        "xy = {}\n",
        "for data in dataset:\n",
        "    if not data.cite.sentences or not data.ref.sentences:\n",
        "        continue\n",
        "    y = data.facet\n",
        "    ref_sents = [''.join([data.ref.sentences[x] for x in data.offsets.ref])]\n",
        "    cite_sents = [''.join([data.cite.sentences[x] for x in data.offsets.cite])]\n",
        "    for sent in ref_sents + cite_sents:\n",
        "        xy[sent] = y\n",
        "\n",
        "len(xy)\n",
        "\n",
        "f = {}\n",
        "for x in xy:\n",
        "    y = xy[x]\n",
        "    y_ = \"_\".join(sorted([x.lower().replace(\"_\", \"\").replace(\"s\", \"\").replace(\" \", \"\") for x in y]))\n",
        "    # print(y)\n",
        "    if \"aim\" in \"\".join(y).lower():\n",
        "        print(x)\n",
        "        print(\"----\")\n",
        "    f[y_] = f.get(y_, 0) + 1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to example.log\n",
            "Loading all folders, skipping 2019 and test set 2018 for now\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Going to load : 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  2%|â–Ž         | 1/40 [00:06<04:22,  6.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  1\n",
            "Hit\n",
            "replace_count:  2\n",
            "Hit\n",
            "replace_count:  3\n",
            "Hit\n",
            "cite_sentence: Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system Appelt et al  and prod uced one of the top scores a recall of 59% and precision of 72% in the MUC6 Coreference Task which evaluated systems' ability to recog nize coreference among noun phrases Sund heim\n",
            " old_cite_sentence: Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system Appelt et al  and prod uced one of the top scores a recall of 59% and precision of 72% in the MUC6 Coreference Task which evaluated systems' ability to recog nize coreference among noun phrases Sund heim\n",
            " author: Sundheim\n",
            "replace_count:  3\n",
            "Hit\n",
            "replace_count:  4\n",
            "Hit\n",
            "replace_count:  5\n",
            "Hit\n",
            "replace_count:  6\n",
            "Hit\n",
            "replace_count:  7\n",
            "Hit\n",
            "replace_count:  8\n",
            "Hit\n",
            "cite_sentence: The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei\n",
            " old_cite_sentence: The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei\n",
            " author: Sundheim\n",
            "replace_count:  8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  5%|â–Œ         | 2/40 [00:12<04:02,  6.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  9\n",
            "Hit\n",
            "replace_count:  10\n",
            "Hit\n",
            "replace_count:  11\n",
            "Hit\n",
            "replace_count:  12\n",
            "Hit\n",
            "replace_count:  13\n",
            "Hit\n",
            "replace_count:  14\n",
            "Hit\n",
            "replace_count:  15\n",
            "Hit\n",
            "cite_sentence: Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2\n",
            " old_cite_sentence: Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2\n",
            " author: Zhao Gildea\n",
            "replace_count:  15\n",
            "Hit\n",
            "Hit\n",
            "Hit\n",
            "replace_count:  16\n",
            "Hit\n",
            "replace_count:  17\n",
            "Hit\n",
            "replace_count:  18\n",
            "Hit\n",
            "replace_count:  19\n",
            "Hit\n",
            "replace_count:  20\n",
            "Hit\n",
            "replace_count:  21\n",
            "Hit\n",
            "replace_count:  22\n",
            "Hit\n",
            "replace_count:  23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|â–Š         | 3/40 [00:35<07:00, 11.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  24\n",
            "Hit\n",
            "replace_count:  25\n",
            "Hit\n",
            "replace_count:  26\n",
            "Hit\n",
            "replace_count:  27\n",
            "Hit\n",
            "replace_count:  28\n",
            "Hit\n",
            "replace_count:  29\n",
            "Hit\n",
            "replace_count:  30\n",
            "Hit\n",
            "replace_count:  31\n",
            "Hit\n",
            "replace_count:  32\n",
            "Hit\n",
            "replace_count:  33\n",
            "Hit\n",
            "replace_count:  34\n",
            "Hit\n",
            "replace_count:  35\n",
            "Hit\n",
            "replace_count:  36\n",
            "Hit\n",
            "replace_count:  37\n",
            "Hit\n",
            "replace_count:  38\n",
            "Hit\n",
            "replace_count:  39\n",
            "Hit\n",
            "replace_count:  40\n",
            "Hit\n",
            "replace_count:  41\n",
            "Hit\n",
            "replace_count:  42\n",
            "Hit\n",
            "replace_count:  43\n",
            "Hit\n",
            "replace_count:  44\n",
            "Hit\n",
            "replace_count:  45\n",
            "Hit\n",
            "replace_count:  46\n",
            "Hit\n",
            "replace_count:  47\n",
            "Hit\n",
            "replace_count:  48\n",
            "Hit\n",
            "replace_count:  49\n",
            "Hit\n",
            "replace_count:  50\n",
            "Hit\n",
            "replace_count:  51\n",
            "Hit\n",
            "replace_count:  52\n",
            "Hit\n",
            "replace_count:  53\n",
            "Hit\n",
            "replace_count:  54\n",
            "Hit\n",
            "replace_count:  55\n",
            "Hit\n",
            "replace_count:  56\n",
            "Hit\n",
            "replace_count:  57\n",
            "Hit\n",
            "replace_count:  58\n",
            "Hit\n",
            "replace_count:  59\n",
            "Hit\n",
            "replace_count:  60\n",
            "Hit\n",
            "replace_count:  61\n",
            "Hit\n",
            "replace_count:  62\n",
            "Hit\n",
            "replace_count:  63\n",
            "Hit\n",
            "replace_count:  64\n",
            "Hit\n",
            "replace_count:  65\n",
            "Hit\n",
            "replace_count:  66\n",
            "Hit\n",
            "replace_count:  67\n",
            "Hit\n",
            "replace_count:  68\n",
            "Hit\n",
            "replace_count:  69\n",
            "Hit\n",
            "replace_count:  70\n",
            "Hit\n",
            "replace_count:  71\n",
            "Hit\n",
            "replace_count:  72\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|â–ˆ         | 4/40 [00:46<06:45, 11.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  73\n",
            "Hit\n",
            "replace_count:  74\n",
            "Hit\n",
            "replace_count:  75\n",
            "Hit\n",
            "replace_count:  76\n",
            "Hit\n",
            "replace_count:  77\n",
            "Hit\n",
            "replace_count:  78\n",
            "Hit\n",
            "replace_count:  79\n",
            "Hit\n",
            "replace_count:  80\n",
            "Hit\n",
            "replace_count:  81\n",
            "Hit\n",
            "replace_count:  82\n",
            "Hit\n",
            "replace_count:  83\n",
            "Hit\n",
            "replace_count:  84\n",
            "Hit\n",
            "replace_count:  85\n",
            "Hit\n",
            "replace_count:  86\n",
            "Hit\n",
            "replace_count:  87\n",
            "Hit\n",
            "replace_count:  88\n",
            "Hit\n",
            "replace_count:  89\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|â–ˆâ–Ž        | 5/40 [00:53<05:54, 10.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  90\n",
            "Hit\n",
            "replace_count:  91\n",
            "Hit\n",
            "Hit\n",
            "cite_sentence: Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: Job- bins and Evett combined word recurrence co-occurrences and a thesaurus; Beeferman et al relied on both lexical modeling and discourse cues; Galley et al made use of word reiteration through lexical chains and discourse cues\n",
            " old_cite_sentence: Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: Job- bins and Evett combined word recurrence co-occurrences and a thesaurus; Beeferman et al relied on both lexical modeling and discourse cues; Galley et al made use of word reiteration through lexical chains and discourse cues\n",
            " author: Jobbins Evett\n",
            "replace_count:  91\n",
            "Hit\n",
            "replace_count:  92\n",
            "Hit\n",
            "replace_count:  93\n",
            "Hit\n",
            "replace_count:  94\n",
            "Hit\n",
            "replace_count:  95\n",
            "Hit\n",
            "cite_sentence: This network could also be used more directly for topic segmentation as in Job- bins and Evett\n",
            " old_cite_sentence: This network could also be used more directly for topic segmentation as in Job- bins and Evett\n",
            " author: Jobbins Evett\n",
            "replace_count:  95\n",
            "Hit\n",
            "Hit\n",
            "cite_sentence: In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the userâ€™s needs are retrieved \n",
            " old_cite_sentence: In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the userâ€™s needs are retrieved \n",
            " author: Jobbins Evett\n",
            "replace_count:  95\n",
            "Hit\n",
            "replace_count:  96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|â–ˆâ–Œ        | 6/40 [01:17<08:01, 14.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  97\n",
            "Hit\n",
            "replace_count:  98\n",
            "Hit\n",
            "replace_count:  99\n",
            "Hit\n",
            "replace_count:  100\n",
            "Hit\n",
            "replace_count:  101\n",
            "Hit\n",
            "replace_count:  102\n",
            "Hit\n",
            "replace_count:  103\n",
            "Hit\n",
            "replace_count:  104\n",
            "Hit\n",
            "replace_count:  105\n",
            "Hit\n",
            "cite_sentence: Log-linear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos and Gimpel andSmith; the idea can be traced back to Pap ineni et al\n",
            " old_cite_sentence: Log-linear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos and Gimpel andSmith; the idea can be traced back to Pap ineni et al\n",
            " author: Gimpel Smith\n",
            "replace_count:  105\n",
            "Hit\n",
            "replace_count:  106\n",
            "Hit\n",
            "replace_count:  107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|â–ˆâ–Š        | 7/40 [01:24<06:35, 11.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  108\n",
            "Hit\n",
            "replace_count:  109\n",
            "Hit\n",
            "replace_count:  110\n",
            "Hit\n",
            "replace_count:  111\n",
            "Hit\n",
            "replace_count:  112\n",
            "Hit\n",
            "replace_count:  113\n",
            "Hit\n",
            "replace_count:  114\n",
            "Hit\n",
            "replace_count:  115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 8/40 [01:59<10:07, 18.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  116\n",
            "Hit\n",
            "replace_count:  117\n",
            "Hit\n",
            "replace_count:  118\n",
            "Hit\n",
            "replace_count:  119\n",
            "Hit\n",
            "replace_count:  120\n",
            "Hit\n",
            "replace_count:  121\n",
            "Hit\n",
            "replace_count:  122\n",
            "Hit\n",
            "replace_count:  123\n",
            "Hit\n",
            "replace_count:  124\n",
            "Hit\n",
            "replace_count:  125\n",
            "Hit\n",
            "replace_count:  126\n",
            "Hit\n",
            "replace_count:  127\n",
            "Hit\n",
            "replace_count:  128\n",
            "Hit\n",
            "replace_count:  129\n",
            "Hit\n",
            "replace_count:  130\n",
            "Hit\n",
            "replace_count:  131\n",
            "Hit\n",
            "replace_count:  132\n",
            "Hit\n",
            "replace_count:  133\n",
            "Hit\n",
            "replace_count:  134\n",
            "Hit\n",
            "replace_count:  135\n",
            "Hit\n",
            "replace_count:  136\n",
            "Hit\n",
            "replace_count:  137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|â–ˆâ–ˆâ–Ž       | 9/40 [02:10<08:29, 16.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  138\n",
            "Hit\n",
            "replace_count:  139\n",
            "Hit\n",
            "replace_count:  140\n",
            "Hit\n",
            "replace_count:  141\n",
            "Hit\n",
            "replace_count:  142\n",
            "Hit\n",
            "replace_count:  143\n",
            "Hit\n",
            "replace_count:  144\n",
            "Hit\n",
            "cite_sentence: The parameters of the model can be estimated from tagged 1 3 4 6 12] or untagged [2 9 11] text\n",
            " old_cite_sentence: The parameters of the model can be estimated from tagged 1 3 4 6 12] or untagged [2 9 11] text\n",
            " author: Kupiec\n",
            "replace_count:  144\n",
            "Hit\n",
            "cite_sentence: One area in which the statistical approach has done parÂ­ ticularly well is automatic part of speech tagging asÂ­ signing each word in an input sentence its proper part of speech 1 2 3 4 6 9 11 12]\n",
            " old_cite_sentence: One area in which the statistical approach has done parÂ­ ticularly well is automatic part of speech tagging asÂ­ signing each word in an input sentence its proper part of speech 1 2 3 4 6 9 11 12]\n",
            " author: Kupiec\n",
            "replace_count:  144\n",
            "Hit\n",
            "replace_count:  145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|â–ˆâ–ˆâ–Œ       | 10/40 [02:27<08:23, 16.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cite_sentence: And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al\n",
            " old_cite_sentence: And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al\n",
            " author: Van Halteren Etal\n",
            "replace_count:  145\n",
            "Hit\n",
            "replace_count:  146\n",
            "Hit\n",
            "replace_count:  147\n",
            "Hit\n",
            "cite_sentence: We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting\n",
            " old_cite_sentence: We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting\n",
            " author: Van Halteren Etal\n",
            "replace_count:  147\n",
            "Hit\n",
            "cite_sentence: Halteren et al  compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging\n",
            " old_cite_sentence: Halteren et al  compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging\n",
            " author: Van Halteren Etal\n",
            "replace_count:  147\n",
            "Hit\n",
            "cite_sentence: Thirdly this approach is compatible with inÂ­ corporating multiple components of the same type to improve performance cf van Halteren et al who found that combining the results of several part of speech taggers increased performance\n",
            " old_cite_sentence: Thirdly this approach is compatible with inÂ­ corporating multiple components of the same type to improve performance cf van Halteren et al who found that combining the results of several part of speech taggers increased performance\n",
            " author: Van Halteren Etal\n",
            "replace_count:  147\n",
            "Hit\n",
            "cite_sentence: Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results\n",
            " old_cite_sentence: Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results\n",
            " author: Van Halteren Etal\n",
            "replace_count:  147\n",
            "Hit\n",
            "replace_count:  148\n",
            "Hit\n",
            "cite_sentence: Parallel to van Halteren et al we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based\n",
            " old_cite_sentence: Parallel to van Halteren et al we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based\n",
            " author: Van Halteren Etal\n",
            "replace_count:  148\n",
            "Hit\n",
            "cite_sentence: In all experiments the TotPrecision voting scheme of van Halteren et al has been used\n",
            " old_cite_sentence: In all experiments the TotPrecision voting scheme of van Halteren et al has been used\n",
            " author: Van Halteren Etal\n",
            "replace_count:  148\n",
            "Hit\n",
            "replace_count:  149\n",
            "Hit\n",
            "replace_count:  150\n",
            "Hit\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally\n",
            " old_cite_sentence: First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans  This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners\n",
            " old_cite_sentence: Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans  This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans  and Brill and Wu  In both approaches different tagger genÂ­ erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans  to WSJ as well it is easier to make a comparison\n",
            " old_cite_sentence: For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans  and Brill and Wu  In both approaches different tagger genÂ­ erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans  to WSJ as well it is easier to make a comparison\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: One of the best methods for tagger combination in van Halteren Zavrel and DaeleÂ­ mans is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers\n",
            " old_cite_sentence: One of the best methods for tagger combination in van Halteren Zavrel and DaeleÂ­ mans is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: The most important result that has undergone a change between van Halteren Zavrel and Daelemans  and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed\n",
            " old_cite_sentence: The most important result that has undergone a change between van Halteren Zavrel and Daelemans  and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: The first is the LOB corpus Johansson which we used in the earlier experiments as well van Halteren Zavrel and Daelemans and which has proved to be a good testing ground\n",
            " old_cite_sentence: The first is the LOB corpus Johansson which we used in the earlier experiments as well van Halteren Zavrel and Daelemans and which has proved to be a good testing ground\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: In van Halteren Zavrel and Daelemans  we used a straightforward imÂ­ plementation of HMM's which turned out to have the worst accuracy of the four competing methods\n",
            " old_cite_sentence: In van Halteren Zavrel and Daelemans  we used a straightforward imÂ­ plementation of HMM's which turned out to have the worst accuracy of the four competing methods\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n",
            "Hit\n",
            "cite_sentence: With LOB and a single 114K tune set van Halteren Zavrel and Daelemans both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context generÂ­ ally leads to better results\n",
            " old_cite_sentence: With LOB and a single 114K tune set van Halteren Zavrel and Daelemans both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context generÂ­ ally leads to better results\n",
            " author: Van Halteren Etal\n",
            "replace_count:  151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|â–ˆâ–ˆâ–Š       | 11/40 [02:34<06:42, 13.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "replace_count:  152\n",
            "Hit\n",
            "cite_sentence: Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time\n",
            " old_cite_sentence: Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time\n",
            " author: Gerdemann King\n",
            "replace_count:  152\n",
            "Hit\n",
            "replace_count:  153\n",
            "Hit\n",
            "replace_count:  154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b7552faa0cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'example.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logging to example.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5b7552faa0cb>\u001b[0m in \u001b[0;36mload_all\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_folders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Going to load :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdatalist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatalist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5b7552faa0cb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_folders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Going to load :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdatalist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatalist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5b7552faa0cb>\u001b[0m in \u001b[0;36mload_folder\u001b[0;34m(folder_root)\u001b[0m\n\u001b[1;32m    152\u001b[0m     articles = {\n\u001b[1;32m    153\u001b[0m         \u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitance_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         for x in os.listdir(citance_dir)}\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitance_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5b7552faa0cb>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    152\u001b[0m     articles = {\n\u001b[1;32m    153\u001b[0m         \u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitance_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         for x in os.listdir(citance_dir)}\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitance_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5b7552faa0cb>\u001b[0m in \u001b[0;36mload_article\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mxml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mparent_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0msentence_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \"\"\"\n\u001b[1;32m   1195\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    595\u001b[0m                     \u001b[0;31m# It can be used to parse the whole source without feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# it with chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size, chars, firstline)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0;31m# decode bytes (those remaining from the last call included)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytebuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UMuKkS1f4RD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# latest tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "global tf_vectorizer\n",
        "\n",
        "\n",
        "def build_model(cite_paper, ref_paper, n=1):\n",
        "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, n), min_df=0, stop_words='english')\n",
        "    sentences = list(cite_paper.sentences.values())\n",
        "    sentences.extend(list(ref_paper.sentences.values()))\n",
        "    tf_vectorizer = tf.fit(sentences)\n",
        "    pickle.dump(tf_vectorizer, open(\"tf-idf_cache/tf_cite_{}_ref_{}_n_{}.pickle\".format(cite_paper.id, ref_paper.id, n), 'wb'))\n",
        "    return tf_vectorizer\n",
        "\n",
        "def encode(sentence):\n",
        "  return sentence\n",
        "\n",
        "def get_similarity_score(cite_paper, ref_paper, sentence1, sentence2, n=1, kernel=\"none\"):\n",
        "    if os.path.exists(\"tf-idf_cache/tf_cite_{}_ref_{}_n_{}.pickle\".format(cite_paper.id, ref_paper.id, n)):\n",
        "        tf_vectorizer = pickle.load(open(\"tf-idf_cache/tf_cite_{}_ref_{}_n_{}.pickle\".format(cite_paper.id, ref_paper.id, n), 'rb'))\n",
        "    else:\n",
        "        tf_vectorizer = build_model(cite_paper, ref_paper, n)\n",
        "\n",
        "    tfidf_1 = tf_vectorizer.transform([sentence1])\n",
        "    tfidf_2 = tf_vectorizer.transform([sentence2])\n",
        "    if kernel == \"none\":\n",
        "        return cosine_similarity(tfidf_1, tfidf_2).item()\n",
        "    elif kernel == \"linear\":\n",
        "        return linear_kernel(tfidf_1, tfidf_2).item()\n",
        "    elif kernel == \"poly_2\":\n",
        "        return polynomial_kernel(tfidf_1, tfidf_2, 2).item()\n",
        "    elif kernel == \"poly_3\":\n",
        "        return linear_kernel(tfidf_1, tfidf_2, 3).item()\n",
        "    elif kernel == \"rbf\":\n",
        "        return rbf_kernel(tfidf_1, tfidf_2).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imwtPpxs4Jch",
        "colab_type": "code",
        "outputId": "963a83ee-5475-43bd-acea-f2835ac04dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# latest bm25\n",
        "!pip install rank_bm25\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "global bm25\n",
        "\n",
        "def build_model(ref_paper):\n",
        "    sentences = ref_paper.sentences.items()\n",
        "    tokenized_sentences = [doc.split(\" \") for doc in sentences]\n",
        "    bm25 = BM25Okapi(tokenized_sentences)\n",
        "    pickle.dump(bm25, open(\"bm_25/bm_25_ref_{}.pickle\".format(ref_paper.id), 'wb'))\n",
        "    return bm25\n",
        "\n",
        "def get_similarity_score(ref_paper, ref_sentence_id, sentence2):\n",
        "    if os.path.exists(\"bm_25/bm_25_ref_{}.pickle\".format(ref_paper.id)):\n",
        "        bm25 = pickle.load(open(\"bm_25/bm_25_ref_{}.pickle\".format(ref_paper.id), 'rb'))\n",
        "    else:\n",
        "        bm25 = build_model(ref_paper)\n",
        "\n",
        "    tokenized_query = sentence2.split(\" \")\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    score = scores[ref_sentence_id]\n",
        "    return score\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/d2/e4/38d03d6d5e2deae8d2838b81d6ba2742475ced42045f5c46aeb00c5fb79c/rank_bm25-0.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.2)\n",
            "Building wheels for collected packages: rank-bm25\n",
            "  Building wheel for rank-bm25 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rank-bm25: filename=rank_bm25-0.2-cp36-none-any.whl size=4162 sha256=d4eb3e115d25725d7a486ff3242ad6a721ceb8796674bdd488e4f265922c9ff4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/0c/1f/78945dd6a5478bbcdb50d73ac96ae5af2ffcdfcd374fd9b1bf\n",
            "Successfully built rank-bm25\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSe8Ih3T4MMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "def get_similarity_score(sentence1, sentence2, n=1, type=1):\n",
        "    # doesn't do anything about frequency of words in a sentence\n",
        "    if type == 1:\n",
        "\n",
        "        if n == 1:\n",
        "            tokens1 = set(re.findall(r'[\\w]+', sentence1.lower()))\n",
        "            tokens2 = set(re.findall(r'[\\w]+', sentence2.lower()))\n",
        "            return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "\n",
        "        elif n == 2:\n",
        "            bigrams1 = set(nltk.bigrams([word for word in sentence1.lower().split()]))\n",
        "            bigrams2 = set(nltk.bigrams([word for word in sentence2.lower().split()]))\n",
        "            return len(bigrams1.intersection(bigrams2)) / len(bigrams1.union(bigrams2))\n",
        "\n",
        "        elif n == 3:\n",
        "            trigrams1 = set(nltk.trigrams([word for word in sentence1.lower().split()]))\n",
        "            trigrams2 = set(nltk.trigrams([word for word in sentence2.lower().split()]))\n",
        "            return len(trigrams1.intersection(trigrams2)) / len(trigrams1.union(trigrams2))\n",
        "\n",
        "    elif type == 2:\n",
        "\n",
        "        if n == 1:\n",
        "            tokens1 = set(re.findall(r'[\\w]+', sentence1.lower()))\n",
        "            tokens2 = set(re.findall(r'[\\w]+', sentence2.lower()))\n",
        "            return len(tokens1.intersection(tokens2)) / len(tokens1)\n",
        "\n",
        "        elif n == 2:\n",
        "            bigrams1 = set(nltk.bigrams([word for word in sentence1.lower().split()]))\n",
        "            bigrams2 = set(nltk.bigrams([word for word in sentence2.lower().split()]))\n",
        "            return len(bigrams1.intersection(bigrams2)) / len(bigrams1)\n",
        "\n",
        "        elif n == 3:\n",
        "            trigrams1 = set(nltk.trigrams([word for word in sentence1.lower().split()]))\n",
        "            trigrams2 = set(nltk.trigrams([word for word in sentence2.lower().split()]))\n",
        "            return len(trigrams1.intersection(trigrams2)) / len(trigrams1)\n",
        "\n",
        "    elif type == 3:\n",
        "\n",
        "        if n == 1:\n",
        "            tokens1 = set(re.findall(r'[\\w]+', sentence1.lower()))\n",
        "            tokens2 = set(re.findall(r'[\\w]+', sentence2.lower()))\n",
        "            return len(tokens1.intersection(tokens2)) / len(tokens2)\n",
        "\n",
        "        elif n == 2:\n",
        "            bigrams1 = set(nltk.bigrams([word for word in sentence1.lower().split()]))\n",
        "            bigrams2 = set(nltk.bigrams([word for word in sentence2.lower().split()]))\n",
        "            return len(bigrams1.intersection(bigrams2)) / len(bigrams2)\n",
        "\n",
        "        elif n == 3:\n",
        "            trigrams1 = set(nltk.trigrams([word for word in sentence1.lower().split()]))\n",
        "            trigrams2 = set(nltk.trigrams([word for word in sentence2.lower().split()]))\n",
        "            return len(trigrams1.intersection(trigrams2)) / len(trigrams2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}