[["We report results for the best and median hyperparameter settings obtained in this way.", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 0], ["(2010) and the posterior regular- ization HMM of Grac\u00b8a et al.", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 0], ["Specifically, for both settings we report results on the median run for each setting.", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 0], ["The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5).", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 1], ["Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 0], ["On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 0], ["We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 0], ["In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 1], ["(2010) and the posterior regular- ization HMM of Grac\u00b8a et al.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 0], ["Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 0], ["In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 0], ["Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 1], ["On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 0], ["In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 0], ["Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 0], ["We consider the unsupervised POS induction problem without the use of a tagging dictionary.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 1], ["On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 0], ["This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 0], ["The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 0], ["For all languages we do not make use of a tagging dictionary.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 1], ["We can only compare with Grac\u00b8a et al.", "Following Lee et al. (2010) we used only the training sections for each language.", 0], ["On each language we investigate the contribution of each component of our model.", "Following Lee et al. (2010) we used only the training sections for each language.", 0], ["Following the setup of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English.", "Following Lee et al. (2010) we used only the training sections for each language.", 0], ["Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.", "Following Lee et al. (2010) we used only the training sections for each language.", 1], ["On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 0], ["Part-of-speech (POS) tag distributions are known to exhibit sparsity \u2014 a word is likely to take a single predominant tag in a corpus.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 0], ["The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 0], ["Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 1], ["(2010) and the posterior regular- ization HMM of Grac\u00b8a et al.", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 0], ["Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 0], ["In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me\u00b4rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac\u00b8a et al., 2009; Berg-Kirkpatrick et al., 2010).", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 0], ["We consider the unsupervised POS induction problem without the use of a tagging dictionary.", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 1], ["Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 0], ["In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me\u00b4rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac\u00b8a et al., 2009; Berg-Kirkpatrick et al., 2010).", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 0], ["(2010) and the posterior regular- ization HMM of Grac\u00b8a et al.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 0], ["We consider the unsupervised POS induction problem without the use of a tagging dictionary.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 1], ["Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 0], ["Specifically, the lexicon is generated as: P (T , W |\u03c8) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 0], ["Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 0], ["We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 1], ["Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 0], ["In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me\u00b4rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac\u00b8a et al., 2009; Berg-Kirkpatrick et al., 2010).", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 0], ["Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 0], ["We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 1], ["This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007).", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 0], ["On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 0], ["Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 0], ["First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 1], ["Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution \u03c8 over tag assignments drawn from DIRICHLET(\u03b2, K ).", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 1], ["During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, \u03b1, \u03b2) \u221d P (T , t, W , w|\u03b1, \u03b2) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, \u03c8, \u03b8, \u03c6, w|\u03b1, \u03b2)d\u03c8d\u03b8d\u03c6 Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior.", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 1], ["A novel element of our model is the ability to capture type-level tag frequencies.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 0], ["However, our full model takes advantage of word features not present in Grac\u00b8a et al.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 0], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 0], ["5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 1], ["Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 0], ["(2010) and the posterior regular- ization HMM of Grac\u00b8a et al.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 0], ["This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 0], ["We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 1], ["On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 0], ["Conditioned on T , features of word types W are drawn.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 0], ["Once HMM parameters (\u03b8, \u03c6) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from \u03c6.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 0], ["The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 1], ["Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 1], ["Recent work has made significant progress on unsupervised POS tagging (Me\u00b4rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009).", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 0], ["Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 0], ["A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types.", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 0], ["We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 1], ["The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 0], ["This task has in fact been performed by human annotators in the DAESO-project.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 0], ["News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 0], ["We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 1], ["Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 0], ["Thanks also to Wauter Bosma for originally mining the headlines from Google News.", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 0], ["In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 0], ["We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 1], ["For the development of our system we use data which was obtained in the DAESO-project.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 0], ["Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 0], ["Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 0], ["Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 1], ["The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 0], ["Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 0], ["This task has in fact been performed by human annotators in the DAESO-project.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 0], ["Our first approach is to use a clustering algorithm to cluster similar headlines.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 1], ["For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 0], ["We use the k-means implementation in the CLUTO1 software package.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 0], ["It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 0], ["The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 1], ["If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 0], ["Table 2 displays the paraphrase detection precision and recall of our two approaches.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 0], ["Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 0], ["For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 1], ["For each news article, the headline and the first 150 characters of the article were stored.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 0], ["We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 0], ["This implies that we focus on obtaining a high precision in the paraphrases collection process.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 0], ["It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 1], ["For instance, if headline A is paired with headline B, and headline B is aligned to headline C , headline A can be aligned to C as Ty pe Precision Recallk m ea ns cl us ter in g 0.91 0.43 clu ste rs on lyk m ea ns cl us ter in g 0.66 0.44 all he ad lin es pa irw ise si mi lar ity 0.93 0.39 clu ste rs on ly pa irw ise si mi lar ity 0.76 0.41 all he ad lin es Table 2: Precision and Recall for both methods Pl ay st ati on 3 m or e ex pe nsi ve th an co m pe tit or P l a y s t a t i o n 3 w i l l b e c o m e m o r e e x p e n s i v e t h a n X b o x 3 6 0 So ny po stp on es Blu Ra y m ov ie s So ny po stp on es co mi ng of blu ra y dv ds Pri ce s Pl ay st ati on 3 kn ow n: fro m 49 9 eu ro s E3 20 06 : Pl ay st ati on 3 fro m 49 9 eu ro s So ny PS 3 wi th Blu R ay for sal e fro m No ve m be r 11 th PS 3 av ail abl e in Eu ro pe fro m No ve m be r 17 th Table 3: Examples of correct (above) and incorrect (below) alignments well.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 0], ["Table 2 displays the paraphrase detection precision and recall of our two approaches.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 0], ["Threshold values as found by optimizing on the development data using again an F0.25-score, are T hlower = 0.2 and T hupper = 0.5.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 0], ["With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(\u03b8) = V 1 \u00b7 V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 1], ["We use this method to collect a large amount of aligned paraphrases in an automatic fashion.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 0], ["This implies that we focus on obtaining a high precision in the paraphrases collection process.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 0], ["Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 0], ["We use an F\u03b2 -score with a \u03b2 of 0.25 as we favour precision over recall.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 1], ["Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 0], ["News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 0], ["However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 0], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 1], ["We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 0], ["The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity.", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 0], ["We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data.", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 0], ["We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 1], ["Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 0], ["Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 0], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 0], ["Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 1], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 0], ["Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 0], ["They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 0], ["Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 1], ["Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 0], ["Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 0], ["A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 0], ["Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 1], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 0], ["Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 0], ["There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 0], ["Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 1], ["Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 0], ["This technique is similar to Hearst and Schu\u00a8 tze (1993) and Widdows (2003).", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 0], ["Ciaramita et al.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 0], ["Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 1], ["A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 0], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 0], ["Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 0], ["Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 1], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 0], ["We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 0], ["We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 0], ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 1], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 0], ["Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu\u00a8 tze (1993) and Widdows (2003).", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 0], ["This technique is similar to Hearst and Schu\u00a8 tze (1993) and Widdows (2003).", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 0], ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 1], ["Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 0], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 0], ["Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 0], ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 1], ["For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 0], ["Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 0], ["The components and their sizes including punctuation are given in Table 3.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 0], ["Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 1], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 0], ["Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu\u00a8 tze (1993) and Widdows (2003).", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 0], ["Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 0], ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 1], ["Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 0], ["A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 0], ["One solution would be to take the intersection between vectors across words for each supersense (i.e. to find the common contexts that these words appear in).", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 0], ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 1], ["Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 0], ["The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 0], ["Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 0], ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 1], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 1], ["Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 1], ["The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 0], ["Confusion networks are generated by choosing one hypothesis as the \u201cskeleton\u201d, and other hypotheses are aligned against it.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 0], ["In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 1], ["In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 0], ["Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007).", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 0], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 0], ["The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 1], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 0], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 0], ["Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 1], ["First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 0], ["Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 0], ["In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 0], ["All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 1], ["All confusion network are connected to a common end node with NULL arcs.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 1], ["The final arcs have a probability of one.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 1], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 0], ["Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 0], ["In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 0], ["Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 1], ["The other hypotheses are aligned against the skeleton.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 1], ["Either votes or some form of confidences are assigned to each word in the network.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 1], ["Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007).", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 0], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 0], ["Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 0], ["On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 1], ["This -best list is then re-scored with the higher order -gram.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 1], ["The second set of weights is used to find the final -best from the re-scored -best list.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 1], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 0], ["A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 0], ["Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 1], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 0], ["A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 0], ["Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 0], ["Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 1], ["This guarantees that the best path will not be found from a network generated for a system with zero weight.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 1], ["A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 0], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 0], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 0], ["Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 1], ["ric since it is based on the rate of edits required to transform the hypothesis into the reference.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 1], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 0], ["In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 0], ["Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 0], ["A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1], ["The algorithm explores better weights iteratively starting from a set of initial weights.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1], ["First, each dimension is optimized using a grid-based line minimization algorithm.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1], ["Then, a new direction based on the changes in the objective function is estimated to speed up the search.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1], ["In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 0], ["Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007).", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 0], ["The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991).", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 0], ["When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 1], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 0], ["In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 0], ["All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 1], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 0], ["Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 0], ["In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 1], ["Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007).", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 0], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 0], ["Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 0], ["On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 1], ["This -best list is then re-scored with the higher order -gram.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 1], ["The second set of weights is used to find the final -best from the re-scored -best list.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 1], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 0], ["All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 0], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 0], ["In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 1], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 0], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 0], ["The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 0], ["A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 1], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 0], ["Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 0], ["System weights may be used to assign a system specific confidence on each word in the network.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 0], ["Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 1], ["ric since it is based on the rate of edits required to transform the hypothesis into the reference.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 1], ["A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 0], ["Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 0], ["In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 0], ["The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 1], ["In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003).", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 0], ["All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 0], ["This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 0], ["To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 1], ["All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 1], ["All confusion network are connected to a common end node with NULL arcs.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 1], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 0], ["However, the other scores are worse in common with the tuning set results.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 0], ["In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 0], ["In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 1], ["The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 0], ["The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 0], ["The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 0], ["Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 1], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 0], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 0], ["A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 0], ["A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 1], ["There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 0], ["and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 0], ["In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 0], ["In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 1], ["There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 0], ["The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 0], ["A unique normalized and real-valued vector \u03b8, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 0], ["Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 1], ["In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 1], ["(a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 0], ["Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I \u03b2 e I a \u03b1 \u03b8 z f J B N M \u03b1 \u03b8 z a a f J B \u03b1 \u03b8 z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 0], ["Formally, we define the following terms1: \u2022 A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. \u2022 A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.\u2022 A document-pair (F, E) refers to two doc uments which are translations of each other.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 0], ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 1], ["The translation lexicon p(f |e) is the key component in this generative process.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 1], ["02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 1], ["In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 0], ["Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters \u03b1 and B in an unsupervised fashion.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 0], ["In the first BiTAM model, we assume that topics are sampled at the sentence-level.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 0], ["02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 1], ["Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 0], ["As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 0], ["With the learned B and \u03b1 fixed, the variational parameters to be computed in Eqn.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 0], ["Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 1], ["Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 1], ["Efficient variational approximation algorithms are designed for inference and parameter estimation.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 0], ["To estimate B, \u03b2 (for BiTAM2) and \u03b1, at most eight variational EM iterations are run on the training data.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 0], ["We use the end-user ter minology for source and target languages.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 0], ["The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 1], ["To reduce the data sparsity problem, we introduce two remedies in our models.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 1], ["First: Laplace smoothing.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 1], ["Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 0], ["The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 0], ["Similar to IBM models, \u201cNull\u201d word is used for the source words which have no translation counterparts in the target language.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 0], ["Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 1], ["Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 1], ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 1], ["Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 0], ["Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 0], ["Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 0], ["Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 1], ["Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 1], ["10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k \u221d ) ) ) ) \u03b4(f, fj )\u03b4(e, ei )\u03c6dnk \u03d5dnji (12) d n=1 j=1 i=1 For \u03b1, close-form update is not available, and we resort to gradient accent as in (Sjo\u00a8 lander et al., 1996) with restarts to ensure each updated \u03b1k >0.", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 0], ["This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)).", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 0], ["The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level.", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 0], ["Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 1], ["Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 1], ["Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 0], ["The proposed models significantly improve the alignment accuracy and lead to better translation qualities.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 0], ["Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 0], ["Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 1], ["For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 1], ["With the learned B and \u03b1 fixed, the variational parameters to be computed in Eqn.", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 0], ["Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003).", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 0], ["The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level.", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 0], ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 1], ["In general, the monolingual model for English can also be a rich topic-mixture.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 0], ["The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 0], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 0], ["Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 1], ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 1], ["For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations.", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 0], ["Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 0], ["An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004).", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 0], ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 1], ["Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 0], ["Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 0], ["Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 0], ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1], ["We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1], ["Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1], ["Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1], ["As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1], ["Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 0], ["A variational inference is used to approximate the true posteriors of these hidden variables.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 0], ["Choosing the number of topics is a model selection problem.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 0], ["To reduce the data sparsity problem, we introduce two remedies in our models.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 1], ["Second: interpolation smoothing.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 1], ["Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = \u03bbBf,e,k +(1\u2212\u03bb)p(f |e).", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 1], ["Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 0], ["We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 0], ["Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 1], ["A non-ergodic Markov chain can bemade ergodic by reserving a small probability for jumping toany other state from the current state (Page et al., 1998).", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 0], ["(Robertson et al.,1992; Salton et al., 1993)), it remains important andyet often overlooked.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 0], ["To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 0], ["In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 1], ["To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 0], ["For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 0], ["In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 1], ["In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 0], ["Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004).", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 0], ["Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 1], ["Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 0], ["We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 0], ["3.1 The LexRank method.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 1], ["Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 0], ["Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 0], ["In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 1], ["To apply LexRank, a similarity graph is producedfor the sentences in an input document set.", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 0], ["Thevalue of d, which we will also refer to as the question bias, is a trade-off between two terms in the Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 0], ["The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences.", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 1], ["As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 0], ["Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 0], ["We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["Alternatively, the sentences canbe returned to the user as a question-focused summary.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["To apply LexRank, a similarity graph is producedfor the sentences in an input document set.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["In thegraph, each node represents a sentence.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1], ["Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 0], ["Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 0], ["3.4 Experiments with topic-sensitive LexRank.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 1], ["The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 0], ["To apply LexRank, a similarity graph is producedfor the sentences in an input document set.", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 0], ["As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 1], ["We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 0], ["As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 0], ["Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 0], ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 1], ["Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 0], ["The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 0], ["Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous \u2013 hence resulting in a loss of recall.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 0], ["From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 1], ["Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 0], ["Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 0], ["In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.)", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 0], ["Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 1], ["Sample seeds used for each semantic relation and sample outputs from Espresso.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 0], ["The number in the parentheses for each relation denotes the total number of seeds.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 0], ["Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 0], ["We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 1], ["Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 0], ["Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 0], ["Systems We compare the results of Espresso with the following two state of the art extraction systems: \u0083 RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.)", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 0], ["Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 1], ["Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 0], ["We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 0], ["We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 0], ["We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point).", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 1], ["Datasets We perform our experiments using the following two datasets: \u0083 TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection.", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 0], ["In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.)", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 0], ["In future work, this parameter can be learned using a development corpus.", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 0], ["Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 1], ["Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 1], ["For example, expanding the relation \u201cnew record of a criminal conviction\u201d part-of \u201cFBI report\u201d, the following new instances are obtained: \u201cnew record\u201d part-of \u201cFBI report\u201d, and \u201crecord\u201d part-of \u201cFBI report\u201d.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 0], ["We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 0], ["Experimental results, for all relations and the two different corpus sizes, show that Espresso greatly outperforms the other two methods on precision.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 0], ["The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 1], ["To date, most research on lexical relation harvesting has focused on is-a and part-of relations.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 0], ["The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 0], ["Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 0], ["Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 1], ["Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 0], ["The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 0], ["To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 0], ["Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 1], ["From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 0], ["The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 0], ["Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 0], ["For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["We evaluate this relation on the TREC9 corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["\u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["We evaluate this relation on the CHEM corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["\u0083 production: This relation occurs when a process or element/object produces a result.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["For example, ammonia produces nitric oxide.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["We evaluate this relation on the CHEM corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1], ["Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 0], ["We propose a limit of 70 words for Arabic parsing evaluations.", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 0], ["To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 0], ["6 Joint Segmentation and Parsing.", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 1], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 0], ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 0], ["6 Joint Segmentation and Parsing.", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 1], ["Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 0], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 0], ["4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun \ufffd '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 0], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 1], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 0], ["It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999).", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 0], ["Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 0], ["6 Joint Segmentation and Parsing.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 1], ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 0], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 0], ["It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999).", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 0], ["Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 1], ["The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 0], ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 0], ["(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 0], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 1], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 0], ["Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 0], ["(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 0], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 1], ["Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation).", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 1], ["Table 9: Dev set results for sentences of length \u2264 70.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 1], ["3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007).", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 0], ["1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009).", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 0], ["10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 0], ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 1], ["We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 0], ["Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 0], ["Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1], ["When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct \ufffd ?f iDafa.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1], ["mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1], ["To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75).", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 0], ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 0], ["English parsing evaluations usually report results on sentences up to length 40.", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 0], ["We propose a limit of 70 words for Arabic parsing evaluations.", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 1], ["The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 0], ["Figure 2: An ATB sample from the human evaluation.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 0], ["In our grammar, features are realized as annotations to basic category labels.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 1], ["We start with noun features since written Arabic contains a very high proportion of NPs.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 1], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74).", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 0], ["We use the default inference parameters.", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 0], ["8 We use head-finding rules specified by a native speaker.", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 1], ["To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 0], ["Modifying the Berkeley parser for Arabic is straightforward.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 0], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006).", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 0], ["By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 1], ["Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 0], ["We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 0], ["Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 0], ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 1], ["We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers.", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 0], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006).", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 0], ["Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings.", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 0], ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 1], ["We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 0], ["Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 0], ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 1], ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 0], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 0], ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 1], ["We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 0], ["On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 0], ["We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 0], ["We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 1], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 0], ["To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 0], ["We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 0], ["Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73).", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 1], ["In the ATB, :: b asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 0], ["Since guess and gold trees may now have different yields, the question of evaluation is complex.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 0], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 0], ["We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 1], ["To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 0], ["We propose a limit of 70 words for Arabic parsing evaluations.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 0], ["For parsing, the most challenging form of ambiguity occurs at the discourse level.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 0], ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 1], ["We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 1], ["The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 0], ["This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 0], ["Modifying the Berkeley parser for Arabic is straightforward.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 0], ["Better Arabic Parsing: Baselines, Evaluations, and Analysis", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 1], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0], ["Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric.", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0], ["We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1], ["For parsing, this is a mistake, especially in the case of interrogatives.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 0], ["To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 0], ["Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 0], ["Better Arabic Parsing: Baselines, Evaluations, and Analysis", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 1], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0], ["Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0], ["It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999).", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1], ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1], ["We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1], ["We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 0], ["The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 0], ["Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 0], ["Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 1], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 0], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 0], ["Gabbard and Kulick (2008) show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 0], ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 1], ["Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 0], ["Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 0], ["Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 0], ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 1], ["Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 0], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 0], ["We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 0], ["The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 1], ["able at http://nlp.stanford.edu/projects/arabic.shtml.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 1], ["7 Unlike Dickinson (2005), we strip traces and only con-.", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 0], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006).", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 0], ["We map the ATB morphological analyses to the shortened \u201cBies\u201d tags for all experiments.", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 0], ["At the phrasal level, we remove all function tags and traces.", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 1], ["In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al., 2008).", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 0], ["This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 0], ["1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009).", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 0], ["The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 1], ["The features we used can be divided into 2 classes: local and global.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 0], ["Such a classification can be seen as a not-always-correct summary of global features.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 0], ["Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 0], ["Global features are extracted from other occurrences of the same token in the whole document.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 1], ["Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 0], ["The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 0], ["In the maximum entropy framework, there is no such constraint.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 0], ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 1], ["Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 0], ["Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 0], ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 0], ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 1], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 0], ["In the maximum entropy framework, there is no such constraint.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 0], ["The probability distribution that satisfies the above property is the one with the highest entropy.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 0], ["We have shown that the maximum entropy framework is able to use global information directly.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 1], ["Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 0], ["For each token , zero, one, or more of the features in each feature group are set to 1.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 0], ["Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 0], ["Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 1], ["Local features are features that are based on neighboring tokens, as well as the token itself.", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 0], ["In fact, training on the official training data is not suitable as the articles in this data set are entirely about aviation disasters, and the test data is about air vehicle launching.", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 0], ["Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 0], ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 1], ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 0], ["The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 0], ["We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 0], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 1], ["Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 1], ["This group contains a large number of features (one for each token string present in the training data).", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 0], ["With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 0], ["For each token , zero, one, or more of the features in each feature group are set to 1.", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 0], ["Global features are extracted from other occurrences of the same token in the whole document.", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 1], ["However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 0], ["Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 0], ["Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 0], ["Global features are extracted from other occurrences of the same token in the whole document.", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 1], ["Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 0], ["The features we used can be divided into 2 classes: local and global.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 0], ["Global features are extracted from other occurrences of the same token in the whole document.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 0], ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 1], ["The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 0], ["Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 0], ["This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 0], ["For all lists except locations, the lists are processed into a list of tokens (unigrams).", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 1], ["Location list is processed into a list of unigrams and bigrams (e.g., New York).", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 1], ["For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 1], ["Multiple features can be used for the same token.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 0], ["We group the features used into feature groups.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 0], ["However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 0], ["The features we used can be divided into 2 classes: local and global.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 1], ["Global features are extracted from other occurrences of the same token in the whole document.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 1], ["Local features are features that are based on neighboring tokens, as well as the token itself.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 0], ["This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 0], ["For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 0], ["Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 1], ["The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 0], ["The system described in this paper is similar to the MENE system of (Borthwick, 1999).", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 0], ["Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 0], ["The features we used can be divided into 2 classes: local and global.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 1], ["Local features are features that are based on neighboring tokens, as well as the token itself.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 1], ["Global features are extracted from other occurrences of the same token in the whole document.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 1], ["The effect of a second reference resolution classifier is not entirely the same as that of global features.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 0], ["As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 0], ["As such, global information from the whole context of a document is important to more accurately recognize named entities.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 0], ["The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 1], ["For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 1], ["This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 0], ["Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 0], ["In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999).", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 0], ["We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 1], ["Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 0], ["With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 0], ["Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 0], ["Global features are extracted from other occurrences of the same token in the whole document.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 1], ["Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 0], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 0], ["For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 0], ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 1], ["We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0], ["The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0], ["The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0], ["If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 1], ["The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 0], ["a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced.", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 0], ["For example, we could choose .m. as the operator and decide by convention which of the two operands plays which role in expressions such as [A .m. B].", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 0], ["Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation.", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 1], ["We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0], ["The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0], ["The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0], ["The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 1], ["If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 0], ["In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 0], ["We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 0], ["In the regular expression calculus there are several operators that involve concatenation.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 1], ["We use the networks in Figure 13 to illustrate the effect of the merge algorithm.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 0], ["C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 0], ["The network created by the operation is shown in Figure 9.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 0], ["3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 1], ["We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 0], ["The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 0], ["Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 0], ["The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 1], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 0], ["Finite-state morphology in the tradition of the Two-Level (Koskenniemi, 1983) and Xerox implementations (Karttunen, 1991; Karttunen, 1994; Beesley and Karttunen, 2000) has been very successful in implementing large-scale, robust and efficient morphological analyzergenerators for concatenative languages, includ ing the commercially important European languages and non-Indo-European examples like 1 I wish to thank Stuart Newton for this example.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 0], ["The implementation is described in Kiraz and GrimleyEvans (1999).", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 0], ["This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 1], ["The source descriptions may also be written in higher-level notations (e.g. lexc (Karttunen, 1993), twolc (Karttunen and Beesley, 1992) and Replace Rules (Karttunen, 1995; Karttunen, 1996; Kempe and Karttunen, 1996)) that are simply helpful short- hands for regular expressions and that compile, using their dedicated compilers, into finite-state networks.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 0], ["The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 0], ["Alternation rules are subsequently composed on the lower side of the result to map the in- terdigitated, but still morphophonemic, strings into real surface strings.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 0], ["All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 1], ["The compile-replace algorithm is a vast improvement in both generality and efficiency, producing the same result in a few minutes.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 0], ["The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 0], ["We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 0], ["The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 1], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 0], ["However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997).", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 0], ["y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect.", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 0], ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 1], ["Simultaneously, the verb was removed from the membership lists of those existing classes.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 0], ["A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 0], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 0], ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 1], ["Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 0], ["The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 0], ["Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 0], ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 1], ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 0], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 0], ["It would be straight\u00ad forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and in\u00ad clude corresponding syntactic information.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 0], ["We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 1], ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 1], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 0], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 0], ["This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs.", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 0], ["We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents.", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 1], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 0], ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 0], ["We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 0], ["However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 1], ["We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 0], ["As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 0], ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 0], ["2.1 Ambiguities in Levin classes.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 1], ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 1], ["Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 0], ["Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 0], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 0], ["However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 1], ["A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 0], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class.", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 0], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 0], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 1], ["However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997).", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 0], ["They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch).", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 0], ["Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 0], ["The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 1], ["The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 1], ["We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 0], ["Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 0], ["Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 0], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 1], ["Levin verb classes are based on an underlying lat\u00ad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 0], ["Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 0], ["Depending on the par\u00ad ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo\u00ad nent Levin classes.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 0], ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 1], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 0], ["However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997).", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 0], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993).", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 0], ["First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 1], ["First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 0], ["As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 0], ["To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be gener\u00ad ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 0], ["Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 1], ["We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes.", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 0], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 0], ["Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 0], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 1], ["Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo\u00ad nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 0], ["As a verb of exerting force, push can appear in the conative alterna\u00ad tion, which emphasizes its force semantic com\u00ad ponent and ability to express an \"attempted\" action where any result that might be associ\u00ad ated- with the verb (e.g., motion) is not nec\u00ad essarily achieved; as a carry verb (used with a goal or directional phrase), push cannot take the conative alternation, which would conflict with the core meaning of the carry verb class (i.e., causation of motion).", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 0], ["Verbs in the slide/roll/run intersec\u00ad tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actu\u00ad ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley).", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 0], ["We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents.", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 1], ["Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 0], ["Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 0], ["We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 0], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 1], ["We base these regular extensions on a fine-grained variation on Levin classes, inter\u00ad sective Levin classes, as a source of semantic components associated with specific adjuncts.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 0], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 0], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993).", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 0], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 1], ["However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997).", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 0], ["We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the litera\u00ad ture (Palmer and Rosenzweig, 1996), (Copes\u00ad take and Sanfilippo, 1993), (Dorr, 1997).", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 0], ["In pursuing this goal, we are currently implement\u00ad ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 0], ["Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes.", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 1], ["The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 0], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 0], ["Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 0], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 1], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 0], ["Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 0], ["However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 0], ["Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 1], ["For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003).", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 0], ["This document contained information on a \u201csmall bomb located outside the of the Russian embassy\u201d and has is not relevant to topic 141, being properly relegated to a lower position.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 0], ["Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 0], ["This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 1], ["As a results, our documents were formed only by lemmas and the next step is the indexing of documents using an IR system.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 0], ["For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus).", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 0], ["It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997).", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 0], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 1], ["Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 0], ["Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 0], ["For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus).", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 0], ["For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101\u2013109, Portland, Oregon, USA, 23 June 2011.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 1], ["Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 0], ["For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003).", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 0], ["One of the great challenges of NLP is the identification of such expressions, \u201chidden\u201d in texts of various genres.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 0], ["Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 1], ["More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 0], ["For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003).", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 0], ["Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 0], ["The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 1], ["For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die).", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 0], ["For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003).", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 0], ["In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using \u201c \u201d (e.g. letter bomb as letter bomb).", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 0], ["We used Zettair to generate the ranked list of documents retrieved in response to each query.", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 1], ["We used the cosine metric to calculate the scores and rank the documents.", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 1], ["The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 0], ["The response function of neural networks is continuous (smooth) at the decision boundaries, allowing them to avoid hard decisions and the complete fragmentation of data associated with decision tree questions.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 0], ["Mast et al. (1996) propose the use of semantic classification trees, a kind of decision tree conditioned on word patterns as features.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 0], ["The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1], ["We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1], ["Table 9 Combined utterance classification accuracies (chance = 35%).", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1], ["Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1], ["Of the DA modeling algorithms described below, Taylor et al. (1998) and Wright (1998) were based on Map Task.", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 0], ["1 While some 1 For the study focusing on prosodic modeling of DAs reported elsewhere (Shriberg et al. 1998), the tag set was further reduced to six categories..", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 0], ["An extensive comparison of the prosodic DA modeling literature with our work can be found in Shriberg et al. (1998).", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 0], ["For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 1], ["Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech.", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 1], ["Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 0], ["Automatic segmentation of spontaneous speech is an open research problem in its own right (Mast et al. 1996; Stolcke and Shriberg 1996).", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 0], ["Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs.", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 0], ["The computation of likelihoods P(EIU ) depends on the types of evidence used.", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 1], ["Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U).", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 1], ["Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 0], ["We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 0], ["Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 0], ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 1], ["For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 1], ["The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance).", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 0], ["Ui,..., Un), where n is the number of utterances in a conversation.", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 0], ["A detailed study aimed at automatic prosodic classification of DAs in the Switchboard domain is available in a companion paper (Shriberg et al. 1998).", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 0], ["A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking.", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 1], ["These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970).", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 1], ["The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture (Core and Allen 1997).", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 0], ["DA frequencies are given as percentages of the total number of utterances in the overall corpus.", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 0], ["As discussed earlier, these domains differ from the Switchboard corpus in being task-oriented.", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 0], ["Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department.", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 1], ["For example, the task of Warnke et al. (1997) was to simultaneously segment and tag DAs, whereas the other results rely on a prior manual segmentation.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 0], ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 0], ["Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 0], ["The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 1], ["While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs).", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 1], ["Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 1], ["In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 0], ["The choice of tasks was motivated by an analysis of confusions committed by a purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, and BACKCHANNELS for AGREEMENTS (and vice versa).", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 0], ["The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 0], ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 1], ["However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 1], ["The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 1], ["This approach is quite expensive, however, as it requires multiple full recognizer or rescoring passes of the input, one for each DA type.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 0], ["In this experiment, only the single best recognizer hypothesis is used, effectively treating it as the true word string.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 0], ["The gender feature that classified speakers as either male or female was used to test for potential inadequacies in F0 normalizations.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 0], ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 1], ["The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 1], ["The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui).", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 1], ["In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 0], ["Kita et al. (1996) made one of the few attempts at unsupervised discovery of dialogue structure, where a finite-state grammar induction algorithm is used to find the topology of the dialogue grammar.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 0], ["Reithinger et al. (1996), for example, used deleted interpolation to smooth the dialogue n-grams.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 0], ["Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 1], ["In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 1], ["a What if p is estimated from data?", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 0], ["Thus, ~ is again less than  and the distribution is again tight.", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 0], ["For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol.", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 0], ["We will show that in both cases the estimated probability is tight.", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 1], ["What if the production probabilities are estimated from data?", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 0], ["a What if p is estimated from data?", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 0], ["Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 0], ["(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator.", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 1], ["Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse.", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 0], ["For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG.", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 0], ["The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one.", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 0], ["We will show that in both cases the estimated probability is tight.", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 1], ["Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions.", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 0], ["We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1.", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 0], ["Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 0], ["If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 1], ["We will show that in both cases the estimated probability is tight.", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 1], ["Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 0], ["As a preliminary to semantics, we need syntax.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 0], ["So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 0], ["The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 1], ["The numerical and date expressions got correct representations.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 1], ["The output of Boxer for this text is shown in Figure 3.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 0], ["Only the box format is shown here \u2014 Boxer is also able to output the DRSs in Prolog or XML encodings.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 0], ["To account for this difference we also need to look at the part of speech that is assigned to a token.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 0], ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 1], ["Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 0], ["In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 0], ["Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 0], ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 1], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 0], ["Boxer is a wide-coverage system for semantic interpretation.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 0], ["The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 0], ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 1], ["Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 1], ["Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 0], ["The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis.", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 0], ["In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 0], ["Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts.", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 1], ["It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 1], ["The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics (Kamp and Reyle, 1993), where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 0], ["So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 0], ["Finally, it also allows us to characterize the meaning of thematic roles independently of the meaning of the verb that describes the event.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 0], ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 1], ["We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 1], ["It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 0], ["The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 0], ["We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 0], ["Boxer is an open-domain tool for computing and reasoning with semantic representations.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 1], ["So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category.", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 0], ["The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989).", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 0], ["There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005).", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 0], ["It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 1], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 0], ["The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 0], ["Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 1], ["For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 1], ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 1], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 0], ["The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 0], ["Our tagger was first evaluated on data from the German Tiger treebank.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 0], ["The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 1], ["The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 0], ["Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 0], ["Note that only the words, but not the POS tags from the test and development data were used, here.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 0], ["3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 1], ["The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 0], ["(2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (Morc\u02c7e), and evaluated them on the Prague Dependency Treebank (PDT 2.0).", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 0], ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 1], ["Results for 2 and for 10 preceding POS tags as context are reported for our tagger.", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 0], ["Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 0], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 0], ["Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 1], ["The number of additional attributes is fixed for each main category.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 0], ["Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 0], ["The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 0], ["Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 1], ["Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 0], ["A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 0], ["Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 1], ["A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 1], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 0], ["The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 0], ["The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 1], ["The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 0], ["The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 0], ["Since the tagger creates a separate tree for each attribute, the probabilities of a set of competing attributes such as masculine, feminine, and neuter will not exactly sum up to 1.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 1], ["We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 0], ["Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 0], ["The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 0], ["Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 1], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 1], ["These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 0], ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 0], ["A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 1], ["Note that only the words, but not the POS tags from the test and development data were used, here.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 0], ["The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or).", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 0], ["Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 0], ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 1], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 0], ["Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 0], ["Thus, pre-pruning with a threshold of 6 was used in the experiments.", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 1], ["In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 0], ["We also evaluated our tagger on the Czech Academic corpus (Hladka\u00b4 et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 0], ["8 In German, the genitive case of arguments is more and.", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 0], ["The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 1], ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 1], ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 0], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 0], ["Note that only the words, but not the POS tags from the test and development data were used, here.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 0], ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 1], ["Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 1], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0], ["In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0], ["All context attributes other than the base POS are always used in combination with the base POS.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 1], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 0], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 0], ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 1], ["The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 0], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 0], ["Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 0], ["We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 1], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 1], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0], ["In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0], ["All context attributes other than the base POS are always used in combination with the base POS.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 1], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 0], ["Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 0], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 1], ["Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 0], ["Results for 2 and for 10 preceding POS tags as context are reported for our tagger.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 0], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 0], ["The probability of an attribute (such as \u201cNom\u201d) is always conditioned on the respective base POS (such as \u201cN\u201d) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 1], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 0], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 0], ["Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 1], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 0], ["Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 0], ["Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 1], ["(Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 0], ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 0], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 1], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 0], ["The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 0], ["Relative to the TnT tagger, however, the accuracy is quite similar for test and development data.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 1], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 0], ["Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 0], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 0], ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 1], ["The best results are obtained with a context size of 10.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 0], ["Our tagger was used with a context size of 10.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 0], ["Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 0], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 1], ["3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 1], ["Czech POS tagging has been extensively studied in the past (Hajic\u02c7 and Vidova\u00b4-Hladka\u00b4, 1998; Hajic\u02c7 et al., 2001; Votrubec, 2006).", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 0], ["Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 0], ["By far the most frequent tagging error was the confusion of nominative and accusative case.", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 0], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 1], ["But it is not difficult to extend our method to handle this problem.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 0], ["Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 0], ["Since the context of the correct translation e is similar to e , is considered as a document in IR.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 0], ["In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 1], ["We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = \u03b1 \u22c5 Pml (tc | Tc (C (e))) + (1 \u2212\u03b1 ) \u22c5 Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 1], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 0], ["In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 0], ["must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e | c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 0], ["To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 1], ["Comparable corpora such as news documents of the same period from different news agencies are readily available.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 0], ["Since the context of the correct translation e is similar to e , is considered as a document in IR.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 0], ["The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 0], ["Comparable corpora refer to texts that are not direct translation but are about the same topic.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 1], ["For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 1], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 0], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 0], ["Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 0], ["We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 1], ["In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 0], ["our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 0], ["The work that is most similar to ours is the recent research of (Huang et al., 2004).", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 0], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 1], ["Comparable corpora such as news documents of the same period from different news agencies are readily available.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 0], ["The context of each candidate translation e' is viewed as a document.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 0], ["The context (i.e., the surrounding words) of c is viewed as a query.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 0], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 1], ["In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 0], ["Comparable corpora such as news documents of the same period from different news agencies are readily available.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 0], ["The size of the English corpus from Jul to Dec The context C(c)of a Chinese word c was col 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 0], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 1], ["Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 0], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 0], ["The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 0], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 1], ["Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 0], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 0], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 0], ["We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 1], ["The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 0], ["So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 0], ["lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 0], ["So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 1], ["The work that is most similar to ours is the recent research of (Huang et al., 2004).", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 0], ["So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 0], ["They attempted to improve named entity translation by combining phonetic and semantic information.", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 0], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 1], ["\u220f t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 0], ["The document with the highest given in Section 4.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 0], ["#e is the total number of English translation candidates in the period.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 0], ["Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 1], ["Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 0], ["In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 0], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 0], ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 1], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 0], ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 0], ["Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 0], ["In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 1], ["We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 0], ["\u220f t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 0], ["So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 0], ["To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 1], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 0], ["It shows that the", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 0], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 0], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 0], ["For example, we want to determine whether a person is at a location, based on the evidence in the context.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 0], ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 0], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 1], ["The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 1], ["The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 0], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 0], ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 1], ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 0], ["For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 1], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0], ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1], ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0], ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0], ["Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1], ["Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1], ["3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 0], ["# of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 0], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 1], ["The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 1], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 0], ["\u2022 ET1DW1: combination of the entity type and the dependent word for M1 \u2022 H1DW1: combination of the head word and the dependent word for M1 \u2022 ET2DW2: combination of the entity type and the dependent word for M2 \u2022 H2DW2: combination of the head word and the dependent word for M2 \u2022 ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP \u2022 ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP \u2022 ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 0], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 0], ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 1], ["It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 0], ["It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 0], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 1], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 0], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 0], ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 1], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 1], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998).", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 0], ["It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 0], ["Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 0], ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 1], ["It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 1], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 0], ["According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 1], ["Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 0], ["For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 0], ["In this paper, we separate the features of base phrase chunking from those of full parsing.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 1], ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 1], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 0], ["We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 0], ["Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 1], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 1], ["We use the official ACE corpus from LDC.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 0], ["set as the last word of the mention.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 1], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 0], ["We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 0], ["Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 1], ["Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 0], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 0], ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 0], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 1], ["Exploring Various Knowledge in Relation Extraction", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 0], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 1], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 0], ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 1], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 1], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 0], ["\u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 0], ["This category of features concerns about the information inherent only in the full parse tree.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 0], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 1], ["We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 0], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 0], ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 1], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 1], ["This may be due to the fact that most of relations in the ACE corpus are quite local.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 0], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 0], ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 1], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 0], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 0], ["In this paper, we separate the features of base phrase chunking from those of full parsing.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 0], ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 1], ["This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 0], ["This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 0], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 1], ["Table 2 also measures the contributions of different features by gradually increasing the feature set.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 0], ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 1], ["extent annotation.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 1], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 0], ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 0], ["\u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 0], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 1], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 0], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 0], ["Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 0], ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 0], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 0], ["Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 0], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 0], ["\u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 0], ["\u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 0], ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 1], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 0], ["Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 0], ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 1], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 0], ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 1], ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 1], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 0], ["It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 0], ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 1], ["Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 0], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 0], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 0], ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 1], ["The semantic relation is determined between two mentions.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 1], ["In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 1], ["Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 0], ["We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 0], ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 1], ["In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 0], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 1], ["\u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 0], ["Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 0], ["Two features are defined to include this information: \u2022 ET1Country: the entity type of M1 when M2 is a country name \u2022 CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 0], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 1], ["This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 1], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 0], ["We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 0], ["Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 0], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 1], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 0], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 0], ["Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 1], ["This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 0], ["In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 0], ["Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 0], ["It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 1], ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 1], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 0], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 0], ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 1], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 0], ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 0], ["Table 5 separates the performance of relation detection from overall performance on the testing set.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 1], ["The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 1], ["It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 0], ["Two features are defined to include this information: \u2022 ET1Country: the entity type of M1 when M2 is a country name \u2022 CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 0], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 0], ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 0], ["This may be due to the fact that most of relations in the ACE corpus are quite local.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 0], ["In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 1], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 0], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 0], ["Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 0], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 1], ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 1], ["Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 0], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 0], ["However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 0], ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 1], ["This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 0], ["This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 0], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 0], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 1], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 1], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 0], ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 0], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 0], ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 1], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 0], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 0], ["It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 0], ["This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 1], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 0], ["While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 0], ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 0], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 1], ["In this paper, we separate the features of base phrase chunking from those of full parsing.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 0], ["For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 0], ["The rest of this paper is organized as follows.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 0], ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 1], ["In this paper, we separate the features of base phrase chunking from those of full parsing.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 0], ["We use the official ACE corpus from LDC.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 0], ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 1], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 0], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 0], ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 0], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 1], ["2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 0], ["Culotta A. and Sorensen J.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 0], ["For example, we want to determine whether a person is at a location, based on the evidence in the context.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 0], ["Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 1], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 0], ["Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 0], ["This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 0], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 1], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 1], ["Copying sharable parts is called redundant copying.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 1], ["In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 0], ["However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 0], ["This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 0], ["Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 1], ["This reduces repeated calculation of substructures.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 1], ["in this method, theretbre, the failure tendency information is acquired by a learning process.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 0], ["In Section 5, a method which uses this generalized strategy is proposed.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 0], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 0], ["The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 1], ["This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 0], ["Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 0], ["For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 0], ["in this method, theretbre, the failure tendency information is acquired by a learning process.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 1], ["That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 1], ["in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 1], ["In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 0], ["The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 0], ["For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 0], ["This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 1], ["This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 1], ["TypeSymbol2 4\u00a2\" '~\u00b0~'~/.~ypeSymbol3 featury \"X~ature5 TypeSymbol4 4r \"~TypeSymbol5 (b) directed graph notation Figure 2: TFS notations Phrase [sub(at ?X2 SignList ] dtrs CHconst Sign U Syn i'oo I syn I head ?Xl . ] ubcat NonEmptySignLIst | ['first ]1 ?\u00d73 Lrest ?X2 J j Phrase -dtrs CHconst hdtr LexicalSignsyn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign ,11 yn Synead Head L~,os N] Irest EmptySignkist Phrase \"syn Syn head ?X1 Head Fpos P Lform Ga ] Lsubcat ?X2 Empl.ySignList dtrs CHconst ccltr ?X3 Sign syn iyn head Head _ [pos N hdtr LexicalSign l-syn Syn l I F head :x~ 7/ Lsubcat [ NonEinptySignList l l P\"\" ~\u00d7~ llll Lrest ?X2 JJjJ Figure 3: Example of TFS unification Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 1], ["The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 0], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 0], ["This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 0], ["Copying sharable parts is called redundant copying.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 1], ["Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 1], ["Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 0], ["In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 0], ["The unification procedure is applied recursively to feature a values of the input nodes.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 0], ["Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 1], ["The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 0], ["7): (1) if nodel ', the dereference result of node/, is current, then CopyNode returns node l\" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1\" and if it returns ,~;everal arc copies, CopyNode creates a new copy node.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 0], ["In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 0], ["For example, a spoken Present.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 1], ["Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 1], ["5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 0], ["Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 0], ["3'he skeleton part is shared by one of the input FSs and the result FS.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 0], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1], ["Copying sharable parts is called redundant copying.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1], ["A better method would nfinimize the copying of sharable varts.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1], ["This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 0], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 0], ["The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 0], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 1], ["Copying sharable parts is called redundant copying.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 1], ["A better method would nfinimize the copying of sharable varts.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 1], ["Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 0], ["in this method, theretbre, the failure tendency information is acquired by a learning process.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 0], ["5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 0], ["Copying sharable parts is called redundant copying.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 1], ["A better method would nfinimize the copying of sharable varts.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 1], ["The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 0], ["It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1\" and returns Nil_.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 0], ["The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: \"<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski's method Input graph GI Input graph 62 \u00a2 .......'77 ........ i : Sobg,'aphs not required to be copied L ...........................................", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 0], ["Copying sharable parts is called redundant copying.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 1], ["5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 1], ["With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 1], ["]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 0], ["Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 0], ["This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 0], ["A better method would nfinimize the copying of sharable varts.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 1], ["3.1 Effects of the Character-based and the.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 0], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 0], ["We found a speed up both in training and test.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 1], ["Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 1], ["of the corpora and these scores, refer to (Emerson, 2005).", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 0], ["This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv.", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 0], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 0], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 1], ["In this section we introduce a confidence measure approach to combine the two results.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 0], ["The effect of the confidence measure is shown in Table 3, where we used \u03b1 = 0.7 and confidence threshold t = 0.8.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 0], ["The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 0], ["We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 1], ["The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 1], ["After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 0], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 0], ["It proves the proposed word-based IOB tagging was very effective.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 0], ["Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 1], ["In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 0], ["If the subset consists of Chinese characters only, it is a character-based IOB tagger.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 0], ["We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 0], ["A confidence measure threshold, t, was defined for making a decision based on the value.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 1], ["If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 1], ["In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 0], ["In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 0], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 0], ["A confidence measure threshold, t, was defined for making a decision based on the value.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1], ["If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1], ["Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1], ["However, the R-iv rates were getting worse in return for higher R-oov rates.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1], ["For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 0], ["By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 0], ["95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 1], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 0], ["We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 0], ["By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 1], ["This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 0], ["For a character-based IOB tagger, there is only one possibility of re-segmentation.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 0], ["Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 1], ["For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 0], ["95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 0], ["In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 1], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 0], ["The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 0], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 1], ["The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 0], ["Qc 2006 Association for Computational Linguistics input \u5498\u38c5\u1bf9\u0523\u0cfc\u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation \u5498 \u38c5 \u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output \u5498\u38c5\u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 1], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 0], ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 0], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 1], ["We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 1], ["We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 1], ["Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 0], ["We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 0], ["In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 0], ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 1], ["We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 1], ["For the bigram features, we only used the previous and the current observations, t\u22121 t0 . As to feature selection, we simply used absolute counts for each feature in the training data.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 0], ["In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 0], ["For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 0], ["For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 1], ["Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 1], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 0], ["In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 0], ["We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 1], ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 0], ["subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 0], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 1], ["We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 1], ["If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 0], ["subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 1], ["In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 0], ["The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 0], ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 0], ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 1], ["If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 1], ["In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 0], ["For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 0], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 0], ["For a character-based IOB tagger, there is only one possibility of re-segmentation.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 1], ["However, there are multiple choices for a subword-based IOB tagger.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 1], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 0], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 0], ["(2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 1], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 0], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 0], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 1], ["When training over 10,000 features on a modest amount of data, we, like Watanabe et al.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 0], ["To remedy this problem, Chiang et al.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 1], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 1], ["Following Chiang et al.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 0], ["We follow Galley et al.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 0], ["(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 0], ["Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 1], ["This grammar can be parsed efficiently using cube pruning (Chiang, 2007).", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 1], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 0], ["These features are somewhat similar to features used by Watanabe et al.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 0], ["A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 0], ["Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 1], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 0], ["This seems in line with the finding of Watanabe et al.", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 0], ["The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 1], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 0], ["Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 0], ["The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 1], ["We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 0], ["For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 0], ["For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 0], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 1], ["Adding the source-side and discount features to Hiero yields a +1.5 B\uf76c\uf765\uf775 improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B\uf76c\uf765\uf775 improvement.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 0], ["For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 0], ["For a rule like NPB(NNP(us) NNP(president) x0:NNP) \u2194 meiguo zongtong x0 the feature node-count-NPB gets value 1, node- count-NNP gets value 2, and all others get 0.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 0], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 1], ["(2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 0], [", ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 0], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 1], ["We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 0], ["When training over 10,000 features on a modest amount of data, we, like Watanabe et al.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 0], ["For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 1], ["For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006).", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 0], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 0], ["Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 1], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 0], ["To remedy this problem, Chiang et al.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 1], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 0], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 1], ["(2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 0], [", ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 0], ["\u2022 For each i, select from the forest a set of hypothesis translations ei1, . . .", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 0], ["Following Chiang et al.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 1], ["We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 0], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 0], ["We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 1], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 0], ["It turns out that in translation hypotheses that move \u201cX said\u201d or \u201cX asked\u201d away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 0], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 1], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 0], ["Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 0], ["When training over 10,000 features on a modest amount of data, we, like Watanabe et al.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 0], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 1], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 0], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 0], ["To remedy this problem, Chiang et al.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 1], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 0], ["To remedy this problem, Chiang et al.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 0], ["A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 0], ["Following Chiang et al.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 1], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 0], ["We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments.", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 0], ["From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) \u2194 x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English).", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 1], ["(2007); here, we are incorporating some of its features directly into the translation model.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 0], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 0], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 0], ["Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 1], ["For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 0], ["features String-to-tree MT offers some unique levers to pull, in terms of target-side features.", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 0], ["We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 1], ["(2008) introduce a structural distortion model, which we include in our experiment.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 0], ["We include two other techniques in our baseline.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 0], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 0], ["Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 1], ["Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 0], ["The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 0], ["(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 1], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 0], ["By contrast, we incorporate features directly into hierarchical and syntax- based decoders.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 0], ["The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 0], ["Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 1], ["This grammar can be parsed efficiently using cube pruning (Chiang, 2007).", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 1], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 0], ["For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 0], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 1], ["4 37.6\u2217\u2217 Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8\u2217 38 .7 39.9\u2217 38.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 0], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997).", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 0], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 0], ["Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 1], ["Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008).", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 0], ["We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 0], ["We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 0], ["Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 1], ["This grammar can be parsed efficiently using cube pruning (Chiang, 2007).", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 1], ["rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 0], ["Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such.", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 0], ["The Potsdam Commentary Corpus", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 0], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 1], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 0], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 0], ["Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 0], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 1], ["The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) \u2022 the full text, \u2022 the annotation values for the activated annotation set (co-reference), \u2022 the actual annotation tiers, and \u2022 the portion of text currently \u2018in focus\u2019 (which also appears underlined in the full text).", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 0], ["It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 0], ["We respond to this on the one hand with a format for its underspecification (see 2.4) and on the other hand with an additional level of annotation that attends only to connectives and their scopes (see 2.5), which is intended as an intermediate step on the long road towards a systematic and objective treatment of rhetorical structure.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 0], ["The corpus has been annotated with six different types of information, which are characterized in the following subsections.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 1], ["Not all the layers have been produced for all the texts yet.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 1], ["Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 1], ["\u2022 Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 0], ["There are still some open issues to be resolved with the format, but it represents a first step.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 0], ["Either save money at any cost - or give priority to education.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 0], ["For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 1], ["rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 0], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 0], ["Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 0], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 1], ["Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 0], ["Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 0], ["There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 0], ["We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 1], ["Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 0], ["One key issue here is to seek a discourse-based model of information structure.", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 0], ["We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 0], ["That is, we can use the discourse parser on PCC texts, emulating for instance a \u201cco-reference oracle\u201d that adds the information from our co-reference annotations.", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 1], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 0], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 0], ["All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 0], ["This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 1], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 1], ["The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 1], ["We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 0], ["rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 0], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 0], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 1], ["It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 0], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 0], ["For illustration, an English translation of one of the commentaries is given in Figure 1.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 0], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 1], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 1], ["The Potsdam Commentary Corpus", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 0], ["The PCC is not the result of a funded project.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 0], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 0], ["The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 1], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 1], ["In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 0], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 0], ["It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 0], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1], ["The corpus has been annotated with six different types of information, which are characterized in the following subsections.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1], ["Not all the layers have been produced for all the texts yet.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1], ["For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1], ["For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 0], ["Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 0], ["rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 0], ["All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 1], ["On the other hand, we are interested in the application of rhetorical analysis or \u2018discourse parsing\u2019 (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 0], ["As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers Su\u00a8ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence.", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 0], ["Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 0], ["Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 1], ["Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 0], ["For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 0], ["Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 0], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 1], ["This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 0], ["It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 0], ["The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus.", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 0], ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 1], ["The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 0], ["The algorithm is based on a graph model representing words and relationships between them.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 0], ["We used the simple graph model based on co-occurrences of nouns in lists (cf.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 0], ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 1], ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 1], ["Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 0], ["Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 0], ["The algorithm is based on a graph model representing words and relationships between them.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 0], ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 1], ["In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 0], ["We used the simple graph model based on co-occurrences of nouns in lists (cf.", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 0], ["Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\".", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 0], ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 1], ["This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 0], ["The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 0], ["This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 0], ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 1], ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 0], ["Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 0], ["The family of such algorithms is described in (Widdows, 2003).", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 0], ["The algorithm is based on a graph model representing words and relationships between them.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 1], ["Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 1], ["The family of such algorithms is described in (Widdows, 2003).", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 0], ["Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 0], ["The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 0], ["Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 1], ["The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 0], ["This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 0], ["However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 0], ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 1], ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 0], ["Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 0], ["This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 0], ["Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\".", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 1], ["Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 0], ["Take the \"best\" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5.", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 0], ["Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 0], ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 1], ["The family of such algorithms is described in (Widdows, 2003).", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 0], ["Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 0], ["This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 0], ["The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 1], ["This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 0], ["Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 0], ["We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 0], ["This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 1], ["This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 0], ["The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 0], ["If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 0], ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 1], ["The algorithm is based on a graph model representing words and relationships between them.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 1], ["Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 1], ["This approach is compared to another reordering scheme presented in (Berger et al., 1996).", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 0], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 0], ["The details are given in (Och and Ney, 2000).", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 0], ["For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 1], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 0], ["A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 0], ["Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 0], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 1], ["When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 1], ["The details are given in (Och and Ney, 2000).", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 0], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 0], ["We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 0], ["The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 1], ["For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 1], ["The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 0], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 0], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 0], ["When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 1], ["This approach leads to a search procedure with complexity O(E3 J4).", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 1], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 0], ["The resulting algorithm has a complexity of O(n!).", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 0], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 0], ["In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 1], ["The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 0], ["The details are given in (Och and Ney, 2000).", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 0], ["Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 0], ["We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 1], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 0], ["In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 0], ["In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 0], ["For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 1], ["In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 0], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 0], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 0], ["We apply a beam search concept as in speech recognition.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 1], ["The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 0], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 0], ["We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 0], ["For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 1], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 0], ["The details are given in (Och and Ney, 2000).", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 0], ["To be short, we omit the target words e; e0 in the formulation of the search hypotheses.", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 0], ["For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 1], ["The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 0], ["The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 0], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 0], ["A dynamic programming recursion similar to the one in Eq. 2 is evaluated.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 1], ["We apply a beam search concept as in speech recognition.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 1], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 0], ["For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 0], ["The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 0], ["For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 1], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 0], ["Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 0], ["We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 0], ["We apply a beam search concept as in speech recognition.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 1], ["The proof is given in (Tillmann, 2000).", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 0], ["The proof is given in (Tillmann, 2000).", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 0], ["The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 0], ["The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words.", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 1], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 0], ["The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 0], ["The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 0], ["This measure has the advantage of being completely automatic.", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 1], ["To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 0], ["We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 0], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 0], ["For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 1], ["A detailed description of the search procedure used is given in this patent.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 0], ["In this case, we have no finite-state restrictions for the search space.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 0], ["The proof is given in (Tillmann, 2000).", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 0], ["We apply a beam search concept as in speech recognition.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 1], ["The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 0], ["A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 0], ["The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max \u00c6;e00 np(jjj0; J) p(\u00c6) p\u00c6(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 0], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 1], ["This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 0], ["This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 0], ["This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 0], ["Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 1], ["Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 1], ["While the accuracy of this approach was as good as any previously published results, the learned models were complex and dicult to interpret, in e?ect acting as very accurate black boxes.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 1], ["We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 0], ["This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 0], ["The words and part of speech associated with each task are shown in Table 1 in column 1.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 0], ["The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 1], ["The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 1], ["We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 1], ["Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 0], ["This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 0], ["These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 0], ["This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 1], ["Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 0], ["Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 0], ["We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 0], ["This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 1], ["If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 0], ["The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 0], ["The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 0], ["However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 1], ["Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 1], ["This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 0], ["These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 0], ["The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 0], ["The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 1], ["Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 1], ["A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 1], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among).", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 0], ["It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 0], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 0], ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 1], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 1], ["decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 0], ["This section presents a method of doing this based on Bayesian classifiers.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 0], ["The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 0], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 1], ["We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 0], ["This problem is addressed in the next section.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 0], ["1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 0], ["Table 1 shows the performance of the baseline method for 18 confusion sets.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 1], ["= abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 0], ["The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 0], ["Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 0], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 1], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 1], ["This section presents a method of doing this based on Bayesian classifiers.", "Golding (1995) builds a classifier based on a rich set of context features.", 0], ["We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].", "Golding (1995) builds a classifier based on a rich set of context features.", 0], ["Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "Golding (1995) builds a classifier based on a rich set of context features.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "Golding (1995) builds a classifier based on a rich set of context features.", 1], ["\\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations.", "Golding (1995) builds a classifier based on a rich set of context features.", 1], ["It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 0], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 0], ["Ya.rowsky proposed decision lists as a way to get the best of both methods.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 0], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 1], ["We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 0], ["\\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 0], ["We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 0], ["Table 1 shows the performance of the baseline method for 18 confusion sets.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1], ["The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1], ["and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1], ["Table 1 shows the performance of the baseline method for 18 confusion sets.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1], ["It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 0], ["Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 0], ["The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 1], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 1], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0], ["For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0], ["There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["The ambiguity among words is modelled by confusion sets.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 0], ["The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 0], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 0], ["The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1], [", c_ 1, c1, ...", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1], [", cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1], ["Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 0], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 0], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 0], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1], ["A method for doing this, based on Bayesian classifiers, was presented.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1], ["It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1], ["On the other hand, words such as chocolate and delicious in the context imply desser\u00b7t. This observation is the basis for the method of context words.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 0], ["Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 0], ["decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 0], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 1], ["We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3).", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 0], ["We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 0], ["The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 1], ["The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 0], ["The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 0], ["decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 1], ["\\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 1], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 0], ["This collection of confusion sets will be used for evaluating the methods throughout the paper.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 0], ["The ambiguity among words is modelled by confusion sets.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 0], ["Table 1 shows the performance of the baseline method for 18 confusion sets.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 1], ["57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u'r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y' re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 0], ["538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 0], ["This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 1], ["The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 1], ["Table 3 shows the results of varying\u00a3 for the usual confusion sets.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 0], ["Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 0], ["The last line of the table gives the total number of occurrences of peace and piece in the training corpus.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 0], ["The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 1], ["and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 1], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 0], ["For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 0], ["On the other hand, words such as chocolate and delicious in the context imply desser\u00b7t. This observation is the basis for the method of context words.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 1], ["To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 0], ["Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 0], ["It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 0], ["Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 1], ["Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj).", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["An ambiguous target word is then classified by finding all collocations that match its context.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 0], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 0], ["Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 0], ["A method for doing this, based on Bayesian classifiers, was presented.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 1], ["It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 1], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 0], ["We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 0], ["We have pursued an alternative approach to the problem of estimating the likelihood terms.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 0], ["Trigrams are at their worst when the words in the confusion set have the same part of speech.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 1], ["In such cases, the Bayesian hybrid method is clearly better.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 1], ["This section presents a method of doing this based on Bayesian classifiers.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 0], ["The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 0], ["Ya.rowsky proposed decision lists as a way to get the best of both methods.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 0], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 1], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 0], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 0], ["This collection of confusion sets will be used for evaluating the methods throughout the paper.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 1], ["A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 0], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity.", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 0], ["This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 1], ["We allow two types of syntactic elements: words, and part-of-speech tags.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 0], ["There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps).", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 0], ["The idea is to make one big list of all features - in this case, context words and collocations.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 0], ["\\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 1], ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 0], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 0], ["This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 1], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 1], ["Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 0], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 0], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 0], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 1], ["Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 0], ["Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj).", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 0], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 0], ["A method is presented for doing this, based on Bayesian classifiers.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 1], ["Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 0], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 0], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 1], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 1], ["We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 0], ["We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 0], ["We smooth the data by adding 1 to the count.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 1], ["The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 1], ["Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 0], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 0], ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 1], ["Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 0], ["In the balance, the reliability metric seemed to give higher performance.", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 0], ["Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 0], ["Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 1], ["Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 0], ["This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 0], ["Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 0], ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 1], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 0], ["Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 0], ["The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 1], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 1], ["A Bayesian hybrid method for context-sensitive spelling correction", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 0], ["It can be seen that trigrams and the Bayesian hybrid method each have their better moments.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 0], ["A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 1], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 1], ["538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 0], ["We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 0], ["The first feature that 46 Training phase (1) (2) (3) (3.5) (4) Propose all possible features as candidat e features.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 0], ["The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 1], ["Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 0], ["This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 0], ["To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 0], ["We conduct the experiments on two different gold standard datasets.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1], ["One is the MicroWNOp corpus, ntu.edu.tw/\u02dccjlin/libsvm/.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1], ["It includes 298 words with 703 objective and 358 subjective WordNet senses.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1], ["Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1], ["Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 0], ["classification vertices in the Mincut approach.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 0], ["In Section 4.5, we shortly discuss results on.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 0], ["We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 1], ["Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 0], ["In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1).", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 0], ["We experiment with different combinations of features and the results are listed in Table 2, prefixed by \u201cSVM\u201d.", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 0], ["We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 1], ["However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 0], ["We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalcea\u2019s dataset respectively.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 0], ["To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 0], ["We supplement WordNet entries with information on the subjectivity of its word senses.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 1], ["We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 0], ["We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 0], ["Section 3 describes our proposed semi-supervised minimum cut framework in detail.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 0], ["Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 1], ["Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 1], ["However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie..", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 1], ["Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNet\u2019s relation structure.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 0], ["4.4 Semi-supervised Graph Mincuts.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 0], ["We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 0], ["We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 1], ["Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 0], ["Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 0], ["Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 0], ["Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 1], ["We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 0], ["However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability.", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 0], ["To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 0], ["Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 1], ["However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 0], ["Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 0], ["There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005).", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 0], ["We supplement WordNet entries with information on the subjectivity of its word senses.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 1], ["An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 0], ["Training a hidden Markov model having this topology corrected all nine instances of the error in the test data.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 0], ["An alternative to uniformly increasing the order of the conditioning is to extend it selectively.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 0], ["The work described here also makes use of a hidden Markov model.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 1], ["State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 0], ["To model the context necessary to correct the error, two extra states are used, as shown in Figure 1.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 0], ["ADJECTIVE DETERMINER To all states NOUN in Basic Network \"Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 0], ["A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words).", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 1], ["A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 1], ["An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 0], ["Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 0], ["It can be seen that only simple forms of prepositional phrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/or determiner.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 0], ["An alternative to uniformly increasing the order of the conditioning is to extend it selectively.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 1], ["Mixed higher- order context can be modeled by introducing explicit state sequences.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 1], ["In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 1], ["An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 0], ["The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed \"category\").", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 0], ["There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 0], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 1], ["An alternative to uniformly increasing the order of the conditioning is to extend it selectively.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 0], ["State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 0], ["In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 0], ["Mixed higher- order context can be modeled by introducing explicit state sequences.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 1], ["The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 1], ["It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 0], ["In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 0], ["A word sequence is considered as being generated from an underlying sequence of categories.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 0], ["In this regard, word equivalence classes were used (Kupiec, 1989).", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 1], ["There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 1], ["In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 0], ["A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 0], ["The first addition was the correct treatment of all non-words in a text.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 0], ["In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 1], ["The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 1], ["For example, 9 errors are from 3 instances of \"... as well as ...\" that arise in the text.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 0], ["It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 0], ["In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 0], ["The model has the advantage that a pre-tagged training corpus is not required.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 1], ["For example, 9 errors are from 3 instances of \"... as well as ...\" that arise in the text.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 0], ["The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 0], ["It is then local word context (embodied in the transition probabilities) which must aid disambiguation of the word.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 0], ["A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 1], ["In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 0], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 0], ["The initial values of the model parameters are calculated from word occurrence probabilities, such that words are initially assumed to function equally probably as any of their possible categories.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 0], ["In this regard, word equivalence classes were used (Kupiec, 1989).", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 1], ["There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 1], ["The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory).", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 0], ["Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 0], ["The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 0], ["A next step is to examine them in pairs.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 1], ["We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 1], ["van Halteren (ed.)", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 0], ["van Halteren 1996).", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 0], ["1998).", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 0], ["In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 1], ["We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 1], ["To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 0], ["Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven).", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 0], ["When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 0], ["A next step is to examine them in pairs.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 1], ["We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 1], ["When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 1], ["6 The question is how large a vote we allow each tagger.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 0], ["Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 0], ["In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 0], ["The most democratic option is to give each tagger one vote (Majority).", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1], ["This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1], ["A next step is to examine them in pairs.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1], ["When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1], ["Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 0], ["Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text.", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 0], ["Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 0], ["The most democratic option is to give each tagger one vote (Majority).", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 1], ["Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 0], ["To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 0], ["Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 0], ["Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 1], ["In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 0], ["A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 0], ["The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 0], ["However, it appears more useful to give more weight to taggers which have proved their quality.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 1], ["This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 1], ["Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 0], ["Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 0], ["In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 0], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 1], ["Also of note is the improvement yielded by the best combination.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 1], ["The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 1], ["1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 0], ["The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory).", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 0], ["To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 0], ["However, it appears more useful to give more weight to taggers which have proved their quality.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 1], ["This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 1], ["The accuracy measurements for all of them are listed in Table 2.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 0], ["Pairwise Voting So far, we have only used information on the performance of individual taggers.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 0], ["Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 0], ["The first choice for this is to use a Memory- Based second level learner.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 1], ["To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 1], ["Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 0], ["To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 0], ["In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 0], ["A next step is to examine them in pairs.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 1], ["We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 1], ["The Viterbi algorithm is used to determine the most probable tag sequence.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 0], ["The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 0], ["Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 0], ["Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 1], ["However, it appears more useful to give more weight to taggers which have proved their quality.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 1], ["For this experiment we have selected four systems, primarily on the basis of availability.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 0], ["This part is used to train the individual tag- gers.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 0], ["Pairwise Voting So far, we have only used information on the performance of individual taggers.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 0], ["The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 1], ["All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 0], ["To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 0], ["As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 0], ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1], ["Also of note is the improvement yielded by the best combination.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1], ["The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1], ["Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1], ["The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 0], ["nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 0], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 0], ["The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 1], ["All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 1], ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 1], ["The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if \"as well as\" is taken to be a coordination conjunction, it is tagged \"as_CC1 well_CC2 as_CC3\", using three related but different ditto tags.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 0], ["In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 0], ["As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 0], ["Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 1], ["After comparison, their outputs are combined using several voting strategies and second stage classifiers.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 1], ["Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 1], ["When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 0], ["Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 0], ["Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 0], ["A next step is to examine them in pairs.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1], ["We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1], ["The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1], ["is usually called stacking (Wolpert 1992).", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1], ["After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 0], ["The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 0], ["It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996).", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 0], ["When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 1], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 1], ["The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement.", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 0], ["To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 0], ["For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 0], ["The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 1], ["In this and the following sections we examine a number of them.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 0], ["Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 0], ["For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 0], ["Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 1], ["Table 2: Accuracy of individual taggers and combination methods.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 1], ["Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 0], ["Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 0], ["It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features).", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 0], ["The first choice for this is to use a Memory- Based second level learner.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1], ["To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1], ["1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1], ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 0], ["Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 0], ["Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 0], ["We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 1], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0], ["We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 1], ["Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 1], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 0], ["Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 0], ["We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 0], ["We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 1], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 0], ["Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 0], ["From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 0], ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 1], ["Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 0], ["We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 0], ["Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 0], ["All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 1], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 0], ["Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 0], ["This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery.", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 0], ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 1], ["We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 1], ["To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy.", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 0], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 0], ["In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 0], ["In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff.", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 1], ["Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 0], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 0], ["In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 1], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 0], ["Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 0], ["Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 0], ["Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 1], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 0], ["To calculate a random baseline, we evaluated 10,000 random clusterings with the same number of verbs and classes as in each of our experimental tasks.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 0], ["Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 0], ["7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 1], ["Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 0], ["4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 0], ["Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 1], ["In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 0], ["Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 0], ["(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 0], ["We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 1], ["This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 1], ["Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 1], ["We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 0], ["Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 0], ["Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 1], ["4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 1], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 0], ["Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 0], ["Learning the argument structure properties of verbs\u2014the semantic roles they assign and their mapping to syntactic positions\u2014is both particularly important and difficult.", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 0], ["All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 1], ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 0], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 0], ["We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 1], ["Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 0], ["We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 0], ["In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d?", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 1], ["We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 0], ["Unsupervised methods such as Dash et al.\u2019s (1997) are appealing because they require no knowledge external to the data.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 0], ["We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 1], ["Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 0], ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 0], ["Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 1], ["Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 1], ["Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 0], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 0], ["In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 0], ["Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 1], ["Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 0], ["In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 0], ["We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 0], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 1], ["The Troll system, which is based on this idea, effectively inqflements type resolution.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 0], ["is of the form: l rl'he \"]'roll ,qysl.em was implemented in Quintus Prolog by Dale (lerdemann and '['hilo (]Stz.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 0], ["THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 0], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 1], ["well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 0], ["C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 0], ["THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 0], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1], ["We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1], ["Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1], ["Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1], ["APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1], ["It would, of course, not be very efficient to work with such large disjunctions of feature structures.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 0], ["The Troll system, which is based on this idea, effectively inqflements type resolution.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 0], ["Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 0], ["1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 1], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 0], ["APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 0], ["well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 0], ["Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 1], ["C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 1], ["The Troll system, which is based on this idea, effectively inqflements type resolution.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 1], ["With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 0], ["Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 0], ["In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 0], ["Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 1], ["As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 0], ["Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 0], ["The compiler and library are implemented in C and an API is available.", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 0], ["The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 1], ["The others are scripts that can be run on both Xerox/PARC and Foma.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 0], ["The compiler and library are implemented in C and an API is available.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 0], ["Foma is free software and will remain under the GNU General Public License.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 0], ["Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 1], ["Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 0], ["For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 0], ["Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 0], ["Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 1], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 0], ["Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 0], ["The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 0], ["Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 1], ["Foma: a finite-state compiler and library", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 0], ["The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 0], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 0], ["Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 1], ["Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 0], ["The second results in a FSM with 221 states and 222 arcs.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 0], ["Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 0], ["Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 1], ["Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 0], ["However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 0], ["The second results in a FSM with 221 states and 222 arcs.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 0], ["This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 1], ["In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 0], ["To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007).", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 0], ["This requires us to calculate the function\u2019s gradient (vector of first derivatives) with respect to \u03b8.11 Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast \u201cinside\u201d DP solution is known (Smith and Eisner, 2006; Wang et al., 2007).", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 0], ["We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009.", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 1], ["3 in Smith and Eisner (2006) for illustrations of the rest.)", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 0], ["Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s).", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 0], ["The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 0], ["We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009.", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 1], ["Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s).", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 0], ["The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 0], ["Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 0], ["We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009.", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 1], ["We recently proposed \u201ccube summing,\u201d an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009).", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 0], ["The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 0], ["Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.)", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 0], ["We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009.", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 1], ["We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation.", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 0], ["Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s).", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 0], ["This requires us to calculate the function\u2019s gradient (vector of first derivatives) with respect to \u03b8.11 Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast \u201cinside\u201d DP solution is known (Smith and Eisner, 2006; Wang et al., 2007).", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 0], ["We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009.", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 1], ["It equates to finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 0], ["Given a sentence s and its parse \u03c4s, at decoding time we seek the target sentence t\u2217, the target tree For a QDG model, the decoding problem has not been addressed before.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 0], ["Given a sentence s and its parse tree \u03c4s, we formulate the translation on the feasibility of inference, including decoding.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 0], ["Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle \u201cnon-local\u201d features using generic techniques that also support efficient parameter estimation.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 1], ["In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 0], ["We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009.", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 0], ["For models without syntactic features, we constrained the decoder to produce dependency trees in which every word\u2019s parent is immediately to its right and ignored syntactic features while scoring structures.", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 0], ["We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura Phrase Syntactic Features: features: +f att \u222a f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 1], ["The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 0], ["Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 0], ["Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in \u03c4t (or a deliberate choice is made by the decoder to translate it to NULL).", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 0], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 1], ["4.1 Translation as Monolingual Parsing.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 0], ["Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 0], ["Given a sentence s and its parse tree \u03c4s, we formulate the translation on the feasibility of inference, including decoding.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 0], ["Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 1], ["We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 0], ["The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 0], ["In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 0], ["Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 1], ["In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 0], ["We recently proposed \u201ccube summing,\u201d an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009).", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 0], ["In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 0], ["Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 1], ["In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 0], ["To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007).", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 0], ["Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 0], ["Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 1], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 0], ["Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 0], ["The data is sorted based on the frequency of the context (\u201ca unit of\u201d appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. \u201cNBC\u201d and \u201cGeneral Electric Co.\u201d appeared 10 times with the context \u201ca unit of\u201d).", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 0], ["We proposed an unsupervised method to discover paraphrases from a large untagged corpus.", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 1], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 0], ["The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 0], ["They cluster NE instance pairs based on the words in the contexts using a bag- of-words method.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 0], ["For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 1], ["We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 1], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 1], ["In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets.", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 0], ["Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 0], ["Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 0], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 1], ["There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 0], ["Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 0], ["As we shall see, most of the linked sets are paraphrases.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 0], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 1], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 1], ["All the sentences have been analyzed by our chunker and NE tag- ger.", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 0], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01].", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 0], ["There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 0], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 1], ["These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 1], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 0], ["The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 0], ["The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 0], ["Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 1], ["We proposed an unsupervised method to discover paraphrases from a large untagged corpus.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 0], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 0], ["After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 0], ["For each pair we also record the context, i.e. the phrase between the two NEs (Step1).", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 1], ["Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 1], ["When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 0], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 0], ["It is a relatively frequent word in the domain, but it can be used in different extraction scenarios.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 0], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 1], ["Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 0], ["Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 0], ["There have been other kinds of efforts to discover paraphrase automatically from corpora.", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 0], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 1], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 0], ["Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 0], ["Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 0], ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 1], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 1], ["Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 0], ["One possibility is to use n-grams based on mutual information.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 0], ["We evaluated the results based on two metrics.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 0], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 1], ["We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 1], ["So, we set a threshold that at least two examples are required to build a link.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 0], ["If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 0], ["We proposed an unsupervised method to discover paraphrases from a large untagged corpus.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 0], ["Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 1], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 0], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 0], ["Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 0], ["There have been other kinds of efforts to discover paraphrase automatically from corpora.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 1], ["The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 0], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand.", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 0], ["In order to solve this problem, a parse tree is needed to understand that \u201cLotus\u201d is not the object of \u201cestimates\u201d.", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 0], ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 1], ["The data is sorted based on the frequency of the context (\u201ca unit of\u201d appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. \u201cNBC\u201d and \u201cGeneral Electric Co.\u201d appeared 10 times with the context \u201ca unit of\u201d).", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 0], ["Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 0], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 0], ["Cluster phrases based on Links We now have a set of phrases which share a keyword.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 1], ["At this step, we will try to link those sets, and put them into a single cluster.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 1], ["In IE, creating the patterns which express the requested scenario, e.g. \u201cmanagement succession\u201d or \u201ccorporate merger and acquisition\u201d is regarded as the hardest task.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 0], ["Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 0], ["When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 0], ["Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2).", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 1], ["For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3).", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 1], ["We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 0], ["We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 0], ["For example, the phrase \u201c's New York-based trust unit,\u201d is not a paraphrase of the other phrases in the \u201cunit\u201d set.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 0], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 1], ["We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 0], ["This result is comparable with the results described in (Baldwin 1997).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 0], ["The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 1], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 1], ["We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 1], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 0], ["Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\".", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 0], ["The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1.", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 1], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 0], ["Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 0], ["The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology.", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 1], ["The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 0], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 0], ["Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 1], ["The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 1], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 0], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 0], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 0], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 1], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 1], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 0], ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 0], ["In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 0], ["If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 1], ["The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\".", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 1], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 1], ["The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 0], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 0], ["As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 0], ["Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 1], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 0], ["Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. \"toner bottle\", \"bottle of toner\", \"the bottle\").", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 0], ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 0], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 1], ["Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent.", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 1], ["Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 0], ["We should point out that the antecedent indicators are preferences and not absolute factors.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 0], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 0], ["Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 1], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 1], ["The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1.", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 0], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 0], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 1], ["The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 0], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 1], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 1], ["Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 0], ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 0], ["Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 0], ["The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 0], ["The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects.", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 0], ["Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 0], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 1], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 0], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 1], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 0], ["The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%.", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 0], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 1], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 0], ["Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\").", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 0], ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 0], ["3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 1], ["The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging).", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 1], ["Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 1], ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 0], ["In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 0], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 1], ["Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\").", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 0], ["Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 0], ["tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test.", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 0], ["There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 1], ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 0], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 0], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 0], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 1], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 0], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\".", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 0], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 0], ["For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 0], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\".", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 1], ["From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 0], ["This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 0], ["In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 1], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 1], ["We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 1], ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 0], ["In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 0], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 1], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 0], ["The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%).", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 0], ["The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 0], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 1], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 1], ["Robust pronoun resolution with limited knowledge", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 0], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 1], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\".", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 1], ["Input is checked against agreement and for a number of antecedent indicators.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 0], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 0], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 1], ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 1], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\".", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 0], ["The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 0], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 1], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 0], ["Input is checked against agreement and for a number of antecedent indicators.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 0], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 1], ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 1], ["We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 0], ["The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 0], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 0], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 1], ["In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 0], ["Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 0], ["tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test.", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 0], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 0], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 0], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 1], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 0], ["If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 0], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 0], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 0], ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 0], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 0], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 1], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 0], ["We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 0], ["Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. \"toner bottle\", \"bottle of toner\", \"the bottle\").", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 1], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 1], ["The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 0], ["Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\".", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 0], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 0], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 1], ["Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 1], ["We should point out that the antecedent indicators are preferences and not absolute factors.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 0], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 0], ["Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 1], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 1], ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 0], ["For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\".", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 1], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 1], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 0], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 0], ["Input is checked against agreement and for a number of antecedent indicators.", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 0], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 1], ["In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994).", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 0], ["From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 0], ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 0], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 1], ["Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 1], ["We should point out that the antecedent indicators are preferences and not absolute factors.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 1], ["We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 0], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 0], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 0], ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 1], ["It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 1], ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 0], ["The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 0], ["As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 1], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 1], ["As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997).", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 0], ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 0], ["For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 0], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 1], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 1], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 0], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 0], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 0], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 1], ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 1], ["As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997).", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 0], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\".", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 0], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 0], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 1], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 0], ["Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 0], ["Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 0], ["In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 1], ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 1], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 0], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 0], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 0], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 1], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 1], ["A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 0], ["The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 0], ["A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994).", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 0], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 1], ["Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result.", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 0], ["Another approach to text segmentation is the detection of semantically related words.", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 0], ["These types can be identified using relation weights (Jobbins and Evett, 1998).", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 0], ["Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 1], ["A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994).", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 0], ["Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 0], ["A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 0], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 1], ["Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 0], ["Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 0], ["Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 0], ["Another approach extracted semantic information from Roget's Thesaurus (RT).", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 1], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 1], ["Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 0], ["This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 0], ["Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 0], ["Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing).", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 1], ["Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 1], ["Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 1], ["Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 0], ["This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 0], ["Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 0], ["The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 1], ["When used in isolation, the performance of each feature was inferior to a combined approach.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 1], ["This fact provides evidence that different lexical relations are detected by each linguistic feature considered.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 1], ["The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 0], ["A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 0], ["This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 0], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 1], ["Text segmentation could also be used as a pre-processing step in automatic summarisation.", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 0], ["These types can be identified using relation weights (Jobbins and Evett, 1998).", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 0], ["Each segment could be summarised individually and then combined to provide an abstract for a document.", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 0], ["The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity.", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 1], ["Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 0], ["In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5).", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 0], ["To evaluate the results, the information retrieval metrics precision and recall were used.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 0], ["The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 1], ["Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 1], ["Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 0], ["The lexical cohesion relations of reiteration and collocation are used to identify related words.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 0], ["Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 0], ["Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 1], ["Identifying semantic relations in a text can be a useful indicator of its conceptual structure.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 1], ["Gibbs sampling for the fertility IBM Model 1 is similar but simpler.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 0], ["The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 0], ["AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 0], ["We built a fertility hidden Markov model by adding fertility to the hidden Markov model.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 1], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 1], ["We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 1], ["We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 0], ["IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 0], ["We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 0], ["Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1], ["Our model is a coherent generative model that combines the HMM and IBM Model 4.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1], ["Our model is much faster than IBM Model 4.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1], ["In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1], ["(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 0], ["1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 0], ["AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 0], ["We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 1], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 1], ["We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 0], ["We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 0], ["We choose t = 1, 5, and 30 for the fertility HMM.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 0], ["Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 1], ["One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent).", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0], ["We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0], ["Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0], ["Our model has only one parameter for each target word, which can be learned more reliably.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1], ["i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)).", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1], ["We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0], ["Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0], [", eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0], ["1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 1], ["We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 0], ["Initially, the fertility IBM Model 1 and fertility HMM did not perform well.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 0], ["For the fertility IBM Model 1, we do not need to estimate the distortion probability.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 0], ["This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 1], ["However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 1], ["Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 1], ["For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . .", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 0], ["(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 0], ["The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 0], ["Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 1], ["Our model is a coherent generative model that combines the HMM and IBM Model 4.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 1], ["It is easier to understand than IBM Model 4 (see Section 3).", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 1], ["IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 0], ["We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 0], ["where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 0], ["We built a fertility hidden Markov model by adding fertility to the hidden Markov model.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 1], ["We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 0], ["IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 0], ["We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 0], ["We built a fertility hidden Markov model by adding fertility to the hidden Markov model.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 1], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 1], ["We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 0], ["We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 0], ["We choose t = 1, 5, and 30 for the fertility HMM.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 0], ["Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 1], ["We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 0], ["(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 0], ["The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 0], ["We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 1], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 1], ["One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent).", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0], ["We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0], ["Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0], ["Our model has only one parameter for each target word, which can be learned more reliably.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1], ["i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)).", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1], ["We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0], ["Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0], [", eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0], ["1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 1], ["Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 0], ["There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 0], ["Because we only sum over fer tilities that are consistent with the alignments, we P (a|a\u2032) = \"\u00a3s c (a|a\u2032; f (s), e(s)) (5)have \"\u00a3f J P (f J |e2I +1) < 1, and our model is de \"\u00a3 \"\u00a3 a s c(a|a\u2032; f (s), e(s)) 1 1 1 \"\u00a3 (s) (s) ficient, similar to IBM Models 3 and 4 (Brown et al., 1993).", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 0], ["We built a fertility hidden Markov model by adding fertility to the hidden Markov model.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1], ["It is similar in some ways to IBM Model 4, but is much easier to understand.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1], ["We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1], ["Gibbs sampling for the fertility IBM Model 1 is similar but simpler.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 0], ["But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 0], ["We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596\u2013605, MIT, Massachusetts, USA, 911 October 2010.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 0], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 1], ["Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 0], ["2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 0], ["However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 1], ["In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 0], ["We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 0], ["Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 0], ["We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 1], ["BABAR employs information extraction techniques to represent and learn role relationships.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 1], ["We evaluated BABAR on two domains: terrorism and natural disasters.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 1], ["We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 0], ["Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected.", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 0], ["We will refer to the semantic classes that co-occur with a caseframe as the semantic expectations of the caseframe.", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 0], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 1], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 0], ["Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 0], ["In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 0], ["We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 1], ["BABAR employs information extraction techniques to represent and learn role relationships.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 1], ["Each pattern represents the role that a noun phrase plays in the surrounding context.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 1], ["In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 0], ["For example, co-occurring caseframes may reflect synonymy (e.g., \u201c<patient> kidnapped\u201d and \u201c<patient> abducted\u201d) or related events (e.g., \u201c<patient> kidnapped\u201d and \u201c<patient> released\u201d).", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 0], ["First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 0], ["Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 1], ["Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 0], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 0], ["However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 0], ["Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 1], ["Ex: Mr. Bush disclosed the policy by reading it...", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 1], ["We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 0], ["Ex: Mr. Bush disclosed the policy by reading it...", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 0], ["Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 0], ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 1], ["Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 0], ["For example, the passive voice pattern \u201c<subject> were kidnapped\u201d and the active voice pattern \u201ckidnapped <direct object>\u201d are merged into a single normalized pattern \u201ckidnapped <patient>\u201d.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, \u201c<agent> kidnapped\u201d or \u201ckidnapped <patient>\u201d), and (2) predicate-argument relations associated with both verbs and nouns (e.g., \u201ckidnapped for <np>\u201d or \u201cvehicle with <np>\u201d).", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 0], ["For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 0], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1], ["Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1], ["We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1], ["Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1], ["One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1], ["Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 0], ["In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 0], ["2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 0], ["A contextual role represents the role that a noun phrase plays in an event or relationship.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 1], ["The goal of our research was to explore the use of contextual role knowledge for coreference resolution.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 0], ["The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 0], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 1], ["2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 1], ["In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 0], ["In this situation, BABAR takes the conservative approach and declines to make a resolution.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 0], ["However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 0], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 1], ["Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 0], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 0], ["In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998).", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 1], ["Its correct antecedent is \u201ca revolver\u201d, which is extracted by the caseframe \u201ckilled with <NP>\u201d.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 0], ["(Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 0], ["BABAR uses a DempsterShafer decision model (Stefik, 1995) to combine the evidence provided by the knowledge sources.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 0], ["Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 1], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 0], ["The goal of our research was to explore the use of contextual role knowledge for coreference resolution.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 0], ["We also performed experiments to evaluate the impact of each type of contextual role knowledge separately.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 0], ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 1], ["Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 0], ["Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 0], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 1], ["The goal of our research was to explore the use of contextual role knowledge for coreference resolution.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 0], ["BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 0], ["Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 1], ["These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 1], ["We evaluated BABAR on two domains: terrorism and natural disasters.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 1], ["Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 0], ["However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 0], ["Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 0], ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 1], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 0], ["The learned patterns are then normalized and applied to the corpus.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 0], ["BABAR employs information extraction techniques to represent and learn role relationships.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 0], ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 1], ["These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 1], ["Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 0], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 0], ["In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998).", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 1], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 0], ["Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 0], ["For pronouns, however, all of the knowledge sources increased recall, often substantially, and with little if any decrease in precision.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 0], ["For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 1], ["If so, the CF Network reports that the anaphor and candidate may be coreferent.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 1], ["Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 0], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 0], ["The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 0], ["The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 1], ["The rationale for treating these semantic labels differently is that they are specific and reliable (as opposed to the WordNet classes, which are more coarse and more noisy due to polysemy).", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 0], ["For example, if X and Y are coreferent, then both X and Y are considered to co-occur with the caseframe that extracts X as well as the caseframe that extracts Y. We will refer to the set of nouns that co-occur with a caseframe as the lexical expectations of the case- frame.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 0], ["For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 0], ["Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1], ["The confidence level is then used as the belief value for the knowledge source.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1], ["Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1], ["The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1], ["The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1], ["From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 0], ["The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 0], ["One common problem was the simple failure to recognize \"hire\" as an indicator of a succession.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 0], ["In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 1], ["The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 0], ["In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 0], ["The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 0], ["The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 1], ["In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 1], ["Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 0], ["Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot?", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 0], ["Looking at the document section scores in table 3, we see that the error score on the body of the text was much lower than on the headline for all but a few systems.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 0], ["In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 1], ["There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 1], ["Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 1], ["The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 0], ["In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 0], ["COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 0], [" Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 1], ["As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 0], ["The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 0], ["One success for the systems as a group is that each of the six smaller ORGANIZATION objects and four smaller PERSON objects (those with just one or two filled slots in the key) was matched perfectly by at least one system; in addition, one larger ORGANIZATION object and two larger PERSON objects were perfectly matched by at least one system.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 0], ["5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 1], ["Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\".", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 0], ["The amount of agreement between the two annotators was found to be 80% recall and 82% precision.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 0], ["It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 0], ["CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1], ["The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1], ["The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1], ["Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1], ["The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 0], ["Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 0], ["Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 0], ["In this article, the management succession scenario will be used as the basis for discussion.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 1], ["The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 1], ["It was also unexpected that one of the systems would match human performance on the task.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 0], ["These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 0], ["TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 0], ["When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 1], ["Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74..", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 1], ["The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 0], ["There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 0], ["Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 0], ["Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 1], ["The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 0], ["The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 0], ["From this table, it may be reasonable to conclude that progress has been made, since the MUC6 performance level is at least as high as for three of the four MUC5 tasks and since that performance level was reached after a much shorter time.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 0], ["No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 1], ["The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 1]]