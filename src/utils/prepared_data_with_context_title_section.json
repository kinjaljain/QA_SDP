[["we perform five runs with different random initialization of sampling state. Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set.", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of Grac\u00b8a et al. (2009). The system of Berg-Kirkpatrick et al.", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, \u03b1, \u03b2) \u221d P (T , t, W , w|\u03b1, \u03b2) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, \u03c8, \u03b8, \u03c6, w|\u03b1, \u03b2)d\u03c8d\u03b8d\u03c6 Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior. Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i). The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5). Performance typically stabilizes across languages after only a few number of iterations. to represent the ith word type emitted by the HMM: P (t(i)|Ti, t(\u2212i), w, \u03b1) \u221d n P (w|Ti, t(\u2212i), w(\u2212i), \u03b1) (tb ,ta ) P (Ti, t(i)|T , W , t(\u2212i), w, \u03b1, \u03b2) = P (T |tb, t(\u2212i), \u03b1)P (ta|T , t(\u2212i), \u03b1) \u2212i (i) i i (\u2212i) P (Ti|W , T \u2212i, \u03b2)P (t |Ti, t , w, \u03b1) All terms are Dirichlet distributions and parameters can be analytically computed from counts in t(\u2212i)where T \u2212i denotes all type-level tag assignment ex cept Ti and t(\u2212i) denotes all token-level tags except and w (\u2212i) (Johnson, 2007).", "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", 1, "Simple Type-Level Unsupervised POS Tagging", "Learning and Inference."], ["On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible. Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank. This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. \u2014 similar results have been observed across multiple languages.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity.", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of Grac\u00b8a et al. (2009). The system of Berg-Kirkpatrick et al.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively. We observe similar trends when using another measure \u2013 type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 \u2013 \u2013 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 \u2013 \u2013 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 \u2013 \u2013 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 \u2013 \u2013 4 0 . 6 7 3 . 2 \u2013 \u2013 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1. 1 1 0.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac\u00b8a et al., 2009; Ravi and Knight, 2009). In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints. In this paper, we make a simplifying assumption of one-tag-per-word. This assumption, however, is not inherent to type-based tagging models.", "More recently, Lee et al. (2010) presented a new type-based model, and also reported very good results.", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac\u00b8a et al., 2009; Ravi and Knight, 2009). In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively. We observe similar trends when using another measure \u2013 type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 \u2013 \u2013 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 \u2013 \u2013 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 \u2013 \u2013 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 \u2013 \u2013 4 0 . 6 7 3 . 2 \u2013 \u2013 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1. 1 1 0.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states.", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["This design does not guarantee \u201cstructural zeros,\u201d but biases towards sparsity. A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (Grac\u00b8a et al., 2009). This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish. On each language we investigate the contribution of each component of our model. For all languages we do not make use of a tagging dictionary. Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45. 2 62.6 45.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010).", 1, "Simple Type-Level Unsupervised POS Tagging", "Experiments."], ["Our second point of comparison is with Grac\u00b8a et al. (2009), who also incorporate a sparsity constraint, but does via altering the model objective using posterior regularization. We can only compare with Grac\u00b8a et al. (2009) on Portuguese (Grac\u00b8a et al. (2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours).", "Following Lee et al. (2010) we used only the training sections for each language.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["We use w erations of sampling (see Figure 2 for a depiction). We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish. On each language we investigate the contribution of each component of our model. For all languages we do not make use of a tagging dictionary. Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45.", "Following Lee et al. (2010) we used only the training sections for each language.", 0, "Simple Type-Level Unsupervised POS Tagging", "Experiments."], ["The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets. Following the setup of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set.", "Following Lee et al. (2010) we used only the training sections for each language.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible.", "Following Lee et al. (2010) we used only the training sections for each language.", 1, "Simple Type-Level Unsupervised POS Tagging", "ABSTRACT"], ["This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["Part-of-speech (POS) tag distributions are known to exhibit sparsity \u2014 a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 0, "Simple Type-Level Unsupervised POS Tagging", "ABSTRACT"], ["Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags. We tokenize MWUs and their POS tags; this reduces the tag set size to 12. See Table 2 for the tag set size of other languages.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible. Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank. This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. \u2014 similar results have been observed across multiple languages.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of Grac\u00b8a et al. (2009). The system of Berg-Kirkpatrick et al.", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["\u2014 similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me\u00b4rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac\u00b8a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level.", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states.", "vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["\u2014 similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me\u00b4rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac\u00b8a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of Grac\u00b8a et al. (2009). The system of Berg-Kirkpatrick et al.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00b8a et al., 2011; Lee et al., 2010).", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["They are set to fixed constants. to explore how well we can induce POS tags using only the one-tag-per-word constraint. Specifically, the lexicon is generated as: P (T , W |\u03c8) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010). Past work however, has typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated these features with token occurrences, typically in an HMM. In our model, we associate these features at the type-level in the lexicon.", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively. We observe similar trends when using another measure \u2013 type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 \u2013 \u2013 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 \u2013 \u2013 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 \u2013 \u2013 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 \u2013 \u2013 4 0 . 6 7 3 . 2 \u2013 \u2013 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1. 1 1 0.", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This assumption, however, is not inherent to type-based tagging models. A promising direction for future work is to explicitly model a distribution over tags for each word type. We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon. The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684). We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments.", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["\u2014 similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me\u00b4rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac\u00b8a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level.", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively. We observe similar trends when using another measure \u2013 type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 \u2013 \u2013 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 \u2013 \u2013 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 \u2013 \u2013 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 \u2013 \u2013 4 0 . 6 7 3 . 2 \u2013 \u2013 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1. 1 1 0.", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This assumption, however, is not inherent to type-based tagging models. A promising direction for future work is to explicitly model a distribution over tags for each word type. We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon. The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684). We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments.", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010).", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity. This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007). There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively. We observe similar trends when using another measure \u2013 type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 \u2013 \u2013 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 \u2013 \u2013 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 \u2013 \u2013 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 \u2013 \u2013 4 0 . 6 7 3 . 2 \u2013 \u2013 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u00b8a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1. 1 1 0.", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007). There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns). Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference. We evaluate our model on seven languages exhibiting substantial syntactic variation.", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["In our model, we associate these features at the type-level in the lexicon. Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint. Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution \u03c8 over tag assignments drawn from DIRICHLET(\u03b2, K ). This alters generation of T as follows: n P (T |\u03c8) = n P (Ti|\u03c8) i=1 Note that this distribution captures the frequency of a tag across word types, as opposed to tokens. The P (T |\u03c8) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary.", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["i=1 (f,v)\u2208Wi For inference, we are interested in the posterior probability over the latent variables in our model. During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, \u03b1, \u03b2) \u221d P (T , t, W , w|\u03b1, \u03b2) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, \u03c8, \u03b8, \u03c6, w|\u03b1, \u03b2)d\u03c8d\u03b8d\u03c6 Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior. Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i). The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5).", "Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)\u2019s one-class HMM.", 1, "Simple Type-Level Unsupervised POS Tagging", "Learning and Inference."], ["(2009). Ablation Analysis We evaluate the impact of incorporating various linguistic features into our model in Table 3. A novel element of our model is the ability to capture type-level tag frequencies. For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR). Across all languages, +PRIOR consistently outperforms 1TW, reducing error on average by 9.1% and 5.9% on best and median settings respectively.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["(2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours). Their best model yields 44.5% one-to-one accuracy, compared to our best median 56.5% result. However, our full model takes advantage of word features not present in Grac\u00b8a et al. (2009). Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming Grac\u00b8a et al.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["6 Results and Analysis. We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of Grac\u00b8a et al.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["0 57.2 43. 3 61.7 38. 5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5). For each language and setting, we report one-to-one (11) and many- to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 11 metric.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of Grac\u00b8a et al. (2009). The system of Berg-Kirkpatrick et al.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010).", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["As is standard, we use a fixed constant K for the number of tagging states. Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word. Conditioned on T , features of word types W are drawn. We refer to (T , W ) as the lexicon of a language and \u03c8 for the parameters for their generation; \u03c8 depends on a single hyperparameter \u03b2. Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters \u03b8 are generated conditioned on tag assignments T . We also draw transition parameters \u03c6.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters \u03b8 are generated conditioned on tag assignments T . We also draw transition parameters \u03c6. Both parameters depend on a single hyperparameter \u03b1. Once HMM parameters (\u03b8, \u03c6) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from \u03c6. The corresponding token words w are drawn conditioned on t and \u03b8.2 Our full generative model is given by: K P (\u03c6, \u03b8|T , \u03b1, \u03b2) = n (P (\u03c6t|\u03b1)P (\u03b8t|T , \u03b1)) t=1 The transition distribution \u03c6t for each tag t is drawn according to DIRICHLET(\u03b1, K ), where \u03b1 is the shared transition and emission distribution hyperparameter. In total there are O(K 2) parameters associated with the transition parameters.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type.", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward na\u00a8\u0131veBayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. Recent work has made significant progress on unsupervised POS tagging (Me\u00b4rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity.", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (Grac\u00b8a et al., 2009). This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model.", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al.", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["They do paraphrase each other to some extent, but their relation can only be understood properly with |{dj : ti \u2208 dj }| |D| is the total number of sentences in the cluster and |{dj : ti \u2208 dj }| is the number of sen tences that contain the term ti. These scores are used in a vector space representation. The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 0, "No title", "Method."], ["In future research we would like to investigate the task of judging paraphrases. The next step we would like to take towards automatic paraphrase generation, is to identify the differences between paraphrases at the constituent level. This task has in fact been performed by human annotators in the DAESO-project. A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way. Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 0, "No title", "Discussion."], ["Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 0, "No title", "Introduction"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).", 1, "No title", "ABSTRACT"], ["News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus.", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 0, "No title", "Introduction"], ["A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way. Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme. Thanks also to Wauter Bosma for originally mining the headlines from Google News.", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 0, "No title", "Acknowledgements"], ["Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event.", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 0, "No title", "Introduction"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].", 1, "No title", "ABSTRACT"], ["Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles. For the development of our system we use data which was obtained in the DAESO-project. This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122\u2013125, Athens, Greece, 30 \u2013 31 March 2009. Qc 2009 Association for Computational Linguistics document, and each original cluster as a collection of documents.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 0, "No title", "Method."], ["We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 0, "No title", "Method."], ["News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 0, "No title", "Introduction"], ["For text-to-text generation it is important to know which words and phrases are semantically close or exchangable in which contexts. While there are various resources available that capture such knowledge at the word level (e.g., synset knowledge in WordNet), this kind of information is much harder to get by at the phrase level. Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].", 1, "No title", "Introduction"], ["The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 0, "No title", "Method."], ["While there are various resources available that capture such knowledge at the word level (e.g., synset knowledge in WordNet), this kind of information is much harder to get by at the phrase level. Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 0, "No title", "Introduction"], ["In future research we would like to investigate the task of judging paraphrases. The next step we would like to take towards automatic paraphrase generation, is to identify the differences between paraphrases at the constituent level. This task has in fact been performed by human annotators in the DAESO-project. A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way. Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 0, "No title", "Discussion."], ["The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package.", "Besides Wubben et al.\u2019s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.", 1, "No title", "Method."], ["We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1. For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function. In each newly obtained cluster all headlines can be aligned to each other. 2.2 Pairwise similarity.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 0, "No title", "Method."], ["Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 0, "No title", "Method."], ["Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. Some examples of correct and incorrect alignments are given in Table 3.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 0, "No title", "Results."], ["The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example.", "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.", 1, "No title", "Method."], ["2.2 Pairwise similarity. Our second approach is to calculate the similarity between pairs of headlines directly. If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair. If it is below the threshold, it is rejected. However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 0, "No title", "Method."], ["In these sub-clusters, there are 6,685 clustered headlines. Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 0, "No title", "Results."], ["While there are various resources available that capture such knowledge at the word level (e.g., synset knowledge in WordNet), this kind of information is much harder to get by at the phrase level. Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 0, "No title", "Introduction"], ["We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1. For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function. In each newly obtained cluster all headlines can be aligned to each other. 2.2 Pairwise similarity.", "However, the proposed system performs better than Wubben et al.\u2019s approaches as well as FCM Clustering for Paraphrase Extraction.", 1, "No title", "Method."], ["The T F \u2217I DF score is then: TF.IDFi = T Fi,j \u00b7 log | Table 1: Part of a sample headline cluster, with sub-clusters and Krahmer, 2007) and will be made available through the Dutch HLT Agency. Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period April\u2013August 2006. For each news article, the headline and the first 150 characters of the article were stored. Roughly 13,000 clusters were retrieved. Table 1 shows part of a (translated) cluster.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 0, "No title", "Method."], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 0, "No title", "ABSTRACT"], ["We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 0, "No title", "Method."], ["Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. Some examples of correct and incorrect alignments are given in Table 3.", "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.", 1, "No title", "Results."], ["Threshold values as found by optimizing on the development data using again an F0.25-score, are T hlower = 0.2 and T hupper = 0.5. An optional final step is to add alignments that are implied by previous alignments. For instance, if headline A is paired with headline B, and headline B is aligned to headline C , headline A can be aligned to C as Ty pe Precision Recallk m ea ns cl us ter in g 0.91 0.43 clu ste rs on lyk m ea ns cl us ter in g 0.66 0.44 all he ad lin es pa irw ise si mi lar ity 0.93 0.39 clu ste rs on ly pa irw ise si mi lar ity 0.76 0.41 all he ad lin es Table 2: Precision and Recall for both methods Pl ay st ati on 3 m or e ex pe nsi ve th an co m pe tit or P l a y s t a t i o n 3 w i l l b e c o m e m o r e e x p e n s i v e t h a n X b o x 3 6 0 So ny po stp on es Blu Ra y m ov ie s So ny po stp on es co mi ng of blu ra y dv ds Pri ce s Pl ay st ati on 3 kn ow n: fro m 49 9 eu ro s E3 20 06 : Pl ay st ati on 3 fro m 49 9 eu ro s So ny PS 3 wi th Blu R ay for sal e fro m No ve m be r 11 th PS 3 av ail abl e in Eu ro pe fro m No ve m be r 17 th Table 3: Examples of correct (above) and incorrect (below) alignments well. We do not add these alignments, because in particular in large clusters when one wrong alignment is made, this process chains together a large amount of incorrect alignments. The 825 clusters in the test set contain 1,751 sub- clusters in total.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 0, "No title", "Method."], ["In these sub-clusters, there are 6,685 clustered headlines. Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 0, "No title", "Results."], ["In the remaining case of a similarity between the two thresholds, similarity is calculated over the contexts of the two headlines, namely the text snippet that was retrieved with the headline. If this similarity exceeds the upper threshold, it is accepted. Threshold values as found by optimizing on the development data using again an F0.25-score, are T hlower = 0.2 and T hupper = 0.5. An optional final step is to add alignments that are implied by previous alignments. For instance, if headline A is paired with headline B, and headline B is aligned to headline C , headline A can be aligned to C as Ty pe Precision Recallk m ea ns cl us ter in g 0.91 0.43 clu ste rs on lyk m ea ns cl us ter in g 0.66 0.44 all he ad lin es pa irw ise si mi lar ity 0.93 0.39 clu ste rs on ly pa irw ise si mi lar ity 0.76 0.41 all he ad lin es Table 2: Precision and Recall for both methods Pl ay st ati on 3 m or e ex pe nsi ve th an co m pe tit or P l a y s t a t i o n 3 w i l l b e c o m e m o r e e x p e n s i v e t h a n X b o x 3 6 0 So ny po stp on es Blu Ra y m ov ie s So ny po stp on es co mi ng of blu ra y dv ds Pri ce s Pl ay st ati on 3 kn ow n: fro m 49 9 eu ro s E3 20 06 : Pl ay st ati on 3 fro m 49 9 eu ro s So ny PS 3 wi th Blu R ay for sal e fro m No ve m be r 11 th PS 3 av ail abl e in Eu ro pe fro m No ve m be r 17 th Table 3: Examples of correct (above) and incorrect (below) alignments well.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 0, "No title", "Method."], ["However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent. Beyond that point, context is needed. With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(\u03b8) = V 1 \u00b7 V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared. If the similarity is higher than the upper threshold, it is accepted. If it is lower than the lower theshold, it is rejected.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.", 1, "No title", "Method."], ["These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 0, "No title", "Introduction"], ["We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 0, "No title", "Method."], ["Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision. We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76. Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases. Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted. This is not a bad thing: we are particularly interested in extracting paraphrase patterns at the constituent level.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 0, "No title", "Discussion."], ["In our case, precision is the number of alignments retrieved from the clusters which are relevant, divided by the total number of retrieved alignments. Recall is the number of relevant retrieved aligments divided by the total number of relevant alignments. We use an F\u03b2 -score with a \u03b2 of 0.25 as we favour precision over recall. We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1.", "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.", 1, "No title", "Method."], ["News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 0, "No title", "Introduction"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 0, "No title", "ABSTRACT"], ["If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair. If it is below the threshold, it is rejected. However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent. Beyond that point, context is needed. With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(\u03b8) = V 1 \u00b7 V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 0, "No title", "Method."], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.", "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.", 1, "No title", "ABSTRACT"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 0, "No title", "ABSTRACT"], ["They do paraphrase each other to some extent, but their relation can only be understood properly with |{dj : ti \u2208 dj }| |D| is the total number of sentences in the cluster and |{dj : ti \u2208 dj }| is the number of sen tences that contain the term ti. These scores are used in a vector space representation. The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines.", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 0, "No title", "Method."], ["Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example. We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters. We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data. The headlines are stemmed using the porter stemmer for Dutch (Kraaij and Pohlmann, 1994). Instead of a word overlap measure as used byHere, C r is a criterion function, which mea sures the ratio of withincluster similarity to betweencluster similarity.", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 0, "No title", "Method."], ["The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example. We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters. We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data. The headlines are stemmed using the porter stemmer for Dutch (Kraaij and Pohlmann, 1994).", "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) \u2217 Ni \u2217 N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.", 1, "No title", "Method."], ["On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set. By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set. Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised. The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder. Table 7 shows the breakdown in performance for each supersense.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30). For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson (2003) call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.", "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons. There are a number of problems our system does not currently handle. Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate). Further, our similarity system does not currently incorporate multi-word terms. We overcome this by using the synonyms of the last word in the multi-word term.", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Other Alternatives and Future Work."], ["They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify. Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003).", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation. Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and Schu\u00a8 tze (1993) and Widdows (2003). However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular. Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary. Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap. Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics. Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earth\u2019s atmosphere. These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Alternatively, the weights can use the ranking of the extracted synonyms. Again these options have been considered below. A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable. The final issue is how to deal with polysemy. Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap. Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics. Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses. Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus. By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen\u2019s configuration for our super- sense tagging experiments. 6.1 Part of Speech Tagging and Chunking. Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994). The only similar performing tool is the Trigrams \u2018n\u2019 Tags tagger (Brants, 2000) which uses a much simpler statistical model. Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Semantic."], ["For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson (2003) call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories. Ciaramita et al.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation. Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and Schu\u00a8 tze (1993) and Widdows (2003). However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["Schu\u00a8 tze\u2019s (1992) WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem). Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word. Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy. He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and Schu\u00a8 tze (1993) and Widdows (2003). However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms. In these cases, our SUFFIX EXAMPLE SUPERSENSEness remoteness attribute -tion, -ment annulment act -ist, -man statesman person -ing, -ion bowling act -ity viscosity attribute -ics, -ism electronics cognition -ene, -ane, -ine arsine substance -er, -or, -ic, -ee, -an mariner person -gy entomology cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories. Ciaramita et al. (2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appear ing directly underneath. Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Roget\u2019s (Yarowsky, 1992).", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6. We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability. Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor. Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003). This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Alternatively, the weights can use the ranking of the extracted synonyms. Again these options have been considered below. A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable. The final issue is how to deal with polysemy. Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earth\u2019s atmosphere. These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003). This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus. Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag. This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available. We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin,2000).", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in S Y S T E M W N 1.6 W N 1.7 .1 Cia ra mit a an d Joh nso n bas eli ne 2 1 % 2 8 % Cia ra mit a an d Joh nso n per cep tro n 5 3 % 5 3 % Si mil arit y bas ed res ult s 6 8 % 6 3 % Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses. This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive. We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003). The experiments were performed by considering all possible configurations of the parameters described above. The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term. Finally, we plan to implement a supervised machine learner to replace the fall- back method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set. We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results. We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis. Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Other Alternatives and Future Work."], ["These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis. Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors. Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu\u00a8 tze (1993) and Widdows (2003). To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6. We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and Schu\u00a8 tze (1993) and Widdows (2003). However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms. In these cases, our SUFFIX EXAMPLE SUPERSENSEness remoteness attribute -tion, -ment annulment act -ist, -man statesman person -ing, -ion bowling act -ity viscosity attribute -ics, -ism electronics cognition -ene, -ane, -ine arsine substance -er, -or, -ic, -ee, -an mariner person -gy entomology cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.", "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN (Hearst and Schu\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability. Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor. Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003). This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus. Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set. By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set. Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised. The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder. Table 7 shows the breakdown in performance for each supersense.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lex- files). For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops. Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs. Lex-files form a set of coarse-grained sense distinctions within WORDNET.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["These results also support Ciaramita and Johnson\u2019s view that abstract concepts like communication, cognition and state are much harder. We would expect the location supersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier. Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help. An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words. This has the advantage of producing a much smaller number of vectors to compare against.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search. The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium\u2019s news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus. The components and their sizes including punctuation are given in Table 3. The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above. C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Corpus."], ["Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise. The rest of the pipeline is described in the next section. Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts. This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in. In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Semantic."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis. Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors. Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu\u00a8 tze (1993) and Widdows (2003). To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6. We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Roget\u2019s (Yarowsky, 1992). These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts. Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses. They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms. Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u2019s ability to cluster words by their semantics.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen\u2019s configuration for our super- sense tagging experiments. 6.1 Part of Speech Tagging and Chunking. Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994). The only similar performing tool is the Trigrams \u2018n\u2019 Tags tagger (Brants, 2000) which uses a much simpler statistical model. Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Semantic."], ["Alternatively, the weights can use the ranking of the extracted synonyms. Again these options have been considered below. A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable. The final issue is how to deal with polysemy. Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["How are words with multiple supersenses handled? Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results. One solution would be to take the intersection between vectors across words for each supersense (i.e. to find the common contexts that these words appear in). However, given the sparseness of the data this may not leave very large context vectors. A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense.", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Other Alternatives and Future Work."], ["He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003). The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense. The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003). They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set. Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30). For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson (2003) call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007). The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER. This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the \u201cskeleton\u201d, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007).", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003).", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Confusion networks are generated by choosing one hypothesis as the \u201cskeleton\u201d, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007). The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER.", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton. Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score. The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems. This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function.", "Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["3.1 Discussion. There are several problems with the previous confusion network decoding approaches. First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one. Equation 6 may be viewed as a log-linear sum of sentence- level features. The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores. Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding. Third, the system weights are independent of the skeleton selection. Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding. For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen. The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities.", "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton. Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified. Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding. For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using \u201ccat sat the mat\u201d as the skeleton, aligning \u201ccat sitting on the mat\u201d and \u201chat on a mat\u201d against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using \u201ccat sat the mat\u201d as the skeleton, aligning \u201ccat sitting on the mat\u201d and \u201chat on a mat\u201d against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word. In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs.", "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.", "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 0, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that. position in the sentence and the number of votes for each word is marked in parentheses.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 0, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["This allows the addition of language model scores by expanding the lattices or re-scoring -best lists. The LM integration should result in more grammatical combination outputs. Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 1, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["The LM integration should result in more grammatical combination outputs. Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks.", "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.", 1, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that. position in the sentence and the number of votes for each word is marked in parentheses.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["Based on vote counts, there are three alternatives in the example: \u201ccat sat on the mat\u201d, \u201ccat on the mat\u201d and \u201ccat sitting on the mat\u201d, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word \u2018sat\u2019 and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, \u201ccat on the mat\u201d. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length. The only difference to word error rate is that the TER allows shifts.", "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding. For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. System combination has been shown to improve classification performance in various tasks.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1, "Improved Word-Level System Combination for Machine Translation", "ABSTRACT"], ["For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used. In this work, modified Powell\u2019s method as proposed by (Brent, 1973) is used. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["In this work, modified Powell\u2019s method as proposed by (Brent, 1973) is used. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search. To improve the chances of finding a global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search. To improve the chances of finding a global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs. Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work.", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Confusion networks are generated by choosing one hypothesis as the \u201cskeleton\u201d, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007). The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER.", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Figure 2: Three confusion networks with prior probabilities. specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991). A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations.", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 0, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.", "Note that the algorithm of Rosti et al. (2007) used N -best lists in the combination.", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding. For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \u201cI saw the blue trees was found\u201d in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding. For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.", "@2009 Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task, e.g. Rosti et al. (2007), each compo\u00ad nent system produces a set of translations, which are then grafted to form a confusion network.", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to \u201cplay\u201d against this, and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations. For example, Rosti et al. (2007) report such an effect.", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991). A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work. System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that. position in the sentence and the number of votes for each word is marked in parentheses.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["Based on vote counts, there are three alternatives in the example: \u201ccat sat on the mat\u201d, \u201ccat on the mat\u201d and \u201ccat sitting on the mat\u201d, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word \u2018sat\u2019 and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, \u201ccat on the mat\u201d. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length. The only difference to word error rate is that the TER allows shifts.", "In addition, it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs. Different alignment methods yield different confusion networks. The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning. As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton. Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors. Also the word-level decoding may break coherent phrases produced by the individual systems.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.", "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER. This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Compared to the baseline system which is also optimized for TER, the BLEU score is improved by 0.97 points. Also, the METEOR score using the METEOR optimized weights is very high. However, the other scores are worse in common with the tuning set results. The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The combination outputs from confusion network decoding may be ungrammatical due to alignment errors. Also the word-level decoding may break coherent phrases produced by the individual systems. In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences. This allows a log-linear addition of arbitrary features such as language model (LM) scores. The LM scores should increase the total log-posterior of more grammatical hypotheses.", "Other scores for the word arc are set as in (Rosti et al., 2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Again, the best scores on each metric are obtained by the combination tuned for that metric. Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F - other combinations are better than any single system on all metrics apart from the METEOR tuned combinations. The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05. terms of METEOR. Compared to the baseline, the BLEU score of the BLEU tuned combination is improved by 1.47 points.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Also, the METEOR score using the METEOR optimized weights is very high. However, the other scores are worse in common with the tuning set results. The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU. Again, the best scores on each metric are obtained by the combination tuned for that metric.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work. System weights may be used to assign a system specific confidence on each word in the network.", "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set. to the pairwise TER alignment described in (Rosti et al., 2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991). A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) \u00b7 p(ei |e).", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions. We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent. Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969\u2013976, Sydney, July 2006.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) \u00b7 p(ei |e).", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["\u2022 A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}. 2.1 Baseline: IBM Model-1. The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004).", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E\u2217 = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity. In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair. A unique normalized and real-valued vector \u03b8, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions. Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["All the plates represent replicates. The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair. (a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair. a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic. Given a specific topic-weight vector \u03b8d for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics. Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I \u03b2 e I a \u03b1 \u03b8 z f J B N M \u03b1 \u03b8 z a a f J B \u03b1 \u03b8 z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and a hexagon denotes a parameter. Un-shaded nodes are hidden variables.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We conclude with a brief discussion in section 6. In statistical machine translation, one typically uses parallel data to identify entities such as \u201cword-pair\u201d, \u201csentence-pair\u201d, and \u201cdocument- pair\u201d. Formally, we define the following terms1: \u2022 A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. \u2022 A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.\u2022 A document-pair (F, E) refers to two doc uments which are translations of each other. Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair. \u2022 A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["2.1 Baseline: IBM Model-1. The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004). We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["83 79 0. 61 16 0. 02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean). The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!\ufffd:South Korean).", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Laplace smoothing is chosen to emphasize more on BiTAM\u2019s strength. Without any smoothing, F- measure drops very quickly over two topics. In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models. We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish. Choosing the number of topics is a model selection problem.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["For example, Chinese words \u201cde\u201d (ffl) , \u201cba\u201d (I\\) and \u201cbei\u201d (%i) generally do not have translations in English. eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair. Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters \u03b1 and B in an unsupervised fashion. Essentially, Eqs. (810) above constitute the E-step, where the posterior estimations of the latent variables are obtained.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["English-French, i.e., e \u2194 f , although our models are tested,in this paper, for EnglishChinese. We use the end-user ter minology for source and target languages. In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics. Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I \u03b2 e I a \u03b1 \u03b8 z f J B N M \u03b1 \u03b8 z a a f J B \u03b1 \u03b8 z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["83 79 0. 61 16 0. 02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean). The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!\ufffd:South Korean).", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments. As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1. A close look at the three BiTAMs does not yield significant difference.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model. Similar improvements over IBM models and HMM are preserved after applying the three kinds of heuristics in the above. As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic. We also test BiTAM3 on large training data, and similar improvements are observed over those of the baseline models (see Table. 5).", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Under a non-symmetric Dirichlet prior, hyperparameter \u03b1 is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1. Better initialization of B can help to avoid local optimal as shown in \u00a7 5.5. With the learned B and \u03b1 fixed, the variational parameters to be computed in Eqn. (810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10\u22125. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["1, p(f |e) is learned via IBM1; \u03bb is estimated via EM on held out data. 4.3 Retrieving Word Alignments. Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA). Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix \u03d5 \u2261 {\u03d5dnji}. UDA uses a French word fdnj (at the jtth position of ntth sentence in the dtth document) to query \u03d5 to get the best aligned English word (by taking the maximum point in a row of \u03d5): adnj = arg max \u03d5dnji .", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). Additional heuristics are applied to further improve the accuracies. Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6. 45 8 15 .7 0 6. 82 2 17 .7 0 6.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["(810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10\u22125. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs. To estimate B, \u03b2 (for BiTAM2) and \u03b1, at most eight variational EM iterations are run on the training data. Figure 2 shows absolute 2\u223c3% better F-measure over iterations of variational EM using two and three topics of BiTAM1 comparing with IBM1. BiTam with Null and Laplace Smoothing Over Var.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["(1) j=1 i=1 1 We follow the notations in (Brown et al., 1993) for. English-French, i.e., e \u2194 f , although our models are tested,in this paper, for EnglishChinese. We use the end-user ter minology for source and target languages. In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k \u221d ) ) ) ) \u03b4(f, fj )\u03b4(e, ei )\u03c6dnk \u03d5dnji (12) d n=1 j=1 i=1 For \u03b1, close-form update is not available, and we resort to gradient accent as in (Sjo\u00a8 lander et al., 1996) with restarts to ensure each updated \u03b1k >0. 4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing.", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003).", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Note that, the sentence-pairs are now connected by the node \u03b8d. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector \u03b8d. Specifically, BiTAM1 assumes that each sentence-pair has one single topic. Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["The conditional \u03d5dnji \u221d exp ) \u03c6dnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where \u03a8(\u00b7) is a digamma function. Note that inthe formulas in \u00a73.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas \u03c6 dnkis the variational param 3.4 Incorporation of Word \u201cNull\u201d. Similar to IBM models, \u201cNull\u201d word is used for the source words which have no translation counterparts in the target language. For example, Chinese words \u201cde\u201d (ffl) , \u201cba\u201d (I\\) and \u201cbei\u201d (%i) generally do not have translations in English. eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair. A unique normalized and real-valued vector \u03b8, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions. Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001).", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["In the M-step, we update \u03b1 and B so that they improve a lower bound of the log-likelihood defined bellow: L(\u03b3, \u03c6, \u03d5; \u03b1, B) = Eq [log p(\u03b8|\u03b1)]+Eq [log p(z|\u03b8)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]\u2212Eq [log q(\u03b8)] \u2212Eq [log q(z)]\u2212Eq [log q(a)]. (11) The close-form iterative updating formula B is: BDA selects iteratively, for each f , the best aligned e, such that the word-pair (f, e) is the maximum of both row and column, or its neighbors have more aligned pairs than the other combpeting candidates.A close check of {\u03d5dnji} in Eqn. 10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k \u221d ) ) ) ) \u03b4(f, fj )\u03b4(e, ei )\u03c6dnk \u03d5dnji (12) d n=1 j=1 i=1 For \u03b1, close-form update is not available, and we resort to gradient accent as in (Sjo\u00a8 lander et al., 1996) with restarts to ensure each updated \u03b1k >0. 4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["3.2 BiTAM2: Monolingual Admixture. In general, the monolingual model for English can also be a rich topic-mixture. This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable.", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["\u2022 A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}. 2.1 Baseline: IBM Model-1. The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004).", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["For simplicity, we assume that the French words fj \u2019s are conditionally independent of each other; the alignment variables aj \u2019s are independent of other variables and are uniformly distributed a priori. Therefore, the distribution for each sentence-pair is: p(fn , an |en , Bzn) = p(fn |en , an , Bzn)p(an |en , Bzn) Jn \u201cNull\u201d is attached to every target sentence to align the source words which miss their translations. Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ). (6) contains only one word: \u201cNull\u201d, and the alignment link a is no longer a hidden variable. Thus, the conditional likelihood for the entire parallel corpus is given by taking the product of the marginal probabilities of each individual document-pair in Eqn.", "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001).", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models. Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models. The proposed models significantly improve the alignment accuracy and lead to better translation qualities. Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Conclusion."], ["This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003).", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics. For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Under a non-symmetric Dirichlet prior, hyperparameter \u03b1 is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1. Better initialization of B can help to avoid local optimal as shown in \u00a7 5.5. With the learned B and \u03b1 fixed, the variational parameters to be computed in Eqn. (810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10\u22125. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs.", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["\u2022 A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}. 2.1 Baseline: IBM Model-1. The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004).", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["5. 3.2 BiTAM2: Monolingual Admixture. In general, the monolingual model for English can also be a rich topic-mixture. This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Note that, the sentence-pairs are now connected by the node \u03b8d. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector \u03b8d. Specifically, BiTAM1 assumes that each sentence-pair has one single topic. Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics. For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation.", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004). We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework. Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E\u2217 = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001).", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["This result suggests a straightforward way to leverage BiTAMs to improve statistical machine translations. In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models. Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models. The proposed models significantly improve the alignment accuracy and lead to better translation qualities. Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Conclusion."], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004). We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework. Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E\u2217 = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity. In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["5.4 Evaluating Word. Alignments We evaluate word alignment accuracies in various settings. Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). Additional heuristics are applied to further improve the accuracies. Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments. As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1. A close look at the three BiTAMs does not yield significant difference. BiTAM3 is slightly better in most settings; BiTAM1 is slightly worse than the other two, because the topics sampled at the sentence level are not very concentrated.", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable. A variational inference is used to approximate the true posteriors of these hidden variables. The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Now e is generated Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable. A variational inference is used to approximate the true posteriors of these hidden variables. The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted. 4.1 Variational Approximation.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models. We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish. Choosing the number of topics is a model selection problem. We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets. The overall computation complexity of the BiTAM is linear to the number of hidden topics.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = \u03bbBf,e,k +(1\u2212\u03bb)p(f |e). (13) As in Eqn.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = \u03bbBf,e,k +(1\u2212\u03bb)p(f |e). (13) As in Eqn. 1, p(f |e) is learned via IBM1; \u03bb is estimated via EM on held out data.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004). In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system. We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Introduction"], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["We built a corpus of 20 multi-document clusters ofcomplex news stories, such as plane crashes, political controversies and natural disasters. The data 1The stationary distribution is unique and the power methodis guaranteed to converge provided that the Markov chain isergodic (Seneta, 1981). A non-ergodic Markov chain can bemade ergodic by reserving a small probability for jumping toany other state from the current state (Page et al., 1998). clusters and their characteristics are shown in Table 1. The news articles were collected from varioussources.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["Currently, we address the question-focused sentence retrieval task. While passage retrieval (PR) isclearly not a new problem (e.g. (Robertson et al.,1992; Salton et al., 1993)), it remains important andyet often overlooked. As noted by (Gaizauskas et al.,2004), while PR is the crucial first step for questionanswering, Q&A research has typically not empha915 Hurricane Isabel's outer bands moving onshoreproduced on 09/18, 6:18 AM 2% SummaryThe North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as faraway as Pennsylvania prepared for possibly ruinous flooding. (2:3) A hurricane warning was in effect from CapeFear in southern North Carolina to the VirginiaMaryland line, and tropical storm warnings extended from South Carolinato New Jersey.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Introduction"], ["Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question. This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system. In the context of answering questions from complex stories, where thereis often more than one correct answer to a question,and where answers are typically time-dependent, weshould focus on maximizing TRDR, which gives us 2For clusters annotated by two judges, all sentences chosenby at least one judge were included. a measure of how many of the relevant sentenceswere identified by the system.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question. In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.", "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1. 4.2 Evaluation metrics and methods. To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment. To compare different configurations of our system to the baselinesystem, we produced extracts at a fixed length of 20sentences. While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for several reasons.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["He or she then generated a list of factualquestions key to understanding the story. Once wecollected the questions for each cluster, two judgesindependently annotated nine of the training clusters. For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question. Once an acceptable rate of inter-judge agreement was verified on the first nine clusters (Kappa (Carletta, 1996) of 0.68), the remaining11 clusters were annotated by one judge each.In some cases, the judges did not find any sentences containing the answer for a given question.Such questions were removed from the corpus. Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["submarine apparently caused the Kursk to sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, has provided his Russiancounterpart with a written summary of whatU.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials said Wednesday.5 There has been no final word on what caused 0.0123 N the submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory that Kursk may have collided with anotherobject is receiving increasingly concrete confirmation. Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003). In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores.", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis. 3.1 The LexRank method. In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["Therefore, there is often morethan one correct answer to a question.We aim to develop a method for sentence retrieval that goes beyond finding sentences that aresimilar to a single query. To this end, we propose to use a stochastic, graph-based method. Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004). In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Introduction"], ["Table 1: Corpus of complex news stories. and sentence numbers of the top 20 sentences. Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question. This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mi-halcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["It is clear that ahigh question bias is needed. However, a small probability for jumping to a node that is lexically similar to the given sentence (rather than the questionitself) is needed. Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions. We appliedall four of these configurations to our unseen development/test data, in order to see if we could furtherdifferentiate their performances. 5.1 Development/testing phase.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "LexRank versus the baseline system."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["This is similar to snippet retrieval (Wu etal., 2004). However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis. 3.1 The LexRank method. In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set.", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively. This model hasproven to be successful in query-based sentence retrieval (Allan et al., 2003), and is used as our competitive baseline in this study (e.g. Tables 4, 5 and7). 3.3 The mixture model.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Table 1: Corpus of complex news stories. and sentence numbers of the top 20 sentences. Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question. This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question. In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["3.1 The LexRank method. In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["The baseline system explained above does not makeuse of any inter-sentence information in a cluster.We hypothesize that a sentence that is similar tothe high scoring sentences in the cluster should alsohave a high score. For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.This idea is captured by the following mixture model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its relevance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster: p(s|q) = d rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) where C is the set of all sentences in the cluster. Thevalue of d, which we will also refer to as the question bias, is a trade-off between two terms in the Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence 1 0.03614457831325301 At least two people are dead, inclu... 0 0.28454242157110576 Officials said the plane was carryin...", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["While we saw in Section 5.1 thatLR[0.20,0.95] may perform better on some questionor cluster types than others, we conclude that it beatsthe competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions. However, in future work, we will continueto improve the performance, perhaps by developing mixed strategies using different configurationsof LexRank. The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences. When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties. An example of such a scenario is illustrated inTables 8 and 9, which show the top ranked sentencesby the baseline and LexRank, respectively for thequestion What caused the Kursk to sink? from theKursk submarine cluster.", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Discussion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003). In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores. We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine). Our system does not rank the input documents, nor is it restricted in terms of the number ofsentences that may be selected from the same document. The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused summary. This is similar to snippet retrieval (Wu etal., 2004).", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Our system does not rank the input documents, nor is it restricted in terms of the number ofsentences that may be selected from the same document. The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused summary. This is similar to snippet retrieval (Wu etal., 2004). However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["3.1 The LexRank method. In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15. Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality.", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004). In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system. We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Introduction"], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["The stationary distributionof a Markov chain can be computed by a simple iterative algorithm, called power method.1 A simpler version of Equation 5, where A is auniform matrix andB is a normalized binary matrix,is known as PageRank (Brin and Page, 1998; Pageet al., 1998) and used to rank the web pages by theGoogle search engine. It was also the model used torank sentences in (Erkan and Radev, 2004). 3.4 Experiments with topic-sensitive LexRank. We experimented with different values of d on ourtraining data. We also considered several thresholdvalues for inter-sentence cosine similarities, wherewe ignored the similarities between the sentencesthat are below the threshold.", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15. Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality.", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["3.1 The LexRank method. In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003). In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores. We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003). In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores. We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered. Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus. In future work, this parameter can be learned using a development corpus.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Experimental results, for all relations and the two different corpus sizes, show that Espresso greatly outperforms the other two methods on precision. However, Espresso fails to match the recall level of RH02 in all but the experiment on the production relation. Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous \u2013 hence resulting in a loss of recall. As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise in the system (e.g, the pattern [X of Y] can ambiguously refer to a part-of, is-a or possession relation). However, generic patterns, while having low precision, yield a high recall, as also reported by [11].", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts. The system architecture is designed with generality in mind, avoiding any relation-specific inference technique. Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["We also plan to investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA. The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for evaluating the outputs of the systems.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Conclusions."], ["In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "ABSTRACT"], ["Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora. From the other side, Espresso requires only weak human supervision. In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["\u0083 ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas\u00b8ca et al., 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds.. Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. E CHEM corpus) and evaluating their quality manually using one human judge2.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. E CHEM corpus) and evaluating their quality manually using one human judge2. For each instance, the judge may assign a score of 1 for correct, 0 for incorrect, and \u00bd for partially correct.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members\u2019 lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. The Espresso algorithm is based on a similar framework to the one adopted in [12].", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Furthermore, it is important to note that there are several other generic patterns, like [X\u2019s Y], from which we expect a similar precision of 50% with a continual increase of recall. This is a very exciting avenue of further investigation. We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text. Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances. There are many avenues of future work.", "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Conclusions."], ["In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "ABSTRACT"], ["The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["We preprocess the corpora using the Alembic Workbench POStagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: \u0083 RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) \u0083 PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members\u2019 lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["4.2. Precision and Recall. We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds.. Table 1.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["As expected, we obtained 1923 instances instead of the 40 reported in Table 7, but precision dropped from 85% to 30%. The challenge, then, is to harness the expressive power of the generic patterns whilst maintaining the precision of Espresso. We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns. Unlike Girju et al. [11] that propose a highly supervised machine learning approach based on selectional restriction, ours is an unsupervised method based on statistical evidence obtained from the Web. At a given iteration in Espresso, the intuition behind our solution is that the Web is large enough that correct instances will be instantiated by many of the currently accepted patterns P. Hence, we can distinguish between generic patterns and incorrect patterns by inspecting the relative frequency distribution of their instances using the patterns in P. More formally, given an instance i produced by a generic or incorrect pattern, we count how many times i instantiates on the Web with every pattern in P, using Google.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Terms are commonly defined as surface representations of stable and key domain concepts [19]. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points. Hence, we can capture relations between complex terms, such as \u201crecord of a criminal conviction\u201d part-of \u201cFBI report\u201d.", "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["In this section, we present a preliminary comparison of Espresso with two state of the art systems on the task of extracting various semantic relations. 4.1.1. Datasets We perform our experiments using the following two datasets: \u0083 TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 \u2013 AP890131, AP890201 \u2013 AP890228, and AP890310 \u2013 AP890319. \u0083 CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2].", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora. From the other side, Espresso requires only weak human supervision. In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus. In future work, this parameter can be learned using a development corpus. Our reliability measure ensures that overly generic patterns, which may potentially have very low precision, are discarded. However, we are currently exploring a web-expansion algorithm that could both help detect generic patterns and also filter out their incorrect instances.", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP\u2019s part- NP].", "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out. A set of new instances is created for each instance i\u2208 I by extracting sub-terminological expressions from x corresponding to the syntactic head of terms. For example, expanding the relation \u201cnew record of a criminal conviction\u201d part-of \u201cFBI report\u201d, the following new instances are obtained: \u201cnew record\u201d part-of \u201cFBI report\u201d, and \u201crecord\u201d part-of \u201cFBI report\u201d. 3.5. Phase 4: Instance filtering.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["4.2. Precision and Recall. We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds.. Table 1.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["4.3. Discussion. Experimental results, for all relations and the two different corpus sizes, show that Espresso greatly outperforms the other two methods on precision. However, Espresso fails to match the recall level of RH02 in all but the experiment on the production relation. Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous \u2013 hence resulting in a loss of recall.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible. To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members\u2019 lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.", "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible. To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.", "Hearst\u2019s method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts. The system architecture is designed with generality in mind, avoiding any relation-specific inference technique. Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["We also plan to investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA. The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for evaluating the outputs of the systems.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Conclusions."], ["4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["\u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["\u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples. The seeds were used for both Espresso as well as RH021.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "PAPER"], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Negra (Dubey and Keller, 2003); English, sections 221 (train) and section 23 (test).", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented.", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented.", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our evaluation includes both weighted and un- weighted lattices. We weight edges using a unigram language model estimated with Good- Turing smoothing. Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005). MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution.", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007). However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance. 4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun \ufffd '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1). more frequently than is done in English. Process nominals name the action of the transitive or ditransitive verb from which they derive.", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsing\u2014including the experiments in the previous section\u2014have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005).", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["A cell in the bottom row of the parse chart is required for each potential whitespace boundary. As we have said, parse quality decreases with sentence length. Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew. Table 9: Dev set results for sentences of length \u2264 70. Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u2019an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005).", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima\u2019an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["87 Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsing\u2014including the experiments in the previous section\u2014have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our evaluation includes both weighted and un- weighted lattices. We weight edges using a unigram language model estimated with Good- Turing smoothing. Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005). MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["87 Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["For each 13 Of course, this weighting makes the PCFG an improper distribution. However, in practice, unknown word models also make the distribution improper. Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["As we have said, parse quality decreases with sentence length. Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew. Table 9: Dev set results for sentences of length \u2264 70. Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference. Each model was able to produce hypotheses for all input sentences.", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["In particular, the decision to represent arguments in verb- initial clauses as VP internal makes VSO and VOS configurations difficult to distinguish. Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop). 3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007). However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance. 4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun \ufffd '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (-) is an elongation character used in Arabic script to justify text. It has no syntactic function.", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs. We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008). For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead). Base NPs are the other significant category of nominal phrases.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add \u2217n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity. Human evaluation is one way to distinguish between the two cases. Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error. The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus. The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Process nominals name the action of the transitive or ditransitive verb from which they derive. The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case. When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct \ufffd ?f iDafa. Gabbard and Kulick (2008) show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set. Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["But for eign learners are often surprised by the verbless predications that are frequently used in Arabic. Although these are technically nominal, they have become known as \u201cequational\u201d sentences. mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences. We also mark all nodes that dominate an SVO configuration (containsSVO). In MSA, SVO usually appears in non-matrix clauses.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings.", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as \ufffd wa and \ufffd fa to link new elements to both preceding clauses and the text as a whole. As a result, Arabic sentences are usually long relative to English, especially after Length English (WSJ) Arabic (ATB) \u2264 20 41.9% 33.7% \u2264 40 92.4% 73.2% \u2264 63 99.7% 92.6% \u2264 70 99.9% 94.9% Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data.", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Negra (Dubey and Keller, 2003); English, sections 221 (train) and section 23 (test).", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions. 7 Unlike Dickinson (2005), we strip traces and only con-. Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7). In our grammar, features are realized as annotations to basic category labels. We start with noun features since written Arabic contains a very high proportion of NPs. genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7). In our grammar, features are realized as annotations to basic category labels. We start with noun features since written Arabic contains a very high proportion of NPs. genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs.", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters. Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings. However, when we pre- tag the input\u2014as is recommended for English\u2014 we notice a 0.57% F1 improvement.", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["First we mark any node that dominates (at any level) a verb sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing (e.g., unaries, base NPs). Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei. 8 We use head-finding rules specified by a native speaker. of Arabic. This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.", "For Arabic, we use the head-finding rules from Green and Manning (2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Various verbal (e.g., \ufffd, .::) and adjectival. suffixes (e.g., \ufffd=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We also collapse unary chains withidentical basic categories like NP \u2192 NP. The pre terminal morphological analyses are mapped to the shortened \u201cBies\u201d tags provided with the tree- bank. Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Each model was able to produce hypotheses for all input sentences. In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus. Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["For example, we might have VP \u2192 VB NP PP, where the NP is the subject. This annotation choice weakens splitIN. We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers. All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al. (2006).", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["We also collapse unary chains withidentical basic categories like NP \u2192 NP. The pre terminal morphological analyses are mapped to the shortened \u201cBies\u201d tags provided with the tree- bank. Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters. Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings. However, when we pre- tag the input\u2014as is recommended for English\u2014 we notice a 0.57% F1 improvement. We use the log-linear tagger of Toutanova et al.", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.", "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets.19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs. We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008). For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead). Base NPs are the other significant category of nominal phrases.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add \u2217n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity. Human evaluation is one way to distinguish between the two cases. Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error. The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus. The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ. The results clearly indicate increased variation in the ATB relative to the WSJ, but care should be taken in assessing the magnitude of the difference. On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample. At the same time, the n-gram error rate is sensitive to samples with extreme n-gram counts. For example, one of the ATB samples was the determiner -\"\" ; dhalik\u201cthat.\u201d The sample occurred in 1507 corpus po sitions, and we found that the annotations were consistent.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["It then computes a normalized Levenshtein edit distance between the extracted chain and the reference. The range of the score is between 0 and 1 (higher is better). We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB. Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation. with the number of exactly matching guess trees.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3). A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990). We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c). 3.1 Gross Statistics. Linguistic intuitions like those in the previous section inform language-specific annotation choices.", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217. Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed.", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75).", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB), and for our test corpus we follow their discussion and use the same data set, the training section of three parts of the ATB (Maamouri et al., 2008a; Maamouri et al., 2009; Maamouri et al., 2008b).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993). In the ATB, :: b asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["\u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment. input token, the segmentation is then performed deterministically given the 1-best analysis. Since guess and gold trees may now have different yields, the question of evaluation is complex. Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsing\u2014including the experiments in the previous section\u2014have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3). A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990). We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c). 3.1 Gross Statistics. Linguistic intuitions like those in the previous section inform language-specific annotation choices.", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Negra (Dubey and Keller, 2003); English, sections 221 (train) and section 23 (test).", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase. All three models evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly. For parsing, the most challenging form of ambiguity occurs at the discourse level. A defining characteristic of MSA is the prevalence of discourse markers to connect and subordinate words and phrases (Ryding, 2005). Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as \ufffd wa and \ufffd fa to link new elements to both preceding clauses and the text as a whole.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217.", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["8 We use head-finding rules specified by a native speaker. of Arabic. This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses. termined by the category of the word that follows it. Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Various verbal (e.g., \ufffd, .::) and adjectival. suffixes (e.g., \ufffd=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "PAPER"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["input token, the segmentation is then performed deterministically given the 1-best analysis. Since guess and gold trees may now have different yields, the question of evaluation is complex. Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (\u03bc Constituents / \u03bc Length). Evalb, the standard parsing metric, is biased toward such corpora (Sampson and Babarczy, 2003). Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic. In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance. 3.2 Inter-annotator Agreement.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "PAPER"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["input token, the segmentation is then performed deterministically given the 1-best analysis. Since guess and gold trees may now have different yields, the question of evaluation is complex. Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005).", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217.", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics. Particles are uninflected. Word Head Of Complement POS 1 '01 inna \u201cIndeed, truly\u201d VP Noun VBP 2 '01 anna \u201cThat\u201d SBAR Noun IN 3 01 in \u201cIf\u201d SBAR Verb IN 4 01 an \u201cto\u201d SBAR Verb IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1). Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues. Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness. Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009).", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al. (2006). Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP \u2192 NP.", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case. When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct \ufffd ?f iDafa. Gabbard and Kulick (2008) show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set. Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase. All three models evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our evaluation includes both weighted and un- weighted lattices. We weight edges using a unigram language model estimated with Good- Turing smoothing. Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005). MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima\u2019an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al. (2006). Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP \u2192 NP.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus. Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine). To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (-) is an elongation character used in Arabic script to justify text.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["If we remove this sample from the evaluation, then the ATB type-level error rises to only 37.4% while the n-gram error rate increases to 6.24%. The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions. 7 Unlike Dickinson (2005), we strip traces and only con-. Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["We also collapse unary chains withidentical basic categories like NP \u2192 NP. The pre terminal morphological analyses are mapped to the shortened \u201cBies\u201d tags provided with the tree- bank. Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4). The ATB is disadvantaged by having fewer trees with longer average 5 LDC A-E catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1). We map the ATB morphological analyses to the shortened \u201cBies\u201d tags for all experiments. yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (\u03bc Constituents / \u03bc Length). Evalb, the standard parsing metric, is biased toward such corpora (Sampson and Babarczy, 2003).", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["(2006). Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP \u2192 NP. The pre terminal morphological analyses are mapped to the shortened \u201cBies\u201d tags provided with the tree- bank.", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["3.2 Inter-annotator Agreement. Annotation consistency is important in any supervised learning task. In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al., 2008). To improve agreement during the revision process, a dual-blind evaluation was performed in which 10% of the data was annotated by independent teams. Maamouri et al.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["8 We use head-finding rules specified by a native speaker. of Arabic. This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses. termined by the category of the word that follows it. Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku\u00a8 bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajic\u02c7 and Zema\u00b4nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages. But Arabic contains a variety of linguistic phenomena unseen in English.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrase- structure treebank.", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier. Such a classification can be seen as a not-always-correct summary of global features. The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document. We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borth- wick (1999) successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages. Recently, statistical NERs have achieved results that are comparable to hand-coded systems. Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance. MUC7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al., 1998; Borthwick, 1999), notably Mikheev' s system, which achieved the best performance of 93.39% on the official NE test data. MENE (Maximum Entropy Named Entity) (Borth- wick, 1999) was combined with Proteus (a hand- coded system), and came in fourth among all MUC 7 participants.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Related Work."], ["Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999). This might be because our features are more comprehensive than those used by Borthwick.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borth- wick (1999) successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique). To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise. The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed. Such constraints are derived from training data, expressing some relationship between features and outcome. The probability distribution that satisfies the above property is the one with the highest entropy. It is unique, agrees with the maximum-likelihood distribution, and has the exponential form (Della Pietra et al., 1997): where refers to the outcome, the history (or context), and is a normalization function. In addition, each feature function is a binary function.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities. The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules. We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997). Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1. This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter). Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1. Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task. The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al., 1999).", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1. 4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document. needs to be in initCaps to be considered for this feature. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["We have estimated the performance of IdentiFinder ' 99 at 200K words of training data from the graphs. For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles. In fact, training on the official training data is not suitable as the articles in this data set are entirely about aviation disasters, and the test data is about air vehicle launching. Both BBN and NYU have tagged their own data to supplement the official training data. Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution (Borthwick, 1999).", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["Both BBN and NYU have tagged their own data to supplement the official training data. Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution (Borthwick, 1999). Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results. The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier.", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["Mikheev et al. (1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities. The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules. We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["Such a classification can be seen as a not-always-correct summary of global features. The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document. We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre. Moreover, if we want to test on a huge test corpus, indexing the whole corpus might prove computationally expensive. Hence we decided to restrict ourselves to only information from the same document.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC).", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["Otherwise, it is set to 0. Lexicon Feature: The string of the token is used as a feature. This group contains a large number of features (one for each token string present in the training data). At most one feature in this group will be set to 1. If is seen infrequently during training (less than a small count), then will not be selected as a feature and all features in this group are set to 0.", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["On the other hand, if it is seen as McCann Pte. Ltd., then organization will be more probable. With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1. Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1. 4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999). This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used.", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document. Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique. For example, if FCC and Federal Communications Commission are both found in a document, then Federal has A begin set to 1, Communications has A continue set to 1, Commission has A end set to 1, and FCC has A unique set to 1. Sequence of Initial Caps (SOIC): In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["For corporate suffixes, a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data. Frequency is calculated by counting the number of distinct previous tokens that each token has (e.g., if Electric Corp. is seen 3 times, and Manufacturing Corp. is seen 5 times during training, and Corp. is not seen with any other preceding tokens, then the \u201cfrequency\u201d of Corp. is 2). The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List. A Person-Prefix-List is compiled in an analogous way. For MUC6, for example, Corporate- Suffix-List is made up of ltd., associates, inc., co, corp, ltd, inc, committee, institute, commission, university, plc, airlines, co., corp. and Person-Prefix- List is made up of succeeding, mr., rep., mrs., secretary, sen., says, minister, dr., chairman, ms. . For a token that is in a consecutive sequence of init then a feature Corporate-Suffix is set to 1.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["If they are found in a list, then a feature for that list will be set to 1. For example, if Barry is not in commonWords and is found in the list of person first names, then the feature PersonFirstName will be set to 1. Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1. For example, if is found in the list of person first names, the feature PersonFirstName is set to 1. Month Names, Days of the Week, and Numbers: If is initCaps and is one of January, February, . . .", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens. This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training. Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones). The zone to which a token belongs is used as a feature.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al., 1999). The sources of our dictionaries are listed in Table 2. For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The sources of our dictionaries are listed in Table 2. For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. A list of words occurring more than 10 times in the training data is also collected (commonWords).", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. A list of words occurring more than 10 times in the training data is also collected (commonWords). Only tokens with initCaps not found in commonWords are tested against each list in Table 2.", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used. We group the features used into feature groups.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used. We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999). This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens. This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training. Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones). The zone to which a token belongs is used as a feature.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1. Otherwise, it is set to 0.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999). This might be because our features are more comprehensive than those used by Borthwick.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["(1997; 1999) do have some statistics that show how IdentiFinder performs when the training data is reduced. Our results show that MENERGI performs as well as IdentiFinder when trained on comparable amount of training data. The system described in this paper is similar to the MENE system of (Borthwick, 1999). It uses a maximum entropy framework and classifies each word given its features. Each name class is subdivided into 4 sub-classes, i.e., N begin, N continue, N end, and N unique.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document. needs to be in initCaps to be considered for this feature. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution (Borthwick, 1999). Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results. The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier. Such a classification can be seen as a not-always-correct summary of global features.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework. The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors). These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999).", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999). We believe it is natural for authors to use abbreviations in subsequent mentions of a named entity (i.e., first \u201cPresident George Bush\u201d then \u201cBush\u201d). As such, global information from the whole context of a document is important to more accurately recognize named entities. Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages. Recently, statistical NERs have achieved results that are comparable to hand-coded systems.", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["Sentence (2) and (3) help to disambiguate one way or the other. If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided. The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. For example, in the sentence that starts with \u201cBush put a freeze on . . .", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided. The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. For example, in the sentence that starts with \u201cBush put a freeze on . . . \u201d, because Bush is the first word, the initial caps might be due to its position (as in \u201cThey put a freeze on . . .", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules. We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997). Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly.", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages. Recently, statistical NERs have achieved results that are comparable to hand-coded systems. Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance. MUC7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al., 1998; Borthwick, 1999), notably Mikheev' s system, which achieved the best performance of 93.39% on the official NE test data. MENE (Maximum Entropy Named Entity) (Borth- wick, 1999) was combined with Proteus (a hand- coded system), and came in fourth among all MUC 7 participants.", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Related Work."], ["The reason why we did not train with both MUC6 and MUC7 training data at the same time is because the task specifications for the two tasks are not identical. As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3. In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999). IdentiFinder ' 99' s results are considerably better than IdentiFinder ' 97' s. IdentiFinder' s performance in MUC7 is published in (Miller et al., 1998). MENE has only been tested on MUC7.", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework. The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors).", "A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["On the other hand, if it is seen as McCann Pte. Ltd., then organization will be more probable. With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1. Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The zone to which a token belongs is used as a feature. For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD). Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0. Case and Zone: If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1. If it is made up of all capital letters, then (allCaps, zone) is set to 1.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique). To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise. The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1. Otherwise, it is set to 0.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation. Most natural languages construct words by concatenating morphemes together in strict orders.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In other applications, it may be useful to design the lexical strings to contain the traditional dictionary citation form, together with linguist-selected \u201ctag\u201d sym Analysis Strings Regular Expression Compiler F S T Word Strings Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages bols like +Noun, +Verb, +SG, +PL, that convey category, person, number, tense, mood, case, etc. Thus the lexical string representing paie, the first-person singular, present indicative form of the French verb payer (\u201cto pay\u201d), might be spelled payer+IndP+SG+P1+Verb. The tag symbols are stored and manipulated just like alphabetic symbols, but they have multicharacter print names. If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally. Following convention, we will often refer to the upper projection of the fst, representing analyses, as the lexical language, a set of lexical strings; and we will refer to the lower projection as the surface language, consisting of surface strings. There are compelling advantages to computing with such finite-state machines, including mathematical elegance, flexibility, and for most natural-language applications, high efficiency and data-compaction.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 1, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine. It expects to find delimited regular expression substrings on a given side (upper or After the special treatment of the regular- expression path is finished, normal processing is resumed in the destination state of the closing ^] arc. For example, the result shown in Figure 9 represents the crossproduct of the two networks shown in Figure 10. a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced. In the linguistic applications presented Lexical: b a g i +Noun +Plural Surface: ^[ { b a g i } ^ 2 ^] Lexical: p e l a b u h a n +Noun +Plural Surface: ^[ { p e l a b u h a n } ^ 2 ^] Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation in the next sections, the two sides of a regular- expression path contain different strings. The upper side contains morphological information; the regular-expression operators appear only on the lower side and are not present in the final result.", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["To introduce the merge operation into the Xerox regular expression calculus we need to choose an operator symbol. Because merge, like subtraction, is a non-commutative operation, we also must distinguish between the template and the filler. For example, we could choose .m. as the operator and decide by convention which of the two operands plays which role in expressions such as [A .m. B]. What we actually have done, perhaps without a sufficiently good motivation, is to introduce two variants of the merge operator, .<m. and .m>., that differ only with respect to whether the template is to the left (.<m.) or to the right (.m>.) of the filler.", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics. The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific", "Thus, we employ the com\u00ad pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication.", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation. Most natural languages construct words by concatenating morphemes together in strict orders.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm. The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions. The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation. To take a simple non-linguistic example, Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular- expression substring.Figure 9: After the Application of Compile Replace lower) of the network. Until an opening delimiter ^[ is encountered, the algorithm constructs a copy of the path it is following.", "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.", 1, "Finite-State Non-Concatenative Morphotactics", "Compile-Replace."], ["In other applications, it may be useful to design the lexical strings to contain the traditional dictionary citation form, together with linguist-selected \u201ctag\u201d sym Analysis Strings Regular Expression Compiler F S T Word Strings Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages bols like +Noun, +Verb, +SG, +PL, that convey category, person, number, tense, mood, case, etc. Thus the lexical string representing paie, the first-person singular, present indicative form of the French verb payer (\u201cto pay\u201d), might be spelled payer+IndP+SG+P1+Verb. The tag symbols are stored and manipulated just like alphabetic symbols, but they have multicharacter print names. If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally. Following convention, we will often refer to the upper projection of the fst, representing analyses, as the lexical language, a set of lexical strings; and we will refer to the lower projection as the surface language, consisting of surface strings. There are compelling advantages to computing with such finite-state machines, including mathematical elegance, flexibility, and for most natural-language applications, high efficiency and data-compaction.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 0, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["For example, if A is a regular expression denoting a language or a relation, A* denotes zero or more and A+ denotes one or more concatenations of A with itself. There are also operators that express a fixed number of concatenations. In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings. The reduplication of any string w can then be notated as {w}^2, and we start by defining a network where the lower-side strings are built by simple concatenation of a prefix ^[, a root enclosed in braces, and an overt-plural suffix ^2 followed by the closing ^]. Figure 11 shows the paths for two Malay plurals in the initial network.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In Arabic, for example, prefixes and suffixes attach to stems in the usual concatenative way, but stems themselves are formed by a process known informally as interdigitation; while in Malay, noun plurals are formed by a process known as full-stem reduplication. Although Arabic and Malay also include prefixation and suffixation that are modeled straightforwardly by concatenation, a complete lexicon cannot be a a:0 *:a *:0 *:0 0:a obtained without non-concatenative processes. We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm. The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions. The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 0, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["We will show a simple and elegant way to do this with strictly finite-state operations. To understand the general solution to full- stem reduplication using the compile-replace algorithm requires a bit of background. In the regular expression calculus there are several operators that involve concatenation. For example, if A is a regular expression denoting a language or a relation, A* denotes zero or more and A+ denotes one or more concatenations of A with itself. There are also operators that express a fixed number of concatenations.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in Beesley and Karttunen [4].", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In effect, Condition 2 preserves any template arc that does not find a match. In that case, the path in the template network advances to a new state while the path in the filler network stays at the current state. We use the networks in Figure 13 to illustrate the effect of the merge algorithm. Figure 13 shows a linear template network and two filler networks, one of which is cyclic. C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We use the networks in Figure 13 to illustrate the effect of the merge algorithm. Figure 13 shows a linear template network and two filler networks, one of which is cyclic. C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14. The three symbols of the filler string are instantiated in the three consonant slots in the CVVCVC template. d V V r V s Figure 14: Intermediate Result.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["resenting the origin and the destination of 0:^[ a * 0:^] the regular-expression path. Figure 8: A Network with a Regular-Expression Substring on the Lower Side The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation. The network created by the operation is shown in Figure 9. When applied in the \u201cupward\u201d direction, the transducer in Figure 9 maps any string of the infinite a* language into the regular expression from which the language was compiled. The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Following the lines of Kataja and Koskenniemi (1988), we could define intermediate networks with regular-expression substrings that indicate the intersection of suitably encoded roots, templates, and vocalizations (for a formal description of what such regular-expression substrings would look like, see Beesley (1998c; 1998b)). However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge. 3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one. The strings of the filler language consist of ordinary symbols such as d, r, s, u, i. The template expressions may contain special class symbols such as C (= consonant) or V (= vowel) that represent a predefined set of ordinary symbols. The objective of the merge operation is to align the template strings with the filler strings and to instantiate the class symbols of the template as the matching filler symbols.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation. Most natural languages construct words by concatenating morphemes together in strict orders.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We shall argue that the morphotactic limitations of the traditional implementations are the direct result of relying solely on the concatenation operation in morphotactic description. We describe a technique, within the Xerox implementation of finite-state morphology, that corrects the limitations at the source, going beyond concatenation to allow the full range of finite-state operations to be used in morphotac- tic description. Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network. This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below. Before illustrating these applications, we will first outline our general approach to finite-state morphology.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 0, "Finite-State Non-Concatenative Morphotactics", "Introduction"], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["Such \u201cconcatenative morphotactics\u201d can be impressively productive, especially in agglutinative languages like Aymara (Figure 11) or Turkish, and in agglutinative/polysynthetic languages like Inuktitut (Figure 2)(Mallon, 1999, 2). In such languages a single word may contain as many morphemes as an average-length English sentence. Finite-state morphology in the tradition of the Two-Level (Koskenniemi, 1983) and Xerox implementations (Karttunen, 1991; Karttunen, 1994; Beesley and Karttunen, 2000) has been very successful in implementing large-scale, robust and efficient morphological analyzergenerators for concatenative languages, includ ing the commercially important European languages and non-Indo-European examples like 1 I wish to thank Stuart Newton for this example. Finnish, Turkish and Hungarian. However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes: \u201cOnly restricted infixation and reduplication can be handled adequately with the present system.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 0, "Finite-State Non-Concatenative Morphotactics", "Introduction"], ["Similarly, the root ktb can combine with template CVVCVC and ui to produce kuutib, the root drs can combine with CVCVC and ui to form duris, and so forth. tiers of McCarthy (1981) as projections of a multi-level transducer and wrote a small Prolog- based prototype that handled the interdigitation of roots, CV-templates and vocalizations into abstract Arabic stems; this general approach, with multi-tape transducers, has been explored and extended by Kiraz in several papers (1994a; 1996; 1994b; 2000) with respect to Syriac and Arabic. The implementation is described in Kiraz and GrimleyEvans (1999). In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages. Thus Akkadian words were formalized as consisting of morphemes, some of which were combined together by intersection and others of which were combined via concatenation.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["This was the key insight: morphotactic description could employ various finite-state operations, not just concatenation; and languages that required only concatenation were just special cases. By extension, the widely noticed limitations of early finite-state implementations in dealing with non-concatenative morphotactics could be traced to their dependence on the concatenation operation in morphotactic descriptions. This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime. Kay (1987) reformalized the autosegmental 5 These patterns combine what McCarthy (1981). issue here due to lack of space.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["2.2 Morphotactics and Alternations. There are two challenges in modeling natural language morphology: \u2022 Morphotactics \u2022 Phonological/Orthographical Alternations Finite-state morphology models both using regular expressions. The source descriptions may also be written in higher-level notations (e.g. lexc (Karttunen, 1993), twolc (Karttunen and Beesley, 1992) and Replace Rules (Karttunen, 1995; Karttunen, 1996; Kempe and Karttunen, 1996)) that are simply helpful short- hands for regular expressions and that compile, using their dedicated compilers, into finite-state networks. In practice, the most commonly separated modules are a lexicon fst, containing lexical strings, and a separately written set of Lexicon Regular Expression Rule Regular Expression Compiler Lexicon FST .o. Rule FST Lexical Transducer (a single FST) Figure 6: Creation of a Lexical Transducer rule fsts that map from the strings in the lexicon to properly spelled surface strings. The lexicon description defines the morphotactics of the language, and the rules define the alternations.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 0, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The corresponding initial surface form is a regular-expression substring, containing two merge operators, that will be compiled and replaced by the interdigitated surface form. The application of the compile-replace operation to the lower side of the initial lexicon yields a transducer that maps the Arabic interdigitated forms directly into their corresponding tripartite analyses and vice versa, as illustrated in Figure 17. Alternation rules are subsequently composed on the lower side of the result to map the in- terdigitated, but still morphophonemic, strings into real surface strings. Although many Arabic templates are widely considered to be pure CV-patterns, it has been argued that certain templates also contain Lexical: k t b =Root C V C V C =Template a + =Voc Surface: k a t a b Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: k u t i b Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: d u u r i s Figure 17: After Applying Compile-Replace to the Lower Side \u201chard-wired\u201d specific vowels and consonants.6 For example, the so-called \u201cFormVIII\u201d template is considered, by some linguists, to contain an embedded t: CtVCVC. The presence of ordinary symbols in the template does not pose any problem for the analysis adopted here.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In the standard notation, the path in Figure 7 is labeled as b i g 0:g +Adj:0 0:e +Comp:r. Lexical transducers are more efficient for analysis and generation than the classical two- level systems (Koskenniemi, 1983) because the morphotactics and the morphological alternations have been precompiled and need not be consulted at runtime. But it would be possible in principle, and perhaps advantageous for some purposes, to view the regular expressions defining the morphology of a language as an un- compiled \u201cvirtual network\u201d. All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime. Most languages build words by simply stringing morphemes (prefixes, roots and suffixes) b i g b i g 0 +Adj g 0 0 +Comp e r together in strict orders. The morphotactic (word building) processes of prefixation and suffixation can be straightforwardly Surface side: Figure 7: A Path in a Transducer for English the epenthetical e in the surface form bigger result from the composition of the original lexicon fst with the rule fst representing the regular morphological alternations in English.", "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.", 1, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["would call templates and vocalizations. The 1996 algorithm that intersected roots and patterns into stems, and substituted the original roots and patterns on just the lower side with the intersected stem, was admittedly rather ad hoc and computationally intensive, taking over two hours to handle about 90,000 stems on a SUN Ultra workstation. The compile-replace algorithm is a vast improvement in both generality and efficiency, producing the same result in a few minutes. Following the lines of Kataja and Koskenniemi (1988), we could define intermediate networks with regular-expression substrings that indicate the intersection of suitably encoded roots, templates, and vocalizations (for a formal description of what such regular-expression substrings would look like, see Beesley (1998c; 1998b)). However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The network created by the operation is shown in Figure 9. When applied in the \u201cupward\u201d direction, the transducer in Figure 9 maps any string of the infinite a* language into the regular expression from which the language was compiled. The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine. It expects to find delimited regular expression substrings on a given side (upper or After the special treatment of the regular- expression path is finished, normal processing is resumed in the destination state of the closing ^] arc. For example, the result shown in Figure 9 represents the crossproduct of the two networks shown in Figure 10. a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In Arabic, for example, prefixes and suffixes attach to stems in the usual concatenative way, but stems themselves are formed by a process known informally as interdigitation; while in Malay, noun plurals are formed by a process known as full-stem reduplication. Although Arabic and Malay also include prefixation and suffixation that are modeled straightforwardly by concatenation, a complete lexicon cannot be a a:0 *:a *:0 *:0 0:a obtained without non-concatenative processes. We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm. The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions. The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 0, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics. The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific versial issue. 7 http://www.x rce.xerox.com /research/mlt t/arabic/ morphotactic problems we have discussed.", "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them. Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains. The only escape from this lim\u00ad itation will be through the use of automated or semi-automated methods of lexical acquisi\u00ad tion. However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the sup\u00ad port of DARPA grant N6600194C-6043, ARO grant DAAH0494G-0426, and CAPES grant 0914/952. One of the most controversial areas has to do with polysemy.", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept. cor e\u00a3. y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect. par t. y e s y e s y e s y e s y e s ye s ye s ye s y e s y e s y e s y e s y e s y e s y e s ind. acti on loc at. inv ers.", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start. 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..\u2022 ,c} such that S C S' and v E ci n ... n dm (subset criterion). Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993). Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch).", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by (Dang ct a!., 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily. However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be gener\u00ad ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb. WordNet does not currently provide a consistent treatment of regular sense exten\u00ad sion (some are listed as separate senses, others are not mentioned at all). It would be straight\u00ad forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and in\u00ad clude corresponding syntactic information. 3.3 Sense extension for manner of. motion Figure 3 shows intersective classes involving two classes of verbs of manner of motion (run and roll verbs) and a class of verbs of existence (me\u00ad ander verbs).", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang ct al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00ad ent refinement of basic Levin classes.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them. Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Even though the Levin verb classes are defined by their syntactic behavior, many reflect seman\u00ad tic distinctions made by WordNet, a classifica\u00ad tion hierarchy defined in terms of purely se\u00ad mantic word relations (synonyms, hypernyms, etc.). When examining in detail the intersec\u00ad tive classes just described, which emphasize not only the individual classes, but also their rela\u00ad tion to other classes, we see a rich semantic lat\u00ad tice much like WordNet. This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs. The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs. WordNet distinguishes two subclasses of cut, differentiated by the type of result: 1.", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.", "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily. However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997). However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap be\u00ad tween Levin and WordNet. The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["For example, flutuar does not take a direct object, so some of the alterna\u00ad tions that are related to its transitive meaning are not present. For these verbs, we have the in\u00ad duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat). As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative. The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily. However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Where break is concerned, the only thing speci\u00ad fied is the resulting change of state where the object becomes separated into pieces. If the result is not achieved, there are no attempted breaking actions that can still be recognized. 2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997). However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993). WordNet is an on\u00ad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\u00ad resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997). However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap be\u00ad tween Levin and WordNet. The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.", "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..\u2022 ,c} such that S C S' and v E ci n ... n dm (subset criterion). Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to.", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English.", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["For in\u00ad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\u00ad tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u2019s original classes, adding an additional level to the hierarchy (Dang et al. 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains. The only escape from this lim\u00ad itation will be through the use of automated or semi-automated methods of lexical acquisi\u00ad tion. However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the sup\u00ad port of DARPA grant N6600194C-6043, ARO grant DAAH0494G-0426, and CAPES grant 0914/952. One of the most controversial areas has to do with polysemy.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993). Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch). The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise. These fringe split verbs appear in several other inter\u00ad sective classes that highlight the force aspect of their meaning.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993). WordNet is an on\u00ad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\u00ad resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch). The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise. These fringe split verbs appear in several other inter\u00ad sective classes that highlight the force aspect of their meaning. Figure 2 depicts the intersection of split, carry and push/pull.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["However the simultaneous potential of mutually exclusive extensions is not a problem. It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative. The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion. Thus they cannot take the conative al\u00ad ternation. 3.2 Comparisons to WordNet.", "participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993). WordNet is an on\u00ad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\u00ad resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in\u00ad strument as an immediate hypernym. This distinction appears in the second-order Levin classes as membership vs. nonmember\u00ad ship in the intersective class with split. Levin verb classes are based on an underlying lat\u00ad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations. Whereas high level semantic relations (syn\u00ad onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class. However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Figure 2 depicts the intersection of split, carry and push/pull. Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo\u00ad nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion. Depending on the par\u00ad ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo\u00ad nent Levin classes. 1. Nora pushed the package to Pamela..", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them. Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains. The only escape from this lim\u00ad itation will be through the use of automated or semi-automated methods of lexical acquisi\u00ad tion. However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the sup\u00ad port of DARPA grant N6600194C-6043, ARO grant DAAH0494G-0426, and CAPES grant 0914/952. One of the most controversial areas has to do with polysemy.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation. First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete. We will be using re\u00ad sources such as dictionaries and online corpora to investigate potential additional members of our classes. Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation (Dorr and Jones, 1996), machine transla\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation. First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete. We will be using re\u00ad sources such as dictionaries and online corpora to investigate potential additional members of our classes. Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["For example, flutuar does not take a direct object, so some of the alterna\u00ad tions that are related to its transitive meaning are not present. For these verbs, we have the in\u00ad duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat). As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative. The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["The fact that the split sense for these verbs does not appear explicitly in WordNet is not surprising since it is only an extended sense of the verbs, and separation is inferred only when the verb occurs with an appropriate adjunct, such as apart. However, apart can also be used with other classes of verbs, including many verbs of motion. To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be gener\u00ad ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb. WordNet does not currently provide a consistent treatment of regular sense exten\u00ad sion (some are listed as separate senses, others are not mentioned at all). It would be straight\u00ad forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and in\u00ad clude corresponding syntactic information.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains.", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["For in\u00ad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\u00ad tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["We also investigated the Portuguese translation of some intersective classes of motion verbs. We selected the slide/roll/run, meander/roll and roll/run intersective classes. Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes. The elements of the slide/roll/run class are rebater (bounce), flutuar (float), rolar ( rolQ and deslizar (slide). The resultative in Portuguese cannot be expressed in the same way as in En\u00ad glish.", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["These fringe split verbs appear in several other inter\u00ad sective classes that highlight the force aspect of their meaning. Figure 2 depicts the intersection of split, carry and push/pull. Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo\u00ad nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion. Depending on the par\u00ad ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo\u00ad nent Levin classes. 1.", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["*Nora pushed at the package to Pamela. Although the Levin classes that make up an intersective class may have conflicting alterna\u00ad tions {e.g., verbs of exerting force can take the conative alternation, while carry verbs cannot), this does not invalidate the semantic regularity of the intersective class. As a verb of exerting force, push can appear in the conative alterna\u00ad tion, which emphasizes its force semantic com\u00ad ponent and ability to express an \"attempted\" action where any result that might be associ\u00ad ated- with the verb (e.g., motion) is not nec\u00ad essarily achieved; as a carry verb (used with a goal or directional phrase), push cannot take the conative alternation, which would conflict with the core meaning of the carry verb class (i.e., causation of motion). The critical point is that, while the verb's meaning can be extended to either \"attempted\" action or directed motion, these two extensions cannot co-occur - they are mutually exclusive. However the simultaneous potential of mutually exclusive extensions is not a problem.", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes. The slide class parti\u00ad tions this roll/run intersection into verbs that can take the transitive alternation and verbs that cannot (drift and glide cannot be causative, because they are not typically externally con\u00ad trollable). Verbs in the slide/roll/run intersec\u00ad tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actu\u00ad ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley). These verbs are listed in the intersective classes with meander verbs of existence. \"Meander Verbs\" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.", "archy is composed of a set of members (linked to their WordNet synsets) and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described (3 004 lemmas) within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic (message) as the direct object (Agent Verb Topic) as in \"John explained trigonometry\" and a frame for Topic and Recipient (Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame (Agent Verb Recipient Topic) in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion, contact, transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during(E )) culmination (end(E )) or consequent (result(E )) stage of an event in a tripartite event structure is similar to that of Moens and Steedman (1988) which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 2.2 PropBank.", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993). WordNet is an on\u00ad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\u00ad resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993). Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch). The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single great\u00ad est limitation on the general application of nat\u00ad ural language processing techniques. In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on a fine-grained variation on Levin classes, inter\u00ad sective Levin classes, as a source of semantic components associated with specific adjuncts. We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them. Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains. The only escape from this lim\u00ad itation will be through the use of automated or semi-automated methods of lexical acquisi\u00ad tion. However, the field has yet to develop a clear consensus on guidelines for a computa\u00ad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\u00ad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the sup\u00ad port of DARPA grant N6600194C-6043, ARO grant DAAH0494G-0426, and CAPES grant 0914/952. One of the most controversial areas has to do with polysemy.", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["me asu re \u2022ad j. per f. \u2022c og n. ob je ct ze ro no m. y e s y e s y e s n o n o y e s y e s y e s y e s n o n o y e s y e s y e s y e s n o n o n o y e s y e s y e s n o n o y e s n o y e s y e s n o n o y e s y e s y e s y e s n o n o y e s Table 2: Portuguese slide/roll/run and roll/run verbs with their alternations tions may depend on which translation is cho\u00ad sen, potentially giving us different clusters, but it is uncertain to what extent this is a factor, and it also requires further investigation. In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class. We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the litera\u00ad ture (Palmer and Rosenzweig, 1996), (Copes\u00ad take and Sanfilippo, 1993), (Dorr, 1997). In pursuing this goal, we are currently implement\u00ad ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998). TAGs have also been applied to Por\u00ad tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994).", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class. We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the litera\u00ad ture (Palmer and Rosenzweig, 1996), (Copes\u00ad take and Sanfilippo, 1993), (Dorr, 1997). In pursuing this goal, we are currently implement\u00ad ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998). TAGs have also been applied to Por\u00ad tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994). We in\u00ad tend to extend this grammar, building a more robust TAG grammar for Portuguese, that will allow us to build an English/Portuguese trans\u00ad fer lexicon using these features.", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes.", "In explormg these quest1ons, we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998) To address these 1ssues, we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm..", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses.", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Levin verb classes are based on an underlying lat\u00ad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations. Whereas high level semantic relations (syn\u00ad onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class. However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes. The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection. In addition, only one verb (pull) has a WordNet sense cor\u00ad responding to the change of state - separation semantic component associated with the split class.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993). WordNet is an on\u00ad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\u00ad resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["After inserting the expression letter bomb twice (because it occurs twice in the original topic), and tv channel that were in dictionary D1 used by the CN index, the relevant document is scored higher and as a consequence is returned in the first position of the ranking(Table 8) . The MAP of this topic has increased 75 percentage points, from 0.2500 in Baseline to 1.000 in the CN index. We see also that the document that was in first position in the Baseline ranking, has its score decreased and was ranked in fourth position in the ranking given by the CN. This document contained information on a \u201csmall bomb located outside the of the Russian embassy\u201d and has is not relevant to topic 141, being properly relegated to a lower position. An interesting fact about this topic is that only the MWE letter bomb influences the result. This was verified as in the index BCN, whose dictionary does not have this MWE, the topic was changed only because of the MWE tv channel and there was no gain or loss for the result.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Worst CN (WCN) - with 17,328 MWEs of D3.."], ["Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The term \u201cmultiword expression\u201d has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["38 04 71 Table 8: Ranking for Topic #141CN In sum, the MWEs insertion seems to improve retrieval bringing more relevant documents, due to a more precise indexing of specific terms. However, the use of these expressions also brought a negative impact for some cases, because some topics require a semantic analysis to return relevant documents (as for example topic 130, which requires relevant documents to mention the causes of the death of Kurt Cobain \u2014 documents which mention his death without mentioning the causes were not considered relevant). This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions. MWEs are found in all genres of texts and their appropriate use is being targeted for study, both in linguistics and computing, due to the different characteristic variations of this type of expression, which ends up causing problems for the success of computational methods that aim their processing. In this work we aimed at achieving a better understanding of several important points associated with the use of Multiword Expressions in IR systems.", "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Conclusions and Future  Work."], ["In addition, it contains the form in which the term appeared in the text (WF) and information of the term in the WordNet (Miller, 1995; Fellbaum, 1998) as SYNSET SCORE and CODE, both not used for <TERM ID=\"GH950102000000-126\" LEMA=\"underworld\" POS=\"NN\"> <WF>underworld</WF> <SYNSET SCORE=\"0.5\" CODE=\"06120171-n\"/> <SYNSET SCORE=\"0.5\" CODE=\"06327598-n\"/> </TERM> Figure 1: Structure of a term in the original documents In this paper, we extracted the terms located in the LEMA attribute, in other words, in their canonical form (e.g. letter bomb for letter bombs). The use of lemmas and not the words (e.g. write for wrote, written, etc.) to the formation of the corpus, avoids linguistic variations that can affect the results of the experiments. As a results, our documents were formed only by lemmas and the next step is the indexing of documents using an IR system. For this task we used a tool called Zettair (Zettair, 2008), which is a compact textual search engine that can be used both for the indexing and for querying text collections. Porter\u2019s Stemmer (Porter, 1997) as implemented in Zettair was also used.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others. For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus). Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment. The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs. In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The term \u201cmultiword expression\u201d has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others. For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus). Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment. The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["In an ideal system, the index terms should correspond to the concepts found in the documents. If indexing is performed only with the atomic terms, there may be a loss of semantic content of the documents. For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101\u2013109, Portland, Oregon, USA, 23 June 2011. Qc 2011 Association for Computational Linguistics return instead irrelevant documents about celestial bodies or carbonated drinks. In order to investigate the effects of indexing of MWEs for IR, the results of queries are analyzed using IR quality metrics.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using \u201c \u201d (e.g. letter bomb as letter bomb). Figure 2 exemplifies this concatenation. Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes. The rationale was that documents containing specific MWEs can be indexed more adequately than those containing the words of the expression separately. As a result, retrieval quality should increase.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["We finish with some conclusions and future work. The concept of Multiword Expression has been widely viewed as a sequence of words that acts as a single unit at some level of linguistic analysis (Calzolari et al., 2002), or as Idiosyncratic interpretations that cross word boundaries (or spaces) (Sag et al., 2002). One of the great challenges of NLP is the identification of such expressions, \u201chidden\u201d in texts of various genres. The difficulties encountered for identifying Multiword Expressions arise for reasons like: \u2022 the difficulty to find the boundaries of a multi- word, because the number of component words may vary, or they may not always occur in a canonical sequence (e.g. rock the boat, rock the seemingly intransigent boat and the bourgeois boat was rocked); \u2022 even some of the core components of an MWE may present some variation (e.g. throw NP to the lions/wolves/dogs/?birds/?butterflies); \u2022 in a multilingual perspective, MWEs of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guardachuva in Portuguese as umbrella in English and not as ?store rain). The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data they are interested (BaezaYates and RibeiroNeto, 1999). Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system. The selection of appropriate indexing terms is a key factor for the quality of IR systems. In an ideal system, the index terms should correspond to the concepts found in the documents.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. This approach is implemented in a lexico- graphic tool called Xtract. More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data they are interested (BaezaYates and RibeiroNeto, 1999). Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system. The selection of appropriate indexing terms is a key factor for the quality of IR systems. In an ideal system, the index terms should correspond to the concepts found in the documents.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["One of the great challenges of NLP is the identification of such expressions, \u201chidden\u201d in texts of various genres. The difficulties encountered for identifying Multiword Expressions arise for reasons like: \u2022 the difficulty to find the boundaries of a multi- word, because the number of component words may vary, or they may not always occur in a canonical sequence (e.g. rock the boat, rock the seemingly intransigent boat and the bourgeois boat was rocked); \u2022 even some of the core components of an MWE may present some variation (e.g. throw NP to the lions/wolves/dogs/?birds/?butterflies); \u2022 in a multilingual perspective, MWEs of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guardachuva in Portuguese as umbrella in English and not as ?store rain). The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years. With the recent increase in efficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing, these can become an aid in improving the performance of MWE detection techniques. In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora.", "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["(2002) defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis. The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997).", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs.", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["3.2 Multiword Expression as Single Terms. In this work, we focused on MWEs composed of exactly two words (i.e. bigrams). In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using \u201c \u201d (e.g. letter bomb as letter bomb). Figure 2 exemplifies this concatenation. Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes.", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["The test collection has a total of 310 query topics. The judgment of whether a document is relevant to a query was assigned according to a list of relevant documents, manually prepared and supplied with the material provided by CLEF. We used Zettair to generate the ranked list of documents retrieved in response to each query. For each query topic, the 1,000 top scoring documents were selected. We used the cosine metric to calculate the scores and rank the documents.", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["We used Zettair to generate the ranked list of documents retrieved in response to each query. For each query topic, the 1,000 top scoring documents were selected. We used the cosine metric to calculate the scores and rank the documents. Finally, to calculate the retrieval evaluation metrics (detailed in Section 3.5) we used the tool trec eval. This tool compares the list of retrieved documents (obtained from Zettair) against the list of relevant documents (provided by CLEF).", "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["Neural networks are worth investigating since they offer potential advantages over decision trees. They can learn decision surfaces that lie at an angle to the axes of the input feature space, unlike standard CART trees, which always split continuous features on one dimension at a time. The response function of neural networks is continuous (smooth) at the decision boundaries, allowing them to avoid hard decisions and the complete fragmentation of data associated with decision tree questions. Most important, however, related work (Ries 1999a) indicated that similarly structured networks are superior classifiers if the input features are words and are therefore a plugin replacement for the language model classifiers described in this paper. Neural networks are therefore a good candidate for a jointly optimized classifier of prosodic and word-level information since one can show that they are a generalization of the integration approach used here.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["N-gram models are likelihood models for DAs, i.e., they compute the conditional probabilities of the word sequence given the DA type. Word-based posterior probability estimators are also possible, although less common. Mast et al. (1996) propose the use of semantic classification trees, a kind of decision tree conditioned on word patterns as features. Finally, Ries (1999a) shows that neural networks using only unigram features can be superior to higher-order n-gram DA models. Warnke et al. (1999) and Ohler, Harbeck, and Niemann (1999) use related discriminative training algorithms for language models.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["The n-gram-based discourse grammars we used have this property. As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure (as well as the implied conditional independences) as a special case of Bayesian belief network (Pearl 1988).", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["5.2.4 Neural Network Classifiers. Although we chose to use decision trees as prosodic classifiers for their relative ease of inspection, we might have used any suitable probabilistic classifier, i.e., any model that estimates the posterior probabilities of DAs given the prosodic features. We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here. Neural networks are worth investigating since they offer potential advantages over decision trees. They can learn decision surfaces that lie at an angle to the axes of the input feature space, unlike standard CART trees, which always split continuous features on one dimension at a time.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The test data was split roughly in half (without speaker overlap), each half was used to separately optimize the parameters, and the best values were then tested on the respective other half. The reported results are from the aggregate outcome on the two test set halves. Table 9 Combined utterance classification accuracies (chance = 35%). The first two columns correspond to Tables 7 and 6, respectively. Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["Table 9 Combined utterance classification accuracies (chance = 35%). The first two columns correspond to Tables 7 and 6, respectively. Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%). Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody only 76.0 76.0 words only 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody only 72.9 72.9 words only 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results. In this experiment we combined the acoustic n-best likelihoods based on recognized words with the Top-5 tree classifier mentioned in Section 5.2.3.", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists of conversations between two speakers with slightly different maps of an imaginary territory. Their task is to help one speaker reproduce a route drawn only on the other speaker's map, all without being able to see each other's maps. Of the DA modeling algorithms described below, Taylor et al. (1998) and Wright (1998) were based on Map Task. The VERBMOBIL corpus consists of two-party scheduling dialogues. A number of the DA m6deling algorithms described below were developed for VERBMOBIL, including those of Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger and Klesen (1997), and Samuel, Carberry, and VijayShanker (1998).", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["This tag set distinguishes 42 mutually exclusive utterance types and was used for the experiments reported here. Table 2 shows the 42 categories with examples and relative frequencies. 1 While some 1 For the study focusing on prosodic modeling of DAs reported elsewhere (Shriberg et al. 1998), the tag set was further reduced to six categories.. of the original infrequent classes were collapsed, the resulting DA type distribution is still highly skewed. This occurs largely because there was no basis for subdividing the dominant DA categories according to task-independent and reliable criteria.", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["Computational approaches to prosodic modeling of DAs have aimed to automatically extract various prosodic parameters--such as duration, pitch, and energy patterns--from the speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others). Some approaches model F0 patterns with techniques such as vector quantization and Gaussian classifiers to help disambiguate utterance types. An extensive comparison of the prosodic DA modeling literature with our work can be found in Shriberg et al. (1998). DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition. Nagata and Morimoto (1994) suggest conditioning word language models on DAs to lower perplexity.", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality.", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality. Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech. These results, besides validating the methods described, are of interest for several reasons. For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14).", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "PAPER"], ["Each utterance unit was identified with one DA, and was annotated with a single DA label. The DA labeling system had special provisions for rare cases where utterances seemed to combine aspects of several DA types. Automatic segmentation of spontaneous speech is an open research problem in its own right (Mast et al. 1996; Stolcke and Shriberg 1996). A rough idea of the difficulty of the segmentation problem on this corpus and using the same definition of utterance units can be derived from a recent study (Shriberg et al. 2000). In an automatic labeling of word boundaries as either utterance or nonboundaries using a combination of lexical and prosodic cues, we obtained 96% accuracy based on correct word transcripts, and 78% accuracy with automatically recognized words.", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["For the speech recognition task, the generalized model gives a clean probabilistic framework for conditioning word probabilities on the conversation context via the underlying DA structure. Unlike previous models that did not address speech recognition or relied only on an intuitive 1-best approximation, our model allows computation of the optimum word sequence by effectively summing over all possible DA sequences as well as all recognition hypotheses throughout the conversation, using evidence from both past and future. Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs. We made a number of significant simplifications to arrive at a computationally and statistically tractable formulation. In this formulation, DAs serve as the hinges that join the various model components, but also decouple these components through statistical independence assumptions.", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Discussion and Issues for Future Research."], ["This can be done using familiar techniques from language modeling for speech recognition, although the sequenced objects in this case are DA labels rather than words; discourse grammars will be discussed in detail in Section 4. 3.1 Dialogue Act Likelihoods. The computation of likelihoods P(EIU ) depends on the types of evidence used. In our experiments we used the following sources of evidence, either alone or in combination: Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W refers to the true (hand-transcribed) words spoken in a conversation. Recognized words: The evidence consists of recognizer acoustics A, and we seek to compute P(A I U).", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["Recognized words: The evidence consists of recognizer acoustics A, and we seek to compute P(A I U). As described later, this involves considering multiple alternative recognized word sequences. Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U). For ease of reference, all random variables used here are summarized in Table 4. The same variables are used with subscripts to refer to individual utterances.", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "PAPER"], ["To enable such research, we need fairly large, standardized corpora that allow comparisons over time and across approaches. Despite its shortcomings, the Switchboard domain could serve this purpose. We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus. The approach combines models for lexical and prosodic realizations of DAs, as well as a statistical discourse 10 The inadequacy of n-gram models for nested discourse structures is pointed out by ChuCarroll (1998), although the suggested solution is a modified n-gram approach.. grammar.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Conclusions."], ["Reithinger et al. (1996), for example, used deleted interpolation to smooth the dialogue n-grams. ChuCarroll (1998) uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition. The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger and Klesen (1997), and Taylor et al. (1998) all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods. Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger (1997) propose using automatically learned stochastic context-free grammars.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["<.1% You are what kind of buff? <.1% I'm sorry. <.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["To expedite the DA labeling task and remain consistent with other Switchboard-based research efforts, we made use of a version of the corpus that had been hand-segmented into sentence-level units prior to our own work and independently of our DA labeling system (Meteer et al. 1995). We refer to the units of this segmentation as utterances. The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance). Each utterance unit was identified with one DA, and was annotated with a single DA label. The DA labeling system had special provisions for rare cases where utterances seemed to combine aspects of several DA types.", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["This means that the likelihood given a complete conversation can be factored into likelihoods given the individual utterances. We use Ui for the ith DA label in the sequence U, i.e., U = (U1 ..... Ui,..., Un), where n is the number of utterances in a conversation. In addition, we use Ei for that portion of the evidence that corresponds to the ith utterance, e.g., the words or the prosody of the ith utterance. Decomposability of the likelihood means that P(EIU) = P(E11 U1).....", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["Second, some utterances are inherently ambiguous based on words alone. For example, some YES-NO-QUESTiONS have word sequences identical to those of STATEMENTS, but can often be distinguished by their final F0 rise. A detailed study aimed at automatic prosodic classification of DAs in the Switchboard domain is available in a companion paper (Shriberg et al. 1998). Here we investigate the interaction of prosodic models with the dialogue grammar and the word-based DA models discussed above. We also touch briefly on alternative machine learning models for prosodic features.", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["WH-QUESTION Well, how old are you? Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970). We expect recognition of backchannels to be useful because of their discourse-structuring role (knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative) and because they seem to occur at certain kinds of syntactic boundaries; detecting a backchannel may thus help in predicting utterance boundaries and surrounding lexical material.", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970). We expect recognition of backchannels to be useful because of their discourse-structuring role (knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative) and because they seem to occur at certain kinds of syntactic boundaries; detecting a backchannel may thus help in predicting utterance boundaries and surrounding lexical material. For an intuition about what backchannels look like, Table 3 shows the most common realizations of the approximately 300 types (35,827 tokens) of backchannel in our Switchboard subset.", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["Each utterance is assigned a unique DA label (shown in column 2), drawn from a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria. The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture (Core and Allen 1997). While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something.", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy. Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department. 36% Uh-huh.", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Researchers using this corpus include Nagata (1992), Nagata and Morimoto (1993, 1994), and Kita et al. (1996). Table 13 shows the most commonly used versions of the tag sets from those three tasks. As discussed earlier, these domains differ from the Switchboard corpus in being task-oriented. Their tag sets are also generally smaller, but some of the same problems of balance occur. For example, in the Map Task domain, 33% of the words occur in 1 of the 12 DAs 0NSTRUCT).", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department. 36% Uh-huh. 19% I think it's great 13% So, -/ 6% That's exactly it.", "These are represented as layers in the DAMSL system (Core & Allen 1997).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Table 14 shows the approximate size of the corpora, the tag set, and tag estimation accuracy rates for various recent models of DA prediction. The results summarized in the table also illustrate the differences in inherent difficulty of the tasks. For example, the task of Warnke et al. (1997) was to simultaneously segment and tag DAs, whereas the other results rely on a prior manual segmentation. Similarly, the task in Wright (1998) and in our study was to determine DA types from speech input, whereas work by others is based on hand-transcribed textual input. Table 13 Dialogue act tag sets used in three other extensively studied corpora.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Reithinger et al. (1996), for example, used deleted interpolation to smooth the dialogue n-grams. ChuCarroll (1998) uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition. The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger and Klesen (1997), and Taylor et al. (1998) all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods. Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger (1997) propose using automatically learned stochastic context-free grammars.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["A SIGNAL-NoN-UNDERSTANDING What did you say? B STATEMENT N C State. The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). A DA represents the meaning of an utterance at the level of illocutionary force (Austin 1962).", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["B STATEMENT N C State. The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). A DA represents the meaning of an utterance at the level of illocutionary force (Austin 1962). Thus, a DA is approximately the equivalent of the speech act of Searle (1969), the conversational game move of Power (1979), or the adjacency pair part of Schegloff (1968) and Saks, Schegloff, and Jefferson (1974).", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Table 1 shows a sample of the kind of discourse structure in which we are interested. Each utterance is assigned a unique DA label (shown in column 2), drawn from a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria. The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture (Core and Allen 1997). While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications.", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["5.3.2 Focused Classifications. To gain a better understanding of the potential for prosodic DA classification independent of the effects of discourse grammar and the skewed DA distribution in Switchboard, we examined several binary DA classification tasks. The choice of tasks was motivated by an analysis of confusions committed by a purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, and BACKCHANNELS for AGREEMENTS (and vice versa). We tested a prosodic classifier, a word-based classifier (with both transcribed and recognized words), and a combined classifier on these two tasks, downsampling the DA distribution to equate the class sizes in each case. Chance performance in all experiments is therefore 50%.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["<.1% You are what kind of buff? <.1% I'm sorry. <.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Prior and related work is summarized in Section 7. Further issues and open problems are addressed in Section 8, followed by concluding remarks in Section 9. The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium. Each conversation involved two randomly selected strangers who had been charged with talking informally about one of several, self- selected general-interest topics. To train our statistical models on this corpus, we combined an extensive effort in human hand-coding of DAs for each utterance, with a variety of automatic and semiautomatic tools.", "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["This leads to the following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ The last step in Equation 12 is justified because, as shown in Figures 1 and 4, the evidence E (acoustics, prosody, words) pertaining to utterances other than i can affect the current utterance only through its DA type Ui. We call this the mixture-of-posteriorsapproach, because it amounts to a mixture of the posterior distributions obtained from DA-specific speech recognizers (Equation 11), using the DA posteriors as weights. This approach is quite expensive, however, as it requires multiple full recognizer or rescoring passes of the input, one for each DA type. A more efficient, though mathematically less accurate, solution can be obtained by combining guesses about the correct DA types directly at the level of the LM. We estimate the distribution of likely DA types for a given utterance using the entire conversation E as evidence, and then use a sentence-level mixture (Iyer, Ostendorf, and Rohlicek 1994) of DA-specific LMs in a single recognizer run.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Speech Recognition."], ["We observe about a 21% relative increase in classification error when using recognizer words; this is remarkably small considering that the speech recognizer used had a word error rate of 41% on the test set. We also compared the n-best DA classification approach to the more straightforward 1-best approach. In this experiment, only the single best recognizer hypothesis is used, effectively treating it as the true word string. The 1-best method increased classification error by about 7% relative to the n-best algorithm (61.5% accuracy with a bigram discourse grammar). 5.2 Dialogue Act Classification Using Prosody.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["noise ratio [SNR]), speaking rate (based on the \"enrate\" measure of Morgan, Fosler, and Mirghafori [1997]), and gender (of both speaker and listener). In the case of utterance duration, the measure correlates both with length in words and with overall speaking rate. The gender feature that classified speakers as either male or female was used to test for potential inadequacies in F0 normalizations. Where appropriate, we included both raw features and values normalized by utterance and/or conversation. We also included features that are the output of the pitch accent and boundary tone event detector of Taylor (2000) (e.g., the number of pitch accents in the utterance).", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["The n-gram-based discourse grammars we used have this property. As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure (as well as the implied conditional independences) as a special case of Bayesian belief network (Pearl 1988).", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure (as well as the implied conditional independences) as a special case of Bayesian belief network (Pearl 1988). Figure 1 shows the variables in the resulting HMM with directed edges representing conditional dependence.", "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Some researchers explicitly used HMM induction techniques to infer dialogue grammars. Woszczyna and Waibel (1994), for example, trained an ergodic HMM using expectation-maximization to model speech act sequencing. Kita et al. (1996) made one of the few attempts at unsupervised discovery of dialogue structure, where a finite-state grammar induction algorithm is used to find the topology of the dialogue grammar. Computational approaches to prosodic modeling of DAs have aimed to automatically extract various prosodic parameters--such as duration, pitch, and energy patterns--from the speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others). Some approaches model F0 patterns with techniques such as vector quantization and Gaussian classifiers to help disambiguate utterance types.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["The model predicted upcoming DAs by using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 2,722 DAs. Many others subsequently relied on and enhanced this n-grams-of-DAs approach, often by applying standard techniques from statistical language modeling. Reithinger et al. (1996), for example, used deleted interpolation to smooth the dialogue n-grams. ChuCarroll (1998) uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing. For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy.", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["(Condition on the first production: with probability q the tree terminates and with probability p it produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.) It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi.", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Y(wn) are available, the EM algorithm can be used to iteratively \"climb\" the likelihood surface (see Section 2). In the simple example here, the estimator converges in one step and is the same ~ as if we had observed the entire parse tree for each wi. Thus, ~ is again less than  and the distribution is again tight. More generally, let G -- (V, T, R, S) denote a context-free grammar with finite variable set V, start symbol S E V, finite terminal set T, and finite production (or rule) set R. (We use R in place of the more typical P to avoid confusion with probabilities.)", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms. Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse. For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol. Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one. The reason is that the derivation tree may have probability greater than zero of never terminating--some mass can be lost to infinity.", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2..", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["The reason is that the derivation tree may have probability greater than zero of never terminating--some mass can be lost to infinity. This phenomenon is well known and well understood, and there are tests for \"tightness\" (by which we mean total probability mass equal to one) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules (see for example Booth and Thompson [1973], Grenander [1976], and Harris [1963]). What if the production probabilities are estimated from data? Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar. It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees.", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["(Condition on the first production: with probability q the tree terminates and with probability p it produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.) It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi.", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2.. Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight. (Wetherell and others use the designation \"consistent\" instead of \"tight,\" but in statistics, consistency refers to the asymptotic correctness of an estimator.) A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol.", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["(2) AEV a s.t. i=1 The function p : R ~ [0,1] subject to (1) that maximizes (2) satisfies: 6 AAp(A ~ a) + f(A ~ a;cai)logp(A ~ a) = 0 AEV   i=1 (A~o~)ER V(B ~/3) E R where {AA}AEV are Lagrange multipliers. Denote the maximum-likelihood estimator by fi: n B AB q- ~i=lf( --+ /3;ca;) = 0 V(S ~ /3) E R f,(B +/3) Since ~ fi(B+/3)=l) fl sA. (8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator. Suppose B E V is unobserved among the parse trees cabc02,-..,can. Then we can assign fi(B --+ fl) arbitrarily, requiring only that (1) be respected.", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", 1, "Estimation of Probabilistic Context-Free Grammars", "Maximum-Likelihood Estimation."], ["We show here that estimated production probabilities always yield proper distributions. Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms. Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse. For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol. Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one.", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar. It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees. For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one. The condition for proper assignment is rather subtle. Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper.", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 0, "Estimation of Probabilistic Context-Free Grammars", "ABSTRACT"], ["Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2..", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi. Consequently: f(A ---+AA;wi) < l[f(A ~ AA;~i) + f(A ~ a;wi)] for each wi, and ~ < .", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Evidently the likelihood is unaffected by the particular assignment of fi(B --~ fl). Furthermore, it is not hard to see that any such B has probability zero of arising in any derivation that is based upon the maximum-likelihood probabilitiesg--~ence the issue of tightness is independent of this assignment. We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1. 3 Consider any sequence of productions that leads from S to B. If the parent (antecedent) of B arose in. the sample, then the last production has ~ probability zero and hence the sequence has probability zero.", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 0, "Estimation of Probabilistic Context-Free Grammars", "Maximum-Likelihood Estimation."], ["2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2.. Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight. (Wetherell and others use the designation \"consistent\" instead of \"tight,\" but in statistics, consistency refers to the asymptotic correctness of an estimator.) A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol.", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]).", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2..", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Completed: 3 (100.00%). Figure 3: Boxer output for Shared Task Text 2 Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008). Boxer was able to produce semantic representation for all text without any further modifications to the software. For each text we briefly say what was good and bad about Boxer\u2019s analysis.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005). 2.1 Combinatory Categorial Grammar. As a preliminary to semantics, we need syntax. Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001). CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression).", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types \u2014 and this must be the case to ensure the semantic composition process proceeds without type clashes \u2014 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 1, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["Good: The named entities were correctly recognised and classified as locations. The various cases of VP coordination all got properly analysed. The numerical and date expressions got correct representations. Bad: The occurrences of the third-person neuter pronouns were not resolved. The preposition \u201cAmid\u201d was not correctly analysed.", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", 1, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["That has been known for some time and it has led to a vaccine that seems to prevent it. Researchers have been looking for other cancers that may be caused by viruses. The output of Boxer for this text is shown in Figure 3. Only the box format is shown here \u2014 Boxer is also able to output the DRSs in Prolog or XML encodings. It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here).", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Researchers have been looking for other cancers that may be caused by viruses. The output of Boxer for this text is shown in Figure 3. Only the box format is shown here \u2014 Boxer is also able to output the DRSs in Prolog or XML encodings. It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here). As we can see from the example and Boxer\u2019s analysis various things go right and various things go wrong.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["For example, the category n/n can correspond to an adjective, a cardinal expression, or even common nouns and proper names (in the compound expressions). In the latter two cases the lexical entry introduces a new discourse referent, in the former two it does not. To account for this difference we also need to look at the part of speech that is assigned to a token. 3.3 Resolution. Boxer implements various presupposition triggers introduced by noun phrases, including personal pronouns, possessive pronouns, reflexive pronouns, emphasising pronouns, demonstrative pronouns, proper names, other-anaphora, definite descriptions.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001). CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001). We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Completed: 3 (100.00%). Figure 3: Boxer output for Shared Task Text 2 Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008). Boxer was able to produce semantic representation for all text without any further modifications to the software. For each text we briefly say what was good and bad about Boxer\u2019s analysis.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["3.1 Preprocessing. The input text needs to be tokenised with one sentence per line. In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007). The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents. An example of a CCG derivation is shown in Figure 2.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Bad: The measure phrase \u201c125 m high\u201d got misinterpreted as noun-noun comn- pound. The definite description \u201cthe fall\u201d was not linked to the falling event mentioned before. Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions. Text 3: John went into a restaurant ... Good: The pronouns were correctly resolved to the proper name \u201cJohn\u201d rather than \u201cthe waiter\u201d, even though this is based on the simple strategy in Boxer to link third- person pronouns to named entities of type human.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001). CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001). We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 0, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["The units of measurement in the last two sentences were not recognised as such. The tricky time expression \u201cmid-80\u2019s\u201d only got a shallow interpretation. Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 0, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001). CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001). We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["277 Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005). 2.1 Combinatory Categorial Grammar.", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", 1, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["Completed: 3 (100.00%). Figure 3: Boxer output for Shared Task Text 2 Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008). Boxer was able to produce semantic representation for all text without any further modifications to the software. For each text we briefly say what was good and bad about Boxer\u2019s analysis.", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising. It was able to produce DRSs for all texts.", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["3.1 Preprocessing. The input text needs to be tokenised with one sentence per line. In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007). The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents. An example of a CCG derivation is shown in Figure 2.", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["277 Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005). 2.1 Combinatory Categorial Grammar.", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 1, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["Finally, a comment on availability of Boxer. All sources of Boxer are available for download and free of noncommercial use. It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", 1, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["<expe > ::= <ref> <expt > ::= <drs> <ref>\u2217 <drs> ::= <condition>\u2217 <condition> ::= <basic> | <complex> <basic> ::= <sym1 >(<expe >) | <sym2 >(<expe >,<expe >) | <named>(<expe >,<nam>,<sort>) <complex> ::= <expt > | <expt >\u21d2<expt > | <expt >\u2228<expt > | <ref>:<expt > DRSs are structures comprising two parts: 1) a set of discourse referents; and 2) a set of conditions constraining the interpretation of the discourse referents. Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs. The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics (Kamp and Reyle, 1993), where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations. The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989). There is only one way to state that an individual is participating in an event\u2014namely by relating it to the event using a binary relation expressing some thematic role.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types \u2014 and this must be the case to ensure the semantic composition process proceeds without type clashes \u2014 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["There is only one way to state that an individual is participating in an event\u2014namely by relating it to the event using a binary relation expressing some thematic role. Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear. Finally, it also allows us to characterize the meaning of thematic roles independently of the meaning of the verb that describes the event. We won\u2019t show the standard translation from DRS to FOL here (Blackburn et al., 2001; Bos, 2004; Kamp and Reyle, 1993). Intuitively, translating DRSs into first-order formulas proceeds as follows: each discourse referent is translated as a first-order quantifier, and all DRS-conditions are translated into a conjunctive formula of FOL.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001). CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001). We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations (Kamp, 1981). DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition. We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1. Discourse Representation Structures (DRSs). 2.", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["The tricky time expression \u201cmid-80\u2019s\u201d only got a shallow interpretation. Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising. It was able to produce DRSs for all texts.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations (Kamp, 1981). DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition. We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1. Discourse Representation Structures (DRSs). 2.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer is distributed with the C&C tools and freely available for research purposes. 277 Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005).", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", 1, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types \u2014 and this must be the case to ensure the semantic composition process proceeds without type clashes \u2014 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs. The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics (Kamp and Reyle, 1993), where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations. The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989). There is only one way to state that an individual is participating in an event\u2014namely by relating it to the event using a binary relation expressing some thematic role. Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear.", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005). 2.1 Combinatory Categorial Grammar. As a preliminary to semantics, we need syntax.", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 0, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["The tricky time expression \u201cmid-80\u2019s\u201d only got a shallow interpretation. Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u00b4 (2008) to deal with issues polysemy and ambiguity.", 1, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech. Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realita\u00a8 t?", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Table 3 shows the results of an evaluation based on the plain STTS tagset. The first result was obtained with TnT trained on Tiger data which was mapped to STTS before. The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS. The third row gives the corresponding figures for our tagger. 5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["A supplementary lexicon was created by analyzing a word list which included all words from the fa\u03b1 pa\u03b1 (t) log pa\u03b1(t) < 1 training, development, and test data with a German computationa l morphology. The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["With athreshold of 10\u22123 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["With athreshold of 10\u22123 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777\u2013784 Manchester, August 2008 context probability of the third POS tag is therefore 0.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["2 8 9 7. 1 7 97.26 97.51 9 7. 3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger.", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Czech POS tagging has been extensively studied in the past (Hajic\u02c7 and Vidova\u00b4-Hladka\u00b4, 1998; Hajic\u02c7 et al., 2001; Votrubec, 2006). Spoustov et al. (2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (Morc\u02c7e), and evaluated them on the Prague Dependency Treebank (PDT 2.0). Morc\u02c7e\u2019s tagging accuracy was 95.12%, 0.3% better than the n-gram tagger. A hybrid system based on four different tagging methods reached an accuracy of 95.68%.", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "We use the following baselines: SVMTool (Gime\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupa\u0142a et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["more replaced by the dative. Table 2: Tagging accuracies on development data in percent. Results for 2 and for 10 preceding POS tags as context are reported for our tagger. much smaller. Table 3 shows the results of an evaluation based on the plain STTS tagset.", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Ferri et al. (2003) describe a more complex backoff smoothing method. Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0). Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes. These two-class trees can be pruned with a fixed pruning threshold.", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities. The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|ti\u22121 ) is internally computed as a product of attribute probabili ties. In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.", "For German, we show results for RFTagger (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category. The number of additional attributes is fixed for each main category. The additional attributes are category-specific. The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Splitting of the POS Tags."], ["The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model. used.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The number of additional attributes is fixed for each main category. The additional attributes are category-specific. The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Splitting of the POS Tags."], ["The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class. In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data. Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not. The best test for each node is selected with the standard information gain criterion.", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Ferri et al. (2003) describe a more complex backoff smoothing method. Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0). Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes. These two-class trees can be pruned with a fixed pruning threshold.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement. With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outperforms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon. A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively. de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7. 2 8 9 7.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class. In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data. Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not. The best test for each node is selected with the standard information gain criterion.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d. \u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not. The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k \u2265 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined.", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger. 5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left.", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja\u00a8rvinen, 1997, pos, morph).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["The number of additional attributes is fixed for each main category. The additional attributes are category-specific. The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Splitting of the POS Tags."], ["The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1). The best tag sequence is computed with the Viterbi algorithm. The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words. The lexical probabilities of unknown words are obtained as follows: The unknown words are divided into four disjoint classes6 with numeric expressions, words starting with an uppercase letter, words starting with a lowercase letter, and a fourth class for the other words. The tagger builds a suffix trie for each class of unknown words using the known word types from that class.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The recursive tree building process terminates if the information gain is 0. The decision tree is pruned with the pruning criterion described below. Since the tagger creates a separate tree for each attribute, the probabilities of a set of competing attributes such as masculine, feminine, and neuter will not exactly sum up to 1. To understand why, assume that there are three trees for the gender attributes. Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS.", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger. 5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model. used.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The tagger builds a suffix trie for each class of unknown words using the known word types from that class. The maximal length of the suffixes is 7. The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous. Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences. 6 In earlier experiments, we had used a much larger number of word classes.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja\u00a8rvinen, 1997).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context. These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute. Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method. The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement. With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outperforms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon. A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively. de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7. 2 8 9 7.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc). Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in). The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or). For evaluation purposes, the refined tags are mapped back to the original tags. This mapping is unambiguous.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["A similar idea was previously presented in Kempe (1994), but apparently never applied again. The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold. A threshold of 6 consistently produced optimal or near optimal results for pre-pruning. Thus, pre-pruning with a threshold of 6 was used in the experiments. The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category.", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon. 5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank, although this information is important for the disambiguation of the case of the next noun phrase. In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc). Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in). The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or).", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The training with a context size of 10 took about 4 minutes. 5.2 Czech Academic Corpus. We also evaluated our tagger on the Czech Academic corpus (Hladka\u00b4 et al., 2007) which contains 652.131 tokens and about 1200 different POS tags. The data was divided into 80% training data, 10% development data and 10% test data. 89 88.9 88.8 Provost & Domingos (2003) noted that well- known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al., 1984) fail to produce accurate probability estimates.", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["7 It was planned to include also the Stanford tagger. (Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data. 8 In German, the genitive case of arguments is more and. more replaced by the dative. Table 2: Tagging accuracies on development data in percent.", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data. Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23. Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["To understand why, assume that there are three trees for the gender attributes. Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d. \u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities. The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|ti\u22121 ) is internally computed as a product of attribute probabili ties. In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime\u00b4nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka\u00b4 et al., 2007) and compared to the TnT tag- ger. 5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["To understand why, assume that there are three trees for the gender attributes. Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d. \u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not.", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777\u2013784 Manchester, August 2008 context probability of the third POS tag is therefore 0.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data. Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23. Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here.", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.)", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["more replaced by the dative. Table 2: Tagging accuracies on development data in percent. Results for 2 and for 10 preceding POS tags as context are reported for our tagger. much smaller. Table 3 shows the results of an evaluation based on the plain STTS tagset.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data. Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23. Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1. This problem 2:N.Reg p=0.999 0:N.Name 0:N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 .... is solved by renormalizing the probabilities. The probability of an attribute (such as \u201cNom\u201d) is always conditioned on the respective base POS (such as \u201cN\u201d) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns. The test 1:ART.Nom checks if the preceding word is a nominative article. assigned to the top node.", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "PAPER"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "PAPER"], ["Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.)", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["This mapping is unambiguous. 7 It was planned to include also the Stanford tagger. (Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data. 8 In German, the genitive case of arguments is more and. more replaced by the dative.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00b4 et al., 2003).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["With athreshold of 10\u22123 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The accuracy of our tagger is lower than on the development data. This could be due to the higher rate of unknown words (10.0% vs. 7.7%). Relative to the TnT tagger, however, the accuracy is quite similar for test and development data. The differences between the two taggers are significant.10 ta gg er de fa ult refined ref.+lexicon Tn T ou r ta gg er 8 3. 4 5 84.11 89.14 8 5.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model. used.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization. Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words? A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Since German is a verb-final language, these tests clearly make sense. Table 4 shows the performance on the test data. Our tagger was used with a context size of 10. The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon. These values were optimal on the development data.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization. Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words?", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["2 8 9 7. 1 7 97.26 97.51 9 7. 3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["used. We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train the SVMTool (or other discriminatively trained tag- gers) without such a restriction given the difficulties to train it with the standard context size. Czech POS tagging has been extensively studied in the past (Hajic\u02c7 and Vidova\u00b4-Hladka\u00b4, 1998; Hajic\u02c7 et al., 2001; Votrubec, 2006). Spoustov et al. (2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (Morc\u02c7e), and evaluated them on the Prague Dependency Treebank (PDT 2.0).", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model. used.", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["4 5 84.11 89.14 8 5. 0 0 85.92 91.07 Table 4: Tagging accuracies on test data. By far the most frequent tagging error was the confusion of nominative and accusative case. If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger. The resulting score of a binomial test is below 0.001.", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pado\u00b4 , 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011).", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["8 of the 43 words are translated to English multi-word phrases (denoted as \u201cphrase\u201d in Table 3). Since our method currently only considers unigram English words, we are not able to find translations for these words. But it is not difficult to extend our method to handle this problem. We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases. The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d).", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["ranked list of candidate words, associating with each candidate a score estimated by the particular method. If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general. Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work. To estimate P(Q | D) , we use the approach of (Ng, 2000). We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 ,..., ek that ap query Q is generated by this model: pear in top M positions in both lists.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["/ \u220ft ct ! can be omitted as this part depends on the query only and thus is the same for all the documents. in the English target corpus. In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. So our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 1, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping. ble of pinyin, api is the ith sylla li is the English letter sequence estimation. We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = \u03b1 \u22c5 Pml (tc | Tc (C (e))) + (1 \u2212\u03b1 ) \u22c5 Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem. We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) \u2211dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t\u2208Tc (C ( e )) lables to English letter sequences. To reduce the search space, we limit the number of English letters that each pinyin syllable can map to as 0, where Pml (\u2022) are the maximu m likelihood esti 1, or 2.", "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).", 1, "Mining New Word Translations from Comparable Corpora", "Translation by context."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. New words such as person names, organization names, technical terms, etc. appear frequently.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus. \u03b1 is set to 0.6 in our experiments. must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e | c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus. Another difference is Onaizan and Knight, 2002b). Knight and Graehl (1998) proposed a probabilistic model for machine transliteration.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 0, "Mining New Word Translations from Comparable Corpora", "Translation by transliteration."], ["Precision and recall for different values of M The past research of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs. Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus. As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus. This list of 43 words is shown in Table 3.", "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003). But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. The English word e document. We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem. More details are given in Section 3. On the other hand, when we only look at the word w itself, we can rely on the pronunciation of w to locate its translation.", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", 1, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. So our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection. \u220f P(tc tc\u2208C ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word. q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Translation by context."], ["Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling. But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling). The work that is most similar to ours is the recent research of (Huang et al., 2004). They attempted to improve named entity translation by combining phonetic and semantic information. Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates. English Service. The size of the English corpus from Jul to Dec The context C(c)of a Chinese word c was col 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes. We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm. lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected.", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing. Unlike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). for each pair of Chinese source word and English translation candidate word. For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c) | C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e | c) (as described in from a half-month period.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm. lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected. The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. The English word e document. We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al. (2011) and Shao and Ng (2004).", 1, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling. But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling). The work that is most similar to ours is the recent research of (Huang et al., 2004). They attempted to improve named entity translation by combining phonetic and semantic information. Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing. Unlike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). for each pair of Chinese source word and English translation candidate word. For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c) | C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e | c) (as described in from a half-month period.", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling). The work that is most similar to ours is the recent research of (Huang et al., 2004). They attempted to improve named entity translation by combining phonetic and semantic information. Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity. It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging.", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["We then n! rank these words e1 , e2 ,..., ek according to the P (Q | D ) = \u220f P (t | D ) c t average of their rank positions in the two lists. \u220f t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output. If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998). More details are is estimated. The document with the highest given in Section 4. Each of the two individual methods provides a P(Q | D) is the one that best matches the query. ranked list of candidate words, associating with each candidate a score estimated by the particular method.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 \u2013 Jul 15, period 2 is Jul 16 \u2013 Jul 31, \u2026, period 12 is Dec 16 \u2013 Dec 31. #c is the total number of new Chinese source words in the period. #e is the total number of English translation candidates in the period. #o is the total number of output English translations. #Cor is the number of correct English translations output.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["We considered these English words as potential translations of the Chinese source words. We call Section 4), which was used to rank the English candidate words based on transliteration. Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2). If no words appear within the top M positions in both ranked lists, then no translation is output. Note that for many Chinese words, only one English word e appeared within the top M positions for both lists.", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003). But parallel corpora are scarce resources, especially for uncommon lan guage pairs.", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["Since our method currently only considers unigram English words, we are not able to find translations for these words. But it is not difficult to extend our method to handle this problem. We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases. The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d). Our method is not able to find 43 (329 + 205) \u00d7 4499 = 362words in all 12 pe these translations.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["We then n! rank these words e1 , e2 ,..., ek according to the P (Q | D ) = \u220f P (t | D ) c t average of their rank positions in the two lists. \u220f t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output. If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing. Unlike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). for each pair of Chinese source word and English translation candidate word. For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c) | C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e | c) (as described in from a half-month period.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["Precision and recall for different values of M The past research of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs. Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus. As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus. This list of 43 words is shown in Table 3.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It consists of 97 documents (~50k words) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set. It shows that the \u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Mentions have three levels: names, nomial expressions or pronouns. The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task. For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight.", "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Most of the features used in our system are based on the work in (Zhou et al., 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M.", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M. (1999).", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information . How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74. 1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33. 3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4. 8 66 .7 74 .7 77 .2 60 .7 68 .0 6 3. 1 4 9.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types. This suggests that relation detection is critical for relation extraction. # of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68. 1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79. 6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight.", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005) 7 : P per formance by ~1.2 in F-measure6 . This suggests the K1 ( , ) a K L ( , ) (1 a ) K C ( , ) (5) usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP. \u2022 ET1DW1: combination of the entity type and the dependent word for M1 \u2022 H1DW1: combination of the head word and the dependent word for M1 \u2022 ET2DW2: combination of the entity type and the dependent word for M2 \u2022 H2DW2: combination of the head word and the dependent word for M2 \u2022 ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP \u2022 ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP \u2022 ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree. This category of features concerns about the information inherent only in the full parse tree. \u2022 PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree \u2022 PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure. \u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase. \u2022 The usefulness of mention level features is quite limited.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["5 4 5. 2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Finally, they also Ours: composite kernel Zhang et al (2006): composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al (2006): convolution tree ke rnel Bunescu et al (2005): shortest path dependency kernel Culotta et al (2004): dependency kernel Zhou et al. (2005): feature-based Kambhatla (2004): feature-based 80.8 (65.2) 77.3 (64.9) 80.1 (63.4) 76.1 (62.4) 65.5 (-) 67.1 (-) 77.2 (63.1) - (63.5) 68.4 (54.9 ) 65.6 (51.2) 63.8 (51.9 ) 62.6 (48.5) 43.8 (-) 35.0 (-) 60.7 (49.5) - (45.2) 74.1 (59.6) 70.9 (57.2) 71.0 (57.1) 68.7 (54.6) 52.5 (-) 45.8 (-) 68.0 (55.5) - (52.8) show that our composite kernel-based system outper forms other composite ke rnel-based systems.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking. It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype. It is not surprising that the performance on the relation type \u201cNEAR\u201d is low because it occurs rarely in both the training and testing data. Others like \u201cPART.Subsidary\u201d and \u201cSOCIAL.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The testing set is held out only for final evaluation. It consists of 97 documents (~50k words) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set. It shows that the \u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["\u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text. Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities.", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2. For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important. The words between the two mentions are classified into three bins: the first word in between, the last word in between and other words in between. Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser (Collins 1999) is employed for full parsing.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["(2004). Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations. During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition. For example, the head word of the name mention \u201cUniversity of Michigan\u201d is \u201cUniversity\u201d.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["(2004). Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 0, "Exploring Various Knowledge in Relation Extraction", "PAPER"], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Zhou et al. (2005) explore various features in relation extraction using SVM.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["\u2022 Chunking features are very useful. It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also includes flags indicating whether the two mentions are in the same NP/PP/VP. \u2022 ET1DW1: combination of the entity type and the dependent word for M1 \u2022 H1DW1: combination of the head word and the dependent word for M1 \u2022 ET2DW2: combination of the entity type and the dependent word for M2 \u2022 H2DW2: combination of the head word and the dependent word for M2 \u2022 ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP \u2022 ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP \u2022 ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree. This category of features concerns about the information inherent only in the full parse tree. \u2022 PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree \u2022 PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path. 4.8 Semantic Resources.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "we call the features used in Zhou et al. (2005) and Kambhatla (2004) flat feature set.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC.", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", 1, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser (Collins 1999) is employed for full parsing.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure. \u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention. Since a pronominal mention (especially neutral pronoun such as \u2018it\u2019 and \u2018its\u2019) contains little information about the sense of the mention, the co- reference chain is used to decide its sense. This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Since a pronominal mention (especially neutral pronoun such as \u2018it\u2019 and \u2018its\u2019) contains little information about the sense of the mention, the co- reference chain is used to decide its sense. This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation. This has an effect of choosing the base phrase contained in the extent annotation.", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["\u2022 Chunking features are very useful. It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results.", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure. \u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase. \u2022 The usefulness of mention level features is quite limited. It only improves the F-measure by 0.8 due to the recall increase.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight. for multi-class classification.", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["\u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight. for multi-class classification.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["\u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.", "Zhao and Grisman (2005) 2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 1, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["However, this paper only uses the binary-class version. For details about SVMLight, please see http://svmlight.joachims.org/ The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For details about SVMLight, please see http://svmlight.joachims.org/ The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words.", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document. 427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "In the future, we would like to use more effective feature sets Zhou et al. (2005) or kernel based similarity measure with LP for relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document. 427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships. Country Name List This is to differentiate the relation subtype \u201cROLE.Citizen-Of\u201d, which defines the relationship between a person and the country of the person\u2019s citizenship, from other subtypes, especially \u201cROLE.Residence\u201d, where defines the relationship between a person and the location in which the person lives. Two features are defined to include this information: \u2022 ET1Country: the entity type of M1 when M2 is a country name \u2022 CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal. This trigger word list is first gathered from WordNet by checking whether a word has the semantic class \u201cperson|\u2026|relative\u201d. Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It also shows that our system performs best on the subtype \u201cSOCIAL.Parent\u201d and \u201cROLE. Citizen-Of\u201d. This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type \u201cAT\u201d although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes.", "Based on his work, Zhou et al (2005) 89 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 89\u201392, Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document. 427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document. 427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types. Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998). Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Table 4 also indicates the low performance on the relation type \u201cAT\u201d although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes. Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight.", "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes. Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships. Country Name List This is to differentiate the relation subtype \u201cROLE.Citizen-Of\u201d, which defines the relationship between a person and the country of the person\u2019s citizenship, from other subtypes, especially \u201cROLE.Residence\u201d, where defines the relationship between a person and the location in which the person lives. Two features are defined to include this information: \u2022 ET1Country: the entity type of M1 when M2 is a country name \u2022 CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal. This trigger word list is first gathered from WordNet by checking whether a word has the semantic class \u201cperson|\u2026|relative\u201d. Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["5 4 5. 2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "Following most of the 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data;.", 1, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For details about SVMLight, please see http://svmlight.joachims.org/ The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words.", "However, most approaches to RE have assumed that the relations\u2019 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "We used Zhou et al.\u2019s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes. We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships. Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.", "Although a bit lower than Zhou et al.\u2019s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that our system performs best on the subtype \u201cSOCIAL.Parent\u201d and \u201cROLE. Citizen-Of\u201d. This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type \u201cAT\u201d although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight. for multi-class classification.", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types. Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998). Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser (Collins 1999) is employed for full parsing.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 and Section 4 describe our approach and various features employed respectively.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Section 3 and Section 4 describe our approach and various features employed respectively. Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser (Collins 1999) is employed for full parsing.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations. During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["5 55 .5 Ka mb hat la (20 04) :fe ature bas ed 6 3. 5 4 5. 2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Advances in Neural Information Processing Systems 14. Cambridge, MA. Culotta A. and Sorensen J. (2004). Dependency tree th parse trees.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Mentions have three levels: names, nomial expressions or pronouns. The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task. For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities. Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.). Mentions have three levels: names, nomial expressions or pronouns. The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["These methods take two DGs as their inputs and give a unification result DG. Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Method In a system where FS unification is applied, there are features whose values fail relatively often in unification with other values and there are features whose values do not fail so often. For example, in Japanese sentence analysis, unification of features for conjugation forms, case markers, and semantic selectional restrictions tends to fail but unification of features for semantic representations does not fail. In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails. For example, when unification of features for case markers does fail, treating these features first avoids treating features for senmntic representations. The SING unification method uses this failure tendency infornmtion.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["(2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small. (3) Unevenness of FS unification failure tendency: in extreme cases, if every feature has the same unification failure tendency, this method has no advantage. However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints. In such cases, the SING unification method obtains efl]ciency gains. The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system. That is, an FS unification method is proposed that introduces a strategy called the e_arly failure \u00a3inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures. This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method). These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method). Section 2 explains typed feature structures (TFSs) and unification on them.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy. 'the method treats features tending to fail in unification first.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6]. Thus treatments such as strategic unification [6] have been developed.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 0, "Strategic Lazy Incremental Copy Graph Unification", "ABSTRACT"], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation. The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing systems mainly based on I.yped feature structure unification such as natural language analysis and generation sysl~ems. Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.", "This observation is the basis for a reordering method proposed by Kogure [1990].", 1, "Strategic Lazy Incremental Copy Graph Unification", "ABSTRACT"], ["Unification fails in treating arcs with common labels more often than in treating arcs with unique labels. Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test. Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method. The efficiency of the LING unification method depends on the proportion of newly created structures in the unification result structures. Two worst eases can be considered: (t) If there are no arcs whose labels are unique to an input node witlh respect to each other, the procedure in LING unification method behaves in the same way as the procedure in the Wroblewski's method.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["The whole subgraph rooted by 6 l/<a c> is then copied. This is because such subgraphs can be modified later. For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated. Incremental Copy Graph Unification PROCEDURE Unify(node1, node2) node1 = Dereference(nodel). node2 = Dereferencelnode2).", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction. As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction. As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag. This is because the type symbol of a 'rFS represents salient information on the whole TFS.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["It substitutes arcs with newly copied nodes for existing arcs. That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem. The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Next, the procedure treats arcs obtained by ComplementArcs. Each arc value is copied and an arc with the same label and the copied value is added to the output node. For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig. 5. The unification procedure is applied recursively to feature a values of the input nodes.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG. Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["U_ Bottom Figure 1: Exainple of a type symbol lattice --2-- peSymb\u00b010 eaturel TypeSymboll ] ]] I feature2 TypeSymbol2 I feature3 ?Tag T ypeSymbol3 ] ]feature4 TypeSymbol4 L [.feature5 TypeSymbol5 TIeature3 7Tag (a) feature-value matrix notation \"?\" i~ the prefix for a tag and TFSs with the same tag are token-identical. TypeSym bol/~ feo~.,o/ I TypeSymboll ~ [. TypeSymbol2 4\u00a2\" '~\u00b0~'~/.~ypeSymbol3 featury \"X~ature5 TypeSymbol4 4r \"~TypeSymbol5 (b) directed graph notation Figure 2: TFS notations Phrase [sub(at ?X2 SignList ] dtrs CHconst Sign U Syn i'oo I syn I head ?Xl . ] ubcat NonEmptySignLIst | ['first ]1 ?\u00d73 Lrest ?X2 J j Phrase -dtrs CHconst hdtr LexicalSignsyn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign ,11 yn Synead Head L~,os N] Irest EmptySignkist Phrase \"syn Syn head ?X1 Head Fpos P Lform Ga ] Lsubcat ?X2 Empl.ySignList dtrs CHconst ccltr ?X3 Sign syn iyn head Head _ [pos N hdtr LexicalSign l-syn Syn l I F head :x~ 7/ Lsubcat [ NonEinptySignList l l P\"\" ~\u00d7~ llll Lrest ?X2 JJjJ Figure 3: Example of TFS unification Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet. A unification example is shown in Fig. 3.", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Typed Feature Structures."], ["I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem. The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not. The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 0, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG. Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy.", " Data-Structure Sharing: Two or more distinct graphs share the same subgral)h by converging nil the same node the noti(m nf", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan. Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification. Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871. These methods uses rooted directed graphs (DGs) to represent FSs.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["In contrast, in a procedure-based system which uses IF-TItEN style rules (i.e., consisting of explicit test and structure-building operations), it is possible to construct the semantic representation (TIIEN par'g) after checking the agreement (IF part). Such a system has the advantage of processing efficiency but the disadvantage of lacking multidirectionality. In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system. That is, an FS unification method is proposed that introduces a strategy called the e_arly failure \u00a3inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures. This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig. 5. The unification procedure is applied recursively to feature a values of the input nodes. The node specified by the feature path <a> fi'om input graph G1 (Gl/<a>) has an arc with the label c and the corresponding node of input graph G2 does not. The whole subgraph rooted by 6 l/<a c> is then copied.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["its arguments and gives one list of arcs whose labels are unique to one input list. The unification procedure first treats arc pairs obtained by SharedArcs. The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom. Next, the procedure treats arcs obtained by ComplementArcs. Each arc value is copied and an arc with the same label and the copied value is added to the output node.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The slot has pairs consisting of nodes and arcs as its value. The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig. 7): (1) if nodel ', the dereference result of node/, is current, then CopyNode returns node l\" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1\" and if it returns ,~;everal arc copies, CopyNode creates a new copy node. It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1\" and returns Nil_. ,',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["It substitutes arcs with newly copied nodes for existing arcs. That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["The efficiency of systems based on ..~uch formalisms, such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies. Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO[Pollard and Sag 861 and JPSG[Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs. For example, a spoken Present. affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan. Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification. Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871. These methods uses rooted directed graphs (DGs) to represent FSs.", "With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current ~That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["ENDIF ENDIE ENDPROCEDURE Figure 6: Incremental copy graph unification procedure The problem with Wroblewski's method is that tile whole result DG is created by using only newly created structures. In the example in Fig. 5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels. This order is related to the unification failure tendency. Unification fails in treating arcs with common labels more often than in treating arcs with unique labels.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["I\"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly. In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts. Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency. I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS. Therefore, Pereira's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification. However, Pereira's method can create skeleton-enviromnent structures that are deeply embedded, for example, in reeursively constructing large phrase structure fl'om their parts.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not. The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted. Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not. The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden (1990) proposed a unification algorithm that uses active data structures, Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug, and Emeie (1991) proposed a lazy-incremental copying (LIC) unification that uses chronological dereference.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["I\"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly. In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts. Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency. I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>).", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["In TFS unification based on Wrobtewski's method, a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively, as shown in Fig. 4. The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY. When a NODE's GENERATION value is equal to the global value specifying the current unit]cation process, the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE's two different slots, FORWARD and COPY, for representing forwarding relationships. A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig. 7): (1) if nodel ', the dereference result of node/, is current, then CopyNode returns node l\" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1\" and if it returns ,~;everal arc copies, CopyNode creates a new copy node. It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1\" and returns Nil_. ,',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results. When a new copy of a node is needed later, the LING unification procedure will actually copy structures using the COPY-DEPENDENCY slot value of the node (in GetOutNode procedure in lJ'ig.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["The procedure assumes the existence of two procedures, namely, SharedArcs and ComplementArcs. The SharedArcs procedure takes two lists of arcs as its arguments and gives two lists of arcs each of which contains arcs whose labels exists in both lists with the same arc label order. The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: \"<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski's method Input graph GI Input graph 62 \u00a2 .......'77 ........ i : Sobg,'aphs not required to be copied L ........................................... Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted. its arguments and gives one list of arcs whose labels are unique to one input list.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>).", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>). To achieve this, I, he LING unification method, which uses copy dependency information, was developed.", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Godden uses active data structures (Lisp closures) to implement lazy evaluation of copying, and Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test. Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method). Section 2 explains typed feature structures (TFSs) and unification on them. Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method. The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method. Section 3 and 4 introduce the LING method and the SING method, respectively.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Unification fails in treating arcs with common labels more often than in treating arcs with unique labels. Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);  provide a better match between the characteristics of the unifiers and those of the linguistic processors.", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Hence, this approach produced lower F-scores. However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Another advantage of the word-based IOB tagging over the character-based is its speed. The subword-based approach is faster because fewer words than characters were labeled. We found a speed up both in training and test. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more delicately.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach.", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv). For detailed info. of the corpora and these scores, refer to (Emerson, 2005). For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv. The authors appreciate the reviewers\u2019 effort and good advice for improving the paper.", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["However, neither was perfect. The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["3.2 Effect of the confidence measure. In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation. The effect of the confidence measure is shown in Table 3, where we used \u03b1 = 0.7 and confidence threshold t = 0.8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based. We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["The subword-based approach is faster because fewer words than characters were labeled. We found a speed up both in training and test. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.", "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging. Each subword is given a prior IOB tag, tw . C Miob (t|w), a \uf8eb M \uf8eb\uf8f6\uf8f6 confidence probability derived in the process of IOB tag exp \uf8ec)' \uf8ec)' \u03bbk fk (ti\u22121 , ti , W ) + )' \u00b5k gk (ti , W )\uf8f7\uf8f7 /Z, \uf8ec\uf8ed i=1 \uf8ec\uf8ed k k \uf8f7\uf8f8 \uf8f7\uf8f8 (1) ging, is defined as Z = )' T =t0 t1 \u00b7\u00b7\u00b7tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 \u00b7\u00b7\u00b7tM ,ti =t P(T |W, wi ) T =t 0 t1 \u00b7\u00b7\u00b7 tM P ( T | W ) where we call fk (ti\u22121 , ti , W ) bigram feature functions because the features trigger the previous observation ti\u22121 where the numerator is a sum of all the observation sequences with word wi labeled as t. \u03b4(tw , tiob )ng denotes the contribution of the dictionary- based segmentation. It is a Kronecker delta function defined as \u03b4(tw , tiob )ng = { 1 if tw = tiob 0 otherwise In Eq. 2, \u03b1 is a weighting between the IOB tagging and the dictionary-based word segmentation.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2. Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tagging was very effective. The IOB tagging approach adopted in this work is not a new idea. It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach.", "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure. In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation. The effect of the confidence measure is shown in Table 3, where we used \u03b1 = 0.7 and confidence threshold t = 0.8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Qc 2006 Association for Computational Linguistics input \u5498\u38c5\u1bf9\u0523\u0cfc\u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation \u5498 \u38c5 \u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output \u5498\u38c5\u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data. We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found the value 0.7 for \u03b1, empirically. By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created. For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.", "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We will give a detailed description of this approach in Section 2. \u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["\u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["We found the value 0.7 for \u03b1, empirically. By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created. For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure.", "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["For detailed info. of the corpora and these scores, refer to (Emerson, 2005). For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["9 5 1 0. 97 1 0. 95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2. Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tagging was very effective.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "(2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv. The authors appreciate the reviewers\u2019 effort and good advice for improving the paper.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, \u201c (Beijing-city)\u201d can be segmented as \u201c (Beijing-city)/O,\u201d or \u201c (Beijing)/B (city)/I,\u201d or \u201d (north)/B (capital)/I (city)/I.\u201d In this work we used forward maximal match (FMM) for disambiguation.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions. However, keep in mind that the dictionary-based approach can produce a higher R-iv rate. We will use this advantage in the confidence measure approach.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "One existing method that is based on sub-word information, Zhang et al. (2006), combines a C R F and a rule-based model.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["For detailed info. of the corpora and these scores, refer to (Emerson, 2005). For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["9 5 1 0. 97 1 0. 95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2. Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tagging was very effective.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["\u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al. (2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The upper numbers are of the character- based and the lower ones, the subword-based. using the FMM, and then labeled with \u201cIOB\u201d tags by the CRFs. The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based. We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["There are several steps to train a subword-based IOB tag- ger. First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193\u2013196, New York, June 2006. Qc 2006 Association for Computational Linguistics input \u5498\u38c5\u1bf9\u0523\u0cfc\u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation \u5498 \u38c5 \u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output \u5498\u38c5\u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data. We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv. The authors appreciate the reviewers\u2019 effort and good advice for improving the paper.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold. In Section 3.2 we will present the experimental segmentation results of the confidence measure approach. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections. The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["The IOB tagging approach adopted in this work is not a new idea. It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used. Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001). Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based. We proved the new approach enhanced the word segmentation significantly.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Of course, backward maximal match (BMM) or other approaches are also applicable. We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach. In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data. We downloaded and used the package \u201cCRF++\u201d from the site \u201chttp://www.chasen.org/\u02dctaku/software.\u201d According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 \u00b7 \u00b7 \u00b7 tM , given the word sequence, W = w0 w1 \u00b7 \u00b7 \u00b7 wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . \u03bbk and \u00b5k are the model parameters corresponding to feature functions fk and gk respectively. The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold. In Section 3.2 we will present the experimental segmentation results of the confidence measure approach. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections. The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["The subscripts are position indicators. 0 means the current word; \u22121, \u22122, the first or second word to the left; 1, 2, the first or second word to the right. For the bigram features, we only used the previous and the current observations, t\u22121 t0 . As to feature selection, we simply used absolute counts for each feature in the training data. We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff. A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Of course, backward maximal match (BMM) or other approaches are also applicable. We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach. In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data. We downloaded and used the package \u201cCRF++\u201d from the site \u201chttp://www.chasen.org/\u02dctaku/software.\u201d According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 \u00b7 \u00b7 \u00b7 tM , given the word sequence, W = w0 w1 \u00b7 \u00b7 \u00b7 wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . \u03bbk and \u00b5k are the model parameters corresponding to feature functions fk and gk respectively. The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known. For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d. Our approaches produced wrong segmentations for labeling inconsistency. Another advantage of the word-based IOB tagging over the character-based is its speed.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["For detailed info. of the corpora and these scores, refer to (Emerson, 2005). For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["of the corpora and these scores, refer to (Emerson, 2005). For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation. Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u201cdict-hybrid.\u201d (Zhang et al., 2006) We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["\u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193\u2013196, New York, June 2006. Qc 2006 Association for Computational Linguistics input \u5498\u38c5\u1bf9\u0523\u0cfc\u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation \u5498 \u38c5 \u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output \u5498\u38c5\u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data. We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters. For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit. We will give a detailed description of this approach in Section 2.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters. For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["By this approach we can change R-oovs and R-ivs and find an optimal tradeoff. This approach will be described in Section 2.2. In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method. Section 3 presents our experimental results. Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit. We will give a detailed description of this approach in Section 2.", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["\u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known. For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d. Our approaches produced wrong segmentations for labeling inconsistency. Another advantage of the word-based IOB tagging over the character-based is its speed.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, \u201c (Beijing-city)\u201d can be segmented as \u201c (Beijing-city)/O,\u201d or \u201c (Beijing)/B (city)/I,\u201d or \u201d (north)/B (capital)/I (city)/I.\u201d In this work we used forward maximal match (FMM) for disambiguation.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, \u201c (Beijing-city)\u201d can be segmented as \u201c (Beijing-city)/O,\u201d or \u201c (Beijing)/B (city)/I,\u201d or \u201d (north)/B (capital)/I (city)/I.\u201d In this work we used forward maximal match (FMM) for disambiguation. Of course, backward maximal match (BMM) or other approaches are also applicable.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH.\u201d However, when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH,\u201d \u201cENQUIRIES\u201d does not occur, but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may. Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed.", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454\u2013462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al. (2007), did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Considering that 1 2 2 \\lw \u2212 w\\l + C i=1 max (fi j \u2212 \u2206hi j \u00b7 wt) (4) 1\u2264 j\u2264n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) \u2194 x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees.", "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time. The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data. This seems in line with the finding of Watanabe et al. (2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data. Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest.", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["(2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["How did the various new features improve the translation quality of our two systems? We begin by examining the discount features. For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights. We see in both cases that one-count rules are strongly penalized, as expected. Reward \u22120.42 a \u22120.13 are \u22120.09 at \u22120.09 on \u22120.05 was \u22120.05 from \u22120.04 \u2019s \u22120.04 by \u22120.04 is \u22120.03 it \u22120.03 its . Penalty +0.67 of +0.56 the +0.47 comma +0.13 period +0.11 in +0.08 for +0.06 to +0.05 will +0.04 and +0.02 as +0.02 have . Bonus \u22120.50 period \u22120.39 VP-C \u22120.36 VB \u22120.31 SG-C \u22120.30 MD \u22120.26 VBG \u22120.25 ADJP\u22120.22LRB \u22120.21 VP-BAR \u22120.20 NPB-BAR \u22120.16 FRAG P en alt y +0.93 IN +0.57 NNP +0.44 NN +0.41 DT +0.34 JJ +0.24 right double quote +0.20 VBZ +0.19 NP +0.16 TO +0.15 ADJP-BAR +0.14 PRN-BAR Table 3: Weights learned for inserting target English words with rules that lack Chinese words.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["ations of all learners to decode the test set. The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1.5 B\uf76c\uf765\uf775 improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B\uf76c\uf765\uf775 improvement. The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes. 6 Analysis.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["(2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["For example, there may be an tendency to generate too many determiners or past-tense verbs. We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols. For a rule like NPB(NNP(us) NNP(president) x0:NNP) \u2194 meiguo zongtong x0 the feature node-count-NPB gets value 1, node- count-NNP gets value 2, and all others get 0. Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side. Sample syntax- based insertion rules are: NPB(DT(the) x0:NN) \u2194 x0 S(x0:NP-C VP(VBZ(is) x1:VP-C)) \u2194 x0 x1 We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [", fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al. (2007), did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["(2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["(2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder. In this section, we describe the new features introduced on top of our baseline systems. Discount features Both of our systems calculate several features based on observed counts of rules in the training data.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [", fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Considering that 1 2 2 \\lw \u2212 w\\l + C i=1 max (fi j \u2212 \u2206hi j \u00b7 wt) (4) 1\u2264 j\u2264n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.", "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We also notice that the-insertion rules sometimes have a good effect, as in the translation \u201cin the bloom of youth,\u201d but other times have a bad effect, as in \u201cpeople seek areas of the conspiracy.\u201d Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk. There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk. We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features. We now turn to features that make use of source-side context.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set: PP \u2192 VBN NP-C PP-BAR \u2192 NP-C IN VP \u2192 NP-C PP CONJP \u2192 RB IN Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category. For example, there may be an tendency to generate too many determiners or past-tense verbs. We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols. For a rule like NPB(NNP(us) NNP(president) x0:NNP) \u2194 meiguo zongtong x0 the feature node-count-NPB gets value 1, node- count-NNP gets value 2, and all others get 0. Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["However, when we apply MIRA with the features already listed, these translation errors all disappear, as demon 38.5 38 37.5 37 36.5 36 35.5 35 Tune Test 0 5 10 15 20 25 Epoch strated by examples 4\u20135 in Figure 1. Why does this happen? It turns out that in translation hypotheses that move \u201cX said\u201d or \u201cX asked\u201d away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear. Therefore, the new features work to discourage these hypotheses. Example 6 shows additionally that commas next to speaking verbs are now correctly deleted.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality. We draw three conclusions from this study. First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al. (2007), did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "This observation con\ufb01rms previous \ufb01ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Considering that 1 2 2 \\lw \u2212 w\\l + C i=1 max (fi j \u2212 \u2206hi j \u00b7 wt) (4) 1\u2264 j\u2264n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["(2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9\u2217 36. 38.", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Experiments."], ["We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) \u2194 x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c).", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT.", "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["(2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["\u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees. e\u2217 = arg max (B\uf76c\uf765\uf775(e) + h(e) \u00b7 w) (2) e Let \u2206hi j = h(e\u2217) \u2212 h(ei j).", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectation- maximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006).", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT.", "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees.", "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["(2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9\u2217 36. 38. 4 37.6\u2217\u2217 Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8\u2217 38 .7 39.9\u2217 38. 39. 6 40.6\u2217\u2217 Table 1: Adding new features with MIRA significantly improves translation accuracy.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls, i, i] Terminal Symbol (X \u2192 \u03b1Ls) \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fj,k \u03b2Ls, i, j] [X, j, Rj,k ] [X \u2192 \u03b1Fj,k \u2022 \u03b2Ls, i, Rj,k ] [X \u2192 \u03b1 \u2022 Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X \u2192 \u03b1Ls\u2022, i, Ri,j ] log p(t|s) = \u03bbm hm(t, s) (3) m Goal [X \u2192 \u03b1Ls\u2022, 0, |V | \u2212 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separately- tunable features for each syntactic category.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["(2007); here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9\u2217 36. 38.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 0, "11,001 New Features for Statistical Machine Translation", "Experiments."], ["(2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees.", "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based. rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries. The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations. In the rhetorical tree, nuclearity information is then used to extract a \u201ckernel tree\u201d that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["Like in the co-reference annotation, G\u00a8otze\u2019s proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet. We use MMAX for this annotation as well. Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 0, "The Potsdam Commentary Corpus", "PAPER"], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 0, "The Potsdam Commentary Corpus", "ABSTRACT"], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 0, "The Potsdam Commentary Corpus", "Introduction"], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Figure 2 shows a screenshot (which is of somewhat limited value, though, as color plays a major role in signalling the different statuses of the information). In the small window on the left, search queries can be entered, here one for an NP that has been annotated on the co-reference layer as bridging. The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) \u2022 the full text, \u2022 the annotation values for the activated annotation set (co-reference), \u2022 the actual annotation tiers, and \u2022 the portion of text currently \u2018in focus\u2019 (which also appears underlined in the full text). Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels. Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then \u201cclick away\u201d those words that are here not used as connectives (such as the conjunction und (\u2018and\u2019) used in lists, or many adverbials that are ambiguous between connective and discourse particle). Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like \u2018for sub- junctor, mark all words up to the next comma as the first segment\u2019), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["One conclusion drawn from this annotation effort was that for humans and machines alike, 2 www.sfs.nphil.unituebingen.de/Elwis/stts/ stts.html 3 www.coli.unisb.de/sfb378/negra-corpus/annotate. html 4 www.wagsoft.com/RSTTool assigning rhetorical relations is a process loaded with ambiguity and, possibly, subjectivity. We respond to this on the one hand with a format for its underspecification (see 2.4) and on the other hand with an additional level of annotation that attends only to connectives and their scopes (see 2.5), which is intended as an intermediate step on the long road towards a systematic and objective treatment of rhetorical structure. 2.4 Underspecified rhetorical structure. While RST (Mann, Thompson 1988) proposed that a single relation hold between adjacent text segments, SDRT (Asher, Lascarides 2003) maintains that multiple relations may hold simultaneously.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future. Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) \u2022 the full text, \u2022 the annotation values for the activated annotation set (co-reference), \u2022 the actual annotation tiers, and \u2022 the portion of text currently \u2018in focus\u2019 (which also appears underlined in the full text). Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels. Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase. 3.2 Stochastic rhetorical analysis. In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).", 1, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English. For effectively annotating connectives/scopes, we found that existing annotation tools were not well-suited, for two reasons: \u2022 Some tools are dedicated to modes of annotation (e.g., tiers), which could only quite un-intuitively be used for connectives and scopes. \u2022 Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them. 5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["In (Reitter, Stede 2003) we went a different way and suggested URML5, an XML format for underspecifying rhetorical structure: a number of relations can be assigned instead of a single one, competing analyses can be represented with shared forests. The rhetorical structure annotations of PCC have all been converted to URML. There are still some open issues to be resolved with the format, but it represents a first step. What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["The significant drop in number of pupils will begin in the fall of 2003. The government has to make a decision, and do it quickly. Either save money at any cost - or give priority to education. Figure 1: Translation of PCC sample commentary (STTS)2. 2.2 Syntactic structure.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.", "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based. rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries. The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations. In the rhetorical tree, nuclearity information is then used to extract a \u201ckernel tree\u201d that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 0, "The Potsdam Commentary Corpus", "Introduction"], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).", 1, "The Potsdam Commentary Corpus", "Introduction"], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["2.5 Connectives with scopes. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes. This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD. This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["\u2022 Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them. 5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then \u201cclick away\u201d those words that are here not used as connectives (such as the conjunction und (\u2018and\u2019) used in lists, or many adverbials that are ambiguous between connective and discourse particle).", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["3.5 Improved models of discourse. structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure. One key issue here is to seek a discourse-based model of information structure. Since Dane\u02c7s\u2019 proposals of \u2018thematic development patterns\u2019, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts. (Hartmann 1984), for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST).", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Thus we are interested not in extraction, but actual generation from representations that may be developed to different degrees of granularity. In order to evaluate and advance this approach, it helps to feed into the knowledge base data that is already enriched with some of the desired information \u2014 as in PCC. That is, we can use the discourse parser on PCC texts, emulating for instance a \u201cco-reference oracle\u201d that adds the information from our co-reference annotations. The knowledge base then can be tested for its relation-inference capabilities on the basis of full-blown co-reference information. Conversely, we can use the full rhetorical tree from the annotations and tune the co-reference module.", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", 1, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 0, "The Potsdam Commentary Corpus", "ABSTRACT"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 0, "The Potsdam Commentary Corpus", "Introduction"], ["We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988). Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised. Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide.", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based. rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries. The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations. In the rhetorical tree, nuclearity information is then used to extract a \u201ckernel tree\u201d that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 0, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004) and the RST Discourse Treebank of Wall Street Journal articles (WSJ) (Carlson et al., 2003).", 1, "The Potsdam Commentary Corpus", "Introduction"], ["5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then \u201cclick away\u201d those words that are here not used as connectives (such as the conjunction und (\u2018and\u2019) used in lists, or many adverbials that are ambiguous between connective and discourse particle). Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like \u2018for sub- junctor, mark all words up to the next comma as the first segment\u2019), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 0, "The Potsdam Commentary Corpus", "Introduction"], ["As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers Su\u00a8ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence. The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences. For illustration, an English translation of one of the commentaries is given in Figure 1. The paper is organized as follows: Section 2 explains the different layers of annotation that have been produced or are being produced. Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 0, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 0, "The Potsdam Commentary Corpus", "PAPER"], ["(Webber et al., 2003)). Again, the idea is that having a picture of syntax, co-reference, and sentence-internal information structure at one\u2019s disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported. The PCC is not the result of a funded project. Instead, the designs of the various annotation layers and the actual annotation work are results of a series of diploma theses, of students\u2019 work in course projects, and to some extent of paid assistentships. This means that the PCC cannot grow particularly quickly.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 0, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 1, "The Potsdam Commentary Corpus", "ABSTRACT"], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase. 3.2 Stochastic rhetorical analysis. In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines. Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method. For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 0, "The Potsdam Commentary Corpus", "ABSTRACT"], ["5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then \u201cclick away\u201d those words that are here not used as connectives (such as the conjunction und (\u2018and\u2019) used in lists, or many adverbials that are ambiguous between connective and discourse particle). Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like \u2018for sub- junctor, mark all words up to the next comma as the first segment\u2019), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future. Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines. Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method. For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%. Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based.", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based. rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries. The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations. In the rhetorical tree, nuclearity information is then used to extract a \u201ckernel tree\u201d that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988). Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised. Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Having explained the various layers of annotation in PCC, we now turn to the question what all this might be good for. This concerns on the one hand the basic question of retrieval, i.e. searching for information across the annotation layers (see 3.1). On the other hand, we are interested in the application of rhetorical analysis or \u2018discourse parsing\u2019 (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5). basically complete, yet some improvements and extensions are still under way. The web-based Annis imports data in a variety of XML formats and tagsets and displays it in a tier-orientedway (optionally, trees can be drawn more ele gantly in a separate window).", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide. (Again, the goal of also in structural features. As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers Su\u00a8ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence. The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences. For illustration, an English translation of one of the commentaries is given in Figure 1.", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 0, "The Potsdam Commentary Corpus", "Introduction"], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide. (Again, the goal of also in structural features.", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", 1, "The Potsdam Commentary Corpus", "Introduction"], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines. Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method. For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%. Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based.", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive). They are also labelled for their topicality (yes / no), and this annotation is accompanied by a confidence value assigned by the annotator (since it is a more subjective matter). Finally, the focus/background partition is annotated, together with the focus question that elicits the corresponding answer.", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova\u00b4 et al., 2009).", 1, "The Potsdam Commentary Corpus", "ABSTRACT"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998). This paper is organised as follows.", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted. They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora. The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus. We prepare an evaluation of our algorithm as applied to the collocation relationships (cf. section 2), and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly.", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 0, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000). The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["In section 2, we present the graph model from which we discover word senses. Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000). The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm. We used the simple graph model based on co-occurrences of nouns in lists (cf. section 2) for our experiment. We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry).", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 0, "Discovering Corpus-Specific Word Senses", "Experimental Results."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000). The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty. The algorithm consists of the following steps: 1. Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6. 2. Recursively remove all nodes of degree one. Then remove the node corresponding with w from G. 3.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted. The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose. The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label.", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm. We used the simple graph model based on co-occurrences of nouns in lists (cf. section 2) for our experiment. We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry).", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 0, "Discovering Corpus-Specific Word Senses", "Experimental Results."], ["Finally, section 6 sketches applications of the algorithm and discusses future work. The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words.", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity. This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998). This paper is organised as follows. In section 2, we present the graph model from which we discover word senses.", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000). The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).", "The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another. In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose. The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label. The family of such algorithms is described in (Widdows, 2003). In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose. The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label. The family of such algorithms is described in (Widdows, 2003). In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["41=0 441=P .4161. sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph of the word mouse Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse. However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense. There are, of course, many more types of polysemy (cf.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 0, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["In section 2, we present the graph model from which we discover word senses. Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000). The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning. This approach to disambiguation combines the benefits of both Yarowsky's (1995) and Schtitze's (1998) approaches. Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used. Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted. They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora.", "As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.", 1, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998). This paper is organised as follows. In section 2, we present the graph model from which we discover word senses.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph of the word mouse Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse. However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense. There are, of course, many more types of polysemy (cf. e.g.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 0, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences.", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features. 1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity. This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Finally, section 6 sketches applications of the algorithm and discusses future work. The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words.", "From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty. The algorithm consists of the following steps: 1. Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6. 2. Recursively remove all nodes of degree one. Then remove the node corresponding with w from G. 3.", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Apply MCL to Gw with a fairly big inflation parameter r which is fixed. 4. Take the \"best\" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5. Go back to 1 with the reduced/devalued set of features F. 6. Go through the final list of clusters L and assign a name to each cluster using a broad-coverage taxonomy (see below).", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000). The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose. The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label. The family of such algorithms is described in (Widdows, 2003). In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another. In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["7. Output the list of class-labels which best represent the different senses of w in the corpus. The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", 1, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features. 1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity. Section 5 describes the experiment and presents a sample of the results. Finally, section 6 sketches applications of the algorithm and discusses future work.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Here we only mention a few direct results of our work. Our algorithm does not only recognise ambiguity, but can also be used to resolve it, because the features shared by the members of each sense cluster provide strong indication of which reading of an ambiguous word is appropriate given a certain context. This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated. The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning. This approach to disambiguation combines the benefits of both Yarowsky's (1995) and Schtitze's (1998) approaches.", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", 1, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["7. Output the list of class-labels which best represent the different senses of w in the corpus. The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise. Below, we outline an algorithm which circumvents the problem of choosing the right parameters.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000). The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task. In this section, we brie y review our translation approach.", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability. The inverted alignment probability p(bijbi\udbc0\udc001; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration. The details are given in (Och and Ney, 2000). The sentence length probability p(JjI) is omitted without any loss in performance. For the inverted alignment probability p(bijbi\udbc0\udc001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.", "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search. Here, we process only full-form words within the translation procedure. the number of permutations carried out for the word reordering is given.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The proof is given in (Tillmann, 2000). 3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996). A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup. A typical situation is shown in Figure 1. When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.", "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability. The inverted alignment probability p(bijbi\udbc0\udc001; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration. The details are given in (Och and Ney, 2000). The sentence length probability p(JjI) is omitted without any loss in performance. For the inverted alignment probability p(bijbi\udbc0\udc001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city. Subsets C of increasing cardinality c are processed. The algorithm works due to the fact that not all permutations of cities have to be considered explicitly. For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored. This algorithm can be applied to statistical machine translation.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Subsets C of increasing cardinality c are processed. The algorithm works due to the fact that not all permutations of cities have to be considered explicitly. For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored. This algorithm can be applied to statistical machine translation. Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["= p(fj je) max \u00c6;e00 j02Cnfjg np(jjj0; J) p(\u00c6) p\u00c6(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j). The resulting algorithm is depicted in Table 1. The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary. 3.1 Word ReOrdering with Verbgroup. Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup. A typical situation is shown in Figure 1. When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg. This approach leads to a search procedure with complexity O(E3 J4). The proof is given in (Tillmann, 2000). 4.1 The Task and the Corpus.", "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities. The resulting algorithm has a complexity of O(n!). However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp. The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["In the following, we assume that this word joining has been carried out. Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup. In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962). The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability. The inverted alignment probability p(bijbi\udbc0\udc001; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration. The details are given in (Och and Ney, 2000). The sentence length probability p(JjI) is omitted without any loss in performance. For the inverted alignment probability p(bijbi\udbc0\udc001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["The details are given in (Tillmann, 2000). For each extension a new position is added to the coverage set. Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $). Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence. The search starts in the hypothesis (I; f;g; 0).", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task. The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words.", "\u2022 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["In the following, we assume that this word joining has been carried out. Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup. In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962). The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The German finite verbs 'bin' (second example) and 'k\u007fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable. In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches.", "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20]; \u2022 alignment template approach [15]; \u2022 cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The German finite verbs 'bin' (second example) and 'k\u007fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable. In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words. The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000). These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition. The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["For the inverted alignment probability p(bijbi\udbc0\udc001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining. The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment. We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training. The word joining is done on the basis of a likelihood criterion. An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["In Eq. (1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.", "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability. The inverted alignment probability p(bijbi\udbc0\udc001; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration. The details are given in (Och and Ney, 2000). The sentence length probability p(JjI) is omitted without any loss in performance. For the inverted alignment probability p(bijbi\udbc0\udc001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) ! (S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account. To be short, we omit the target words e; e0 in the formulation of the search hypotheses. There are 13 types of extensions needed to describe the verbgroup reordering. The details are given in (Tillmann, 2000).", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words. The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000). These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition. The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction. Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction. A dynamic programming recursion similar to the one in Eq. 2 is evaluated. In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions. To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered. Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches.", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "The decoding algorithm employed for this chunk + weight \u00d7 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The final score is obtained from: max e;e0 j2fJ\udbc0\udc00L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence. The complexity of the quasimonotone search is O(E3 J (R2+LR)). The proof is given in (Tillmann, 2000). 3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg. This approach leads to a search procedure with complexity O(E3 J4). The proof is given in (Tillmann, 2000). 4.1 The Task and the Corpus. We have tested the translation system on the Verbmobil task (Wahlster 1993).", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities. The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei\udbc0\udc001 = e0. The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words. For \u00c6 = 1, a new target language word is generated using the trigram language model p(eje0; e00). For \u00c6 = 0, no new target word is generated, while an additional source sentence position is covered.", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words. The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000). These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition. The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["On average, 6 reference translations per automatic translation are available. The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken. This measure has the advantage of being completely automatic. SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person. For the error counts, a range from 0:0 to 1:0 is used.", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches.", "no . of se nt en ce s 2 0 0 no . of ru nn in g w or ds 2 05 5 no . of w or d for ms no . of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996). A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction. A dynamic programming recursion similar to the one in Eq. 2 is evaluated. In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg. This approach leads to a search procedure with complexity O(E3 J4).", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The final score is obtained from: max e;e0 j2fJ\udbc0\udc00L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence. The complexity of the quasimonotone search is O(E3 J (R2+LR)). The proof is given in (Tillmann, 2000). 3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search. Here, we process only full-form words within the translation procedure. the number of permutations carried out for the word reordering is given.", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The search starts in the hypothesis (I; f;g; 0). f;g denotes the empty set, where no source sentence position is covered. The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max \u00c6;e00 np(jjj0; J) p(\u00c6) p\u00c6(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j). f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ \udbc0\udc00L; ; Jg. The final score is obtained from: max e;e0 j2fJ\udbc0\udc00L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . . , wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "ABSTRACT"], ["Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient. The bigram must have occurred 5 or more times to be included as a feature. This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process. The training and test data are converted to feature vectors where each feature represents the occurrence of one of the bigrams that belong in the feature set. This representation of the training data is the actual input to the learning algorithms.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Method."], ["This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7. 1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coecient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner. If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Analysis of Experimental Results."], ["We use the Weka (Witten and Frank, 2000) implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classi?er. Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml. Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. Ten teams participated in the supervised learning portion of this event. Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarri?", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Data."], ["The following process is repeated for each task. Capitalization and punctuation are removed from the training and test data. Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient. The bigram must have occurred 5 or more times to be included as a feature. This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Method."], ["One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. While the accuracy of this approach was as good as any previously published results, the learned models were complex and dicult to interpret, in e?ect acting as very accurate black boxes. Our experience has been that variations in learning algorithms are far less signi?cant contributors to disambiguation accuracy than are variations in the feature set. In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Discussion."], ["Test instances are disambiguated by ?nding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features. An instance of an ambiguous word is dis- ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context. We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er. The majority classi?er assigns the most common sense in the training data to every instance in the test data. A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Learning Decision Trees."], ["It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution. One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements. This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning. Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison. The experimental data is discussed, and then the empirical results are presented.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb. Thus, there are 36 tasks involving the disambiguation of 29 di?erent words. The words and part of speech associated with each task are shown in Table 1 in column 1. Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided. The number of test and training instances for each task are shown in columns 2 and 4.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Data."], ["However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources. These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy. It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in (Smadja et al., 1996). We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse. Decision trees are among the most widely used machine learning algorithms.", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution. One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements. This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning. Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison. The experimental data is discussed, and then the empirical results are presented.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases. Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram. These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in (Smadja et al., 1996).", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text. We are optimistic that this is viable, since bigram features are easy to identify in raw text. This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words. The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Conclusion."], ["Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump. One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. While the accuracy of this approach was as good as any previously published results, the learned models were complex and dicult to interpret, in e?ect acting as very accurate black boxes. Our experience has been that variations in learning algorithms are far less signi?cant contributors to disambiguation accuracy than are variations in the feature set.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Discussion."], ["In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text. We are optimistic that this is viable, since bigram features are easy to identify in raw text. This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words. The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Conclusion."], ["1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coecient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner. If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses. In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Analysis of Experimental Results."], ["We are optimistic that this is viable, since bigram features are easy to identify in raw text. This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words. The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota. We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Conclusion."], ["The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks. In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice Coecient. The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither. The Dice Coecient is based strictly on the event where w 1 and w 2 occur together in a bigram. There are 6 tasks where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, and sanction-p.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Results."], ["These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance. (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions. However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of data. We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical signi?cance when the bi- gram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of ?t statistics. We perform tests using X 2 , G 2 , and Fisher's exact test for each bigram.", "The convergence is quicker for X2 than G2.\u201d In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["This was motivated by the success of decision stumps in performing disambiguation based on a single bigram feature. In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features. This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7. 1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coecient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Analysis of Experimental Results."], ["A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases. Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram. These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in (Smadja et al., 1996).", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["The value of n 12 shows how often bigrams occur where big is the ?rst word and cat is not the second. The counts in n +1 and n 1+ indicate how often words big and cat occur as the ?rst and second words of any bigram in the corpus. The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family. (Cressie and Read, 1984) introduce the power divergence family of goodness of ?t statistics. A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources. These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bigrams, which we de?ne to be two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation of Bigram Counts consecutive words that occur in a text. The distributional characteristics of bigrams are fairly consistent across corpora; a majority of them only occur one time. Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen. We explore two alternatives, the power divergence family of goodness of ?t statistics and the Dice Coecient, an information theoretic measure related to point- wise Mutual Information. Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 ? 2 contingency table.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family. (Cressie and Read, 1984) introduce the power divergence family of goodness of ?t statistics. A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic. These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance. (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence. But as the reliability and U(xjy) metrics indicate, it is not completely clear how the metric should be defined. This problem is addressed in the next section. 3.5 Hybrid method 2: Bayesian classifiers. The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The reason this was not done in the work reported here is that setting this confidence threshold involves a certain subjective factor (which depends on the user's \"irritability threshold\"). Our simplifying assumption allows us to measure performance objectively, by the single parameter of prediction accuracy. 1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set. For instance, cat might have the confusion set {lwf,car\u00b7, ... }, hat might have {cat,had, ...", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!) = max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid co\u00ad occurs 10 times with desert and 1 time with dessert in the training corpus.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The second tests for collocations - patterns of words and part-of-speech tags around the target word. The context-word and collocation methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context.", "Golding (1995) builds a classifier based on a rich set of context features.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence con\u00ad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence.", "Golding (1995) builds a classifier based on a rich set of context features.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context. The methods handle multiple confusion sets by applying the same technique to each confusion set independently.", "Golding (1995) builds a classifier based on a rich set of context features.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "Golding (1995) builds a classifier based on a rich set of context features.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.", "Golding (1995) builds a classifier based on a rich set of context features.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["3.4 Hybrid method 1: Decision lists. Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\u00ad cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence con\u00ad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["The main parameter to tune for the method of context words is k, the half-width of the context window. Previous work [Yarowsky, 1994] shows that sma.ller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities. \\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax. In the rest of this paper, this value of k will be used. 2 We are interpreting the condition \"c. occurs within a \u00b1k-word window of w;\" as a binary feature - either it happens, or it does not.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00ad ing and Schabes, 1996).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["along with some guerrilla fighting in the desert. two ladies who lay pinkly nude beside him in the desert Matching part-of-speech tags (here, PREP) against the sentence is done by first tagging each word in the sentence with its set of possible part-of-speech tags, obtained from a dictionary. For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set. The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ). The method of collocations was implemented in much the same way as the method of context words.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Further research is needed to understand the circumstances under which each metric performs best. Focusing for now on the reliability metric, Table 6 shows that the method of decision lists does, by and large, accomplish what it set out to do - namely, outperform either component method alone. There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence. But as the reliability and U(xjy) metrics indicate, it is not completely clear how the metric should be defined. This problem is addressed in the next section.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with. There are several ways to obtain such a collection.", "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj). The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework. The task is to pick the word Wi that is most probable, given the context words Cj observed within a \u00b1k-word window of the target word. The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The task is to pick the word Wi that is most probable, given the context words Cj observed within a \u00b1k-word window of the target word. The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], [", c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus.", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: p(c-b\u00b7\u00b7 .,ck I si) =IT p(cj I Si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["The context-word and collocation methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Yarowsky [11] proposed decision list as a way to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is. Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["One clue about the identity of an ambiguous target word comes from the words around it. For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert. On the other hand, words such as chocolate and delicious in the context imply desser\u00b7t. This observation is the basis for the method of context words. The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow. C on fu si on se t No.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Since the number of collocations grows exponentially with e, it was only practical to vary f from 1 to 3. We tried this on some practice confusion sets, and found that a.ll values of\u00a3 gave roughly comparable performance. We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3). Table 3 shows the results of varying\u00a3 for the usual confusion sets. There is no clear winner; each value of\u00a3 did best for certain confusion sets.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context. The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus. Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2. 5 84 0 its , it' s 19 .5 1 3. 57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u'r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y' re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets. The \"Most frequent word\" column gives the word in the confusion set that occurred most frequently in the training corpus. (In subsequent tables, confusion sets will be referred to by their most frequent word.)", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["5 3 0 0.840 0.840 0.840 O . G 9. 0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0.. 538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction. 4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["}, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u2020\u2021 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u2020\u2021 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We tried this on some practice confusion sets, and found that a.ll values of\u00a3 gave roughly comparable performance. We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3). Table 3 shows the results of varying\u00a3 for the usual confusion sets. There is no clear winner; each value of\u00a3 did best for certain confusion sets. Table 5 gives examples of the collocations learned for {peace, piece} with\u00a3= 2.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus. Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases. For example, the members of the confusion set {I, me} occurred 840 times in the test corpus, the breakdown being 744 I and 96 me. The baseline method predicted I every time, and thus was right 744 times, for a score of 744/840 = 0.886.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Table 5: Excerpts from the sorted list of 98 collocations learned for {peace, piece} with \u00a3 = 2. Each line gives a collocation, and the number of peace and piece occurrences it matched. The last line of the table gives the total number of occurrences of peace and piece in the training corpus. 49 population. Applying the U(xjy) metric to the arid example, the value returned now depends on the number of occurrences of deser\u00b7t and dessert in the training corpus.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus.", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["where dessert was misspelled as desert. This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Run time (1) Initialize the probability for each word in the confusion set to its prior probability. (2) Go through the sorted list of features that was saved during training. For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities. (3) Choose the word in the confusion set with the highest probability. Figure 2: Outline of the method of collocations.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["One clue about the identity of an ambiguous target word comes from the words around it. For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert. On the other hand, words such as chocolate and delicious in the context imply desser\u00b7t. This observation is the basis for the method of context words. The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow. C on fu si on se t No.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help. By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time. To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set. If the observed association is not judged to be significant,3 then c is discarded. The significance level is currently set to 0.05.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["In other words, c is ignored if it practically never occurs within the context of any w;, or if it practically always occurs within the context of every w;. In the former case, we have insufficient data to measure its presence; in the latter, its absence. Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set. For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help. By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with. There are several ways to obtain such a collection.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj). The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity. Gale et al. interpolate between the two so as to minimize the overall inaccuracy.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ). The method of collocations was implemented in much the same way as the method of context words. The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;. An ambiguous target word is then classified by finding all collocations that match its context. Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's).", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The method of collocations was implemented in much the same way as the method of context words. The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;. An ambiguous target word is then classified by finding all collocations that match its context. Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's). 45 for each word in the confusion set.", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Feature-based approaches, such as Bayesian clas\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence con\u00ad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity. Gale et al. interpolate between the two so as to minimize the overall inaccuracy. We have pursued an alternative approach to the problem of estimating the likelihood terms. We start with the observation that there is no need to use every word in the \u00b1k-word window to discriminate among the words in the confusion set. If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech. In this case, trigrams can distinguish between the words only by their prior probabilities\u00ad this follows from the way the method calculates sentence probabilities. Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["In this case, trigrams can distinguish between the words only by their prior probabilities\u00ad this follows from the way the method calculates sentence probabilities. Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method. In such cases, the Bayesian hybrid method is clearly better. On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence.", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["3.4 Hybrid method 1: Decision lists. Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\u00ad cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus. Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "We adopt the Bayesian hybrid method, which we will call Bayes, having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how\u00ad ever, the version used here uses an improved smooth\u00ad ing technique, which is mentioned briefly below.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["One is based on finding words in the dictionary that are one typo away from each other [Mays et al., 1991).1 Another finds words that have the same or similar pronunciations. Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of \"\\Vords Commonly Confused\" in the back of the Random House unabridged dictionary [Fiexner, 1983]. A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error). We will make the simplifying assumption that both kinds of errors are equally bad. In practice, however, false negatives are much worse, as users get irritated by programs that badger them with bogus complaints.", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax.", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["}, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["In the work reported here, the method of collocations was used to capture order dependencies. A collocation expresses a pattern of syntactic elements around the target word. We allow two types of syntactic elements: words, and part-of-speech tags. Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords \u00b1 3 \u00b1 6 \u00b1 1 2 \u00b1 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 .. 5 7 5 0.575 0.585 0.498 0 . 7 5 9 0.697 0.671 0.586 0 . 5 3 0 0.530 0.521 0.557 0 . 6 9 5 0.526 0.516 0.558 0 . 7 5 4 0.705 0..574 0.574 0 . 7 2 6 0.623 0.557 0.466 0 . 2 9 0 0.290 0.290 0.435 0 . 4 5 5 0.2.50 0.364 0.318 A vg no.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Table 5 gives examples of the collocations learned for {peace, piece} with\u00a3= 2. A good deal of redundancy can be seen among the collocations. There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps). Many of the collocations a.t the end of the list appear to be overgeneral and irrelevant. 3.4 Hybrid method 1: Decision lists.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\u00ad cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making. An ambiguous target word is then classified by running down the list and matching each feature against the target context.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["}, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["3.5 Hybrid method 2: Bayesian classifiers. The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations. In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj). The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity. Gale et al. interpolate between the two so as to minimize the overall inaccuracy.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS9801638, IIS 0085836 and SBR987345.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research. We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Acknowledgements"], ["Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj).", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["It can be thought of as the feature's reliability a.t picking out that w; from the others in the confusion set. 5Jn fact, we guarantee that this inequalit.y holds by performing smoothing before calculating strength. We smooth the data by adding 1 to the count. of how many times each feature was observed for each w;. 47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["(In subsequent tables, confusion sets will be referred to by their most frequent word.) The \"Baseline\" column gives the prediction accuracy of the baseline system on the test corpus. Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework. The task is to pick the word Wi that is most probable, given the context words Cj observed within a \u00b1k-word window of the target word. The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k.", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["To compare the two strength metrics, we tried both on some practice confusion sets. Sometimes one metric did substantially better, sometimes the other. In the balance, the reliability metric seemed to give higher performance. This metric is therefore the one that will be used from here on. It was also used for all experiments involving the method of collocations.", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The second tests for collocations - patterns of words and part-of-speech tags around the target word. The context-word and collocation methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax.", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This obviates the need for resolving conflicts between features. Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!)", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right. Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one. Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength. This preserves the strongest non-conflicting evidence as the basis for our answer. The strength of a collocation reflects its reliability for decision-making; a further discussion of strength is deferred to Section 3.4.", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["This obviates the need for resolving conflicts between features. Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!)", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The overlapping portion is the factor they have in common, and thus represents their lack of independence. This is only a heuristic because we could imagine collocations that do not overlap, but still conflict. Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right. Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one. Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "PAPER"], ["Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech. In this case, trigrams can distinguish between the words only by their prior probabilities\u00ad this follows from the way the method calculates sentence probabilities.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research. We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["5 3 0 0.840 0.840 0.840 O . G 9. 0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0.. 538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction. 4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence con\u00ad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making. An ambiguous target word is then classified by running down the list and matching each feature against the target context. The first feature that 46 Training phase (1) (2) (3) (3.5) (4) Propose all possible features as candidat e features. Count occurren ces of each candidat e feature in the training corpus. P r u n e f e a t u r e s t h a t h a v e i n s u f f i c i e n t d a t a . o r a r e u n i n f o r m a t i v e d i s c r i m i n a t o r s . S o r t t h e r e m a i n i n g f e a t u r e s i n o r d e r o f d e c r e a s i n g s t r e n g t h . Store the list of features (and their associat ed statistics ) for use at run time.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve. We run our best Mincut LRMSL algorithm with two different settings on Wiebe. Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset. At the recall of50% we achieve a precision of 83.6% (in compari son to their precision of 55% at the same recall). Our F-score is 0.63 (vs. 0.52).", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive\u2014having a positive electric charge;\u201cprotons are positive\u201d (objective) (2) plus, positive\u2014involving advantage or good; \u201ca plus (or positive) factor\u201d (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous. Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006). Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 0, "No title", "Introduction"], ["Method 1 (NoSL) assigns the same constant weight of 1.0 to all Word- Net relations. Method 2 (SL) reflects different degrees of preserving subjectivity. To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008). This method 5 We now describe the above steps in more detail. uses a list of subjective words (SL) to classify each Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A).", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 0, "No title", "After graph construction  we then employ a."], ["4 Experiments and Evaluation. 4.1 Datasets. We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/\u02dccjlin/libsvm/. Linear kernel and probability estimates are used in this work.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1, "No title", "After graph construction  we then employ a."], ["4.1 Datasets. We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/\u02dccjlin/libsvm/. Linear kernel and probability estimates are used in this work. http://www.cs.pitt.edu/mpq a subjective 0.24 0.83 religio us similar-to 0.81 scrupulo us 0.76 0.17 objective baseline.8 Three different feature types are used.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1, "No title", "After graph construction  we then employ a."], ["Relati on Feature s (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjectiv e or objective part of the training set, respectiv ely. This provides a non graph 0.16 0.84 flicker Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6. It includes 298 words with 703 objective and 358 subjective WordNet senses. The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses. As the MicroWNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1, "No title", "Available at."], ["In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Re lati on # unl ab ele d da ta Ac cu ra cy {\u2205 } 0 75 .3 % {si milar to } 41 8 79 .1 % {si milar to, ant on ym } 51 4 79 .5 % {si milarto, antonym, direct-hypernym, direct hy po ny m } 2, 72 1 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, also se e, ext en ded ant on ym } 3, 00 4 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, al so se e, ex te nd ed an to ny m, d eri ved fr o m , at tri bu te , d o m ai n, d o m ain m e m be r} 3, 22 0 84 .6 % 89 Option1 87 Option2. 85 83 81 79 77 75 0 500 1000 1500 2000 2500 3000 3500 Size of Unlabeled Data Figure 3: Learning curve with different sizes of unlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2). Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2). For comparison to Wiebe and Mihalcea (2006), we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description). Wiebe and Mihalcea (2006) report their results in precision and recall curves for subjective senses, such as a precision of about 55% at a recall of 50% for subjective senses.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.", 1, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008).", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 0, "No title", "Introduction"], ["This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008). pubs/papers/goldstandard.total.acl06. classification vertices in the Mincut approach. 9 Available at http://www.d.umn.edu/\u02dctpederse/. similarity.html.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses. As the MicroWNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4. In Section 4.5, we shortly discuss results on. Wiebe&Mihalcea\u2019s dataset. 4.2 Baseline and Evaluation.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 0, "No title", "Available at."], ["Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.", 1, "No title", "ABSTRACT"], ["Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008).", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 0, "No title", "Introduction"], ["U . Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%). The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small MicroWNOp dataset. In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1). 4.5 Comparison to Prior Approaches. In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Re lati on # unl ab ele d da ta Ac cu ra cy {\u2205 } 0 75 .3 % {si milar to } 41 8 79 .1 % {si milar to, ant on ym } 51 4 79 .5 % {si milarto, antonym, direct-hypernym, direct hy po ny m } 2, 72 1 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, also se e, ext en ded ant on ym } 3, 00 4 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, al so se e, ex te nd ed an to ny m, d eri ved fr o m , at tri bu te , d o m ai n, d o m ain m e m be r} 3, 22 0 84 .6 % 89 Option1 87 Option2.", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective. For example, the sense uncompromising, inflexible\u2014not making concessions is subjective, as \u201cuncompromising\u201d is a monosemous word and also in SL. We experiment with different combinations of features and the results are listed in Table 2, prefixed by \u201cSVM\u201d. All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Mark- ert (2008). However, improvements by adding more features remain small.", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 0, "No title", "Available   at   http://www.comp.leeds.ac.uk/."], ["Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.", 1, "No title", "ABSTRACT"], ["At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 0, "No title", "Related Work."], ["Wiebe&Mihalcea\u2019s dataset. 4.2 Baseline and Evaluation. We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalcea\u2019s dataset respectively. We use the McNemar test at the significance level of 5% for significance statements. All evaluations are carried out by 10-fold cross-validation.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 0, "No title", "Available at."], ["Method 1 (NoSL) assigns the same constant weight of 1.0 to all Word- Net relations. Method 2 (SL) reflects different degrees of preserving subjectivity. To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008). This method 5 We now describe the above steps in more detail. uses a list of subjective words (SL) to classify each Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A).", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 0, "No title", "After graph construction  we then employ a."], ["We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).", 1, "No title", "ABSTRACT"], ["However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 0, "No title", "ABSTRACT"], ["The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 0, "No title", "Introduction"], ["The SVM is also used as a baseline and its features are described in Section 4.3. As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie..", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 1, "No title", "After graph construction  we then employ a."], ["As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 1, "No title", "After graph construction  we then employ a."], ["Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective. We then count how often two senses related via a given relation have the same or a different subjectivity label.", "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.", 1, "No title", "After graph construction  we then employ a."], ["Wiebe and Mi- halcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005). One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification. Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNet\u2019s relation structure. 3.1 Minimum Cuts: The Main Idea. Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items should be grouped in the same cut.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 0, "No title", "Related Work."], ["3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations. Likewise, LMNoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list). 4.4 Semi-supervised Graph Mincuts. Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in MicroWNOp as unlabeled data. We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL).", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["3.2 Why might Semi-supervised Minimum. Cuts Work? We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. First, our problem satisfies two major conditions necessary for using minimum cuts. It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).", 1, "No title", "Semi-Supervised Mincuts."], ["There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 0, "No title", "Related Work."], ["(2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative).", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 0, "No title", "Related Work."], ["As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 0, "No title", "After graph construction  we then employ a."], ["in the sentence \u201cThis deal is a positive development for our company.\u201d gives a strong indication that 1 All examples in this paper are from WordNet 2.0.. 1 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1\u20139, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.", 1, "No title", "Introduction"], ["However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 0, "No title", "Related Work."], ["Method 1 (NoSL) assigns the same constant weight of 1.0 to all Word- Net relations. Method 2 (SL) reflects different degrees of preserving subjectivity. To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008). This method 5 We now describe the above steps in more detail. uses a list of subjective words (SL) to classify each Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A).", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 0, "No title", "After graph construction  we then employ a."], ["in the sentence \u201cThis deal is a positive development for our company.\u201d gives a strong indication that 1 All examples in this paper are from WordNet 2.0.. 1 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1\u20139, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.", "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.", 1, "No title", "Introduction"], ["At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 0, "No title", "Related Work."], ["(2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative).", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 0, "No title", "Related Work."], ["The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005). Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed. Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005).", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 0, "No title", "Introduction"], ["We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).", 1, "No title", "ABSTRACT"], ["The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The \"augmented network\" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}). Training a hidden Markov model having this topology corrected all nine instances of the error in the test data. An important point to note is that improving the model detail in this manner does not forcibly correct the error. The actual patterns of category usage must be distinct in the language.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This has the great advantage of eliminating the pre-tagged corpus. It minimizes the resources required, facilitates experimentation with different word categories, and is easily adapted for use with other languages. The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989).", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The \"augmented network\" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}). Training a hidden Markov model having this topology corrected all nine instances of the error in the test data.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Unless transition probabilities are highly constraining, the higher probability paths will tend to go through the to-infstate. This situation may be addressed in several ways, the simplest being to initially assign zero transition probabilities from the to-infstate to states other than verbs and the adverb state. ADJECTIVE DETERMINER To all states NOUN in Basic Network \"Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network. The lexical context available for modeling a word's category is solely the category of the preceding word (expressed via the transition probabilities P(Ci [ Ci1). Such limited context does not adequately model the constraint present in local word context.", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This problem could be resolved by tying corresponding transitions together. Alternatively, investigation of a trainable grammar (Baker, 1979; Fujisaki et al., 1989) may be a fruitful way to further develop the model in terms of grammatical components. A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words). A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary. In the document, 142 words were tagged as unknown (their possible categories were not known).", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Alternatively, investigation of a trainable grammar (Baker, 1979; Fujisaki et al., 1989) may be a fruitful way to further develop the model in terms of grammatical components. A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words). A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary. In the document, 142 words were tagged as unknown (their possible categories were not known). A total of 1,526 words had ambiguous categories (i.e. 40% of the document).", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The basic network cannot model the dependency of the number of the verb on its subject, which precedes it by a prepositional phrase. To model such dependency across the phrase, the networks shown in Figure 2 can be used. It can be seen that only simple forms of prepositional phrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/or determiner. The final transitions in the networks serve to discriminate between the correct and incorrect category assignment given the selected preceding context. As in the previous section, the corrections are not programmed into the model.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail. The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network.", "Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled. The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed \"category\"). Application areas include speech recognition/synthesis and information retrieval. Several workers have addressed the problem of tagging text.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Mar\u00ad cken 1990).", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data. Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci). In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary. In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model. Obviously, a trade-off is involved.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail. The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network. As an example, we may consider a systematic error made by the basic model.", "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network. As an example, we may consider a systematic error made by the basic model. It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data. Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci). In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary. In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model. Obviously, a trade-off is involved.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["State-dependent probabilities of the form P(Wi = Wa ] Ci = cz) represent the probability that word Wa is seen, given category c~. For instance, the word \"dog\" can be seen in the states noun and verb, and only has a nonzero probability in those states. A word sequence is considered as being generated from an underlying sequence of categories. Of all the possible category sequences from which a given word sequence can be generated, the one which maximizes the probability of the words is used. The Viterbi algorithm (Viterbi, 1967) will find this category sequence.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.", "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data. Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci). In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary. In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model. Obviously, a trade-off is involved.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["A stochastic method for assigning part-of-speech categories to unrestricted English text has been described. It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text. It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters. I would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributions to this work.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This indicates that modal auxiliaries can be recognized as a natural class via their pattern of usage. Extending the Basic Model The basic model was used as a benchmark for successive improvements. The first addition was the correct treatment of all non-words in a text. This includes hyphenation, punctuation, numbers and abbreviations. New categories were added for number, abbreviation, and comma.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["For example, many noun/verb ambiguities in front of past participles were incorrectly tagged as verbs. The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably. In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably. The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories. This leaves 50% of the corpus for training all the other equivalence classes.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably. In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably. The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories. This leaves 50% of the corpus for training all the other equivalence classes. Editing the Transition Structure A common error in the basic model was the assignment of the word \"to\" to the to-infcategory (\"to\" acting as an infinitive marker) instead of preposition before noun phrases.", "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious. In cases where the choice of tag was unclear (as often occurs in idioms), the tag was ruled as incorrect. For example, 9 errors are from 3 instances of \"... as well as ...\" that arise in the text. It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson (1987). Typical errors beyond the scope of the model described here are exemplified by incorrect adverbial and prepositional assignment.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text. It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters. I would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributions to this work. I am also indebted to Sheldon Nicholl of the Univ. of Illinois, for his comments and valuable insight.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus. It minimizes the resources required, facilitates experimentation with different word categories, and is easily adapted for use with other languages.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.", "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious. In cases where the choice of tag was unclear (as often occurs in idioms), the tag was ruled as incorrect. For example, 9 errors are from 3 instances of \"... as well as ...\" that arise in the text. It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson (1987). Typical errors beyond the scope of the model described here are exemplified by incorrect adverbial and prepositional assignment.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["As an example, we may consider a systematic error made by the basic model. It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The \"augmented network\" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}).", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["For example, \"dog\" is more likely to be a noun than a verb and \"see\" is more likely to be a verb than a noun. However they are both members of the equivalence class noun-or-verb, and so are considered to behave identically. It is then local word context (embodied in the transition probabilities) which must aid disambiguation of the word. In practice, word context provides significant constraint, so the trade-off appears to be a remarkably favorable one. The Basic Model The development of the model was guided by evaluation against a simple basic model (much of the development of the model was prompted by an analysis of the errors in its hehaviour).", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The tagger could be extended by further category refinements (e.g. inclusion of a gerund category), and the single pronoun category currently causes erroneous tags for adjacent words. With respect to the problem of unknown words, alternative category assignments for them could be made by using the context embodied in transition probabilities. A stochastic method for assigning part-of-speech categories to unrestricted English text has been described. It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data. Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci). In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary. In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model. Obviously, a trade-off is involved.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi (1983). By exploiting the fact that the matrix of probabilities P(Eqvi I Ci) is sparse, a considerable improvement can be gained over the basic training algorithm in which iterations are made over all states. The initial values of the model parameters are calculated from word occurrence probabilities, such that words are initially assumed to function equally probably as any of their possible categories. Superlative and comparative adjectives were collapsed into a single adjective category, to economize on the overall number of categories. (If desired, after tagging the finer category can be replaced).", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.", "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Of all the four systems, this one has access to the most information: contextual information (the words and tags in a window spanning three positions before and after the focus word) as well as lexical information (the existence of words formed by suffix/prefix addition/deletion). However, the actual use of this information is severely limited in that the individual information items can only be combined according to the patterns laid down in the rule templates. The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory). During the training phase, cases containing information about the word, the context and the correct tag are stored in memory. During tagging, the case most similar to that of the focus word is retrieved from the memory, which is indexed on the basis of the Information Gain of each feature, and the accompanying tag is selected.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Now there are more varied systems available, a variety which we hope will lead to better combination effects. For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies. The Viterbi algorithm is used to determine the most probable tag sequence.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies. The Viterbi algorithm is used to determine the most probable tag sequence. Since this model has no facilities for handling unknown words, a Memory-Based system (see below) is used to propose distributions of potential tags for words not in the lexicon.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The reasons for this choice are several. First of all, tagging is a widely researched and well-understood task (cf. van Halteren (ed.) 1998). Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited. Now there are more varied systems available, a variety which we hope will lead to better combination effects.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["First of all, tagging is a widely researched and well-understood task (cf. van Halteren (ed.) 1998). Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["It shows that for 99.22% of Tune, at least one tagger selects the correct tag. However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging. We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation. All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune. The patterns between the brackets give the distribution of correct/incorrect tags over the systems.", "Like Van Halteren et al. (1998), we evaluated two features combinations.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic. Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven). Data driven methods appear to be the more popular. This can be explained by the fact that, in general, hand crafting an explicit model is rather difficult, especially since what is being modelled, natural language, is not (yet) well- understood.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely.", "Van Halteren et al. (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti.", "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["1998). Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996).", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority).", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem. 9Tags (Memory-Based) scores significantly worse than TagPair (p=0.0274) and not significantly better than Precision-Recall (p=0.2766). 1Tags+Word could not be handled by C5.0 due to the huge number of feature values.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction). Conclusion. Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems. Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf. Ratnaparkhi 1996 on such effects in the Penn Treebank corpus).", "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11). After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone. A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority).", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors. In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers. It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996). The underlying assumption is twofold.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%).", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Of all the four systems, this one has access to the most information: contextual information (the words and tags in a window spanning three positions before and after the focus word) as well as lexical information (the existence of words formed by suffix/prefix addition/deletion). However, the actual use of this information is severely limited in that the individual information items can only be combined according to the patterns laid down in the rule templates. The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory). During the training phase, cases containing information about the word, the context and the correct tag are stored in memory. During tagging, the case most similar to that of the focus word is retrieved from the memory, which is indexed on the basis of the Information Gain of each feature, and the accompanying tag is selected.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision). 7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare.", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies. The Viterbi algorithm is used to determine the most probable tag sequence. Since this model has no facilities for handling unknown words, a Memory-Based system (see below) is used to propose distributions of potential tags for words not in the lexicon. The second system is the Transformation Based Learning system as described by Brill (19941; henceforth tagger R, for Rules).", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Both this system and Brill's system are used with the default settings that are suggested in their documentation. 2Ratnaparkhi's Java implementation of this system is available at ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/ The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986). The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types. Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited. A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["tag in each case. We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune). Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune.", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited. Now there are more varied systems available, a variety which we hope will lead to better combination effects. For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if \"as well as\" is taken to be a coordination conjunction, it is tagged \"as_CC1 well_CC2 as_CC3\", using three related but different ditto tags. by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision). 7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf. Table 2 below) certainly still leaves room for improvement, although tagger E surprises us with an accuracy well above any results reported so far and makes us less confident about the gain to be accomplished with combination.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%).", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf. Ratnaparkhi 1996 on such effects in the Penn Treebank corpus). Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages. But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements. Our thanks go to the creators of the tagger generators used here for making their systems available.", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608. 12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags. The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging. We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation. All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune. The patterns between the brackets give the distribution of correct/incorrect tags over the systems. tag in each case.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.", "Furthermore, the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The difficulty of the tagging task can be judged by the two baseline measurements in Table 2 below, representing a completely random choice from the potential tags for each token (Random) and selection of the lexically most likely tag (LexProb). For our experiment, we divide the corpus into three parts. The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if \"as well as\" is taken to be a coordination conjunction, it is tagged \"as_CC1 well_CC2 as_CC3\", using three related but different ditto tags. by taking the first eight utterances of every ten. This part is used to train the individual tag- gers.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf. Table 2 below) certainly still leaves room for improvement, although tagger E surprises us with an accuracy well above any results reported so far and makes us less confident about the gain to be accomplished with combination.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality. Obviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997). Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11). After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone. A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608. 12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["1Tags+Word could not be handled by C5.0 due to the huge number of feature values. Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers. It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996). The underlying assumption is twofold. First, the combined votes will make the system more robust to the quirks of each learner's particular bias.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997). Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags. The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997). Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Both this system and Brill's system are used with the default settings that are suggested in their documentation. 2Ratnaparkhi's Java implementation of this system is available at ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/ The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986). The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types. Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate.", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune). Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997). Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["When abstracting away from individual tags, precision and recall are equal and measure how many tokens are tagged correctly; in this case we also use the more generic term accuracy. 6In our experiment, a random selection from among the winning tags is made whenever there is a tie. Table 2: Accuracy of individual taggers and combination methods. But we have even more information on how well the taggers perform. We not only know whether we should believe what they propose (precision) but also know how often they fail to recognize the correct tag (recall).", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Obviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["For unknown words, the single position before and after, three suffix letters, and information about capitalization and presence of a hyphen or a digit are used. The fourth and final system is the MXPOST system as described by Ratnaparkhi (19962; henceforth tagger E, for Entropy). It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features). A beam search is then used to find the highest probability tag sequence. Both this system and Brill's system are used with the default settings that are suggested in their documentation.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997). Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem.", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word.24 With the increased amount of training material, addition of the context gener\u00ad ally leads to better results.", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002). By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages.", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003). As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).", "Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be \u201ceasier\u201d than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together. It is a question for future research to explore the effect of these variables in clustering performance. We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried (Dash et al., 1997) did not prove useful, because of the problem of having no consistent threshold for feature inclusion.", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies. Learning the argument structure properties of verbs\u2014the semantic roles they assign and their mapping to syntactic positions\u2014is both particularly important and difficult. A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000). However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise. Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others. This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach. Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies. Learning the argument structure properties of verbs\u2014the semantic roles they assign and their mapping to syntactic positions\u2014is both particularly important and difficult. A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003). As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments. 4.1 Clustering Parameters. We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. In performing hierarchical clustering, both a vector distance measure and a cluster distance (\u201clinkage\u201d) measure are specified. We used the simple Euclidean distance for the former, and Ward linkage for the latter.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["5.2 Manual Feature Selection. One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We used the chunker (partial parser) of Abney (1991) to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it. Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame. From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments. 4.1 Clustering Parameters. We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003). We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process.", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["5.2 Manual Feature Selection. One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs). All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments). 3.3 Feature Extraction. All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind. Thus, the features serve as approximations to the underlying distinctions among classes. Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details. Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb. We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases). Interestingly, was generally very high, indicating that there is structure in the data, but not what matches our classification. This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery. We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling). We might also ask, would any subset of verbs do as well?", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process.", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods. We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes. To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy. In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff. However, we did experiment with (as in Strehl et al., 2000), and found that performance was generally better (even on our measure, described below, that discounts oversplitting).", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002).", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes. To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy. In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff. However, we did experiment with (as in Strehl et al., 2000), and found that performance was generally better (even on our measure, described below, that discounts oversplitting). This supports our intuition that the approach may enable us to find more consistent clusters at a finer grain, without too much fragmentation.", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind. Thus, the features serve as approximations to the underlying distinctions among classes. Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details. Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb. We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner. Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993). To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.", "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003). We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["5.2 Manual Feature Selection. One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993). To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data.", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1. To calculate a random baseline, we evaluated 10,000 random clusterings with the same number of verbs and classes as in each of our experimental tasks. Because the achieved depends on the precise size of clusters, we calculated mean over the best scenario (with equal-sized clusters), yielding a conservative estimate (i.e., an upper bound) of the baseline. These figures are reported with our results in Table 2 below.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["5.2 Manual Feature Selection. One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["1, 26. 3, 26. 7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2). 3.2 Verb Selection. Our experimental verbs were selected as follows.", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["These figures are reported with our results in Table 2 below. 4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good. Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification. The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between them\u2014 there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not. It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["However, it is useful as a relative measure of good-. ness, in comparing clusterings arising from different feature sets. 4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1.", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner. Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993). To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003). We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["5.3 Unsupervised Feature Selection. In order to deal with excessive dimensionality, Dash et al. (1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task. Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2. This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder. Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks. More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2. This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder. Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks. More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set. The increased reduction in error rate is particularly striking for the 2-way tasks, of 37% for the Seed set compared to 20% for the Ling set.", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["The higher (.89 vs. .33) reflects the better separation of the data. regard to the target classes. We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters. Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster). A value of 0 suggests that a point is not clearly in a particular cluster.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["These figures are reported with our results in Table 2 below. 4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good. Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification. The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between them\u2014 there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not. It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["However, it is useful as a relative measure of good-. ness, in comparing clusterings arising from different feature sets. 4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative.", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003). We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well. Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies. Learning the argument structure properties of verbs\u2014the semantic roles they assign and their mapping to syntactic positions\u2014is both particularly important and difficult. A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23. All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results. C5.0 is supervised accuracy; Base is on random clusters. Full is full feature set; Ling is manually selected subset; Seed is seed-verb-selected set.", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies. Learning the argument structure properties of verbs\u2014the semantic roles they assign and their mapping to syntactic positions\u2014is both particularly important and difficult. A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003). As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind. Thus, the features serve as approximations to the underlying distinctions among classes. Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details. Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb. We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task. We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties. We find that the unsupervised method we tried cannot be consistently applied to our data. However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "ABSTRACT"], ["By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages. However, a general feature space means that most features will be irrelevant to any given verb discrimination task. In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d? In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand. In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner.", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003). As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["While we could have selected a threshold that might work reasonably well with our data, we would have little confidence that it would work well in general, considering the inconsistent pattern of results. 5.4 Semi-Supervised Feature Selection. Unsupervised methods such as Dash et al.\u2019s (1997) are appealing because they require no knowledge external to the data. However, in many aspects of computational linguistics, it has been found that a small amount of labelled data contains sufficient information to allow us to go beyond the limits of completely unsupervised approaches. In our domain in particular, verb class discovery \u201cin a vacuum\u201d is not necessary.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be \u201ceasier\u201d than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together. It is a question for future research to explore the effect of these variables in clustering performance. We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried (Dash et al., 1997) did not prove useful, because of the problem of having no consistent threshold for feature inclusion.", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions. It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks.", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["5.2 Manual Feature Selection. One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002).", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1.", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002).", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as \u201cperson\u201d by our chunker (Abney, 1991). We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails?", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["is a constra.int of the following fornl : i[' a.n object is of ;~ cert;fin kind then ill deserves certa.in fea.tures with wdues of cert~till kinds An FCI~ stat:ing tha,2: a. verb must h~we v and N t'eatures with values A- and -respectively is a.ll example of a. conjunctive FCI{. A disjunctive I\"CI{. is of the form: l rl'he \"]'roll ,qysl.em was implemented in Quintus Prolog by Dale (lerdemann and '['hilo (]Stz. if an object is of a. cel'taiu kiud then it deserves cerl;a.in [ca,1;tll'C~s with vMues of certa.hi kinds, or it deserves cerl.ahi (pei'ha.liS other) fea.1;u res \\vil, h viiiues of terra.in (perlla.ps other) kinds, or ... (31:it i:leserw.;s i:erl;a.in (lmrhal)S other) fea,1;llres wil.h Vi, l.[ll(~S o[ certain (perha.ps other) khi,<ls lo::I exa~]nple, the following F(',|/.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch. !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "PAPER"], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,. Sag (rorthcoming) [14].. well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed. Unfortunately, well-typability is not sufficient to ensure that disjunctive FCRs are satisfied. Consider, For exam- pie, our encoding of the disjunctive FCR p and suppose that 99 is the fe, ature structure t[f : +,9 : -].", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch. !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "PAPER"], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7).", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Type resolution, on the other hand, always considers species. Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F').", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F'). Uni/ication of sets of fca.ture structures is defined here ill the standard way: S t2 ,S\" = {1\"[ I\"' 6 S and l\"\" G S\" and 1\" = 1\"' H 1\"\"}.", "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails?", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7).", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["a.n N)propria.teness specification. Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7]. 1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities. The a.lnount of dis- junction is a.lso kept small by the technique of unlilli,g described in [9]. This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in 4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "INTRODUCTION"], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Type resolution, on the other hand, always considers species. Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F').", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,. Sag (rorthcoming) [14].. well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed. Unfortunately, well-typability is not sufficient to ensure that disjunctive FCRs are satisfied. Consider, For exam- pie, our encoding of the disjunctive FCR p and suppose that 99 is the fe, ature structure t[f : +,9 : -].", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where.", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails?", "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 0, "Foma: a finite-state compiler and library", "Efficiency."], ["For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (\u2203x)(x \u2208 L \u2227 (\u2203y)(y \u2208 L \u2227 (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to \u2208 and \u2227, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes. This practice stems back from the earliest two-level compilers (Karttunen et al., 1987).", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 0, "Foma: a finite-state compiler and library", "Basic Regular Expressions."], ["Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005). One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit.", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", 1, "Foma: a finite-state compiler and library", "Introduction"], ["Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes. This practice stems back from the earliest two-level compilers (Karttunen et al., 1987). Below is a simple example of the format: Multichar_Symbols +Pl +Sing LEXICON Root Nouns; LEXICON Nouns cat Plural; church Plural; LEXICON Plural +Pl:%\u02c6s #; +Sing #;", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 0, "Foma: a finite-state compiler and library", "Building morphological analyzers."], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 0, "Foma: a finite-state compiler and library", "ABSTRACT"], ["Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions. However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 0, "Foma: a finite-state compiler and library", "Introduction"], ["It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005). One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", 1, "Foma: a finite-state compiler and library", "Introduction"], ["The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries. North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions. However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 0, "Foma: a finite-state compiler and library", "Introduction"], ["The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged. GNU AT&T Foma xfst flex fsm 4 \u03a3\u2217a\u03a315 0.216s 16.23s 17.17s 1.884s \u03a3\u2217a\u03a320 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions.", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", 1, "Foma: a finite-state compiler and library", "Introduction"], ["One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions (Hulden, 2008). For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (\u2203x)(x \u2208 L \u2227 (\u2203y)(y \u2208 L \u2227 (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to \u2208 and \u2227, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 0, "Foma: a finite-state compiler and library", "Basic Regular Expressions."], ["Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines. For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 0, "Foma: a finite-state compiler and library", "Efficiency."], ["The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton. This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately. Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma. It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort. Retaining backwards compatibility with Xerox/PARC and at the same time extending the formalism means that one is often able to construct finite-state networks in equivalent various ways, either through ASCII-based operators or through the Unicode-based extensions.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 0, "Foma: a finite-state compiler and library", "Introduction"], ["Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata. However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", 1, "Foma: a finite-state compiler and library", "Efficiency."], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 0, "Foma: a finite-state compiler and library", "ABSTRACT"], ["Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines. For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 0, "Foma: a finite-state compiler and library", "Efficiency."], ["It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005). One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 0, "Foma: a finite-state compiler and library", "Introduction"], ["One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions.", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", 1, "Foma: a finite-state compiler and library", "Introduction"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 0, "Foma: a finite-state compiler and library", "PAPER"], ["It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005). One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 0, "Foma: a finite-state compiler and library", "Introduction"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 0, "Foma: a finite-state compiler and library", "ABSTRACT"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", 1, "Foma: a finite-state compiler and library", "ABSTRACT"], ["For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["GNU AT&T Foma xfst flex fsm 4 \u03a3\u2217a\u03a315 0.216s 16.23s 17.17s 1.884s \u03a3\u2217a\u03a320 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits. The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005). One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 0, "Foma: a finite-state compiler and library", "Introduction"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", 1, "Foma: a finite-state compiler and library", "ABSTRACT"], ["One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions (Hulden, 2008). For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (\u2203x)(x \u2208 L \u2227 (\u2203y)(y \u2208 L \u2227 (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to \u2208 and \u2227, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 0, "Foma: a finite-state compiler and library", "Basic Regular Expressions."], ["For instance, by default, for efficiency reasons, Foma determinizes and minimizes automata between nearly every incremental operation. Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata. However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 0, "Foma: a finite-state compiler and library", "Automata visualization and."], ["GNU AT&T Foma xfst flex fsm 4 \u03a3\u2217a\u03a315 0.216s 16.23s 17.17s 1.884s \u03a3\u2217a\u03a320 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits. The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions. The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton. This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately. Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma. It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort.", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", 1, "Foma: a finite-state compiler and library", "Introduction"], ["(2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab.", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. Our base system uses features as discussed in \u00a72. To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the \u201cgrow-diag-final-and\u201d heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized align ments.", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["11\u201313: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008). Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA). This requires us to calculate the function\u2019s gradient (vector of first derivatives) with respect to \u03b8.11 Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast \u201cinside\u201d DP solution is known (Smith and Eisner, 2006; Wang et al., 2007). It runs in O(mn2) time and O(mn) space. Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences.", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (\u201ca(\u03c4t(j)) = \u03c4s(a(j))\u201d corresponds to their \u201cparent-child\u201d configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2).", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the \u201cbackbone\u201d model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation.", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the \u201cbackbone\u201d model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation.", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments.", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["5.3 Handling Non-Local Features. So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., f phr , f N for N > 1, f zth, f sunc ). We recently proposed \u201ccube summing,\u201d an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item. Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features.", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation. Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.) As usual, the normalization constant is not required for decoding; it suffices to solve: t , a ) = argmax \u03b8 g(s, \u03c4 , a, t, \u03c4 ) (8)Which translations are possible depends heav ily on the configurations that the QDG permits. (t\u2217, \u03c4 \u2217 \u2217 T s t t,\u03c4t ,aa Formally, for a parent-child pair (t\u03c4t (j), tj ) in \u03c4t, we consider the relationship between a(\u03c4t(j)) and a(j), the source-side words to which t\u03c4t (j) and tj align.", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation. Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.) As usual, the normalization constant is not required for decoding; it suffices to solve: t , a ) = argmax \u03b8 g(s, \u03c4 , a, t, \u03c4 ) (8)Which translations are possible depends heav ily on the configurations that the QDG permits.", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the \u201cbackbone\u201d model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation.", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["11\u201313: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008). Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA). This requires us to calculate the function\u2019s gradient (vector of first derivatives) with respect to \u03b8.11 Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast \u201cinside\u201d DP solution is known (Smith and Eisner, 2006; Wang et al., 2007). It runs in O(mn2) time and O(mn) space. Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences.", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["Non-local features will present a challenge for decoding and training (\u00a74.3). Given a sentence s and its parse \u03c4s, at decoding time we seek the target sentence t\u2217, the target tree For a QDG model, the decoding problem has not been addressed before. It equates to finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known. The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist. A major advantage of DP is that, with small modifications, summing over structures is also possible with \u201cinside\u201d DP algorithms.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["Note that \u201cnon locality\u201d is relative to a choice of formalism; in \u00a72 we did not commit to any formalism, so it is only now that we can describe phrase and N -gram features as non-local. Non-local features will present a challenge for decoding and training (\u00a74.3). Given a sentence s and its parse \u03c4s, at decoding time we seek the target sentence t\u2217, the target tree For a QDG model, the decoding problem has not been addressed before. It equates to finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known. The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research. (Table 1 explains notation.) Given a sentence s and its parse tree \u03c4s, we formulate the translation on the feasibility of inference, including decoding. Typically these feature functions are chosen to factor into local parts of the overall structure. We next define some key features used in current MT systems, explaining how they factor.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["cmu.edu/Quipu. We presented feature-rich MT using a principled probabilistic framework that separates features from inference. Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle \u201cnon-local\u201d features using generic techniques that also support efficient parameter estimation. Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints. We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper.", "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, \u03b3, \u03c6, \u03c4\u03c6, a) for an in put sentence s and its parse \u03c4s, i.e., finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,\u03c4s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Conclusion."], ["(2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab.", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality. 6.5 Varying k During Decoding. For models without syntactic features, we constrained the decoder to produce dependency trees in which every word\u2019s parent is immediately to its right and ignored syntactic features while scoring structures. This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate. Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.) The tree-to-tree syntactic features gtree 2 in our model are binary features f qg that fire for particular QG configurations. We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura Phrase Syntactic Features: features: +f att \u222a f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU). tions involving root words and NULL-alignments more finely. There are 14 features in this category.", "(2007) Ar\u2192En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al.", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["of s should be aligned to some part of t (alignment to NULL incurs an explicit cost). Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated. Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in \u03c4t (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in \u03c4s apart from a single recently aligned word.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features.", "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["We will exploit this in training(\u00a75). Efficient summing opens up many possibilities for training \u03b8, such as likelihood and pseudo likelihood, and provides principled ways to handle hidden variables during learning. 4.1 Translation as Monolingual Parsing. We decode by performing lattice parsing on a lattice encoding the set of possible translations. The lattice is a weighted \u201csausage\u201d lattice that permits sentences up to some maximum length \u00a3; \u00a3 is derived from the source sentence length.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["We turn next to the \u201cbackbone\u201d model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation. Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.)", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research. (Table 1 explains notation.) Given a sentence s and its parse tree \u03c4s, we formulate the translation on the feasibility of inference, including decoding. Typically these feature functions are chosen to factor into local parts of the overall structure. We next define some key features used in current MT systems, explaining how they factor.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["\u00fcbersetzen: sie:you sie:you konnten:could translate \u00fcbersetzen: translate \u00fcbersetzen: konnten:couldn es:it sie :you translated translated konnten:might es:it sie:let sie:them ?:? \u00fcbersetzen: translate es:it konnten:could es:it NULL:to ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them. selected at each position and a dependency tree over them. 4.2 Source-Side Coverage Features.", "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["(2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["Features are often implied by a choice of formalism. dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.", "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["(2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab.", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["5.3 Handling Non-Local Features. So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., f phr , f N for N > 1, f zth, f sunc ). We recently proposed \u201ccube summing,\u201d an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item. Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features.", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2).", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation. illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism. We have validated cube summing and decoding as practical methods for approximate inference. Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses\u2019 phrase segmentation variable), and, of course, additional feature representations.", "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["(2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab.", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. Our base system uses features as discussed in \u00a72. To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the \u201cgrow-diag-final-and\u201d heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized align ments.", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["For models without syntactic features, we constrained the decoder to produce dependency trees in which every word\u2019s parent is immediately to its right and ignored syntactic features while scoring structures. This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate. Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice. Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig. 2.", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998).Our approach permits an alternative to mini mum error-rate training (MERT; Och, 2003); it is For our target-language syntactic features g syn , discriminative but handles latent structure and regularization in more principled ways. The pseudo- likelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA\u2019s inner loop faster than MERT\u2019s inner loop. Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output. 6.1 Data and Evaluation. We use the GermanEnglish portion of the Basic Travel Expression Corpus (BTEC).", "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019). Figure 2 shows examples of extracted NE pair instances and their contexts. The data is sorted based on the frequency of the context (\u201ca unit of\u201d appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. \u201cNBC\u201d and \u201cGeneral Electric Co.\u201d appeared 10 times with the context \u201ca unit of\u201d). Step 2. Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word \u201cunit\u201d for the phrase \u201ca unit of\u201d).", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.", "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["They first collect the NE instance pairs and contexts, just like our method. However, the next step is clearly different. They cluster NE instance pairs based on the words in the contexts using a bag- of-words method. In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30. Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue. In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets. The accuracies for link were 73% and 86% on two evaluated domains. These results are promising and there are several avenues for improving on these results.", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. If the expression is a word or a short phrase (like \u201ccorporation\u201d and \u201ccompany\u201d), it is called a \u201csynonym\u201d. There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One possibility is to use n-grams based on mutual information. If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate. Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain. As was explained in the results section, \u201cstrength\u201d or \u201cadd\u201d are not desirable keywords in the CC-domain. In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link \u201cIBM\u201d and \u201cLotus\u201d) (Step 4). As we shall see, most of the linked sets are paraphrases. This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995. They contain about 200M words (25M, 110M, 40M and 19M words, respectively). All the sentences have been analyzed by our chunker and NE tag- ger. The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory. 3.2 Results.", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["The number of NE instance pairs used in their experiment is less than half of our method. There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. If the expression is a word or a short phrase (like \u201ccorporation\u201d and \u201ccompany\u201d), it is called a \u201csynonym\u201d. There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019). Figure 2 shows examples of extracted NE pair instances and their contexts.", "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["For example, the phrase \u201c's New York-based trust unit,\u201d is not a paraphrase of the other phrases in the \u201cunit\u201d set. As you can see in the figure, the accuracy for the domain is quite high except for the \u201cagree\u201d set, which contains various expressions representing different relationships for an IE application. The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set. The results, along with the total number of phrases, are shown in Table 1. D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02]. The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs. This can be repeated several times to collect a list of author / book title pairs and expressions. However, those methods need initial seeds, so the relation between entities has to be known in advance.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["We concentrate on those sets. Among these 32 sets, we found the following pairs of sets which have two or more links. Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances. buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct. We will describe the evaluation of such clusters in the next subsection.", "For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview. Before explaining our method in detail, we present a brief overview in this subsection.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["First, from a large corpus, we extract all the NE instance pairs. Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, \u201cIBM plans to acquire Lotus\u201d. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, \u201cIBM plans to acquire Lotus\u201d. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2).", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["All the links in the \u201cCC-domain are shown in Step 4 in subsection 3.2. Out of those 15 links, 4 are errors, namely \u201cbuy - pay\u201d, \u201cacquire - pay\u201d, \u201cpurchase - stake\u201d \u201cacquisition - stake\u201d. When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event. The similar explanation applies to the link to the \u201cstake\u201d set. We checked whether the discovered links are listed in WordNet.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords. Also, \u201cagree\u201d in the CC-domain is not a desirable keyword. It is a relatively frequent word in the domain, but it can be used in different extraction scenarios. In this domain the major scenarios involve the things they agreed on, rather than the mere fact that they agreed. \u201cAgree\u201d is a subject control verb, which dominates another verb whose subject is the same as that of \u201cagree\u201d; the latter verb is generally the one of interest for extraction.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u2019t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results. The number of NE instance pairs used in their experiment is less than half of our method. There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach.", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "PAPER"], ["This problem arises because our keywords consist of only one word. Sometime, multiple words are needed, like \u201cvice chairman\u201d, \u201cprime minister\u201d or \u201cpay for\u201d (\u201cpay\u201d and \u201cpay for\u201d are different senses in the CC-domain). One possibility is to use n-grams based on mutual information. If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate. Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["We will describe the evaluation of such clusters in the next subsection. 3.3 Evaluation Results. We evaluated the results based on two metrics. One is the accuracy within a set of phrases which share the same keyword; the other is the accuracy of links. We picked two domains, the CC-domain and the \u201cPerson \u2013 Company\u201d domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview.", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["and \u201cH\u201d represents \u201cHanson Plc\u201d. x EG, has agreed to be bought by H x EG, now owned by H x H to acquire EG x H\u2019s agreement to buy EG Three of those phrases are actually paraphrases, but sometime there could be some noise; such as the second phrase above. So, we set a threshold that at least two examples are required to build a link. More examples are shown in Figure 5. Notice that the CC-domain is a special case.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["At this step, we will try to link those sets, and put them into a single cluster. Our clue is the NE instance pairs. If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases. For example, the two NEs \u201cEastern Group Plc\u201d and \u201cHanson Plc\u201d have the following contexts. Here, \u201cEG\u201d represents \u201cEastern Group Plc\u201d.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["Also, the method of using keywords rules out phrases which don\u2019t contain popular words in the domain. We are not claiming that this method is almighty. Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases. Applications The discovered paraphrases have multiple applications. One obvious application is information extraction.", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, \u201cIBM plans to acquire Lotus\u201d. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2).", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results. The number of NE instance pairs used in their experiment is less than half of our method. There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Using structural information As was explained in the results section, we extracted examples like \u201cSmith estimates Lotus\u201d, from a sentence like \u201cMr. Smith estimates Lotus will make profit this quarter\u2026\u201d. In order to solve this problem, a parse tree is needed to understand that \u201cLotus\u201d is not the object of \u201cestimates\u201d. Chunking is not enough to find such relationships. This remains as future work.", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019). Figure 2 shows examples of extracted NE pair instances and their contexts. The data is sorted based on the frequency of the context (\u201ca unit of\u201d appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. \u201cNBC\u201d and \u201cGeneral Electric Co.\u201d appeared 10 times with the context \u201ca unit of\u201d). Step 2. Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word \u201cunit\u201d for the phrase \u201ca unit of\u201d).", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Step 2. Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word \u201cunit\u201d for the phrase \u201ca unit of\u201d). Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword. We used the TF/ITF metric to identify keywords. keywords Step 3 Sets of phrases based on keywords Step 4 Links between sets of phrases All the contexts collected for a given domain are gathered in a bag and the TF/ITF scores are calculated for all the words except stopwords in the bag.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Figure 4 shows some such phrase sets based on keywords in the CC-domain. Step 4. Cluster phrases based on Links We now have a set of phrases which share a keyword. However, there are phrases which express the same meanings even though they do not share the same keyword. For example, in Figure 3, we can see that the phrases in the \u201cbuy\u201d, \u201cacquire\u201d and \u201cpurchase\u201d sets are mostly paraphrases.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["However, there are phrases which express the same meanings even though they do not share the same keyword. For example, in Figure 3, we can see that the phrases in the \u201cbuy\u201d, \u201cacquire\u201d and \u201cpurchase\u201d sets are mostly paraphrases. At this step, we will try to link those sets, and put them into a single cluster. Our clue is the NE instance pairs. If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.", "Yet, many of these templates share a similar meaning, e.g. \u2018X accommodate up to Y \u2019, \u2018X can accommodate up to Y \u2019, \u2018X will accommodate up to Y \u2019, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Applications The discovered paraphrases have multiple applications. One obvious application is information extraction. In IE, creating the patterns which express the requested scenario, e.g. \u201cmanagement succession\u201d or \u201ccorporate merger and acquisition\u201d is regarded as the hardest task. The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set. Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["We picked two domains, the CC-domain and the \u201cPerson \u2013 Company\u201d domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate. It is not easy to make a clear definition of \u201cparaphrase\u201d. Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria. If two phrases can be used to express the same relationship within an information extraction application (\u201cscenario\u201d), these two phrases are paraphrases. Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["All the links in the \u201cCC-domain are shown in Step 4 in subsection 3.2. Out of those 15 links, 4 are errors, namely \u201cbuy - pay\u201d, \u201cacquire - pay\u201d, \u201cpurchase - stake\u201d \u201cacquisition - stake\u201d. When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event. The similar explanation applies to the link to the \u201cstake\u201d set. We checked whether the discovered links are listed in WordNet.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2). For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link \u201cIBM\u201d and \u201cLotus\u201d) (Step 4).", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2). For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link \u201cIBM\u201d and \u201cLotus\u201d) (Step 4). As we shall see, most of the linked sets are paraphrases.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue. We focus on phrases which connect two Named Entities (NEs), and proceed in two stages. The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets. The second stage links sets which involve the same pairs of individual NEs.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "ABSTRACT"], ["While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue. In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["Evaluation within a set The evaluation of paraphrases within a set of phrases which share a keyword is illustrated in Figure 4. For each set, the phrases with bracketed frequencies are considered not paraphrases in the set. For example, the phrase \u201c's New York-based trust unit,\u201d is not a paraphrase of the other phrases in the \u201cunit\u201d set. As you can see in the figure, the accuracy for the domain is quite high except for the \u201cagree\u201d set, which contains various expressions representing different relationships for an IE application. The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview.", "As for paraphrase, Sekine\u2019s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["\"Non-prepositional\" noun phrases A \"pure\", \"non-prepositional\" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ). Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording. Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\". This preference can be explained in terms of sali\u00ad ence from the point of view of the centering theory. The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects.", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1. Examine the current sentence and the two pre\u00ad. ceding sentences (if available).", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997).", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u2019s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u2019s Anaphora Resolution. System. 3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred. If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent.", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\". Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990). Example: Press the keyi down and turn the volume up...", "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not). Lexically reiterated items include re\u00ad peated synonymous noun phrases which may often be preceded by definite articles or demonstratives. Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. \"toner bottle\", \"bottle of toner\", \"the bottle\"). Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we con\u00ad sider it as the preferred candidate (1, 0). \"Non-prepositional\" noun phrases A \"pure\", \"non-prepositional\" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ).", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Look for noun phrases3 only to the left of the anaphor4 2. Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\".", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent.", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997).", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent. For in\u00ad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1. Examine the current sentence and the two pre\u00ad. ceding sentences (if available).", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov\u2019s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u201dantecedent indicators\u201d).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The sample texts con\u00ad tained 180 pronouns among which were 120 in\u00ad stances of exophoric reference (most being zero pro\u00ad nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Look for noun phrases3 only to the left of the anaphor4 2. Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\".", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\". This preference can be explained in terms of sali\u00ad ence from the point of view of the centering theory. The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\".", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared.", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 81\u201390, Hyderabad, India, January 2008. Qc 2008 Asian Federation of Natural Language Processing algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation.", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6. 3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994). Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994). Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3). The evaluation indicated 83.6% success rate.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3). The evaluation indicated 83.6% success rate. The \"Baseline subject\" model tested on the same data scored 33.9% recall and 67.9% precision, whereas \"Baseline most recent\" scored 66.7%.", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6. 3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994). Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997).", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent.", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology.", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\".", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj. where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998). The sample texts con\u00ad tained 180 pronouns among which were 120 in\u00ad stances of exophoric reference (most being zero pro\u00ad nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 0, "Robust pronoun resolution with limited knowledge", "PAPER"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP (Kennedy and Boguraev, 1996), Baldwin\u2019s pronoun resolution method (Baldwin, 1997) and Mitkov\u2019s knowledge-poor pronoun resolution approach (Mitkov, 1998b).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "Mitkov\u2019s approach Mitkov\u2019s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s (1996) parser-free algorithm, Baldwin\u2019s (1997) CogNiac and Mitkov\u2019s (1998b) knowledge-poor approach.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "Mitkov\u2019s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997). For the time being, we are using the same scores for Polish.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%. Given that our knowledge\u00ad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\u00ad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997).", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent.", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "For example, in the following sentence, mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership.\u201d \u201cAmerican Medical Association\u201d, \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same object.Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase \u2013 which is the scope of this paper as well.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent. If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent. If immediate reference does not hold, propose the candidate with higher score for collocational pattern. If collocational pattern suggests a tie or does not hold, select the candidate with higher score for indicating verbs.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997). For the time being, we are using the same scores for Polish.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not). Lexically reiterated items include re\u00ad peated synonymous noun phrases which may often be preceded by definite articles or demonstratives. Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. \"toner bottle\", \"bottle of toner\", \"the bottle\"). Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we con\u00ad sider it as the preferred candidate (1, 0). \"Non-prepositional\" noun phrases A \"pure\", \"non-prepositional\" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ).", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1. Examine the current sentence and the two pre\u00ad. ceding sentences (if available).", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent. For in\u00ad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not). 21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}.", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent. If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr\u00abandez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u2019s algorithm for pronoun resolution (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent. For in\u00ad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997). For the time being, we are using the same scores for Polish.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u201cit\u201d occurrences, and assigns animacy.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997). For the time being, we are using the same scores for Polish. The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998).", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\".", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998). Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997). For the time being, we are using the same scores for Polish. The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998).", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well (Mitkov & Belguith 1998).", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["The sample texts con\u00ad tained 180 pronouns among which were 120 in\u00ad stances of exophoric reference (most being zero pro\u00ad nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Our evalua\u00ad tion, based on 63 examples (anaphors) from a tech\u00ad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%. Given that our knowledge\u00ad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\u00ad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%. Given that our knowledge\u00ad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\u00ad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon. Typically, our preference-based model proved superior to both baseline models when the antece\u00ad dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["The subject change point occurred at sentence 13, just two sentences after a predicted subject change at sentence 11. In this investigation, word repetition alone achieved better results than using either collocation or relation weights individually. The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text. The objective of the current investigation was to determine whether all troughs coincide with a subject change. The troughs placed by the algorithm were compared to the segmentations identified by test subjects for the same texts.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 1: Locating Subject Change."], ["Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching. Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994).", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["collocation 6.3 3.83 35 (83.3%) Table 1. Comparison of segmentation algorithm using different linguistic features. Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result. A total of 41 out of a possible 42 known subject change points were identified from the least number of troughs placed per text (7.I). For the text where the known subject change point went undetected, a total of three troughs were placed at sentences 6, 11 and 18.", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 1: Locating Subject Change."], ["Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented.", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by look\u00ad up in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. \"orange oranges\").", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Comparison of troughs to segmentation points placed by the test subjects. changes. Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62. These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation. As an example, a text segmentation algorithm developed by Hearst ( 1994) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61.", "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching. Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994).", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998).", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex).", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991). Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. troughs placed subject change linguistic feature points located average std.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 0, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget's Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Roget\u2019s Thesaurus.", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights. This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects. Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "ABSTRACT"], ["Ties can be anaphoric or cataphoric, and located at both the sentential and suprasentential level. Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role. Consequently, it was not further considered. Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing). The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers. A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel).", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998).", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by look\u00ad up in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. \"orange oranges\"). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991). Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. troughs placed subject change linguistic feature points located average std.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated. This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text. 1 Background Theory: Lexical Cohesion. Cohesion concerns how words in a text are related.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998).", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect. Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows. The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows. The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered. Areas for improving the segmentation algorithm include incorporation of a threshold for troughs.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered. Areas for improving the segmentation algorithm include incorporation of a threshold for troughs. Currently, all troughs indicate a subject change, however, minor fluctuations in scores may be discounted.", "This evaluation is also a weak point as card(Wl \u2229 Wr ) only relies on word reiteration. As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["A method is presented for segmenting text into subtopic areas. The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity. The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 0, "Text Segmentation Using Reiteration and Collocation", "ABSTRACT"], ["Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated. This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text. 1 Background Theory: Lexical Cohesion. Cohesion concerns how words in a text are related.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text.", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by look\u00ad up in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. \"orange oranges\").", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text pmtions by matching high frequency terms.", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex. The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991).", "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["Future work with this algorithm should include application to longer documents. With trough thresholding the segments identified in longer documents could detect significant subject changes. Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation. As an example, a text segmentation algorithm developed by Hearst ( 1994) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61. In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5). Each text was only 500 words in length and was related to a specific subject area. These factors limited the degree of subject change that occurred.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997). The degree to which a segmentation point was 'missed' by a trough, for instance, is not considered.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects. Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document.", "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved [1].", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget's Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A method is presented for segmenting text into subtopic areas. The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity. The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights. This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "ABSTRACT"], ["orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex).", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation. General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role.", "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., \u201ccar\u201d and \u201cdrive\u201d, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here. Al ig n m en t M o d e l P R A E R e n \u2192 c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007). The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128).", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["2 3 1. 1 Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en\u2192cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn \u2192en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["There have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993). Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al.", "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128).", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["1 Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en\u2192cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn \u2192en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison. The training time for each model is calculated from scratch.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency (Brown et al., 1993; Och and Ney, 2003) from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e). Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words. Table 1, Figure 1, and Figure 2 shows the AER results for different models.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)).", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence.", "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level. Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . . , fJ and a f J , we have \"\u00a3I \u03c6i + \u03c6\u01eb = J . target sentence eI 1 = e1, e2, . . . , eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions. Following Brown et al. (1993), we assume that each source word is aligned to exactly one target word.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Statistical Word Alignment Models."], ["The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing. More samples give no further improvement. Initially, the fertility IBM Model 1 and fertility HMM did not perform well. If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e). Hence, smoothing is needed.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["(2I +1)J \u03b4(fj , f )\u03b4(ei, e) J ! , we have: c(a|a\u2032; f J , e2I +1) = j P\u02dc(aJ |f J , e2I +1) \u00d7 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n \u03bb(ei)\u03c6i e\u2212\u03bb(ei ) \u00d7 i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u00d7 c(\u03c6|e; f1 , e1 ) = \u03b4(aj , a)\u03b4(aj\u22121, a\u2032) j P\u02dc(a1 |f1 , e1 ) \u00d7 J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 \u03c6 \u03b4(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)\u03b4(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency. We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability. Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977). The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Expectation Maximization Algorithm."], ["During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6. This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6. This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step. In this case, the fertility hidden Markov model is not faster than the HMM.", "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We need more the HMM do in the EM algorithms. The algorithm aJ J J 1 for the E-step on one machine (all machines are independent) is in Algorithm 1. For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . . , J ) is O(tI J ). This is the same complexity as the HMM if t is O(I ), and it has lower complexity if t is a constant.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Words from position I + 1 to 2I + 1 in the target sentence are all empty words. We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["However, IBM Model 4 is so complex that most researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box. The complexity in IBM Model 4 makes it hard to understand and to improve. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. We also want it to be accurate and computationally efficient. There have been many years of research on word alignment.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596\u2013605, MIT, Massachusetts, USA, 911 October 2010.", "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["2 3 1. 1 Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en\u2192cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn \u2192en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . . , fJ and a f J , we have \"\u00a3I \u03c6i + \u03c6\u01eb = J . target sentence eI 1 = e1, e2, . . .", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Statistical Word Alignment Models."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["2 3 1. 1 Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en\u2192cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn \u2192en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["1 Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en\u2192cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn \u2192en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison. The training time for each model is calculated from scratch.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency (Brown et al., 1993; Och and Ney, 2003) from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably.", "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["Replacing 1 with a1 \u03c6\u01eb ! J i ! \u03c6\u01eb ! J i ! (2I +1)J \u03b4(fj , f )\u03b4(ei, e) J ! , we have: c(a|a\u2032; f J , e2I +1) = j P\u02dc(aJ |f J , e2I +1) \u00d7 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n \u03bb(ei)\u03c6i e\u2212\u03bb(ei ) \u00d7 i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u00d7 c(\u03c6|e; f1 , e1 ) = \u03b4(aj , a)\u03b4(aj\u22121, a\u2032) j P\u02dc(a1 |f1 , e1 ) \u00d7 J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 \u03c6 \u03b4(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)\u03b4(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency. We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability. Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Expectation Maximization Algorithm."], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Our model is thus much faster than IBM Model 4. Our model is also easier to understand than IBM Model 4. The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4. While better word alignment results do not necessarily correspond to better translation quality, our translation results are comparable in translation quality to both the HMM and IBM Model 4. Acknowledgments We would like to thank Tagyoung Chung, Matt Post, and the anonymous reviewers for helpful comments.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Conclusion."], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e). Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words. Table 1, Figure 1, and Figure 2 shows the AER results for different models.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)).", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level. Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . . , fJ and a f J , we have \"\u00a3I \u03c6i + \u03c6\u01eb = J . target sentence eI 1 = e1, e2, . . . , eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions. Following Brown et al. (1993), we assume that each source word is aligned to exactly one target word.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Statistical Word Alignment Models."], ["The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.", "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tar get language. However, IBM Model 4 is so complex that most researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3).", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["The expected fertility for a nonempty word ei is \u03bb(ei), and the expected fertil over fertilities that agree with the alignments: ity for all empty words is I \u03bb(\u01eb). Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), \u03bb(e), \u03be1(e) , \u03be2(a )) 1 1 1 = P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) = P\u02dc \u2032 aJ e 2I +1, f J ) log \u2032 P (aJ , f J | e2I +1) 1 1 ,\u03c6\u01eb 1 1 1 1 1 1 J 1 1 1 1 \u2248 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) \u00d7 \u2212 \u03be1(e)( P (f |e) \u2212 1) 1 1 1 1 I \uf8eb J \uf8f6 e f n \u03b4 \uf8ed i=1 j=1 \u03b4(aj , i), \u03c6i\uf8f8 \u00d7 \u2212 \u03be2(a\u2032)( a\u2032 a P (a|a\u2032) \u2212 1) \uf8eb 2I +1 J \uf8f6 Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 \u03b4 \uf8ed i=I +1 j=1 \u03b4(aj , i), \u03c6\u01eb\uf8f8 (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, \u03c6\u01eb and each P (f |e) = \"\u00a3s c (f |e; f (s), e(s)) (4) \u03c6i are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments. Because we only sum over fer tilities that are consistent with the alignments, we P (a|a\u2032) = \"\u00a3s c (a|a\u2032; f (s), e(s)) (5)have \"\u00a3f J P (f J |e2I +1) < 1, and our model is de \"\u00a3 \"\u00a3 a s c(a|a\u2032; f (s), e(s)) 1 1 1 \"\u00a3 (s) (s) ficient, similar to IBM Models 3 and 4 (Brown et al., 1993). We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion \u03bb(e) = s c(\u03c6| e; f , e ) s c(k|e; f (s), e(s)) (6) probability: the distortion probability is 0 if fertility where s is the number of bilingual sentences, andis not consistent with alignments, and uniform oth c(f |e; f J 2I +1 \u02dc J J 2I +1 erwise. The total number of consistent fertility and 1 , e1 ) = P (a1 |f1 , e1 ) \u00d7 J alignments is J ! .", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here. Al ig n m en t M o d e l P R A E R e n \u2192 c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["Due to space constraints, we are unable to provide details for IBM Models 3, 4 and 5; see Brown et al. (1993) and Och and Ney (2003). But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency (Brown et al., 1993; Och and Ney, 2003) from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Statistical Word Alignment Models."], ["It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596\u2013605, MIT, Massachusetts, USA, 911 October 2010. Qc 2010 Association for Computational Linguistics estimation. Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993).", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place. (b) After they were released...", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases. This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics. In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used. 7 Acknowledgements. This work was supported in part by the National Science Foundation under grant IRI9704240.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["In this section, we describe how contextual role knowledge is represented and learned. Section 2.1 describes how BABAR generates training examples to use in the learning process. We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents. Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples. 2.1 Reliable Case Resolutions.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["Given a document to process, BABAR uses four modules to perform coreference resolution. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released. The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["4 Evaluation Results. 4.1 Corpora. We evaluated BABAR on two domains: terrorism and natural disasters. We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuter\u2019s text collection8 that had a subject code corresponding to natural disasters. For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X \u2229Y =S 1 \u2212 ) m1 (X ) \u2217 m2 (Y ) m1 (X ) \u2217 m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).", "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, management succession systems must distinguish between a person who is fired and a person who is hired. Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime. We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. For example, kidnapping victims should be extracted from the subject of the verb \u201ckidnapped\u201d when it occurs in the passive voice (the shorthand representation of this pattern would be \u201c<subject> were kidnapped\u201d).", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags. This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain. Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected. However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse. For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent.", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Figure 2 shows examples of lexical expectations that were learned for both domains. collected too. We will refer to the semantic classes that co-occur with a caseframe as the semantic expectations of the caseframe. Figure 3 shows examples of semantic expectations that were learned. For example, BABAR learned that agents that \u201cassassinate\u201d or \u201cinvestigate a cause\u201d are usually humans or groups (i.e., organizations).", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place. (b) After they were released...", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases. This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics. In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used. 7 Acknowledgements. This work was supported in part by the National Science Foundation under grant IRI9704240.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released. The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data. Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics.", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Proper names that match are resolved with each other. The second case involves existential noun phrases (Allen, 1995), which are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse. In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood. For example, a story can mention \u201cthe FBI\u201d, \u201cthe White House\u201d, or \u201cthe weather\u201d without any prior referent in the story. Although these existential NPs do not need a prior referent, they may occur multiple times in a document.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot. conceptual relationship in the discourse. For example, co-occurring caseframes may reflect synonymy (e.g., \u201c<patient> kidnapped\u201d and \u201c<patient> abducted\u201d) or related events (e.g., \u201c<patient> kidnapped\u201d and \u201c<patient> released\u201d). We do not attempt to identify the types of relationships that are found. BABAR merely identifies caseframes that frequently co-occur in coreference resolutions.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["3 The Coreference Resolution Model. Given a document to process, BABAR uses four modules to perform coreference resolution. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions. 3.1 General Knowledge Sources.", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions. (Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Although these existential NPs do not need a prior referent, they may occur multiple times in a document. By definition, each existential NP uniquely specifies an object or concept, so we can infer that all instances of the same existential NP are coreferent (e.g., \u201cthe FBI\u201d always refers to the same entity). Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved. Table 1 briefly describes the seven syntactic heuristics used by BABAR to resolve noun phrases. Words and punctuation that appear in brackets are considered optional.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["Ex: He was found in San Jose, where ... Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding.", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["In this section, we describe how contextual role knowledge is represented and learned. Section 2.1 describes how BABAR generates training examples to use in the learning process. We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents. Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples. 2.1 Reliable Case Resolutions.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["Ex: He was found in San Jose, where ... Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags. This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain. Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected. However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse. For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Ideally we\u2019d like to know the thematic role of each extracted noun phrase, but AutoSlog does not generate thematic roles. As a (crude) approximation, we normalize the extraction patterns with respect to active and passive voice and label those extractions as agents or patients. For example, the passive voice pattern \u201c<subject> were kidnapped\u201d and the active voice pattern \u201ckidnapped <direct object>\u201d are merged into a single normalized pattern \u201ckidnapped <patient>\u201d.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, \u201c<agent> kidnapped\u201d or \u201ckidnapped <patient>\u201d), and (2) predicate-argument relations associated with both verbs and nouns (e.g., \u201ckidnapped for <np>\u201d or \u201cvehicle with <np>\u201d). We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus. The learned patterns are then normalized and applied to the corpus.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent. Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes. 2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too. an event.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, management succession systems must distinguish between a person who is fired and a person who is hired. Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime. We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. For example, kidnapping victims should be extracted from the subject of the verb \u201ckidnapped\u201d when it occurs in the passive voice (the shorthand representation of this pattern would be \u201c<subject> were kidnapped\u201d).", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime. We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. For example, kidnapping victims should be extracted from the subject of the verb \u201ckidnapped\u201d when it occurs in the passive voice (the shorthand representation of this pattern would be \u201c<subject> were kidnapped\u201d). The types of patterns produced by AutoSlog are outlined in (Riloff, 1996).", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["If these sets do not overlap, then the words cannot be coreferent. The semantic caseframe expectations are used in two ways. One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves. Given an anaphor and candidate, BABAR checks (1) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate, and (2) whether the semantic classes of the candidate intersect with the semantic ex pectations of the caseframe that extracts the anaphor. If one of these checks fails then this knowledge source reports that the candidate is not a viable antecedent for the anaphor.", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics. BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions. In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned. Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features. Our coreference resolver also incorporates an existential noun phrase recognizer and a DempsterShafer probabilistic model to make resolution decisions.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor. If either case is true, then CFLex reports that the anaphor and candidate might be coreferent. 2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations. Semantic expectations are analogous to lexical expectations except that they represent semantic classes rather than nouns. For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place.", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The learned patterns are then normalized and applied to the corpus. This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot.", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases. This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics. In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used. 7 Acknowledgements. This work was supported in part by the National Science Foundation under grant IRI9704240.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["If no candidate satisfies this condition (which is often the case), then the anaphor is left unresolved. One of the strengths of the DempsterShafer model is its natural ability to recognize when several credible hypotheses are still in play. In this situation, BABAR takes the conservative approach and declines to make a resolution. 4 Evaluation Results. 4.1 Corpora.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions. (Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions.", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["ments contained 322 anaphoric links. For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links. In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998). We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships. Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in \u03b8 is correct, so eliminating all of the hypotheses violates this assumption.", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["(S2) The burglar fired the gun three times and fled. \u201cThe gun\u201d will be extracted by the caseframe \u201cfired <patient>\u201d. Its correct antecedent is \u201ca revolver\u201d, which is extracted by the caseframe \u201ckilled with <NP>\u201d. If \u201cgun\u201d and \u201crevolver\u201d refer to the same object, then it should also be acceptable to say that Fred was \u201ckilled with a gun\u201d and that the burglar \u201cfireda revolver\u201d. During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions. (Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). These systems rely on a training corpus that has been manually annotated with coreference links.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Consequently, we cut their evidence values in half to lessen their influence. 3.2 The DempsterShafer Decision Model. BABAR uses a DempsterShafer decision model (Stefik, 1995) to combine the evidence provided by the knowledge sources. Our motivation for using DempsterShafer is that it provides a well-principled framework for combining evidence from multiple sources with respect to competing hypotheses. In our situation, the competing hypotheses are the possible antecedents for an anaphor.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions. 3.1 General Knowledge Sources.", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections. Table 5: Individual Performance of KSs for Disasters (e.g., \u201cthe mayor\u201d vs. \u201cthe journalist\u201d). We also performed experiments to evaluate the impact of each type of contextual role knowledge separately. Tables 4 and 5 show BABAR\u2019s performance when just one contextual role knowledge source is used at a time. For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "However, the use of related verbs is similar in spirit to Bean and Riloff\u2019s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot. conceptual relationship in the discourse.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Juraf- sky, 2011), where they serve the central unit of semantic analysis.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Table 2 shows BABAR\u2019s performance. We measured recall (Rec), precision (Pr), and the F-measure (F) with recall and precision equally weighted. BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters. We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus. Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PAPER"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["4 Evaluation Results. 4.1 Corpora. We evaluated BABAR on two domains: terrorism and natural disasters. We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuter\u2019s text collection8 that had a subject code corresponding to natural disasters. For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X \u2229Y =S 1 \u2212 ) m1 (X ) \u2217 m2 (Y ) m1 (X ) \u2217 m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PAPER"], ["Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, the passive voice pattern \u201c<subject> were kidnapped\u201d and the active voice pattern \u201ckidnapped <direct object>\u201d are merged into a single normalized pattern \u201ckidnapped <patient>\u201d.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, \u201c<agent> kidnapped\u201d or \u201ckidnapped <patient>\u201d), and (2) predicate-argument relations associated with both verbs and nouns (e.g., \u201ckidnapped for <np>\u201d or \u201cvehicle with <np>\u201d). We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus. The learned patterns are then normalized and applied to the corpus. This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship.", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution.", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["ments contained 322 anaphoric links. For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links. In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998). We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships. Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in \u03b8 is correct, so eliminating all of the hypotheses violates this assumption.", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Tables 4 and 5 show BABAR\u2019s performance when just one contextual role knowledge source is used at a time. For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision. For pronouns, however, all of the knowledge sources increased recall, often substantially, and with little if any decrease in precision. This result suggests that all of contextual role KSs can provide useful information for resolving anaphora. Tables 4 and 5 also show that putting all of the contextual role KSs in play at the same time produces the greatest performance gain.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent. Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source. For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us. Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The scoping heuristics are based on the anaphor type: for reflexive pronouns the scope is the current clause, for relative pronouns it is the prior clause following its VP, for personal pronouns it is the anaphor\u2019s sentence and two preceding sentences, and for definite NPs it is the anaphor\u2019s sentence and eight preceding sentences. The semantic agreement KS eliminates some candidates, but also provides positive evidence in one case: if the candidate and anaphor both have semantic tags human, company, date, or location that were assigned via NER or the manually labeled dictionary entries. The rationale for treating these semantic labels differently is that they are specific and reliable (as opposed to the WordNet classes, which are more coarse and more noisy due to polysemy). KS Function Ge nde r filters candidate if gender doesn\u2019t agree. Nu mb er filters candidate if number doesn\u2019t agree.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus. For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NP\u2019s caseframe. For example, if X and Y are coreferent, then both X and Y are considered to co-occur with the caseframe that extracts X as well as the caseframe that extracts Y. We will refer to the set of nouns that co-occur with a caseframe as the lexical expectations of the case- frame. Figure 2 shows examples of lexical expectations that were learned for both domains. collected too.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the \u03c72 table to obtain a confidence level. The confidence level is then used as the belief value for the knowledge source. For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent. 3 The Coreference Resolution Model. Given a document to process, BABAR uses four modules to perform coreference resolution.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["BABAR uses the log-likelihood statistic (Dunning, 1993) to evaluate the strength of a co-occurrence relationship. For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the \u03c72 table to obtain a confidence level. The confidence level is then used as the belief value for the knowledge source. For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent. 3 The Coreference Resolution Model.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Given a document to process, BABAR uses four modules to perform coreference resolution. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus. Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us. Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Once the scenario had been identified, the ranked retrieval method was used, and the ranked list was sampled at different points to collect approximately 200 relevant and 200 nonrelevant articles, representing a variety of article types (feature articles, brief notices, editorials, etc.). From those candidate articles, the training and test sets were selected blindly, with later checks and corrections for imbalances in the relevant/nonrelevant categories and in article types. From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks. The selection was again done blindly, with later checks to ensure that the set was fairly representative in terms of article length and type. Note that although Named Entity, Coreference and Template Element are defined as domain-independent tasks, the articles that were used for MUC6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well. The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors. Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies. The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "ACKNOWLEDGEMENTS"], ["Note also that even the best system on the third event was unable to determine that the succession event was occurring at McCannEfickson; in addition, it only partially captured the full title of the post. To its credit, however, it did recognize that the event was relevant; only two systems produced output that is recognizable as pertaining to this event. One common problem was the simple failure to recognize \"hire\" as an indicator of a succession. Two systems never filled the OTHER_ORG slot or its dependent slot, REL OTHER_ORG, despite the fact that data to fill those slots was often present; over half the IN_AND_OUT objects in the answer key contain data for those two slots. Almost without exception, systems did more poorly on those two slots than on any others in the SUCCESSION_EVENT and IN_AND_OUT objects; the best scores posted were 70% error on OTHER_ORG (median score of 79%) and 72% error on REL_OTHER ORG (median of 86%).", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations. Other sources of excitement are the spinoff efforts that the NE and CO tasks have inspired that bring these tasks and their potential applications to the attention of new research groups and new customer groups. In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure. Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well.", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Test abstract The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision. The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well. The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors. Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies. The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "ACKNOWLEDGEMENTS"], ["The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years. The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The example passage covers a broad spectrum of the phenomena included in the task. At one end of the spectrum are the proper names and aliases, which are inherently definite and whose referent may appear anywhere in the text. In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus. On the periphery of the central phenomena are markables whose status as coreferring expressions is determined by syntax, such as predicate nominals (\"Motor Vehicles International is the biggest American auto exporter to Latin America\") and 100 90 80 70 60 50 40 30 20 10 0 0 10 20 30 appositives (\"MVI, the first company to announce such a move since the passage of the new international trade agreement\"). At the far end of the spectrum are bare common nouns, such as the prenominal \"company\" in the example, whose status as a referring expression may be questionable.", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS, Av. \u00a9 2001 Association for Computational Linguistics interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["As defined for MUC6, the ST task presents a significant challenge in terms of system portability, in that the test procedure requ~ed that all domain-specific development be done in a period of one month. For past MUC evaluations, the formal run had been conducted using the same scenario as the dry run, and the task definition was released well before the dry run. Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels. However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1. The standardized template structure minimizes the amount of idiosyncratic programming required to produce the expected types of objects, links, and slot fills.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Frequently, at least one can be found in close proximity to an organization's name, e.g., as an appositive (\"Creative Artists Agency, the big Hollywood talent agency\"). Nonetheless, performance is much lower on this slot than on others. Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot? One reason for low performance is that an organization may be identified in a text solely by a descriptor, i.e., without a fill for the ORG_NAME slot and therefore without the usual local clues that the NP is in fact a relevant descriptor. It is, of course, also possible that a text may identify an organization solely by name.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The TYPE slot, however, is a more difficult slot for ENAMEX than for the other subcategories. It involves a three-way distinction for ENAMEX and only a two-way distinction for NUMEX and TIMEX, and it offers the possibility of confusing names of one type with names of another, especially the possibility of confusing organization names with person names. Looking at the document section scores in table 3, we see that the error score on the body of the text was much lower than on the headline for all but a few systems. There was just one system that posted a higher error score on the body than on the headline, the baseline NMSU CRL configuration, and the difference in scores is largely due to the fact that the system overgenerated to a greater extent on the body than on the headline. Its basic strategy for 96.42 0 95.66 0 0 7 7 94.92 0 0 8 8 94.00 0 0 20 9 93.65 0 2 16 10 93.33 0 4 38 9 92.88 0 0 18 10 92.74 0 0 22 11 92.61 100 0 18 9 91.20 0 0 30 13 90.84 3 11 19 14 89.06 3 4 28 18 88.19 0 0 22 20 85.82 0 6 18 21 85.73 0 44 53 21 84.95 0 0 50 21 Table 3.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns. CO Results on Some Aspects of Task and on \"Walkthrough Article\" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns. CO Results on Some Aspects of Task and on \"Walkthrough Article\" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way. Two useful attributes for the equivalence class as a whole would be one to distinguish individual coreference from type coreference and one to identify the general semantic type of the class (organization, person, location, time, currency, etc.).", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.. The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA. The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration. The changes occurred only in performance on identifying organizations. BBN conducted a comparative test in which the experimental configuration used a larger lexicon than the baseline configuration, but the exact nature of the difference is not known and the performance differences are very small.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision. The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["\") as a monetary value rather than ignoring it as a weight. In addition, a number of errors identifying entity names were made; some of those errors also showed up as errors on the Template Element task and are described in a later section of this paper. COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe. The variety of high-frequency phenomena covered by the task is partially represented in the following hypothetical example, where all bracketed text segments are considered coreferential: 428 [Motor Vehicles International Corp.] announced a major management shakeup .... [MVI] said the chief executive officer has resigned ....", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers. EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.  Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.  Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence from anywhere in the text.  Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The following breakdowns of overall scores on NE are computed:  by slot, i.e., for performance across tag elements, across TYPE attributes, and across tag strings;  by subcategorization, i.e., for performance on each TYPE attribute separately;  by document section, i.e., for performance on distinct subparts of the article, as identified by the SGML tags contained in the original text: <HL> (\"headline\"), <DD> (\"document date\"), <DATELINE>, and <TXT> (the body of the article). NE Results Overall Fifteen sites participated in the NE evaluation, including two that submitted two system configurations for testing and one that submitted four, for a total of 20 systems. As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites. On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well. It was also unexpected that one of the systems would match human performance on the task.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["descriptor identify or the SCENARIO TEMPLATE A Scenario Template (ST) task captures domain-and task-specific information. Three scenarios were defined in the course of MUC6: (1) a scenario concerning the event of organizations placing orders to buy aircraft with aircraft manufacturers (the \"aircraft order\" scenario); (2) a scenario concerning the event of contract negotiations between labor unions and companies (the \"labor negotiations\" scenario); (3) a scenario concerning changes in corporate managers occupying executive posts (the \"management succession\" scenario). The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation. One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["But the problems are certainly tractable; none of the fifteen TE entities in the key (ten ORGANIZATION entities and five PERSON entities) was miscategofized by all of the systems. In addition to miscategorization errors, the walkthrough text provides other interesting examples of system errors at the object level and the slot level, plus a number of examples of system successes. One success for the systems as a group is that each of the six smaller ORGANIZATION objects and four smaller PERSON objects (those with just one or two filled slots in the key) was matched perfectly by at least one system; in addition, one larger ORGANIZATION object and two larger PERSON objects were perfectly matched by at least one system. Thus, each of the five PERSON objects in the key and seven of the ten ORGANIZATION objects in the key were matched perfectly by at least one system. The three larger ORGANIZATION objects that none of the systems got perfectly correct are for the McCannErickson, Creative Artists Agency, and Coca-Cola companies.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["(The reverse is not the case, i.e., ORG_COUNTRY may be filled even if ORG_LOCALE is not, but this situation is relatively rare.) Since a missing or spurious ORG_LOCALE is likely to incur the same error in ORG_COUNTRY, the error scores for the two slots are understandably similar. 5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision.. ((XI 90  ,,v 80   04 ~) 5O 4O 20 10 0 .. 0 10 220 30 41) 50 60 7(1 80 91) Recall I(X) Figure 4.", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Statistically, large differences of up to 15 points may not be reflected as a difference in the ranking of the systems. Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]). Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Also, the descriptor is not always close to the name, and some discourse processing may be requ~ed in order to identify it --this is likely to increase the opportunity for systems to miss the information. A third significant reason is that the response fill had to match the key fill exactly in order to be counted correct; there was no allowance made in the scoring software for assigning full or partial credit if the response fill only partially matched the key fill. It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys. TE Results on \"Walkthrough Article\" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems. Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting. Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\"", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\" As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["This period comprised the \"evaluation epoch.\" As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants. The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query. It can also be used to do unranked, Boolean retrievals. The Boolean retrieval method was used in the initial probing of the corpus to identify candidates for the Scenario Template task, because the Boolean retrieval is relatively fast, and the unranked results are easy to scan to get a feel for the variety of nonrelevant as well as relevant documents that match all or some of the query terms.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["All the participating sites also submitted systems for evaluation on the TE and NE tasks. All but one of the development teams (UDurham) had members who were veterans of MUC5. Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant. Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object. The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT \" Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the annotators sometimes found di.cult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other.", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\" As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["All the participating sites also submitted systems for evaluation on the TE and NE tasks. All but one of the development teams (UDurham) had members who were veterans of MUC5. Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant. Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object. The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT \" Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["A variety of proper name types were excluded, e.g. product names. The range of numerical and temporal expressions covered by the task was also limited; one notable example is the restriction of temporal expressions to exclude \"relative\" time expressions such as \"last week\". Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation. Some work on expanding the scope of the NE task has been carried out in the context of a foreign- language NE evaluation conducted in the spring of 1996. This evaluation is called the MET (Multilingual Named Entity) and, like MUC6, was carried out under the auspices of the Tipster Text program.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation. One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion. The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure. At the top level is the TEMPLATE object, of which there is one instantiated for every document.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion. The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure. At the top level is the TEMPLATE object, of which there is one instantiated for every document. This object points down to one or more SUCCESSION_EVENT objects if the document meets the event relevance criteria given in the task documentation.", "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites. On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well. It was also unexpected that one of the systems would match human performance on the task. Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC. This test measures the amount of variability between the annotators.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The document section results show 0% error on Document Date and Dateline, 7% error on Headline, and 6% error on Text. The subcategory error scores were 6% on Organization, 1% on Person, and 4% on Location, 8% on Date, and 0% on Money and Percent. These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates. Analysis of the results shows that some Date errors were a result of simple oversight (e.g., \"fiscal 1994\") and others were a consequence of forgetting or misinterpreting the task guidelines with respect to determining the maximal span of the date expression (e.g., tagging \"fiscal 1993's second quarter\" and \"Aug. 1\" separately, rather than tagging \"fiscal 1993's second quarter, ended Aug. 1\" as a single expression in accordance with the task guidelines). NE Results on \"Walkthrough Article\" In the answer key for the walkthrough article there are 69 ENAMEX tags (including a few optional ones), six TIMEX tags and six NUMEX tags.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Human performance was measured in terms of variability between the outputs produced by the two NRaD and SAIC evaluators for 30 of the articles in the test set (the same 30 articles that were used for NE and CO testing). Using the scoring method in which one annotator's draft key serves as the \"key\" and the other annotator's draft key serves as the \"response,\" the overall consistency score was 93.14 on the F-measure, with 93% recall and 93% precision. TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5. Figure 6 indicates the relative amount of error contributed by each of the slots in the ORGANIZATION object. It is evident that the more linguistic processing necessary to fill a slot, the harder the slot is to fill correctly.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC. This test measures the amount of variability between the annotators. When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%. The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%. In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1.", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%. In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1. Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74..  It represents just one style of writing \"the Chrysler division\" (currently, only \"Chrysler\" (journalistic) and has a basic basic toward financial news and a specific bias toward the topic of the Scenario Template task.  It was very small (only 30 articles).", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years. The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume. All except the Scenario Template task are defined independently of any particular domain. This paper surveys the results of the evaluation on each task and, to a more limited extent, across tasks.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["TE Results on \"Walkthrough Article\" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems. Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing. There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations. Errors of these kinds result in a penalty at the object level, since the extracted information is contained in the wrong type of object. Examples of each of these types of error appear below, along with the number of systems that committed the error.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Nearly half the sites chose to participate in all four tasks, and all but one site participated in at least one SGML task and one extraction task. The variety of tasks designed for MUC6 reflects the interests of both participants and sponsors in assessing and furthering research that can satisfy some urgent text processing needs in the very near term and can lead to solutions to more challenging text understanding problems in the longer term. Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem. Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns. The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["They can also be quite long and complex and can even have internal punctuation such as a commas or an ampersand. Sometimes it is difficult to distinguish them from names of other types, especially from person names. Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.. The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA. The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration.", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem. Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns. The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting. Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Commercial systems are available already that include identification of those defined for this MUC6 task, and since a number of systems performed very well for MUC6, it is evident that high performance is probably within reach of any development site that devotes enough effort to the task. Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one. The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks. The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task. The association of shortened forms of the name with the full name depends on techniques that could be used for NE and CO as well as for TE.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4. Highest P&R F-Measure scores posted for MUC6 and MUC5 ST tasks Note that table 4 shows four top scores for MUC5, one for each language-domain pair: English Joint Ventures (EJV), Japanese Joint Ventures (JJV), English Microelectronics (EME), and Japanese Microelectronics (JME). From this table, it may be reasonable to conclude that progress has been made, since the MUC6 performance level is at least as high as for three of the four MUC5 tasks and since that performance level was reached after a much shorter time. ST Results on Some Aspects of Task and on \"Walkthrough Article\" Three succession events are reported in the walkthrough article. Successful interpretation of three sentences from the walkthrough article is necessary for high performance on these events.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]). Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed. Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed. Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations. MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."]]