[["we perform five runs with different random initialization of sampling state. Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set.", "Following ##CITATION##  we report the best and median settings of hyperparameters based on the F- score in addition to inferred values ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.", "Following ##CITATION##  we report the best and median settings of hyperparameters based on the F- score in addition to inferred values ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets. Following the setup of Johnson  , we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora   which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set.", "Following ##CITATION##  we report the best and median settings of hyperparameters based on the F- score in addition to inferred values ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, \u03b1, \u03b2) \u221d P (T , t, W , w|\u03b1, \u03b2) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, \u03c8, \u03b8, \u03c6, w|\u03b1, \u03b2)d\u03c8d\u03b8d\u03c6 Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior. Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i). The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5). Performance typically stabilizes across languages after only a few number of iterations. to represent the ith word type emitted by the HMM: P  .", "Following ##CITATION##  we report the best and median settings of hyperparameters based on the F- score in addition to inferred values ", 1, "Simple Type-Level Unsupervised POS Tagging", "Learning and Inference."], ["On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible. Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank. This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. \u2014 similar results have been observed across multiple languages.", "This property is not strictly true of linguistic data but is a good approximation: as ##CITATION##  note assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages ", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "This property is not strictly true of linguistic data but is a good approximation: as ##CITATION##  note assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["The corresponding token words w are drawn conditioned on t and \u03b8.2 Our full generative model is given by: K P (\u03c6, \u03b8|T , \u03b1, \u03b2) = n (P (\u03c6t|\u03b1)P (\u03b8t|T , \u03b1)) t=1 The transition distribution \u03c6t for each tag t is drawn according to DIRICHLET(\u03b1, K ), where \u03b1 is the shared transition and emission distribution hyperparameter. In total there are O(K 2) parameters associated with the transition parameters. In contrast to the Bayesian HMM, \u03b8t is not drawn from a distribution which has support for each of the n word types. Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then \u03b8t is drawn from DIRICHLET(\u03b1, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4). Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (\u03c6, \u03b8) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|\u03c6, \u03b8) = P (T , W , \u03b8, \u03c8, \u03c6, t, w|\u03b1, \u03b2) = P (T , W , \u03c8|\u03b2) [Lexicon] \uf8eb n n \uf8ed (w,t)\u2208(w,t) j \uf8f6 P (tj |\u03c6tj\u22121 )P (wj |tj , \u03b8tj )\uf8f8 P (\u03c6, \u03b8|T , \u03b1, \u03b2) [Parameter] P (w, t|\u03c6, \u03b8) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively.", "This property is not strictly true of linguistic data but is a good approximation: as ##CITATION##  note assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages ", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity.", "This property is not strictly true of linguistic data but is a good approximation: as ##CITATION##  note assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages ", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function  . In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "More recently ##CITATION##   presented a new type-based model and also reported very good results ", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "More recently ##CITATION##   presented a new type-based model and also reported very good results ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["See Table 2 for the tag set size of other languages. With the exception of the Dutch data set, no other processing is performed on the annotated tags. 6 Results and Analysis. We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness.", "More recently ##CITATION##   presented a new type-based model and also reported very good results ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints. In this paper, we make a simplifying assumption of one-tag-per-word. This assumption, however, is not inherent to type-based tagging models.", "More recently ##CITATION##   presented a new type-based model and also reported very good results ", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function  . In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "As in previous work   we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Another thread of relevant research has explored the use of features in unsupervised POS induction  . These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1.", "As in previous work   we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "As in previous work   we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states.", "As in previous work   we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["This design does not guarantee \u201cstructural zeros,\u201d but biases towards sparsity. A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments  . This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight  , who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j This is the approach taken by ##CITATION##   ", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j This is the approach taken by ##CITATION##   ", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states. Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word. Conditioned on T , features of word types W are drawn. We refer to (T , W ) as the lexicon of a language and \u03c8 for the parameters for their generation; \u03c8 depends on a single hyperparameter \u03b2.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j This is the approach taken by ##CITATION##   ", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish. On each language we investigate the contribution of each component of our model. For all languages we do not make use of a tagging dictionary. Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45. 2 62.6 45.", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j This is the approach taken by ##CITATION##   ", 1, "Simple Type-Level Unsupervised POS Tagging", "Experiments."], ["We use w erations of sampling (see Figure 2 for a depiction). We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish. On each language we investigate the contribution of each component of our model. For all languages we do not make use of a tagging dictionary. Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45.", "Following ##CITATION##   we used only the training sections for each language ", 0, "Simple Type-Level Unsupervised POS Tagging", "Experiments."], ["The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets. Following the setup of Johnson  , we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora   which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set.", "Following ##CITATION##   we used only the training sections for each language ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.", "Following ##CITATION##   we used only the training sections for each language ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible.", "Following ##CITATION##   we used only the training sections for each language ", 1, "Simple Type-Level Unsupervised POS Tagging", "ABSTRACT"], ["Part-of-speech (POS) tag distributions are known to exhibit sparsity \u2014 a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech ##CITATION## this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction", 0, "Simple Type-Level Unsupervised POS Tagging", "ABSTRACT"], ["Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags. We tokenize MWUs and their POS tags; this reduces the tag set size to 12. See Table 2 for the tag set size of other languages.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech ##CITATION## this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible. Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank. This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech ##CITATION## this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["On several languages, we report performance exceeding that of more complex state-of-the art systems.1 Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits \u201cone tag per discourse\u201d sparsity \u2014 words are likely to select a single predominant tag in a corpus, even when several tags are possible. Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank. This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. \u2014 similar results have been observed across multiple languages.", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech ##CITATION## this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["We experiment with four values for each hyperparameter resulting in 16 (\u03b1, \u03b2) combinations: \u03b1 \u03b2 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations In each run, we performed 30 iterations of Gibbs sampling for the type assignment variables W .4 We use the final sample for evaluation. Evaluation Metrics We report three metrics to evaluate tagging performance. As is standard, we report the greedy one-to-one   and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags. We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-. encodes the one tag per word constraint and is uni form over type-level tag assignments.", "vised POS induction algorithm ##CITATION##11 Our mapping algorithm then learns the connection between these clusters and universal tags", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags. We tokenize MWUs and their POS tags; this reduces the tag set size to 12. See Table 2 for the tag set size of other languages. With the exception of the Dutch data set, no other processing is performed on the annotated tags.", "vised POS induction algorithm ##CITATION##11 Our mapping algorithm then learns the connection between these clusters and universal tags", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. Recent work has made significant progress on unsupervised POS tagging  . Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods.", "vised POS induction algorithm ##CITATION##11 Our mapping algorithm then learns the connection between these clusters and universal tags", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states.", "vised POS induction algorithm ##CITATION##11 Our mapping algorithm then learns the connection between these clusters and universal tags", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.   and the posterior regular- ization HMM of Grac\u00b8a et al.  .", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation but despite recent progress the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications Berg- Kirkpatrick et al Grac\u00b8a et al ##CITATION##", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.   and the posterior regular- ization HMM of Grac\u00b8a et al.  . The system of Berg-Kirkpatrick et al.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation but despite recent progress the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications Berg- Kirkpatrick et al Grac\u00b8a et al ##CITATION##", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["However, our full model takes advantage of word features not present in Grac\u00b8a et al.  . Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming Grac\u00b8a et al.  . Ablation Analysis We evaluate the impact of incorporating various linguistic features into our model in Table 3.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation but despite recent progress the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications Berg- Kirkpatrick et al Grac\u00b8a et al ##CITATION##", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity. We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states.", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation but despite recent progress the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications Berg- Kirkpatrick et al Grac\u00b8a et al ##CITATION##", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.   and the posterior regular- ization HMM of Grac\u00b8a et al.  .", "Systems for inducing syntactic categories often make use of morpheme-like features such as word-final characters Smith and Eisner Haghighi and Klein Berg-Kirkpatrick et al ##CITATION## or model words at the character-level Clark; Blunsom and Cohn but do not include morphemes explicitly", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["  and the posterior regular- ization HMM of Grac\u00b8a et al.  . The system of Berg-Kirkpatrick et al.   reports the best unsupervised results for English. We consider two variants of Berg-Kirkpatrick et al.", "Systems for inducing syntactic categories often make use of morpheme-like features such as word-final characters Smith and Eisner Haghighi and Klein Berg-Kirkpatrick et al ##CITATION## or model words at the character-level Clark; Blunsom and Cohn but do not include morphemes explicitly", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["  also report results on English, but on the reduced 17 tag set, which is not comparable to ours). Their best model yields 44.5% one-to-one accuracy, compared to our best median 56.5% result. However, our full model takes advantage of word features not present in Grac\u00b8a et al.  . Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming Grac\u00b8a et al.", "Systems for inducing syntactic categories often make use of morpheme-like features such as word-final characters Smith and Eisner Haghighi and Klein Berg-Kirkpatrick et al ##CITATION## or model words at the character-level Clark; Blunsom and Cohn but do not include morphemes explicitly", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This assumption, however, is not inherent to type-based tagging models. A promising direction for future work is to explicitly model a distribution over tags for each word type. We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon. The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684). We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments.", "Systems for inducing syntactic categories often make use of morpheme-like features such as word-final characters Smith and Eisner Haghighi and Klein Berg-Kirkpatrick et al ##CITATION## or model words at the character-level Clark; Blunsom and Cohn but do not include morphemes explicitly", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction  . These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na\u00a8\u0131veBayes approach also yields substantial performance gains, without the associated training complexity.", "Several unsupervised POS induction systems make use of morphological features Blunsom and Cohn ##CITATION## Berg-Kirkpatrick et al Clark Christodoulopoulos et al and this approach has been empirically proved to be helpful Christodoulopoulos et al", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["  and the posterior regular- ization HMM of Grac\u00b8a et al.  . The system of Berg-Kirkpatrick et al.   reports the best unsupervised results for English. We consider two variants of Berg-Kirkpatrick et al.", "Several unsupervised POS induction systems make use of morphological features Blunsom and Cohn ##CITATION## Berg-Kirkpatrick et al Clark Christodoulopoulos et al and this approach has been empirically proved to be helpful Christodoulopoulos et al", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.   and the posterior regular- ization HMM of Grac\u00b8a et al.  .", "Several unsupervised POS induction systems make use of morphological features Blunsom and Cohn ##CITATION## Berg-Kirkpatrick et al Clark Christodoulopoulos et al and this approach has been empirically proved to be helpful Christodoulopoulos et al", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["This assumption, however, is not inherent to type-based tagging models. A promising direction for future work is to explicitly model a distribution over tags for each word type. We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon. The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684). We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments.", "Several unsupervised POS induction systems make use of morphological features Blunsom and Cohn ##CITATION## Berg-Kirkpatrick et al Clark Christodoulopoulos et al and this approach has been empirically proved to be helpful Christodoulopoulos et al", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity. This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler  . There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).", "Recently ##CITATION##  combined the one class per word type constraint Brown et al in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al \u2019s one-class HMM   ", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["The corresponding token words w are drawn conditioned on t and \u03b8.2 Our full generative model is given by: K P (\u03c6, \u03b8|T , \u03b1, \u03b2) = n (P (\u03c6t|\u03b1)P (\u03b8t|T , \u03b1)) t=1 The transition distribution \u03c6t for each tag t is drawn according to DIRICHLET(\u03b1, K ), where \u03b1 is the shared transition and emission distribution hyperparameter. In total there are O(K 2) parameters associated with the transition parameters. In contrast to the Bayesian HMM, \u03b8t is not drawn from a distribution which has support for each of the n word types. Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then \u03b8t is drawn from DIRICHLET(\u03b1, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4). Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (\u03c6, \u03b8) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|\u03c6, \u03b8) = P (T , W , \u03b8, \u03c8, \u03c6, t, w|\u03b1, \u03b2) = P (T , W , \u03c8|\u03b2) [Lexicon] \uf8eb n n \uf8ed (w,t)\u2208(w,t) j \uf8f6 P (tj |\u03c6tj\u22121 )P (wj |tj , \u03b8tj )\uf8f8 P (\u03c6, \u03b8|T , \u03b1, \u03b2) [Parameter] P (w, t|\u03c6, \u03b8) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively.", "Recently ##CITATION##  combined the one class per word type constraint Brown et al in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al \u2019s one-class HMM   ", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function  . In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "Recently ##CITATION##  combined the one class per word type constraint Brown et al in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al \u2019s one-class HMM   ", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler  . There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns). Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference. We evaluate our model on seven languages exhibiting substantial syntactic variation.", "Recently ##CITATION##  combined the one class per word type constraint Brown et al in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al \u2019s one-class HMM   ", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["In our model, we associate these features at the type-level in the lexicon. Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint. Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution \u03c8 over tag assignments drawn from DIRICHLET(\u03b2, K ). This alters generation of T as follows: n P (T |\u03c8) = n P (Ti|\u03c8) i=1 Note that this distribution captures the frequency of a tag across word types, as opposed to tokens. The P (T |\u03c8) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary.", "Recently ##CITATION##  combined the one class per word type constraint Brown et al in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al \u2019s one-class HMM   ", 1, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["i=1 (f,v)\u2208Wi For inference, we are interested in the posterior probability over the latent variables in our model. During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, \u03b1, \u03b2) \u221d P (T , t, W , w|\u03b1, \u03b2) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, \u03c8, \u03b8, \u03c6, w|\u03b1, \u03b2)d\u03c8d\u03b8d\u03c6 Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior. Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i). The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5).", "Recently ##CITATION##  combined the one class per word type constraint Brown et al in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al \u2019s one-class HMM   ", 1, "Simple Type-Level Unsupervised POS Tagging", "Learning and Inference."], [" . Ablation Analysis We evaluate the impact of incorporating various linguistic features into our model in Table 3. A novel element of our model is the ability to capture type-level tag frequencies. For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR). Across all languages, +PRIOR consistently outperforms 1TW, reducing error on average by 9.1% and 5.9% on best and median settings respectively.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of ##CITATION##   That model incorrectly assumed independence of the conditional sampling distributions resulting in a accuracy of 664% well below that of our model  ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. Recent work has made significant progress on unsupervised POS tagging  . Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of ##CITATION##   That model incorrectly assumed independence of the conditional sampling distributions resulting in a accuracy of 664% well below that of our model  ", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["6 Results and Analysis. We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.   and the posterior regular- ization HMM of Grac\u00b8a et al.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of ##CITATION##   That model incorrectly assumed independence of the conditional sampling distributions resulting in a accuracy of 664% well below that of our model  ", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["0 57.2 43. 3 61.7 38. 5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5). For each language and setting, we report one-to-one (11) and many- to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 11 metric.", "It is also interesting to compare the bigram PYP1HMM to the closely related model of ##CITATION##   That model incorrectly assumed independence of the conditional sampling distributions resulting in a accuracy of 664% well below that of our model  ", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.", "Similar constraints have been developed for part-of-speech tagging ##CITATION## Christodoulopoulos et al and the power of type-based sampling has been demonstrated even in the absence of explicit model constraints Liang et al", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.   and the posterior regular- ization HMM of Grac\u00b8a et al.  . The system of Berg-Kirkpatrick et al.", "Similar constraints have been developed for part-of-speech tagging ##CITATION## Christodoulopoulos et al and the power of type-based sampling has been demonstrated even in the absence of explicit model constraints Liang et al", 0, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function  . In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.", "Similar constraints have been developed for part-of-speech tagging ##CITATION## Christodoulopoulos et al and the power of type-based sampling has been demonstrated even in the absence of explicit model constraints Liang et al", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "Similar constraints have been developed for part-of-speech tagging ##CITATION## Christodoulopoulos et al and the power of type-based sampling has been demonstrated even in the absence of explicit model constraints Liang et al", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["As is standard, we use a fixed constant K for the number of tagging states. Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word. Conditioned on T , features of word types W are drawn. We refer to (T , W ) as the lexicon of a language and \u03c8 for the parameters for their generation; \u03c8 depends on a single hyperparameter \u03b2. Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters \u03b8 are generated conditioned on tag assignments T . We also draw transition parameters \u03c6.", "Here W t refers to the set of word types that are generated by tag t In other words conditioned on tag t we can only generate word w from the set of word types in W t which is generated earlier ##CITATION##", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters \u03b8 are generated conditioned on tag assignments T . We also draw transition parameters \u03c6. Both parameters depend on a single hyperparameter \u03b1. Once HMM parameters (\u03b8, \u03c6) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from \u03c6. The corresponding token words w are drawn conditioned on t and \u03b8.2 Our full generative model is given by: K P (\u03c6, \u03b8|T , \u03b1, \u03b2) = n (P (\u03c6t|\u03b1)P (\u03b8t|T , \u03b1)) t=1 The transition distribution \u03c6t for each tag t is drawn according to DIRICHLET(\u03b1, K ), where \u03b1 is the shared transition and emission distribution hyperparameter. In total there are O(K 2) parameters associated with the transition parameters.", "Here W t refers to the set of word types that are generated by tag t In other words conditioned on tag t we can only generate word w from the set of word types in W t which is generated earlier ##CITATION##", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["In total there are O(K 2) parameters associated with the transition parameters. In contrast to the Bayesian HMM, \u03b8t is not drawn from a distribution which has support for each of the n word types. Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then \u03b8t is drawn from DIRICHLET(\u03b1, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4). Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (\u03c6, \u03b8) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|\u03c6, \u03b8) = P (T , W , \u03b8, \u03c8, \u03c6, t, w|\u03b1, \u03b2) = P (T , W , \u03c8|\u03b2) [Lexicon] \uf8eb n n \uf8ed (w,t)\u2208(w,t) j \uf8f6 P (tj |\u03c6tj\u22121 )P (wj |tj , \u03b8tj )\uf8f8 P (\u03c6, \u03b8|T , \u03b1, \u03b2) [Parameter] P (w, t|\u03c6, \u03b8) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively. Since the parameter and token components will remain fixed throughout experiments, we briefly describe each.", "Here W t refers to the set of word types that are generated by tag t In other words conditioned on tag t we can only generate word w from the set of word types in W t which is generated earlier ##CITATION##", 0, "Simple Type-Level Unsupervised POS Tagging", "Generative Story."], ["In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.", "Here W t refers to the set of word types that are generated by tag t In other words conditioned on tag t we can only generate word w from the set of word types in W t which is generated earlier ##CITATION##", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type.", "Here W t refers to the set of word types that are generated by tag t In other words conditioned on tag t we can only generate word w from the set of word types in W t which is generated earlier ##CITATION##", 1, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward na\u00a8\u0131veBayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. Recent work has made significant progress on unsupervised POS tagging  . Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity.", "Second learning categories has been cast as unsupervised part-of-speech tagging task recent work includes Ravi and Knight  ##CITATION##  Lamar et al ", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments  . This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight  , who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model.", "Second learning categories has been cast as unsupervised part-of-speech tagging task recent work includes Ravi and Knight  ##CITATION##  Lamar et al ", 0, "Simple Type-Level Unsupervised POS Tagging", "Related Work."], ["This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. \u2014 similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system  . These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable.", "Second learning categories has been cast as unsupervised part-of-speech tagging task recent work includes Ravi and Knight  ##CITATION##  Lamar et al ", 0, "Simple Type-Level Unsupervised POS Tagging", "Introduction"], ["The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive.", "Second learning categories has been cast as unsupervised part-of-speech tagging task recent work includes Ravi and Knight  ##CITATION##  Lamar et al ", 1, "Simple Type-Level Unsupervised POS Tagging", "##other##"], ["They do paraphrase each other to some extent, but their relation can only be understood properly with |{dj : ti \u2208 dj }| |D| is the total number of sentences in the cluster and |{dj : ti \u2208 dj }| is the number of sen tences that contain the term ti. These scores are used in a vector space representation. The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by ##CITATION##  ", 0, "No title", "Method."], ["In future research we would like to investigate the task of judging paraphrases. The next step we would like to take towards automatic paraphrase generation, is to identify the differences between paraphrases at the constituent level. This task has in fact been performed by human annotators in the DAESO-project. A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way. Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme.", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by ##CITATION##  ", 0, "No title", "Discussion."], ["Paraphrase generation has already proven to be valuable for Question Answering  , Machine Translation   and the evaluation thereof  , but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases  .", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by ##CITATION##  ", 0, "No title", "Introduction"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by ##CITATION##  ", 1, "No title", "ABSTRACT"], ["A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way. Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme. Thanks also to Wauter Bosma for originally mining the headlines from Google News.", "##CITATION## have compared Clustering against pairwise matching for extracting paraphrases from news corpora ", 0, "No title", "Acknowledgements"], ["News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases  . We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus.", "##CITATION## have compared Clustering against pairwise matching for extracting paraphrases from news corpora ", 0, "No title", "Introduction"], ["Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering  , Machine Translation   and the evaluation thereof  , but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event.", "##CITATION## have compared Clustering against pairwise matching for extracting paraphrases from news corpora ", 0, "No title", "Introduction"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "##CITATION## have compared Clustering against pairwise matching for extracting paraphrases from news corpora ", 1, "No title", "ABSTRACT"], ["Where previous work has focused on aligning news-items at the paragraph and sentence level  , we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles. For the development of our system we use data which was obtained in the DAESO-project. This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122\u2013125, Athens, Greece, 30 \u2013 31 March 2009. Qc 2009 Association for Computational Linguistics document, and each original cluster as a collection of documents.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by ##CITATION## ", 0, "No title", "Method."], ["We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level  , we choose to focus on aligning the headlines of news articles.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by ##CITATION## ", 0, "No title", "Method."], ["For instance, in a cluster of four sentences, 4) = 6 alignments can be made. In our case, precision is the number of alignments retrieved from the clusters which are relevant, divided by the total number of retrieved alignments. Recall is the number of relevant retrieved aligments divided by the total number of relevant alignments. We use an F\u03b2 -score with a \u03b2 of 0.25 as we favour precision over recall. We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by ##CITATION## ", 0, "No title", "Method."], ["For text-to-text generation it is important to know which words and phrases are semantically close or exchangable in which contexts. While there are various resources available that capture such knowledge at the word level (e.g., synset knowledge in WordNet), this kind of information is much harder to get by at the phrase level. Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering  , Machine Translation   and the evaluation thereof  , but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.", "The obtained results have been compared with that of the Paraphrase Acquisition system developed by ##CITATION## ", 1, "No title", "Introduction"], ["The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni   is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example.", "Besides ##CITATION##\u2019s systems a Fuzzy C-Means FCM clustering approach has also been adopted", 0, "No title", "Method."], ["In future research we would like to investigate the task of judging paraphrases. The next step we would like to take towards automatic paraphrase generation, is to identify the differences between paraphrases at the constituent level. This task has in fact been performed by human annotators in the DAESO-project. A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way. Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme.", "Besides ##CITATION##\u2019s systems a Fuzzy C-Means FCM clustering approach has also been adopted", 0, "No title", "Discussion."], ["We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1. For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function. In each newly obtained cluster all headlines can be aligned to each other. 2.2 Pairwise similarity.", "Besides ##CITATION##\u2019s systems a Fuzzy C-Means FCM clustering approach has also been adopted", 0, "No title", "Method."], ["The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package.", "Besides ##CITATION##\u2019s systems a Fuzzy C-Means FCM clustering approach has also been adopted", 1, "No title", "Method."], ["We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1. For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function. In each newly obtained cluster all headlines can be aligned to each other. 2.2 Pairwise similarity.", "##CITATION## have attempted k-means clustering while the proposed system uses fuzzy clustering", 0, "No title", "Method."], ["Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni   is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge.", "##CITATION## have attempted k-means clustering while the proposed system uses fuzzy clustering", 0, "No title", "Method."], ["Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. Some examples of correct and incorrect alignments are given in Table 3.", "##CITATION## have attempted k-means clustering while the proposed system uses fuzzy clustering", 0, "No title", "Results."], ["The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni   is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example.", "##CITATION## have attempted k-means clustering while the proposed system uses fuzzy clustering", 1, "No title", "Method."], ["2.2 Pairwise similarity. Our second approach is to calculate the similarity between pairs of headlines directly. If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair. If it is below the threshold, it is rejected. However, as Barzilay and Elhadad   have pointed out, sentence mapping in this way is only effective to a certain extent.", "However the proposed system performs better than ##CITATION##\u2019s approaches as well as FCM Clustering for Paraphrase Extraction", 0, "No title", "Method."], ["In these sub-clusters, there are 6,685 clustered headlines. Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.", "However the proposed system performs better than ##CITATION##\u2019s approaches as well as FCM Clustering for Paraphrase Extraction", 0, "No title", "Results."], ["Where previous work has focused on aligning news-items at the paragraph and sentence level  , we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles. For the development of our system we use data which was obtained in the DAESO-project. This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122\u2013125, Athens, Greece, 30 \u2013 31 March 2009. Qc 2009 Association for Computational Linguistics document, and each original cluster as a collection of documents.", "However the proposed system performs better than ##CITATION##\u2019s approaches as well as FCM Clustering for Paraphrase Extraction", 0, "No title", "Method."], ["We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1. For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function. In each newly obtained cluster all headlines can be aligned to each other. 2.2 Pairwise similarity.", "However the proposed system performs better than ##CITATION##\u2019s approaches as well as FCM Clustering for Paraphrase Extraction", 1, "No title", "Method."], ["The T F \u2217I DF score is then: TF.IDFi = T Fi,j \u00b7 log | Table 1: Part of a sample headline cluster, with sub-clusters and Krahmer, 2007) and will be made available through the Dutch HLT Agency. Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period April\u2013August 2006. For each news article, the headline and the first 150 characters of the article were stored. Roughly 13,000 clusters were retrieved. Table 1 shows part of a (translated) cluster.", "The proposed system the existing systems ##CITATION## and FCM Clustering approach were tested on this dataset", 0, "No title", "Method."], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "The proposed system the existing systems ##CITATION## and FCM Clustering approach were tested on this dataset", 0, "No title", "ABSTRACT"], ["We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level  , we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles.", "The proposed system the existing systems ##CITATION## and FCM Clustering approach were tested on this dataset", 0, "No title", "Method."], ["Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. Some examples of correct and incorrect alignments are given in Table 3.", "The proposed system the existing systems ##CITATION## and FCM Clustering approach were tested on this dataset", 1, "No title", "Results."], ["In these sub-clusters, there are 6,685 clustered headlines. Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al    741 752 913 824 Mihalcea et al    703 696 977 813 Proposed system variant WSD threshold = 02 top 50% 664 810 652 723 Cosine similarity threshold = 07 ##CITATION##  625 805 566 665 k-Means clustering ##CITATION##  606 706 710 708 FCM clustering 362 681 95 167 Table 6 Performance of proposed system on MSRVDC Dataset 1  ", 0, "No title", "Results."], ["The headlines are stemmed using the porter stemmer for Dutch  . Instead of a word overlap measure as used byHere, C r is a criterion function, which mea sures the ratio of withincluster similarity to betweencluster similarity. As soon as P K 1(k) ex ceeds a threshold, k \u2212 1 is selected as the optimum number of clusters. To find the optimal threshold value for cluster- stopping, optimization is performed on the development data. Our optimization function is an F - score: (1 + \u03b22) \u00b7  , we use a modified F\u03b2 = (\u03b22 precision + recall) T F \u2217I DF word score as was suggested by Nelken \u00b7 and Shieber  .", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al    741 752 913 824 Mihalcea et al    703 696 977 813 Proposed system variant WSD threshold = 02 top 50% 664 810 652 723 Cosine similarity threshold = 07 ##CITATION##  625 805 566 665 k-Means clustering ##CITATION##  606 706 710 708 FCM clustering 362 681 95 167 Table 6 Performance of proposed system on MSRVDC Dataset 1  ", 0, "No title", "Method."], ["The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni   is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al    741 752 913 824 Mihalcea et al    703 696 977 813 Proposed system variant WSD threshold = 02 top 50% 664 810 652 723 Cosine similarity threshold = 07 ##CITATION##  625 805 566 665 k-Means clustering ##CITATION##  606 706 710 708 FCM clustering 362 681 95 167 Table 6 Performance of proposed system on MSRVDC Dataset 1  ", 0, "No title", "Method."], ["However, as Barzilay and Elhadad   have pointed out, sentence mapping in this way is only effective to a certain extent. Beyond that point, context is needed. With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(\u03b8) = V 1 \u00b7 V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared. If the similarity is higher than the upper threshold, it is accepted. If it is lower than the lower theshold, it is rejected.", "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al    741 752 913 824 Mihalcea et al    703 696 977 813 Proposed system variant WSD threshold = 02 top 50% 664 810 652 723 Cosine similarity threshold = 07 ##CITATION##  625 805 566 665 k-Means clustering ##CITATION##  606 706 710 708 FCM clustering 362 681 95 167 Table 6 Performance of proposed system on MSRVDC Dataset 1  ", 1, "No title", "Method."], ["These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases  . We use this method to collect a large amount of aligned paraphrases in an automatic fashion. We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.", "This method described in earlier work ##CITATION##  was reported to yield a precision of 076 and a recall of 041 on clustering actual Dutch paraphrases in a headline corpus ", 0, "No title", "Introduction"], ["We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level  , we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles.", "This method described in earlier work ##CITATION##  was reported to yield a precision of 076 and a recall of 041 on clustering actual Dutch paraphrases in a headline corpus ", 0, "No title", "Method."], ["Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision. We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76. Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases. Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted. This is not a bad thing: we are particularly interested in extracting paraphrase patterns at the constituent level.", "This method described in earlier work ##CITATION##  was reported to yield a precision of 076 and a recall of 041 on clustering actual Dutch paraphrases in a headline corpus ", 0, "No title", "Discussion."], ["In our case, precision is the number of alignments retrieved from the clusters which are relevant, divided by the total number of retrieved alignments. Recall is the number of relevant retrieved aligments divided by the total number of relevant alignments. We use an F\u03b2 -score with a \u03b2 of 0.25 as we favour precision over recall. We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1.", "This method described in earlier work ##CITATION##  was reported to yield a precision of 076 and a recall of 041 on clustering actual Dutch paraphrases in a headline corpus ", 1, "No title", "Method."], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines.", "So-called comparable monolingual corpora for instance independently written news reports describing the same event in which some pairs of sentences exhibit partial semantic overlap have also been investigated Shinyama et al Barzilay and Lee Shen et al ##CITATION## The first manually collected paraphrase corpus is the Microsoft Research Paraphrase MSRP Corpus Dolan et al consisting of 5801 sentence pairs sampled from a larger corpus of news articles", 0, "No title", "ABSTRACT"], ["In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. Some examples of correct and incorrect alignments are given in Table 3. Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision. We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76. Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases.", "So-called comparable monolingual corpora for instance independently written news reports describing the same event in which some pairs of sentences exhibit partial semantic overlap have also been investigated Shinyama et al Barzilay and Lee Shen et al ##CITATION## The first manually collected paraphrase corpus is the Microsoft Research Paraphrase MSRP Corpus Dolan et al consisting of 5801 sentence pairs sampled from a larger corpus of news articles", 0, "No title", "Discussion."], ["If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair. If it is below the threshold, it is rejected. However, as Barzilay and Elhadad   have pointed out, sentence mapping in this way is only effective to a certain extent. Beyond that point, context is needed. With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(\u03b8) = V 1 \u00b7 V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared.", "So-called comparable monolingual corpora for instance independently written news reports describing the same event in which some pairs of sentences exhibit partial semantic overlap have also been investigated Shinyama et al Barzilay and Lee Shen et al ##CITATION## The first manually collected paraphrase corpus is the Microsoft Research Paraphrase MSRP Corpus Dolan et al consisting of 5801 sentence pairs sampled from a larger corpus of news articles", 0, "No title", "Method."], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.", "So-called comparable monolingual corpora for instance independently written news reports describing the same event in which some pairs of sentences exhibit partial semantic overlap have also been investigated Shinyama et al Barzilay and Lee Shen et al ##CITATION## The first manually collected paraphrase corpus is the Microsoft Research Paraphrase MSRP Corpus Dolan et al consisting of 5801 sentence pairs sampled from a larger corpus of news articles", 1, "No title", "ABSTRACT"], ["For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).", "To investigate the effect of the amount of training data on results we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled inand aligned using i i=LD18 \u2217 Ni \u2217 N I STitfidf scores over headline clusters and Cosine sim ilarity as described in   resulting in an extra 612158 aligned headlines", 0, "No title", "ABSTRACT"], ["They do paraphrase each other to some extent, but their relation can only be understood properly with |{dj : ti \u2208 dj }| |D| is the total number of sentences in the cluster and |{dj : ti \u2208 dj }| is the number of sen tences that contain the term ti. These scores are used in a vector space representation. The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines.", "To investigate the effect of the amount of training data on results we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled inand aligned using i i=LD18 \u2217 Ni \u2217 N I STitfidf scores over headline clusters and Cosine sim ilarity as described in   resulting in an extra 612158 aligned headlines", 0, "No title", "Method."], ["Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example. We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters. We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data. The headlines are stemmed using the porter stemmer for Dutch  . Instead of a word overlap measure as used byHere, C r is a criterion function, which mea sures the ratio of withincluster similarity to betweencluster similarity.", "To investigate the effect of the amount of training data on results we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled inand aligned using i i=LD18 \u2217 Ni \u2217 N I STitfidf scores over headline clusters and Cosine sim ilarity as described in   resulting in an extra 612158 aligned headlines", 0, "No title", "Method."], ["The total intra-cluster variances is minimized by the function k V = (xj \u2212 \u00b5i)2 i=1 xj \u2208Si where \u00b5i is the centroid of all the points xj \u2208 Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni   is used to find the optimal k for each sub-cluster: C r(k) \u2212 mean(C r[1...\u2206K ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...\u2206K ]) three headlines shown in the example. We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters. We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data. The headlines are stemmed using the porter stemmer for Dutch  .", "To investigate the effect of the amount of training data on results we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled inand aligned using i i=LD18 \u2217 Ni \u2217 N I STitfidf scores over headline clusters and Cosine sim ilarity as described in   resulting in an extra 612158 aligned headlines", 1, "No title", "Method."], ["On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set. By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set. Our results significantly outperform Ciaramita and Johnson   on both test sets even though our system is unsupervised. The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder. Table 7 shows the breakdown in performance for each supersense.", "Another related task is supersense tagging Ciaramita and Johnson ##CITATION## Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30). For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson   call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita   has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.", "Another related task is supersense tagging Ciaramita and Johnson ##CITATION## Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Another related task is supersense tagging Ciaramita and Johnson ##CITATION## Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson   present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.", "Another related task is supersense tagging Ciaramita and Johnson ##CITATION## Ciaramita and Altun", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Although we could adapt our method for use with an automatically induced inventory our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets as Ciaramita and Johnson  and ##CITATION##  do with unknown nouns", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons. There are a number of problems our system does not currently handle. Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate). Further, our similarity system does not currently incorporate multi-word terms. We overcome this by using the synonyms of the last word in the multi-word term.", "Although we could adapt our method for use with an automatically induced inventory our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets as Ciaramita and Johnson  and ##CITATION##  do with unknown nouns", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Other Alternatives and Future Work."], ["They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify. Our evaluation will use exactly the same test sets as Ciaramita and Johnson  .", "Although we could adapt our method for use with an automatically induced inventory our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets as Ciaramita and Johnson  and ##CITATION##  do with unknown nouns", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation. Grefenstette   claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and Schu\u00a8 tze   and Widdows  . However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.", "Although we could adapt our method for use with an automatically induced inventory our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets as Ciaramita and Johnson  and ##CITATION##  do with unknown nouns", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular. Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary. Bur- gun and Bodenreider   compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap. Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics. Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet which seems to suffer from several serious limitations ##CITATION## and typically overlaps to a rather limited extent with the output of automatic acquisition methods", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earth\u2019s atmosphere. These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet which seems to suffer from several serious limitations ##CITATION## and typically overlaps to a rather limited extent with the output of automatic acquisition methods", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["For instance, entity is less frequent than many concepts it subsumes. This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners. Hearst and Schu\u00a8 tze   flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size. These categories are used to label paragraphs with topics, effectively repeating Yarowsky\u2019s   experiments using the their categories rather than Roget\u2019s thesaurus. Schu\u00a8 tze\u2019s   WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet which seems to suffer from several serious limitations ##CITATION## and typically overlaps to a rather limited extent with the output of automatic acquisition methods", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["Bur- gun and Bodenreider   compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap. Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics. Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses. Ciaramita and Johnson   found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus. By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet which seems to suffer from several serious limitations ##CITATION## and typically overlaps to a rather limited extent with the output of automatic acquisition methods", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson   present a tagger which uses synonym set glosses as annotated training examples.", "There are however approaches to the complementary problem of determining the closest known sense for unknown words Widdows ##CITATION## Burchardt et al which can be viewed as the logical next step after unknown sense detection", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson   call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita   has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories. Ciaramita et al.", "There are however approaches to the complementary problem of determining the closest known sense for unknown words Widdows ##CITATION## Burchardt et al which can be viewed as the logical next step after unknown sense detection", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["The supersense guessing rules are given in Table 5. If none of the rules match, then the default supersense artifact is assigned. The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection. Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6. There are many parameters to consider: \u2022 how many extracted synonyms to use; \u2022 how to weight each synonym\u2019s vote; \u2022 whether unreliable synonyms should be filtered out; \u2022 how to deal with polysemous synonyms.", "There are however approaches to the complementary problem of determining the closest known sense for unknown words Widdows ##CITATION## Burchardt et al which can be viewed as the logical next step after unknown sense detection", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation. Grefenstette   claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and Schu\u00a8 tze   and Widdows  . However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.", "There are however approaches to the complementary problem of determining the closest known sense for unknown words Widdows ##CITATION## Burchardt et al which can be viewed as the logical next step after unknown sense detection", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["Ciaramita and Johnson   found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus. By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult. consistency when classifying similar words into categories. For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earth\u2019s atmosphere. These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.", "Possibilities include associating items with similar existing senses   or clustering them into approximate senses", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created. For example, antigen is the subject of represent. Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head. If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created. If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created.", "Possibilities include associating items with similar existing senses   or clustering them into approximate senses", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Semantic."], ["Schu\u00a8 tze\u2019s   WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem). Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word. Widdows   uses a similar technique to insert words into the WORDNET hierarchy. He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging.", "Possibilities include associating items with similar existing senses   or clustering them into approximate senses", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6. We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability. Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor. Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson  . This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.", "Possibilities include associating items with similar existing senses   or clustering them into approximate senses", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Alternatively, the weights can use the ranking of the extracted synonyms. Again these options have been considered below. A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable. The final issue is how to deal with polysemy. Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik  ?", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages eg Roark and Charniak Ci aramita ##CITATION## it has been used in only one previous study on semantic classification of Chinese unknown words Chen and Lin", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson   present a tagger which uses synonym set glosses as annotated training examples.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages eg Roark and Charniak Ci aramita ##CITATION## it has been used in only one previous study on semantic classification of Chinese unknown words Chen and Lin", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earth\u2019s atmosphere. These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages eg Roark and Charniak Ci aramita ##CITATION## it has been used in only one previous study on semantic classification of Chinese unknown words Chen and Lin", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson  . This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus. Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag. This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available. We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages eg Roark and Charniak Ci aramita ##CITATION## it has been used in only one previous study on semantic classification of Chinese unknown words Chen and Lin", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "More re cently the task of automatic supersense tagging has emerged for English Ciaramita and Johnson ##CITATION## Ciaramita and Altun Paa\u00df and Reichartz as well as for Italian Picca et al Picca et al Attardi et al and Chinese Qiu et al languages with WordNetsmapped to English WordNet3 In principle we be lieve supersenses ought to apply to nouns and verbsin any language and need not depend on the avail ability of a semantic lexicon4 In this work we focuson the noun SSTs summarized in figure 2 and ap plied to an Arabic sentence in figure 1", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in S Y S T E M W N 1.6 W N 1.7 .1 Cia ra mit a an d Joh nso n bas eli ne 2 1 % 2 8 % Cia ra mit a an d Joh nso n per cep tro n 5 3 % 5 3 % Si mil arit y bas ed res ult s 6 8 % 6 3 % Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses. This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive. We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson  . The experiments were performed by considering all possible configurations of the parameters described above. The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.", "More re cently the task of automatic supersense tagging has emerged for English Ciaramita and Johnson ##CITATION## Ciaramita and Altun Paa\u00df and Reichartz as well as for Italian Picca et al Picca et al Attardi et al and Chinese Qiu et al languages with WordNetsmapped to English WordNet3 In principle we be lieve supersenses ought to apply to nouns and verbsin any language and need not depend on the avail ability of a semantic lexicon4 In this work we focuson the noun SSTs summarized in figure 2 and ap plied to an Arabic sentence in figure 1", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set. Some examples from the test sets are given in Table 2 with their supersenses. We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search. The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium\u2019s news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus. The components and their sizes including punctuation are given in Table 3.", "More re cently the task of automatic supersense tagging has emerged for English Ciaramita and Johnson ##CITATION## Ciaramita and Altun Paa\u00df and Reichartz as well as for Italian Picca et al Picca et al Attardi et al and Chinese Qiu et al languages with WordNetsmapped to English WordNet3 In principle we be lieve supersenses ought to apply to nouns and verbsin any language and need not depend on the avail ability of a semantic lexicon4 In this work we focuson the noun SSTs summarized in figure 2 and ap plied to an Arabic sentence in figure 1", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Corpus."], ["These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "More re cently the task of automatic supersense tagging has emerged for English Ciaramita and Johnson ##CITATION## Ciaramita and Altun Paa\u00df and Reichartz as well as for Italian Picca et al Picca et al Attardi et al and Chinese Qiu et al languages with WordNetsmapped to English WordNet3 In principle we be lieve supersenses ought to apply to nouns and verbsin any language and need not depend on the avail ability of a semantic lexicon4 In this work we focuson the noun SSTs summarized in figure 2 and ap plied to an Arabic sentence in figure 1", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Thus some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN Hearst and Schu\u00a8 tze Peters et al Mihalcea and Moldo- van Agirre et al and on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["For instance, entity is less frequent than many concepts it subsumes. This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners. Hearst and Schu\u00a8 tze   flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size. These categories are used to label paragraphs with topics, effectively repeating Yarowsky\u2019s   experiments using the their categories rather than Roget\u2019s thesaurus. Schu\u00a8 tze\u2019s   WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).", "Thus some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN Hearst and Schu\u00a8 tze Peters et al Mihalcea and Moldo- van Agirre et al and on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["We intend to extend our experiments beyond the Ciaramita and Johnson   set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results. We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows   using latent semantic analysis. Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors. Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu\u00a8 tze   and Widdows  . To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.", "Thus some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN Hearst and Schu\u00a8 tze Peters et al Mihalcea and Moldo- van Agirre et al and on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Other Alternatives and Future Work."], ["He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson   implement a super- sense tagger based on the multi-class perceptron classifier  , which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.", "Thus some research has been focused on deriving different sense groupings to overcome the fine\u2013 grained distinctions of WN Hearst and Schu\u00a8 tze Peters et al Mihalcea and Moldo- van Agirre et al and on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Ciaramita and Altun", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability. Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor. Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson  . This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus. Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.", "In contrast some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Kohomban and Lee and Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Conclusion."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "In contrast some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Kohomban and Lee and Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity. Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing   and class-based smoothing  , to text classification   and question answering  . In particular, WORDNET   has significantly influenced research in NLP. Unfortunately, these resource are extremely time- consuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.", "In contrast some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Kohomban and Lee and Ciaramita and Altun", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson   implement a super- sense tagger based on the multi-class perceptron classifier  , which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.", "In contrast some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD Segond et al Ciaramita and Johnson Villarejo et al ##CITATION## Kohomban and Lee and Ciaramita and Altun", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lex- files). For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops. Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs. Lex-files form a set of coarse-grained sense distinctions within WORDNET.", "Distributed representations are useful in capturing such meaning for individual words Sato et al Maas and Ng ##CITATION##", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["These results also support Ciaramita and Johnson\u2019s view that abstract concepts like communication, cognition and state are much harder. We would expect the location supersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier. Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help. An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words. This has the advantage of producing a much smaller number of vectors to compare against.", "Distributed representations are useful in capturing such meaning for individual words Sato et al Maas and Ng ##CITATION##", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search. The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium\u2019s news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus. The components and their sizes including punctuation are given in Table 3. The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above. C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3.", "Distributed representations are useful in capturing such meaning for individual words Sato et al Maas and Ng ##CITATION##", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Corpus."], ["Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise. The rest of the pipeline is described in the next section. Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts. This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in. In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.", "Distributed representations are useful in capturing such meaning for individual words Sato et al Maas and Ng ##CITATION##", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Semantic."], ["The weight could also be divided by the rank (RANK) to penalise supersenses further down the list. The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties. The extracted synonyms are filtered before contributing to the vote with their supersense(s). This filtering involves checking that the synonym\u2019s frequency and number of contexts are large enough to ensure it is reliable. We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonym\u2019s frequency and the number of contexts it appears in.", "Supersense tagging   evaluates a model\u2019s ability to cluster words by their semantics", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Results."], ["The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson   present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.", "Supersense tagging   evaluates a model\u2019s ability to cluster words by their semantics", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "ABSTRACT"], ["Ciaramita and Johnson   believe that the key sense distinctions are still maintained by supersenses. They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms. Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy. Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself. Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary.", "Supersense tagging   evaluates a model\u2019s ability to cluster words by their semantics", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "Supersense tagging   evaluates a model\u2019s ability to cluster words by their semantics", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Alternatively, the weights can use the ranking of the extracted synonyms. Again these options have been considered below. A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable. The final issue is how to deal with polysemy. Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik  ?", "A concept analogous to our notion of meta sense ie senses beyond single words has been used in previous work on class-based WSD Yarowsky ##CITATION## Izquierdo et al and indeed the CAM might be used for class-based WSD as well", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Approach."], ["However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term. Finally, we plan to implement a supervised machine learner to replace the fall- back method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set. We intend to extend our experiments beyond the Ciaramita and Johnson   set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results. We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows   using latent semantic analysis. Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.", "A concept analogous to our notion of meta sense ie senses beyond single words has been used in previous work on class-based WSD Yarowsky ##CITATION## Izquierdo et al and indeed the CAM might be used for class-based WSD as well", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Other Alternatives and Future Work."], ["Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word. Widdows   uses a similar technique to insert words into the WORDNET hierarchy. He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson   implement a super- sense tagger based on the multi-class perceptron classifier  , which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.", "A concept analogous to our notion of meta sense ie senses beyond single words has been used in previous work on class-based WSD Yarowsky ##CITATION## Izquierdo et al and indeed the CAM might be used for class-based WSD as well", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson   implement a super- sense tagger based on the multi-class perceptron classifier  , which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.", "A concept analogous to our notion of meta sense ie senses beyond single words has been used in previous work on class-based WSD Yarowsky ##CITATION## Izquierdo et al and indeed the CAM might be used for class-based WSD as well", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Previous Work."], ["Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data. Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.", "Previous work on prediction at the supersense level Ciaramita and Johnson ##CITATION## has focused on lexical acquisition nouns exclusively thus aiming at word type classification rather than tagging", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Our evaluation will use exactly the same test sets as Ciaramita and Johnson  . The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense. The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson  . They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set. Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set.", "Previous work on prediction at the supersense level Ciaramita and Johnson ##CITATION## has focused on lexical acquisition nouns exclusively thus aiming at word type classification rather than tagging", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Evaluation."], ["Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30). For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson   call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita   has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.", "Previous work on prediction at the supersense level Ciaramita and Johnson ##CITATION## has focused on lexical acquisition nouns exclusively thus aiming at word type classification rather than tagging", 0, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Supersenses."], ["These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u2019s hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.", "Previous work on prediction at the supersense level Ciaramita and Johnson ##CITATION## has focused on lexical acquisition nouns exclusively thus aiming at word type classification rather than tagging", 1, "Supersense Tagging of Unknown Nouns using Semantic Similarity", "Introduction"], ["Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from  , the new method improves the BLEU scores significantly.", "In this two-pass method translation performance hinges on the N-best hypotheses that are generated in the first pass since rescoring occurs on these so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance This technique is called system combination Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION## ", 0, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU. Again, the best scores on each metric are obtained by the combination tuned for that metric. Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F - other combinations are better than any single system on all metrics apart from the METEOR tuned combinations. The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.", "In this two-pass method translation performance hinges on the N-best hypotheses that are generated in the first pass since rescoring occurs on these so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance This technique is called system combination Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION## ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.", "In this two-pass method translation performance hinges on the N-best hypotheses that are generated in the first pass since rescoring occurs on these so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance This technique is called system combination Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION## ", 0, "Improved Word-Level System Combination for Machine Translation", "ABSTRACT"], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "In this two-pass method translation performance hinges on the N-best hypotheses that are generated in the first pass since rescoring occurs on these so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance This technique is called system combination Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION## ", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "Confusion network and re-decoding have been well studied in the combination of different MT systems Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while  . Recently, confusion network decoding for MT system combination has been proposed  . To generate confusion networks, hypotheses have to be aligned against each other. In  , Levenshtein alignment was used to generate the network.", "Confusion network and re-decoding have been well studied in the combination of different MT systems Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems.", "Confusion network and re-decoding have been well studied in the combination of different MT systems Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "ABSTRACT"], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "Confusion network and re-decoding have been well studied in the combination of different MT systems Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The skeleton defines the word order of the combination output. Minimum Bayes risk  . The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER. This work was extended in   by introducing system weights for word confidences.", "Bangalore et al   Sim et al   ##CITATION##  and ##CITATION##  chose the hypothesis that best agrees with other hypotheses on average as the skeleton    ", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Also, a more heuristic alignment method has been proposed in a different system combination approach  . A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the \u201cskeleton\u201d, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk  .", "Bangalore et al   Sim et al   ##CITATION##  and ##CITATION##  chose the hypothesis that best agrees with other hypotheses on average as the skeleton    ", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Minimum Bayes risk  . The average TER score was computed between each system\u2019s -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER. This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton.", "Bangalore et al   Sim et al   ##CITATION##  and ##CITATION##  chose the hypothesis that best agrees with other hypotheses on average as the skeleton    ", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "Bangalore et al   Sim et al   ##CITATION##  and ##CITATION##  chose the hypothesis that best agrees with other hypotheses on average as the skeleton    ", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Recently, confusion network decoding for MT system combination has been proposed  . To generate confusion networks, hypotheses have to be aligned against each other. In  , Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In  , different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++  .", "Bangalore et al   used a WER based alignment and Sim et al   ##CITATION##  and ##CITATION##  used minimum Translation Error Rate TER based alignment to build the confusion network    ", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length. The only difference to word error rate is that the TER allows shifts. A shift of a sequence of words is counted as a single edit. The minimum translation edit alignment is usually found through a beam search.", "Bangalore et al   used a WER based alignment and Sim et al   ##CITATION##  and ##CITATION##  used minimum Translation Error Rate TER based alignment to build the confusion network    ", 0, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["The only difference to word error rate is that the TER allows shifts. A shift of a sequence of words is counted as a single edit. The minimum translation edit alignment is usually found through a beam search. When multiple references are provided, the edits from the closest reference are divided by the average reference length. Full test set scores are obtained by accumulating the edits and the average reference lengths.", "Bangalore et al   used a WER based alignment and Sim et al   ##CITATION##  and ##CITATION##  used minimum Translation Error Rate TER based alignment to build the confusion network    ", 0, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton. Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score. The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems. This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities  . Other evaluation metrics may also be used as the MBR loss function.", "Bangalore et al   used a WER based alignment and Sim et al   ##CITATION##  and ##CITATION##  used minimum Translation Error Rate TER based alignment to build the confusion network    ", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "In recent several years the system combination methods based on confusion networks developed rapidly Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##; ##CITATION## He et al which show state-of-the-art performance in benchmarks", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The following three rows correspond to the improved confusion network decoding with different optimization metrics. As expected, the scores on the metric used in tuning are the best on that metric. Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning. However, the METEOR tuning yields extremely high TER and low BLEU scores. This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR Ch in es e tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 56 .5 6 55 .8 8 58 .3 5 57 .0 9 57 .6 9 56 .1 1 29 .3 9 30 .4 5 32 .8 8 36 .1 8 33 .8 5 36 .6 4 54 .5 4 54 .3 6 56 .7 2 57 .1 1 58 .2 8 58 .9 0 no we ig hts ba sel in e 53 .1 1 53 .4 0 37 .7 7 38 .5 2 59 .1 9 59 .5 6 T E R t u n e d B L E U t u n e d M T R t u n e d 52 .1 3 53 .0 3 70 .2 7 36 .8 7 39 .9 9 28 .6 0 57 .3 0 58 .9 7 63 .1 0 Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04.", "In recent several years the system combination methods based on confusion networks developed rapidly Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##; ##CITATION## He et al which show state-of-the-art performance in benchmarks", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in  . However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "In recent several years the system combination methods based on confusion networks developed rapidly Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##; ##CITATION## He et al which show state-of-the-art performance in benchmarks", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "In recent several years the system combination methods based on confusion networks developed rapidly Bangalore et al Matusov et al Sim et al ##CITATION##; ##CITATION##; ##CITATION## He et al which show state-of-the-art performance in benchmarks", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["3.1 Discussion. There are several problems with the previous confusion network decoding approaches. First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one. Equation 6 may be viewed as a log-linear sum of sentence- level features. The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score.", "While in lattice decoding a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc Similar to the features in ##CITATION##  the features adopted by lattice-based model are arc posterior probability language model probability the number of null arcs the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores. Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding. Third, the system weights are independent of the skeleton selection. Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton.", "While in lattice decoding a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc Similar to the features in ##CITATION##  the features adopted by lattice-based model are arc posterior probability language model probability the number of null arcs the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton. Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified. Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.", "While in lattice decoding a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc Similar to the features in ##CITATION##  the features adopted by lattice-based model are arc posterior probability language model probability the number of null arcs the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "While in lattice decoding a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc Similar to the features in ##CITATION##  the features adopted by lattice-based model are arc posterior probability language model probability the number of null arcs the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words ", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.", "While in lattice decoding a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc Similar to the features in ##CITATION##  the features adopted by lattice-based model are arc posterior probability language model probability the number of null arcs the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words ", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen. The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities.", "While in lattice decoding a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc Similar to the features in ##CITATION##  the features adopted by lattice-based model are arc posterior probability language model probability the number of null arcs the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words ", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton. Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified. Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.", "Each arc has different confidences concerned with different systems and the confidence of system s is denoted by psarc psarc is increased by 1/k + 1 if the hypothesis ranking k in the system s contains the arc ##CITATION##; He et al ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights. 3.1 Discussion. There are several problems with the previous confusion network decoding approaches.", "Each arc has different confidences concerned with different systems and the confidence of system s is denoted by psarc psarc is increased by 1/k + 1 if the hypothesis ranking k in the system s contains the arc ##CITATION##; He et al ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in  . However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "Each arc has different confidences concerned with different systems and the confidence of system s is denoted by psarc psarc is increased by 1/k + 1 if the hypothesis ranking k in the system s contains the arc ##CITATION##; He et al ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "Each arc has different confidences concerned with different systems and the confidence of system s is denoted by psarc psarc is increased by 1/k + 1 if the hypothesis ranking k in the system s contains the arc ##CITATION##; He et al ", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using \u201ccat sat the mat\u201d as the skeleton, aligning \u201ccat sitting on the mat\u201d and \u201chat on a mat\u201d against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word.", "Each arc has different confidences concerned with different systems and the confidence of system s is denoted by psarc psarc is increased by 1/k + 1 if the hypothesis ranking k in the system s contains the arc ##CITATION##; He et al ", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using \u201ccat sat the mat\u201d as the skeleton, aligning \u201ccat sitting on the mat\u201d and \u201chat on a mat\u201d against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word. In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs.", "Each arc has different confidences concerned with different systems and the confidence of system s is denoted by psarc psarc is increased by 1/k + 1 if the hypothesis ranking k in the system s contains the arc ##CITATION##; He et al ", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using \u201ccat sat the mat\u201d as the skeleton, aligning \u201ccat sitting on the mat\u201d and \u201chat on a mat\u201d against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word. In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs.", "QcAssociation for Computational Linguistics System Combination: In a typical system combination task eg ##CITATION##  each component system produces a set of translations which are then grafted to form a confusion network ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "QcAssociation for Computational Linguistics System Combination: In a typical system combination task eg ##CITATION##  each component system produces a set of translations which are then grafted to form a confusion network ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "QcAssociation for Computational Linguistics System Combination: In a typical system combination task eg ##CITATION##  each component system produces a set of translations which are then grafted to form a confusion network ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list.", "QcAssociation for Computational Linguistics System Combination: In a typical system combination task eg ##CITATION##  each component system produces a set of translations which are then grafted to form a confusion network ", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation.", "QcAssociation for Computational Linguistics System Combination: In a typical system combination task eg ##CITATION##  each component system produces a set of translations which are then grafted to form a confusion network ", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.", "QcAssociation for Computational Linguistics System Combination: In a typical system combination task eg ##CITATION##  each component system produces a set of translations which are then grafted to form a confusion network ", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score. The last two terms are not completely independent but seem to help based on experimental results. The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice.", "We build our confusion networks using the method of ##CITATION##  but instead of forming alignments using the tercom script Snover et al we create alignments that minimize invWER Leusch et al a form of edit distance that permits properly nested block movements of substrings ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "We build our confusion networks using the method of ##CITATION##  but instead of forming alignments using the tercom script Snover et al we create alignments that minimize invWER Leusch et al a form of edit distance that permits properly nested block movements of substrings ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "We build our confusion networks using the method of ##CITATION##  but instead of forming alignments using the tercom script Snover et al we create alignments that minimize invWER Leusch et al a form of edit distance that permits properly nested block movements of substrings ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "We build our confusion networks using the method of ##CITATION##  but instead of forming alignments using the tercom script Snover et al we create alignments that minimize invWER Leusch et al a form of edit distance that permits properly nested block movements of substrings ", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration. Similarly, weights which maximize BLEU or METEOR may be optimized. The same Powell\u2019s method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in  . A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used.", "The procedure described by ##CITATION##   has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment ", 0, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR Ch in es e tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 56 .5 6 55 .8 8 58 .3 5 57 .0 9 57 .6 9 56 .1 1 29 .3 9 30 .4 5 32 .8 8 36 .1 8 33 .8 5 36 .6 4 54 .5 4 54 .3 6 56 .7 2 57 .1 1 58 .2 8 58 .9 0 no we ig hts ba sel in e 53 .1 1 53 .4 0 37 .7 7 38 .5 2 59 .1 9 59 .5 6 T E R t u n e d B L E U t u n e d M T R t u n e d 52 .1 3 53 .0 3 70 .2 7 36 .8 7 39 .9 9 28 .6 0 57 .3 0 58 .9 7 63 .1 0 Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04. score. Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision. The Arabic test set results are shown in Table 2. The TER and BLEU optimized combination results beat all single system scores on all metrics.", "The procedure described by ##CITATION##   has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Confusion network decoding usually requires finding the path with the highest confidence in the network. Based on vote counts, there are three alternatives in the example: \u201ccat sat on the mat\u201d, \u201ccat on the mat\u201d and \u201ccat sitting on the mat\u201d, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word \u2018sat\u2019 and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, \u201ccat on the mat\u201d. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length.", "The procedure described by ##CITATION##   has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment ", 0, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["This allows the addition of language model scores by expanding the lattices or re-scoring -best lists. The LM integration should result in more grammatical combination outputs. Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights.", "The procedure described by ##CITATION##   has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment ", 1, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["The LM integration should result in more grammatical combination outputs. Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks.", "The procedure described by ##CITATION##   has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment ", 1, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["Confusion network decoding usually requires finding the path with the highest confidence in the network. Based on vote counts, there are three alternatives in the example: \u201ccat sat on the mat\u201d, \u201ccat on the mat\u201d and \u201ccat sitting on the mat\u201d, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word \u2018sat\u2019 and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, \u201ccat on the mat\u201d. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length.", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings For this ##CITATION##   use the tercom script   which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another  ", 0, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese. It also seems like METEOR should not be used in tuning due to high insertion rate and low precision. It would be interesting to know which tuning metric results in the best translations in terms of human judgment. However, this would require time consuming evaluations such as human mediated TER post-editing  . The improved confusion network decoding approach allows arbitrary features to be used in the combination.", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings For this ##CITATION##   use the tercom script   which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another  ", 0, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified. Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights. 3.1 Discussion.", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings For this ##CITATION##   use the tercom script   which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another  ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate  has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that. position in the sentence and the number of votes for each word is marked in parentheses.", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings For this ##CITATION##   use the tercom script   which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another  ", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["Based on vote counts, there are three alternatives in the example: \u201ccat sat on the mat\u201d, \u201ccat on the mat\u201d and \u201ccat sitting on the mat\u201d, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word \u2018sat\u2019 and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, \u201ccat on the mat\u201d. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length. The only difference to word error rate is that the TER allows shifts.", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings For this ##CITATION##   use the tercom script   which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another  ", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. System combination has been shown to improve classification performance in various tasks. There are several approaches for combining classifiers.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 0, "Improved Word-Level System Combination for Machine Translation", "ABSTRACT"], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in  . However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score. The last two terms are not completely independent but seem to help based on experimental results. The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. System combination has been shown to improve classification performance in various tasks.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 1, "Improved Word-Level System Combination for Machine Translation", "ABSTRACT"], ["For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used. In this work, modified Powell\u2019s method as proposed by   is used. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["In this work, modified Powell\u2019s method as proposed by   is used. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search. To improve the chances of finding a global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search. To improve the chances of finding a global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs. Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration.", "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of ##CITATION##   ", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["The baseline corresponds to Equation 5 with TER tuned weights. The following three rows correspond to the improved confusion network decoding with different optimization metrics. As expected, the scores on the metric used in tuning are the best on that metric. Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning. However, the METEOR tuning yields extremely high TER and low BLEU scores.", "Note that the algorithm of ##CITATION##   used N -best lists in the combination ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Figure 2: Three confusion networks with prior probabilities. specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. The optimization of the system and feature weights may be carried out using -best lists as in  . A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations.", "Note that the algorithm of ##CITATION##   used N -best lists in the combination ", 0, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU. Again, the best scores on each metric are obtained by the combination tuned for that metric. Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F - other combinations are better than any single system on all metrics apart from the METEOR tuned combinations. The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.", "Note that the algorithm of ##CITATION##   used N -best lists in the combination ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  . When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.", "Note that the algorithm of ##CITATION##   used N -best lists in the combination ", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in  . However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "This large grammatical difference may produce a longer sentence with spuriously inserted words as in \u201cI saw the blue trees was found\u201d in Figure 1c ##CITATION##  partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network  ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "This large grammatical difference may produce a longer sentence with spuriously inserted words as in \u201cI saw the blue trees was found\u201d in Figure 1c ##CITATION##  partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network  ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs.", "This large grammatical difference may produce a longer sentence with spuriously inserted words as in \u201cI saw the blue trees was found\u201d in Figure 1c ##CITATION##  partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network  ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "This large grammatical difference may produce a longer sentence with spuriously inserted words as in \u201cI saw the blue trees was found\u201d in Figure 1c ##CITATION##  partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network  ", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "Our baseline confusion network system has an additional penalty feature hp m which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton normalized by the number of nodes in the network ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in  . However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities.", "Our baseline confusion network system has an additional penalty feature hp m which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton normalized by the number of nodes in the network ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton. Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified. Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.", "Our baseline confusion network system has an additional penalty feature hp m which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton normalized by the number of nodes in the network ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "Our baseline confusion network system has an additional penalty feature hp m which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton normalized by the number of nodes in the network ##CITATION##", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using \u201ccat sat the mat\u201d as the skeleton, aligning \u201ccat sitting on the mat\u201d and \u201chat on a mat\u201d against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word. In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs.", "Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task eg ##CITATION##  each compo\u00ad nent system produces a set of translations which are then grafted to form a confusion network ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section. Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.", "Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task eg ##CITATION##  each compo\u00ad nent system produces a set of translations which are then grafted to form a confusion network ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task eg ##CITATION##  each compo\u00ad nent system produces a set of translations which are then grafted to form a confusion network ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list.", "Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task eg ##CITATION##  each compo\u00ad nent system produces a set of translations which are then grafted to form a confusion network ", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation.", "Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task eg ##CITATION##  each compo\u00ad nent system produces a set of translations which are then grafted to form a confusion network ", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.", "Association for Computational Linguistics System Combination: In a typical system combi\u00ad nation task eg ##CITATION##  each compo\u00ad nent system produces a set of translations which are then grafted to form a confusion network ", 1, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "If measures with this property are used to tune a typical statistical MT system it can sometimes be observed that the MT system learns to \u201cplay\u201d against this and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations For example ##CITATION##  report such an effect  ", 0, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["The LM integration should result in more grammatical combination outputs. Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks.", "If measures with this property are used to tune a typical statistical MT system it can sometimes be observed that the MT system learns to \u201cplay\u201d against this and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations For example ##CITATION##  report such an effect  ", 0, "Improved Word-Level System Combination for Machine Translation", "Conclusions."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities  . Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  . When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis.", "If measures with this property are used to tune a typical statistical MT system it can sometimes be observed that the MT system learns to \u201cplay\u201d against this and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations For example ##CITATION##  report such an effect  ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["This work was extended in   by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.", "If measures with this property are used to tune a typical statistical MT system it can sometimes be observed that the MT system learns to \u201cplay\u201d against this and might even learn to produce translations which show the \u201cgood\u201d features without actually being good translations For example ##CITATION##  report such an effect  ", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities  . Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  . When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["In speech recognition, confusion network decoding   has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while  . Recently, confusion network decoding for MT system combination has been proposed  . To generate confusion networks, hypotheses have to be aligned against each other.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. The optimization of the system and feature weights may be carried out using -best lists as in  . A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.", "The handicap of using a single reference can be addressed by constructing a lattice of reference translations\u2013this technique has been used to combine the output of multiple translation systems ##CITATION##", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities  . Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  . When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis.", "In addition it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination   a task for which TERp may be even better suited", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Qc 2007 Association for Computational Linguistics potheses in  . The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach  . A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the \u201cskeleton\u201d, and other hypotheses are aligned against it.", "In addition it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination   a task for which TERp may be even better suited", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work. System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems\u2019 relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In  , the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.", "In addition it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination   a task for which TERp may be even better suited", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate  has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that. position in the sentence and the number of votes for each word is marked in parentheses.", "In addition it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination   a task for which TERp may be even better suited", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["Based on vote counts, there are three alternatives in the example: \u201ccat sat on the mat\u201d, \u201ccat on the mat\u201d and \u201ccat sitting on the mat\u201d, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word \u2018sat\u2019 and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, \u201ccat on the mat\u201d. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length. The only difference to word error rate is that the TER allows shifts.", "In addition it may also be used as a general-purpose string alignment tool\u2014TER has been used for aligning multiple system outputs to each other for MT system combination   a task for which TERp may be even better suited", 1, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate   was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312\u2013319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in  . The alignments from TER are consistent as they do not depend on the test set size.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model Matusov et al or edit distance alignments allowing shifts ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  . When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model Matusov et al or edit distance alignments allowing shifts ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model Matusov et al or edit distance alignments allowing shifts ##CITATION##", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs. Different alignment methods yield different confusion networks. The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning. As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton. Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score.", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model Matusov et al or edit distance alignments allowing shifts ##CITATION##", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors. Also the word-level decoding may break coherent phrases produced by the individual systems.", "As in   confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["In  , Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In  , different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++  . The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.", "As in   confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task. The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration. The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks. All four reference translations available for the tuning and test sets were used.", "As in   confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["The second set of weights is used to find the final -best from the re-scored -best list. As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs.", "As in   confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one.", "As in   confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.", "As in   confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models", 1, "Improved Word-Level System Combination for Machine Translation", "Multiple Confusion Network Decoding."], ["Compared to the baseline system which is also optimized for TER, the BLEU score is improved by 0.97 points. Also, the METEOR score using the METEOR optimized weights is very high. However, the other scores are worse in common with the tuning set results. The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU.", "Other scores for the word arc are set as in  ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Full test set scores are obtained by accumulating statistics over all test sentences. The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate  has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that.", "Other scores for the word arc are set as in  ", 0, "Improved Word-Level System Combination for Machine Translation", "Evaluation Metrics."], ["The baseline corresponds to Equation 5 with TER tuned weights. The following three rows correspond to the improved confusion network decoding with different optimization metrics. As expected, the scores on the metric used in tuning are the best on that metric. Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning. However, the METEOR tuning yields extremely high TER and low BLEU scores.", "Other scores for the word arc are set as in  ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["The combination outputs from confusion network decoding may be ungrammatical due to alignment errors. Also the word-level decoding may break coherent phrases produced by the individual systems. In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences. This allows a log-linear addition of arbitrary features such as language model (LM) scores. The LM scores should increase the total log-posterior of more grammatical hypotheses.", "Other scores for the word arc are set as in  ", 1, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["Again, the best scores on each metric are obtained by the combination tuned for that metric. Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F - other combinations are better than any single system on all metrics apart from the METEOR tuned combinations. The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05. terms of METEOR. Compared to the baseline, the BLEU score of the BLEU tuned combination is improved by 1.47 points.", "The first syscomb pw corresponds BLEU System deen fren worst 1184 1631 best 2830 3313 syscomb 2905 3363 Table 3: NIST BLEU scores on the GermanEnglish deen and French-English fren Europarl tesset to the pairwise TER alignment described in   ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Also, the METEOR score using the METEOR optimized weights is very high. However, the other scores are worse in common with the tuning set results. The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU. Again, the best scores on each metric are obtained by the combination tuned for that metric.", "The first syscomb pw corresponds BLEU System deen fren worst 1184 1631 best 2830 3313 syscomb 2905 3363 Table 3: NIST BLEU scores on the GermanEnglish deen and French-English fren Europarl tesset to the pairwise TER alignment described in   ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision. The Arabic test set results are shown in Table 2. The TER and BLEU optimized combination results beat all single system scores on all metrics. The best results on a given metric are again obtained by the combination optimized for the corresponding metric. It should be noted that the TER optimized combination has significantly higher BLEU score than the TER optimized baseline.", "The first syscomb pw corresponds BLEU System deen fren worst 1184 1631 best 2830 3313 syscomb 2905 3363 Table 3: NIST BLEU scores on the GermanEnglish deen and French-English fren Europarl tesset to the pairwise TER alignment described in   ", 0, "Improved Word-Level System Combination for Machine Translation", "Results."], ["When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work. System weights may be used to assign a system specific confidence on each word in the network.", "The first syscomb pw corresponds BLEU System deen fren worst 1184 1631 best 2830 3313 syscomb 2905 3363 Table 3: NIST BLEU scores on the GermanEnglish deen and French-English fren Europarl tesset to the pairwise TER alignment described in   ", 1, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations Such a technique has been used with TER to combine the output of multiple translation systems ##CITATION## ", 0, "Improved Word-Level System Combination for Machine Translation", "Log-Linear Combination with Arbitrary."], ["This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities  . Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  . When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In  , simple score was assigned to the word coming from the th- best hypothesis.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations Such a technique has been used with TER to combine the output of multiple translation systems ##CITATION## ", 0, "Improved Word-Level System Combination for Machine Translation", "Confusion Network Decoding."], ["In speech recognition, confusion network decoding   has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while  . Recently, confusion network decoding for MT system combination has been proposed  . To generate confusion networks, hypotheses have to be aligned against each other.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations Such a technique has been used with TER to combine the output of multiple translation systems ##CITATION## ", 0, "Improved Word-Level System Combination for Machine Translation", "Introduction"], ["specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. The optimization of the system and feature weights may be carried out using -best lists as in  . A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.", "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations Such a technique has been used with TER to combine the output of multiple translation systems ##CITATION## ", 1, "Improved Word-Level System Combination for Machine Translation", "Weights Optimization."], ["Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) \u00b7 p(ei |e).", "There are only a few studies on document-level SMT Representative work includes Zhao et al  Tam et al  Carpuat    ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions. We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random.", "There are only a few studies on document-level SMT Representative work includes Zhao et al  Tam et al  Carpuat    ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent. Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969\u2013976, Sydney, July 2006.", "There are only a few studies on document-level SMT Representative work includes Zhao et al  Tam et al  Carpuat    ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida   stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "There are only a few studies on document-level SMT Representative work includes Zhao et al  Tam et al  Carpuat    ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) \u00b7 p(ei |e).", "Zhao et al   assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model It shows that the performance of word alignment can be improved with the help of document-level information which indirectly improves the quality of SMT  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Similar to IBM models, \u201cNull\u201d word is used for the source words which have no translation counterparts in the target language. For example, Chinese words \u201cde\u201d (ffl) , \u201cba\u201d (I\\) and \u201cbei\u201d (%i) generally do not have translations in English. eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair. Following a variational EM scheme  , we estimate the model parameters \u03b1 and B in an unsupervised fashion. Essentially, Eqs.", "Zhao et al   assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model It shows that the performance of word alignment can be improved with the help of document-level information which indirectly improves the quality of SMT  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E\u2217 = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity. In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair. A unique normalized and real-valued vector \u03b8, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions. Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions.", "Zhao et al   assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model It shows that the performance of word alignment can be improved with the help of document-level information which indirectly improves the quality of SMT  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "Zhao et al   assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model It shows that the performance of word alignment can be improved with the help of document-level information which indirectly improves the quality of SMT  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida   stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "Zhao et al   assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model It shows that the performance of word alignment can be improved with the help of document-level information which indirectly improves the quality of SMT  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["All the plates represent replicates. The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair. (a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair. a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic. Given a specific topic-weight vector \u03b8d for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by ##CITATION## Basically the BiTAM model consists of topic-dependent transla tion lexicons modeling P rc|e k where c e and k denotes the source Chinese word target English word and the topic index respectively ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics. Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I \u03b2 e I a \u03b1 \u03b8 z f J B N M \u03b1 \u03b8 z a a f J B \u03b1 \u03b8 z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and a hexagon denotes a parameter. Un-shaded nodes are hidden variables.", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by ##CITATION## Basically the BiTAM model consists of topic-dependent transla tion lexicons modeling P rc|e k where c e and k denotes the source Chinese word target English word and the topic index respectively ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We conclude with a brief discussion in section 6. In statistical machine translation, one typically uses parallel data to identify entities such as \u201cword-pair\u201d, \u201csentence-pair\u201d, and \u201cdocument- pair\u201d. Formally, we define the following terms1: \u2022 A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. \u2022 A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.\u2022 A document-pair (F, E) refers to two doc uments which are translations of each other. Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair. \u2022 A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}.", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by ##CITATION## Basically the BiTAM model consists of topic-dependent transla tion lexicons modeling P rc|e k where c e and k denotes the source Chinese word target English word and the topic index respectively ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes   and non-overlapping bilingual word-clusters   with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by ##CITATION## Basically the BiTAM model consists of topic-dependent transla tion lexicons modeling P rc|e k where c e and k denotes the source Chinese word target English word and the topic index respectively ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["2.1 Baseline: IBM Model-1. The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions   in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations  . We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by ##CITATION## Basically the BiTAM model consists of topic-dependent transla tion lexicons modeling P rc|e k where c e and k denotes the source Chinese word target English word and the topic index respectively ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["83 79 0. 61 16 0. 02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean). The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!\ufffd:South Korean).", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by ##CITATION## Basically the BiTAM model consists of topic-dependent transla tion lexicons modeling P rc|e k where c e and k denotes the source Chinese word target English word and the topic index respectively ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["English-French, i.e., e \u2194 f , although our models are tested,in this paper, for EnglishChinese. We use the end-user ter minology for source and target languages. In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics. Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I \u03b2 e I a \u03b1 \u03b8 z f J B N M \u03b1 \u03b8 z a a f J B \u03b1 \u03b8 z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.", "We adopted K = 3 topics following the setting in ##CITATION##", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) \u00b7 p(ei |e). (1) j=1 i=1 1 We follow the notations in   for. English-French, i.e., e \u2194 f , although our models are tested,in this paper, for EnglishChinese. We use the end-user ter minology for source and target languages.", "We adopted K = 3 topics following the setting in ##CITATION##", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["The co-occurrence count, however, only favors \u201cHanGuo\u201d, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context. Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small. With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3. 5.4 Evaluating Word. Alignments We evaluate word alignment accuracies in various settings.", "We adopted K = 3 topics following the setting in ##CITATION##", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["83 79 0. 61 16 0. 02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean). The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!\ufffd:South Korean).", "We adopted K = 3 topics following the setting in ##CITATION##", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments. As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1. A close look at the three BiTAMs does not yield significant difference.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model. Similar improvements over IBM models and HMM are preserved after applying the three kinds of heuristics in the above. As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic. We also test BiTAM3 on large training data, and similar improvements are observed over those of the baseline models (see Table. 5).", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Under a non-symmetric Dirichlet prior, hyperparameter \u03b1 is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1. Better initialization of B can help to avoid local optimal as shown in \u00a7 5.5. With the learned B and \u03b1 fixed, the variational parameters to be computed in Eqn. (810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10\u22125. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["1, p(f |e) is learned via IBM1; \u03bb is estimated via EM on held out data. 4.3 Retrieving Word Alignments. Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA). Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix \u03d5 \u2261 {\u03d5dnji}. UDA uses a French word fdnj (at the jtth position of ntth sentence in the dtth document) to query \u03d5 to get the best aligned English word (by taking the maximum point in a row of \u03d5): adnj = arg max \u03d5dnji .", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). Additional heuristics are applied to further improve the accuracies. Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6. 45 8 15 .7 0 6. 82 2 17 .7 0 6.", "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "##CITATION##  note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["(810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10\u22125. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs. To estimate B, \u03b2 (for BiTAM2) and \u03b1, at most eight variational EM iterations are run on the training data. Figure 2 shows absolute 2\u223c3% better F-measure over iterations of variational EM using two and three topics of BiTAM1 comparing with IBM1. BiTam with Null and Laplace Smoothing Over Var.", "##CITATION##  note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["(1) j=1 i=1 1 We follow the notations in   for. English-French, i.e., e \u2194 f , although our models are tested,in this paper, for EnglishChinese. We use the end-user ter minology for source and target languages. In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics.", "##CITATION##  note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k \u221d ) ) ) ) \u03b4(f, fj )\u03b4(e, ei )\u03c6dnk \u03d5dnji (12) d n=1 j=1 i=1 For \u03b1, close-form update is not available, and we resort to gradient accent as in   with restarts to ensure each updated \u03b1k >0. 4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity  . To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing.", "##CITATION##  note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity  . To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.", "##CITATION##  note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity  . To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing.", "##CITATION##  note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Note that, the sentence-pairs are now connected by the node \u03b8d. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector \u03b8d. Specifically, BiTAM1 assumes that each sentence-pair has one single topic. Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions.", "Our approach is inspired by the recent studies ##CITATION## ##CITATION## Tam et al Gong and Zhou Ruiz and Federico which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["The conditional \u03d5dnji \u221d exp ) \u03c6dnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where \u03a8(\u00b7) is a digamma function. Note that inthe formulas in \u00a73.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas \u03c6 dnkis the variational param 3.4 Incorporation of Word \u201cNull\u201d. Similar to IBM models, \u201cNull\u201d word is used for the source words which have no translation counterparts in the target language. For example, Chinese words \u201cde\u201d (ffl) , \u201cba\u201d (I\\) and \u201cbei\u201d (%i) generally do not have translations in English. eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair.", "Our approach is inspired by the recent studies ##CITATION## ##CITATION## Tam et al Gong and Zhou Ruiz and Federico which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions. We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random.", "Our approach is inspired by the recent studies ##CITATION## ##CITATION## Tam et al Gong and Zhou Ruiz and Federico which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "Our approach is inspired by the recent studies ##CITATION## ##CITATION## Tam et al Gong and Zhou Ruiz and Federico which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation.", "Our approach is inspired by the recent studies ##CITATION## ##CITATION## Tam et al Gong and Zhou Ruiz and Federico which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes   and non-overlapping bilingual word-clusters   with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Our approach is inspired by the recent studies ##CITATION## ##CITATION## Tam et al Gong and Zhou Ruiz and Federico which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model ##CITATION## 2007 presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model ##CITATION## 2007 presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair. A unique normalized and real-valued vector \u03b8, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions. Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model ##CITATION## 2007 presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  .", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model ##CITATION## 2007 presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  . Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model ##CITATION## 2007 presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["3.2 BiTAM2: Monolingual Admixture. In general, the monolingual model for English can also be a rich topic-mixture. This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable.", "\u2022 In addition to the utilization of in-domain monolingual corpora our method is different from the previous works   in the following aspects: 1 we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; 2 rather than modeling topic-dependent translation lexicons in the training process we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs so our method can also be directly applied to topic-dependent phrase probability modeling", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent. Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969\u2013976, Sydney, July 2006. Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.", "\u2022 In addition to the utilization of in-domain monolingual corpora our method is different from the previous works   in the following aspects: 1 we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; 2 rather than modeling topic-dependent translation lexicons in the training process we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs so our method can also be directly applied to topic-dependent phrase probability modeling", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["In the M-step, we update \u03b1 and B so that they improve a lower bound of the log-likelihood defined bellow: L(\u03b3, \u03c6, \u03d5; \u03b1, B) = Eq [log p(\u03b8|\u03b1)]+Eq [log p(z|\u03b8)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]\u2212Eq [log q(\u03b8)] \u2212Eq [log q(z)]\u2212Eq [log q(a)]. (11) The close-form iterative updating formula B is: BDA selects iteratively, for each f , the best aligned e, such that the word-pair (f, e) is the maximum of both row and column, or its neighbors have more aligned pairs than the other combpeting candidates.A close check of {\u03d5dnji} in Eqn. 10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k \u221d ) ) ) ) \u03b4(f, fj )\u03b4(e, ei )\u03c6dnk \u03d5dnji (12) d n=1 j=1 i=1 For \u03b1, close-form update is not available, and we resort to gradient accent as in   with restarts to ensure each updated \u03b1k >0. 4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity  .", "\u2022 In addition to the utilization of in-domain monolingual corpora our method is different from the previous works   in the following aspects: 1 we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; 2 rather than modeling topic-dependent translation lexicons in the training process we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs so our method can also be directly applied to topic-dependent phrase probability modeling", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  . Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.", "\u2022 In addition to the utilization of in-domain monolingual corpora our method is different from the previous works   in the following aspects: 1 we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; 2 rather than modeling topic-dependent translation lexicons in the training process we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs so our method can also be directly applied to topic-dependent phrase probability modeling", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["For simplicity, we assume that the French words fj \u2019s are conditionally independent of each other; the alignment variables aj \u2019s are independent of other variables and are uniformly distributed a priori. Therefore, the distribution for each sentence-pair is: p(fn , an |en , Bzn) = p(fn |en , an , Bzn)p(an |en , Bzn) Jn \u201cNull\u201d is attached to every target sentence to align the source words which miss their translations. Specifically, the latent Dirichlet allocation   can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ). (6) contains only one word: \u201cNull\u201d, and the alignment link a is no longer a hidden variable. Thus, the conditional likelihood for the entire parallel corpus is given by taking the product of the marginal probabilities of each individual document-pair in Eqn.", "\u2022 In addition to the utilization of in-domain monolingual corpora our method is different from the previous works   in the following aspects: 1 we use a different topic model \u2014 HTMM which has different assumption from PLSA and LDA; 2 rather than modeling topic-dependent translation lexicons in the training process we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs so our method can also be directly applied to topic-dependent phrase probability modeling", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  .", "To exploit topic information for statistical machine translation SMT researchers have proposed various topic-specific lexicon translation models ##CITATION## ##CITATION## Tam et al to improve translation quality", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models. Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models. The proposed models significantly improve the alignment accuracy and lead to better translation qualities. Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.", "To exploit topic information for statistical machine translation SMT researchers have proposed various topic-specific lexicon translation models ##CITATION## ##CITATION## Tam et al to improve translation quality", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Conclusion."], ["With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  . Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "To exploit topic information for statistical machine translation SMT researchers have proposed various topic-specific lexicon translation models ##CITATION## ##CITATION## Tam et al to improve translation quality", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  . Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.", "To exploit topic information for statistical machine translation SMT researchers have proposed various topic-specific lexicon translation models ##CITATION## ##CITATION## Tam et al to improve translation quality", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics. For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida   stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.", "To exploit topic information for statistical machine translation SMT researchers have proposed various topic-specific lexicon translation models ##CITATION## ##CITATION## Tam et al to improve translation quality", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Under a non-symmetric Dirichlet prior, hyperparameter \u03b1 is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1. Better initialization of B can help to avoid local optimal as shown in \u00a7 5.5. With the learned B and \u03b1 fixed, the variational parameters to be computed in Eqn. (810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10\u22125. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs.", "Sentences should be translated in consistence with their topics  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["The co-occurrence count, however, only favors \u201cHanGuo\u201d, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context. Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small. With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3. 5.4 Evaluating Word. Alignments We evaluate word alignment accuracies in various settings.", "Sentences should be translated in consistence with their topics  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics. For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida   stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.", "Sentences should be translated in consistence with their topics  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes   and non-overlapping bilingual word-clusters   with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Sentences should be translated in consistence with their topics  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["5. 3.2 BiTAM2: Monolingual Admixture. In general, the monolingual model for English can also be a rich topic-mixture. This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated", "Topic modeling has received some use in SMT for instance Bilingual LSA adaptation Tam et al and the BiTAM model ##CITATION## which uses a bilingual topic model for learning alignment", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["Note that, the sentence-pairs are now connected by the node \u03b8d. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector \u03b8d. Specifically, BiTAM1 assumes that each sentence-pair has one single topic. Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable \u03b8d, referred to as the topic-weight vector of the document, can take values in the (K \u22121)-simplex following a probability density: to the proposed distributions.", "Topic modeling has received some use in SMT for instance Bilingual LSA adaptation Tam et al and the BiTAM model ##CITATION## which uses a bilingual topic model for learning alignment", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "Topic modeling has received some use in SMT for instance Bilingual LSA adaptation Tam et al and the BiTAM model ##CITATION## which uses a bilingual topic model for learning alignment", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based   and interlingua-based   approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects.", "Topic modeling has received some use in SMT for instance Bilingual LSA adaptation Tam et al and the BiTAM model ##CITATION## which uses a bilingual topic model for learning alignment", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes   and non-overlapping bilingual word-clusters   with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "Topic modeling has received some use in SMT for instance Bilingual LSA adaptation Tam et al and the BiTAM model ##CITATION## which uses a bilingual topic model for learning alignment", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["The conditional \u03d5dnji \u221d exp ) \u03c6dnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where \u03a8(\u00b7) is a digamma function. Note that inthe formulas in \u00a73.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas \u03c6 dnkis the variational param 3.4 Incorporation of Word \u201cNull\u201d. Similar to IBM models, \u201cNull\u201d word is used for the source words which have no translation counterparts in the target language. For example, Chinese words \u201cde\u201d (ffl) , \u201cba\u201d (I\\) and \u201cbei\u201d (%i) generally do not have translations in English. eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair.", "To avoid the need for hard decisions about domain membership some have used topic modeling to improve SMT performance eg using latent semantic analysis Tam et al or \u2018biTAM\u2019 ##CITATION##", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation.", "To avoid the need for hard decisions about domain membership some have used topic modeling to improve SMT performance eg using latent semantic analysis Tam et al or \u2018biTAM\u2019 ##CITATION##", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent. Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969\u2013976, Sydney, July 2006. Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.", "To avoid the need for hard decisions about domain membership some have used topic modeling to improve SMT performance eg using latent semantic analysis Tam et al or \u2018biTAM\u2019 ##CITATION##", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes   and non-overlapping bilingual word-clusters   with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "To avoid the need for hard decisions about domain membership some have used topic modeling to improve SMT performance eg using latent semantic analysis Tam et al or \u2018biTAM\u2019 ##CITATION##", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  .", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "ABSTRACT"], ["This result suggests a straightforward way to leverage BiTAMs to improve statistical machine translations. In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models. Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models. The proposed models significantly improve the alignment accuracy and lead to better translation qualities. Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Conclusion."], ["These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes   and non-overlapping bilingual word-clusters   with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics   and text modeling  . Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Introduction"], ["The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations  . We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework. Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E\u2217 = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity. In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Notations and Baseline."], ["Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Bilingual Topic AdMixture Model."], ["5.4 Evaluating Word. Alignments We evaluate word alignment accuracies in various settings. Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). Additional heuristics are applied to further improve the accuracies. Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments. As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1. A close look at the three BiTAMs does not yield significant difference. BiTAM3 is slightly better in most settings; BiTAM1 is slightly worse than the other two, because the topics sampled at the sentence level are not very concentrated.", "In ##CITATION## three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["Now e is generated Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable. A variational inference is used to approximate the true posteriors of these hidden variables. The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted. 4.1 Variational Approximation.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models. We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish. Choosing the number of topics is a model selection problem. We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets. The overall computation complexity of the BiTAM is linear to the number of hidden topics.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Experiments."], ["This is realized by using the same topic-weight vector \u03b8d and the same topic indicator zdn sampled according to \u03b8d, as described in \u00a73.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and \u03b8 is intractable. A variational inference is used to approximate the true posteriors of these hidden variables. The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events  ", 0, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity  . To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = \u03bbBf,e,k +(1\u2212\u03bb)p(f |e). (13) As in Eqn.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = \u03bbBf,e,k +(1\u2212\u03bb)p(f |e). (13) As in Eqn. 1, p(f |e) is learned via IBM1; \u03bb is estimated via EM on held out data.", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events  ", 1, "BiTAM: Bilingual Topic AdMixture Models forWord Alignment", "Learning and Inference."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model which is similar as the Topic-sensitive LexRank ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model which is similar as the Topic-sensitive LexRank ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR  and analyzing sentiments in text  . In  , we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system. We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model which is similar as the Topic-sensitive LexRank ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Introduction"], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model which is similar as the Topic-sensitive LexRank ##CITATION##", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank   and Total ReciprocalDocument Rank  .MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question. This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system. In the context of answering questions from complex stories, where thereis often more than one correct answer to a question,and where answers are typically time-dependent, weshould focus on maximizing TRDR, which gives us 2For clusters annotated by two judges, all sentences chosenby at least one judge were included. a measure of how many of the relevant sentenceswere identified by the system.", "Its weight twij is calculated by tf \u00b7 idf  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["3 0.28454242157110576 Rescue officials said that at least th... Graph Figure 2: LexRank example: sentence similaritygraph with a cosine threshold of 0.15. equation and is determined empirically. For highervalues of d, we give more importance to the relevance to the question compared to the similarity tothe other sentences in the cluster. The denominatorsin both terms are for normalization, which are described below.", "Its weight twij is calculated by tf \u00b7 idf  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused summary. This is similar to snippet retrieval  . However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis. 3.1 The LexRank method.", "Its weight twij is calculated by tf \u00b7 idf  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question. In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.", "Its weight twij is calculated by tf \u00b7 idf  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1. 4.2 Evaluation metrics and methods. To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment. To compare different configurations of our system to the baselinesystem, we produced extracts at a fixed length of 20sentences. While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for several reasons.", "  discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["He or she then generated a list of factualquestions key to understanding the story. Once wecollected the questions for each cluster, two judgesindependently annotated nine of the training clusters. For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question. Once an acceptable rate of inter-judge agreement was verified on the first nine clusters   of 0.68), the remaining11 clusters were annotated by one judge each.In some cases, the judges did not find any sentences containing the answer for a given question.Such questions were removed from the corpus. Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1.", "  discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["submarine apparently caused the Kursk to sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, has provided his Russiancounterpart with a written summary of whatU.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials said Wednesday.5 There has been no final word on what caused 0.0123 N the submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory that Kursk may have collided with anotherobject is receiving increasingly concrete confirmation. Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval  . In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores.", "  discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "  discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question. To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDFweightedword overlap. In our experiments, themethod achieves a TRDR score that is significantly higher than that of the baseline.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection Mihalcea and Tarau text summarization Erkan and Radev Mi-halcea word sense disambiguation Mihalcea et al Mihalcea sentiment analysis Pang and Lee and sentence retrieval for question answering ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "ABSTRACT"], ["Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1. 4.2 Evaluation metrics and methods. To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment. To compare different configurations of our system to the baselinesystem, we produced extracts at a fixed length of 20sentences. While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for several reasons.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection Mihalcea and Tarau text summarization Erkan and Radev Mi-halcea word sense disambiguation Mihalcea et al Mihalcea sentiment analysis Pang and Lee and sentence retrieval for question answering ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["While we saw in Section 5.1 thatLR[0.20,0.95] may perform better on some questionor cluster types than others, we conclude that it beatsthe competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions. However, in future work, we will continueto improve the performance, perhaps by developing mixed strategies using different configurationsof LexRank. The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences. When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties. An example of such a scenario is illustrated inTables 8 and 9, which show the top ranked sentencesby the baseline and LexRank, respectively for thequestion What caused the Kursk to sink? from theKursk submarine cluster.", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection Mihalcea and Tarau text summarization Erkan and Radev Mi-halcea word sense disambiguation Mihalcea et al Mihalcea sentiment analysis Pang and Lee and sentence retrieval for question answering ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Discussion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection Mihalcea and Tarau text summarization Erkan and Radev Mi-halcea word sense disambiguation Mihalcea et al Mihalcea sentiment analysis Pang and Lee and sentence retrieval for question answering ##CITATION##", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["It is clear that ahigh question bias is needed. However, a small probability for jumping to a node that is lexically similar to the given sentence (rather than the questionitself) is needed. Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions. We appliedall four of these configurations to our unseen development/test data, in order to see if we could furtherdifferentiate their performances. 5.1 Development/testing phase.", "These algorithms are all based on the query-sensitive LexRank OtterBacher et al", 0, "Using Random Walks for Question-focused Sentence Retrieval", "LexRank versus the baseline system."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "These algorithms are all based on the query-sensitive LexRank OtterBacher et al", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["This is similar to snippet retrieval  . However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis. 3.1 The LexRank method. In  , the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set.", "These algorithms are all based on the query-sensitive LexRank OtterBacher et al", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "These algorithms are all based on the query-sensitive LexRank OtterBacher et al", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively. This model hasproven to be successful in query-based sentence retrieval  , and is used as our competitive baseline in this study (e.g. Tables 4, 5 and7). 3.3 The mixture model.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency TF\u00d7ISF ##CITATION##:quiring a specific strategy", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency TF\u00d7ISF ##CITATION##:quiring a specific strategy", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In  , the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency TF\u00d7ISF ##CITATION##:quiring a specific strategy", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question. In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1) log(tfw,q + 1)  idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.", "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency TF\u00d7ISF ##CITATION##:quiring a specific strategy", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["3.1 The LexRank method. In  , the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "A topic-sensitiveLexRank is proposed in ##CITATION##As in LexRank the set of sentences in a documentcluster is represented as a graph where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["The baseline system explained above does not makeuse of any inter-sentence information in a cluster.We hypothesize that a sentence that is similar tothe high scoring sentences in the cluster should alsohave a high score. For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.This idea is captured by the following mixture model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its relevance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster: p(s|q) = d rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) where C is the set of all sentences in the cluster. Thevalue of d, which we will also refer to as the question bias, is a trade-off between two terms in the Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence 1 0.03614457831325301 At least two people are dead, inclu... 0 0.28454242157110576 Officials said the plane was carryin...", "A topic-sensitiveLexRank is proposed in ##CITATION##As in LexRank the set of sentences in a documentcluster is represented as a graph where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["While we saw in Section 5.1 thatLR[0.20,0.95] may perform better on some questionor cluster types than others, we conclude that it beatsthe competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions. However, in future work, we will continueto improve the performance, perhaps by developing mixed strategies using different configurationsof LexRank. The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences. When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties. An example of such a scenario is illustrated inTables 8 and 9, which show the top ranked sentencesby the baseline and LexRank, respectively for thequestion What caused the Kursk to sink? from theKursk submarine cluster.", "A topic-sensitiveLexRank is proposed in ##CITATION##As in LexRank the set of sentences in a documentcluster is represented as a graph where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Discussion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "A topic-sensitiveLexRank is proposed in ##CITATION##As in LexRank the set of sentences in a documentcluster is represented as a graph where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval  . In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores. We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine). Our system does not rank the input documents, nor is it restricted in terms of the number ofsentences that may be selected from the same document. The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused summary. This is similar to snippet retrieval  .", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["Our system does not rank the input documents, nor is it restricted in terms of the number ofsentences that may be selected from the same document. The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused summary. This is similar to snippet retrieval  . However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["3.1 The LexRank method. In  , the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["In  , the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15. Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality.", "To apply LexRank to query-focused context a topic-sensitive version of LexRank isproposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR  and analyzing sentiments in text  . In  , we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system. We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.", "A topic- sensitive LexRank is proposed in ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Introduction"], ["The stationary distributionof a Markov chain can be computed by a simple iterative algorithm, called power method.1 A simpler version of Equation 5, where A is auniform matrix andB is a normalized binary matrix,is known as PageRank   and used to rank the web pages by theGoogle search engine. It was also the model used torank sentences in  . 3.4 Experiments with topic-sensitive LexRank. We experimented with different values of d on ourtraining data. We also considered several thresholdvalues for inter-sentence cosine similarities, wherewe ignored the similarities between the sentencesthat are below the threshold.", "A topic- sensitive LexRank is proposed in ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "0.1973852892722677 Milan fire brigade officials said that..."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "A topic- sensitive LexRank is proposed in ##CITATION##", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "A topic- sensitive LexRank is proposed in ##CITATION##", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15. Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality.", "The degree of a given node is an indication of how much important the sentence is To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed in  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["3.1 The LexRank method. In  , the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "The degree of a given node is an indication of how much important the sentence is To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed in  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "The degree of a given node is an indication of how much important the sentence is To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed in  ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "The degree of a given node is an indication of how much important the sentence is To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed in  ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher. We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.", "Afterwards our approach is evaluated against two existing approaches which rely on the conventional semantic network and are able to capture binary relations only The other one is based on topic-sensitive LexRank ##CITATION## called title-sensitive PageRank here ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Conclusion."], ["Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question.", "Afterwards our approach is evaluated against two existing approaches which rely on the conventional semantic network and are able to capture binary relations only The other one is based on topic-sensitive LexRank ##CITATION## called title-sensitive PageRank here ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question. In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.", "Afterwards our approach is evaluated against two existing approaches which rely on the conventional semantic network and are able to capture binary relations only The other one is based on topic-sensitive LexRank ##CITATION## called title-sensitive PageRank here ", 0, "Using Random Walks for Question-focused Sentence Retrieval", "Our approach: topic-sensitive LexRank."], ["We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline. Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi  , which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).", "Afterwards our approach is evaluated against two existing approaches which rely on the conventional semantic network and are able to capture binary relations only The other one is based on topic-sensitive LexRank ##CITATION## called title-sensitive PageRank here ", 1, "Using Random Walks for Question-focused Sentence Retrieval", "Formal description of the problem."], ["In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered. Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus. In future work, this parameter can be learned using a development corpus.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives see ##CITATION## for a discussion of such generic patterns", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["In this phase, Espresso selects among the patterns P those that are most reliable. Intuitively, a reliable pattern is one that is both highly precise and one that extracts many instances. The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision). We thus prefer patterns that are highly associated with the input patterns I'. Pointwise mutual information [4] is a commonly used metric for measuring the strength of association between two events x and y: pmi(x, y ) = log P(x, y ) P(x)P(y ) We define the reliability of a pattern p, r\u03c0(p), as its average strength of association across each input instance i in I', weighted by the reliability of each instance i: \u239b \u239e \u2211\u239c pmi(i, p) \u2217 r (i )\u239f \u239c r ( p ) = i\u2208I \u2032 \u239d max \u03b9 \u239f pmi \u23a0 \u03c0 I \u2032 where r\u03b9(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum pointwise mutual information between all patterns and all instances.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives see ##CITATION## for a discussion of such generic patterns", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["To address this, we multiply pmi(i, p) with the discounting factor suggested in [16]. The set of highest n scoring patterns P', according to r\u03c0(p), are then selected and retained for the next phase, where n is the number of patterns of the previous iteration incremented by 1. In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered. Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives see ##CITATION## for a discussion of such generic patterns", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts. The system architecture is designed with generality in mind, avoiding any relation-specific inference technique. Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora.", "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives see ##CITATION## for a discussion of such generic patterns", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["We also plan to investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA. The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for evaluating the outputs of the systems.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances This approach is very common in both the NLP and Semantic Web communities   ", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Conclusions."], ["In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances This approach is very common in both the NLP and Semantic Web communities   ", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "ABSTRACT"], ["Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora. From the other side, Espresso requires only weak human supervision. In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances This approach is very common in both the NLP and Semantic Web communities   ", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["\u0083 ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II.", "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances This approach is very common in both the NLP and Semantic Web communities   ", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds.. Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. E CHEM corpus) and evaluating their quality manually using one human judge2.", "Typically algorithms are compared using one set of handpicked seeds for each category ##CITATION## McIntosh and Curran", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. E CHEM corpus) and evaluating their quality manually using one human judge2. For each instance, the judge may assign a score of 1 for correct, 0 for incorrect, and \u00bd for partially correct.", "Typically algorithms are compared using one set of handpicked seeds for each category ##CITATION## McIntosh and Curran", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples. The seeds were used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample outputs from Espresso.", "Typically algorithms are compared using one set of handpicked seeds for each category ##CITATION## McIntosh and Curran", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Furthermore, it is important to note that there are several other generic patterns, like [X\u2019s Y], from which we expect a similar precision of 50% with a continual increase of recall. This is a very exciting avenue of further investigation. We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text. Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances. There are many avenues of future work.", "Typically algorithms are compared using one set of handpicked seeds for each category ##CITATION## McIntosh and Curran", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Conclusions."], ["In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning Brin Riloff and Jones Agichtein and Gravano Ravichandran and Hovy Etzioni et al ##CITATION## Bunescu and Mooney Rozenfeld and Feldman", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "ABSTRACT"], ["We preprocess the corpora using the Alembic Workbench POStagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: \u0083 RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) \u0083 PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning Brin Riloff and Jones Agichtein and Gravano Ravichandran and Hovy Etzioni et al ##CITATION## Bunescu and Mooney Rozenfeld and Feldman", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["\u0083 ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning Brin Riloff and Jones Agichtein and Gravano Ravichandran and Hovy Etzioni et al ##CITATION## Bunescu and Mooney Rozenfeld and Feldman", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning Brin Riloff and Jones Agichtein and Gravano Ravichandran and Hovy Etzioni et al ##CITATION## Bunescu and Mooney Rozenfeld and Feldman", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Phase 1: Pattern discovery. The pattern discovery phase takes as input a set of instances I' and produces as output a set of lexical patterns P. For the first iteration I' = Is, the set of initial seeds. In order to induce P, we apply a slight modification to the approach presented in [20]. For each input instance i = {x, y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then generalized into a set of new sentences SGx,y by replacing all terminological expressions by a terminological label (TR). For example: \u201cBecause/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y\u201d is generalized as: \u201cBecause/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y\u201d All substrings linking terms x and y are then extracted from the set SGx,y, and overall frequencies are computed.", "To generalize the task we first determine noun phrases in the data following the definition in ##CITATION##", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: pmi(i, p) = log x, p, y x,*, y *, p,* where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual information is biased towards infrequent events. To address this, we multiply pmi(i, p) with the discounting factor suggested in [16]. The set of highest n scoring patterns P', according to r\u03c0(p), are then selected and retained for the next phase, where n is the number of patterns of the previous iteration incremented by 1. In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one.", "To generalize the task we first determine noun phrases in the data following the definition in ##CITATION##", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members\u2019 lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.", "To generalize the task we first determine noun phrases in the data following the definition in ##CITATION##", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Terms are commonly defined as surface representations of stable and key domain concepts [19]. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points. Hence, we can capture relations between complex terms, such as \u201crecord of a criminal conviction\u201d part-of \u201cFBI report\u201d.", "To generalize the task we first determine noun phrases in the data following the definition in ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["In this section, we present a preliminary comparison of Espresso with two state of the art systems on the task of extracting various semantic relations. 4.1.1. Datasets We perform our experiments using the following two datasets: \u0083 TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 \u2013 AP890131, AP890201 \u2013 AP890228, and AP890310 \u2013 AP890319. \u0083 CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2].", "In a completely separate stream of work ##CITATION## investigated the extraction of axioms from the text using the statistical text harvesting paradigm", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus. In future work, this parameter can be learned using a development corpus. Our reliability measure ensures that overly generic patterns, which may potentially have very low precision, are discarded. However, we are currently exploring a web-expansion algorithm that could both help detect generic patterns and also filter out their incorrect instances.", "In a completely separate stream of work ##CITATION## investigated the extraction of axioms from the text using the statistical text harvesting paradigm", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["For each semantic relation, we manually extracted a set of seed examples. The seeds were used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample outputs from Espresso. 4.2. Precision and Recall.", "In a completely separate stream of work ##CITATION## investigated the extraction of axioms from the text using the statistical text harvesting paradigm", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.", "In a completely separate stream of work ##CITATION## investigated the extraction of axioms from the text using the statistical text harvesting paradigm", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP\u2019s part- NP].", "In a completely separate stream of work ##CITATION## investigated the extraction of axioms from the text using the statistical text harvesting paradigm", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14]. \u0083 ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.", "In   they report the results", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. E CHEM corpus) and evaluating their quality manually using one human judge2. For each instance, the judge may assign a score of 1 for correct, 0 for incorrect, and \u00bd for partially correct.", "In   they report the results", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["\u2020 Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances. \u2020 Relative recall is given in relation to ESP. Tables 2 \u2013 8 reports the total number of instances, precision, and relative recall of each system on the TREC9 and CHEM corpora. The relative recall is always given in relation to the Espresso system. For example, in Table 2, RH02 has a relative recall of 5.31 with Espresso, which means that the RH02 system output 5.31 times more correct relations than Espresso (at a cost of much Table 8. System performance on the production relation on the CHEM dataset.", "In   they report the results", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "In   they report the results", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible. To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches.", "As pointed out by ##CITATION##  most ontology extraction systems so far have focused on generalised is-a or part-of relationships", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "As pointed out by ##CITATION##  most ontology extraction systems so far have focused on generalised is-a or part-of relationships", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.", "As pointed out by ##CITATION##  most ontology extraction systems so far have focused on generalised is-a or part-of relationships", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members\u2019 lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.", "As pointed out by ##CITATION##  most ontology extraction systems so far have focused on generalised is-a or part-of relationships", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.", "Hearst\u2019s method has since then been followed by the most successful systems such as the Espresso system proposed by ##CITATION##  based on bootstrapping", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm begins a four-phase loop.", "Hearst\u2019s method has since then been followed by the most successful systems such as the Espresso system proposed by ##CITATION##  based on bootstrapping", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "The Espresso Algorithm."], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "Hearst\u2019s method has since then been followed by the most successful systems such as the Espresso system proposed by ##CITATION##  based on bootstrapping", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.", "Hearst\u2019s method has since then been followed by the most successful systems such as the Espresso system proposed by ##CITATION##  based on bootstrapping", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts. The system architecture is designed with generality in mind, avoiding any relation-specific inference technique. Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Introduction"], ["A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Relevant Work."], ["We also plan to investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA. The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for evaluating the outputs of the systems.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 0, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Conclusions."], ["4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \u0083 succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["We evaluate this relation on the TREC9 corpus. \u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["\u0083 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["We evaluate this relation on the CHEM corpus. \u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["\u0083 production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples. The seeds were used for both Espresso as well as RH021.", "Pennachiotti and Pantel developed a system that extracts the relations such as is-a part-of and succession from the Trec9 corpus and is-a part-of production and reaction from a chemistry corpus ##CITATION##", 1, "A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations", "Experimental Results."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.", "Joint segmentation and parsing was also investigated for Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "PAPER"], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB  ; CTB6  ; Negra  ; English, sections 221 (train) and section 23 (test).", "Joint segmentation and parsing was also investigated for Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "Joint segmentation and parsing was also investigated for Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented.", "Joint segmentation and parsing was also investigated for Arabic ##CITATION##", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "Indeed we have used it to solve the problem of parsing while recovering null elements in both English and Chinese Cai Chiang and Goldberg and others have used it for the joint segmentation and parsing of Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design  . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages.", "Indeed we have used it to solve the problem of parsing while recovering null elements in both English and Chinese Cai Chiang and Goldberg and others have used it for the joint segmentation and parsing of Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic  , but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (-) is an elongation character used in Arabic script to justify text. It has no syntactic function.", "Indeed we have used it to solve the problem of parsing while recovering null elements in both English and Chinese Cai Chiang and Goldberg and others have used it for the joint segmentation and parsing of Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented.", "Indeed we have used it to solve the problem of parsing while recovering null elements in both English and Chinese Cai Chiang and Goldberg and others have used it for the joint segmentation and parsing of Arabic ##CITATION##", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our evaluation includes both weighted and un- weighted lattices. We weight edges using a unigram language model estimated with Good- Turing smoothing. Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models  .13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation  . MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution.", "This makes the word sequence unobserved to the parser which has to infer both the syntactic-structure and the token segmentation6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation and the output of the initial model is fed as the input to a second stage parser This is a popular approach in parsing systems for Arabic and Chinese   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "This makes the word sequence unobserved to the parser which has to infer both the syntactic-structure and the token segmentation6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation and the output of the initial model is fed as the input to a second stage parser This is a popular approach in parsing systems for Arabic and Chinese   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "This makes the word sequence unobserved to the parser which has to infer both the syntactic-structure and the token segmentation6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation and the output of the initial model is fed as the input to a second stage parser This is a popular approach in parsing systems for Arabic and Chinese   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsing\u2014including the experiments in the previous section\u2014have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "This makes the word sequence unobserved to the parser which has to infer both the syntactic-structure and the token segmentation6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation and the output of the initial model is fed as the input to a second stage parser This is a popular approach in parsing systems for Arabic and Chinese   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics. Particles are uninflected.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances   It is also used for Arabic   and other languages   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances   It is also used for Arabic   and other languages   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances   It is also used for Arabic   and other languages   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented.", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances   It is also used for Arabic   and other languages   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al   Sima\u2019an   and Hall   and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith   Goldberg and Tsarfaty   and ##CITATION##   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["input token, the segmentation is then performed deterministically given the 1-best analysis. Since guess and gold trees may now have different yields, the question of evaluation is complex. Cohen and Smith   chose a metric like SParseval   that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al   Sima\u2019an   and Hall   and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith   Goldberg and Tsarfaty   and ##CITATION##   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al   Sima\u2019an   and Hall   and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith   Goldberg and Tsarfaty   and ##CITATION##   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization  , insensitivity to morphology  , and the effect of variable word order  . Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al   Sima\u2019an   and Hall   and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith   Goldberg and Tsarfaty   and ##CITATION##   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits.", "Recently ##CITATION##   report on an extensive set of experiments with several kinds of tree annotations and refinements and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser both when assuming gold word segmentation", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "Recently ##CITATION##   report on an extensive set of experiments with several kinds of tree annotations and refinements and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser both when assuming gold word segmentation", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["87 Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.", "Recently ##CITATION##   report on an extensive set of experiments with several kinds of tree annotations and refinements and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser both when assuming gold word segmentation", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "Recently ##CITATION##   report on an extensive set of experiments with several kinds of tree annotations and refinements and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser both when assuming gold word segmentation", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsing\u2014including the experiments in the previous section\u2014have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "The best reported results for parsing Arabic when the gold word segmentation is not known however are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser resulting in an F-score of 79% F1 for sentences of up to 70 words  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our evaluation includes both weighted and un- weighted lattices. We weight edges using a unigram language model estimated with Good- Turing smoothing. Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models  .13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation  . MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution.", "The best reported results for parsing Arabic when the gold word segmentation is not known however are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser resulting in an F-score of 79% F1 for sentences of up to 70 words  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["87 Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.", "The best reported results for parsing Arabic when the gold word segmentation is not known however are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser resulting in an F-score of 79% F1 for sentences of up to 70 words  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "The best reported results for parsing Arabic when the gold word segmentation is not known however are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser resulting in an F-score of 79% F1 for sentences of up to 70 words  ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["For each 13 Of course, this weighting makes the PCFG an improper distribution. However, in practice, unknown word models also make the distribution improper. Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87  -style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories.", "The best reported results for parsing Arabic when the gold word segmentation is not known however are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser resulting in an F-score of 79% F1 for sentences of up to 70 words  ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["As we have said, parse quality decreases with sentence length. Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty   found that unweighted lattices were more effective for Hebrew. Table 9: Dev set results for sentences of length \u2264 70. Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference. Each model was able to produce hypotheses for all input sentences.", "The best reported results for parsing Arabic when the gold word segmentation is not known however are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser resulting in an F-score of 79% F1 for sentences of up to 70 words  ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic  , but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (-) is an elongation character used in Arabic script to justify text. It has no syntactic function.", "As for work on Arabic MSA results have been reported on the PATB Kulick Gabbard and Marcus Diab ##CITATION## the Prague Dependency Treebank PADT Buchholz and Marsi Nivre and the CATiB Habash and Roth", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented. The ATB segmentation scheme is one of many alternatives.", "As for work on Arabic MSA results have been reported on the PATB Kulick Gabbard and Marcus Diab ##CITATION## the Prague Dependency Treebank PADT Buchholz and Marsi Nivre and the CATiB Habash and Roth", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations. Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work. We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions. The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship. This paper is based on work supported in part by DARPA through IBM.", "As for work on Arabic MSA results have been reported on the PATB Kulick Gabbard and Marcus Diab ##CITATION## the Prague Dependency Treebank PADT Buchholz and Marsi Nivre and the CATiB Habash and Roth", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design  . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages.", "As for work on Arabic MSA results have been reported on the PATB Kulick Gabbard and Marcus Diab ##CITATION## the Prague Dependency Treebank PADT Buchholz and Marsi Nivre and the CATiB Habash and Roth", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add \u2217n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity. Human evaluation is one way to distinguish between the two cases. Following Dickinson  , we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error. The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus. The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["Our baseline for all sentence lengths is 5.23% F1 higher than the best previous result. The difference is due to more careful S-NOM NP NP NP VP VBG :: b NP restoring NP ADJP NN :: b NP NN NP NP ADJP DTJJ ADJP DTJJ NN :: b NP NP NP ADJP ADJP DTJJ J ..i NN :: b NP NP NP ADJP ADJP DTJJ NN _;\ufffd NP PRP DTJJ DTJJ J ..i _;\ufffd PRP J ..i NN _;\ufffd NP PRP DTJJ NN _;\ufffd NP PRP DTJJ J ..i role its constructive effective (b) Stanford (c) Berkeley (d) Bik el (a) Reference Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmentation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa  . In the ATB, :: b asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun.", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs. We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB  . For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead). Base NPs are the other significant category of nominal phrases.", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75).", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Process nominals name the action of the transitive or ditransitive verb from which they derive. The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case. When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct \ufffd ?f iDafa. Gabbard and Kulick   show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set. Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["But for eign learners are often surprised by the verbless predications that are frequently used in Arabic. Although these are technically nominal, they have become known as \u201cequational\u201d sentences. mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences. We also mark all nodes that dominate an SVO configuration (containsSVO). In MSA, SVO usually appears in non-matrix clauses.", "Recently ##CITATION##   analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short idafa constructions and verbal or equational clauses", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings.", "For better comparison with work of others we adopt the suggestion made by ##CITATION##   to evaluate the parsing quality on sentences up to 70 tokens long", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design  . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages.", "For better comparison with work of others we adopt the suggestion made by ##CITATION##   to evaluate the parsing quality on sentences up to 70 tokens long", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as \ufffd wa and \ufffd fa to link new elements to both preceding clauses and the text as a whole. As a result, Arabic sentences are usually long relative to English, especially after Length English (WSJ) Arabic (ATB) \u2264 20 41.9% 33.7% \u2264 40 92.4% 73.2% \u2264 63 99.7% 92.6% \u2264 70 99.9% 94.9% Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data.", "For better comparison with work of others we adopt the suggestion made by ##CITATION##   to evaluate the parsing quality on sentences up to 70 tokens long", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB  ; CTB6  ; Negra  ; English, sections 221 (train) and section 23 (test).", "For better comparison with work of others we adopt the suggestion made by ##CITATION##   to evaluate the parsing quality on sentences up to 70 tokens long", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions. 7 Unlike Dickinson  , we strip traces and only con-. Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).", "The Arabic grammar features come from ##CITATION##  which contains an ablation study similar to Table 2", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits.", "The Arabic grammar features come from ##CITATION##  which contains an ablation study similar to Table 2", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["For example, we might have VP \u2192 VB NP PP, where the NP is the subject. This annotation choice weakens splitIN. We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley   and Bikel   parsers. All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al.  .", "The Arabic grammar features come from ##CITATION##  which contains an ablation study similar to Table 2", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7). In our grammar, features are realized as annotations to basic category labels. We start with noun features since written Arabic contains a very high proportion of NPs. genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter.", "The Arabic grammar features come from ##CITATION##  which contains an ablation study similar to Table 2", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7). In our grammar, features are realized as annotations to basic category labels. We start with noun features since written Arabic contains a very high proportion of NPs. genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs.", "The Arabic grammar features come from ##CITATION##  which contains an ablation study similar to Table 2", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters. Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings. However, when we pre- tag the input\u2014as is recommended for English\u2014 we notice a 0.57% F1 improvement.", "For Arabic we use the head-finding rules from ##CITATION##  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Various verbal (e.g., \ufffd, .::) and adjectival. suffixes (e.g., \ufffd=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters.", "For Arabic we use the head-finding rules from ##CITATION##  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).", "For Arabic we use the head-finding rules from ##CITATION##  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["First we mark any node that dominates (at any level) a verb sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing (e.g., unaries, base NPs). Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei. 8 We use head-finding rules specified by a native speaker. of Arabic. This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.", "For Arabic we use the head-finding rules from ##CITATION##  ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Various verbal (e.g., \ufffd, .::) and adjectival. suffixes (e.g., \ufffd=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB  ; CTB6  ; Negra  ; English, sections 221 (train) and section 23 (test).", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Each model was able to produce hypotheses for all input sentences. In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.", "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing  ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization  , insensitivity to morphology  , and the effect of variable word order  . Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 20% F1  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["Each model was able to produce hypotheses for all input sentences. In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 20% F1  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus. Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 20% F1  ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 20% F1  ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters. Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings. However, when we pre- tag the input\u2014as is recommended for English\u2014 we notice a 0.57% F1 improvement. We use the log-linear tagger of Toutanova et al.", "We previously showed optimal Berkeley parser   pa- rameterizations for both the Arabic   and French   data sets19 For Arabic our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Various verbal (e.g., \ufffd, .::) and adjectival. suffixes (e.g., \ufffd=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters.", "We previously showed optimal Berkeley parser   pa- rameterizations for both the Arabic   and French   data sets19 For Arabic our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["For example, we might have VP \u2192 VB NP PP, where the NP is the subject. This annotation choice weakens splitIN. We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley   and Bikel   parsers. All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al.  .", "We previously showed optimal Berkeley parser   pa- rameterizations for both the Arabic   and French   data sets19 For Arabic our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design  . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages.", "We previously showed optimal Berkeley parser   pa- rameterizations for both the Arabic   and French   data sets19 For Arabic our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add \u2217n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity. Human evaluation is one way to distinguish between the two cases. Following Dickinson  , we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error. The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus. The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.", "Recently ##CITATION##  analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short Idafa constructions and verbal or equational clauses", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["Our baseline for all sentence lengths is 5.23% F1 higher than the best previous result. The difference is due to more careful S-NOM NP NP NP VP VBG :: b NP restoring NP ADJP NN :: b NP NN NP NP ADJP DTJJ ADJP DTJJ NN :: b NP NP NP ADJP ADJP DTJJ J ..i NN :: b NP NP NP ADJP ADJP DTJJ NN _;\ufffd NP PRP DTJJ DTJJ J ..i _;\ufffd PRP J ..i NN _;\ufffd NP PRP DTJJ NN _;\ufffd NP PRP DTJJ J ..i role its constructive effective (b) Stanford (c) Berkeley (d) Bik el (a) Reference Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmentation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa  . In the ATB, :: b asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun.", "Recently ##CITATION##  analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short Idafa constructions and verbal or equational clauses", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs. We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB  . For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead). Base NPs are the other significant category of nominal phrases.", "Recently ##CITATION##  analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short Idafa constructions and verbal or equational clauses", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design  . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages.", "Recently ##CITATION##  analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar including labels for short Idafa constructions and verbal or equational clauses", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "We allow the parser to produce empty elements by means of lattice-parsing Chappelier et al a general processing community Hall Chappelier et al and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew Goldberg and Tsarfaty Goldberg and Elhadad and Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217. Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed.", "We allow the parser to produce empty elements by means of lattice-parsing Chappelier et al a general processing community Hall Chappelier et al and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew Goldberg and Tsarfaty Goldberg and Elhadad and Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning  . Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "We allow the parser to produce empty elements by means of lattice-parsing Chappelier et al a general processing community Hall Chappelier et al and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew Goldberg and Tsarfaty Goldberg and Elhadad and Arabic ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Less frequently studied is the interplay among language, annotation choices, and parsing model design  . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages.", "We allow the parser to produce empty elements by means of lattice-parsing Chappelier et al a general processing community Hall Chappelier et al and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew Goldberg and Tsarfaty Goldberg and Elhadad and Arabic ##CITATION##", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ. The results clearly indicate increased variation in the ATB relative to the WSJ, but care should be taken in assessing the magnitude of the difference. On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample. At the same time, the n-gram error rate is sensitive to samples with extreme n-gram counts. For example, one of the ATB samples was the determiner -\"\" ; dhalik\u201cthat.\u201d The sample occurred in 1507 corpus po sitions, and we found that the annotations were consistent.", "Recent work has therefore focused on the importance of detecting errors in the treebank ##CITATION## and methods for finding such errors automatically eg", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["It then computes a normalized Levenshtein edit distance between the extracted chain and the reference. The range of the score is between 0 and 1 (higher is better). We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB. Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation. with the number of exactly matching guess trees.", "Recent work has therefore focused on the importance of detecting errors in the treebank ##CITATION## and methods for finding such errors automatically eg", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "Recent work has therefore focused on the importance of detecting errors in the treebank ##CITATION## and methods for finding such errors automatically eg", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3). A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives  . We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c). 3.1 Gross Statistics. Linguistic intuitions like those in the previous section inform language-specific annotation choices.", "Recent work has therefore focused on the importance of detecting errors in the treebank ##CITATION## and methods for finding such errors automatically eg", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217. Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed.", "##CITATION##   discuss annotation consistency in the Penn Arabic Treebank ATB and for our test corpus we follow their discussion and use the same data set the training section of three parts of the ATB Maamouri et al; Maamouri et al Maamouri et al", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning  . Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).", "##CITATION##   discuss annotation consistency in the Penn Arabic Treebank ATB and for our test corpus we follow their discussion and use the same data set the training section of three parts of the ATB Maamouri et al; Maamouri et al Maamouri et al", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives  . The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.", "##CITATION##   discuss annotation consistency in the Penn Arabic Treebank ATB and for our test corpus we follow their discussion and use the same data set the training section of three parts of the ATB Maamouri et al; Maamouri et al Maamouri et al", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (\u00a72). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75).", "##CITATION##   discuss annotation consistency in the Penn Arabic Treebank ATB and for our test corpus we follow their discussion and use the same data set the training section of three parts of the ATB Maamouri et al; Maamouri et al Maamouri et al", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa  . In the ATB, :: b asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.", "Measuring recall is tricky even using the errors identified in ##CITATION##   as \u201cgold\u201d errors", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We start with noun features since written Arabic contains a very high proportion of NPs. genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs. We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB  . For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).", "Measuring recall is tricky even using the errors identified in ##CITATION##   as \u201cgold\u201d errors", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "Measuring recall is tricky even using the errors identified in ##CITATION##   as \u201cgold\u201d errors", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3). A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives  . We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c). 3.1 Gross Statistics. Linguistic intuitions like those in the previous section inform language-specific annotation choices.", "Measuring recall is tricky even using the errors identified in ##CITATION##   as \u201cgold\u201d errors", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "Recently ##CITATION##  demonstrated the effectiveness of lattice-parsing for parsing Arabic", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB  ; CTB6  ; Negra  ; English, sections 221 (train) and section 23 (test).", "Recently ##CITATION##  demonstrated the effectiveness of lattice-parsing for parsing Arabic", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase. All three models evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly. For parsing, the most challenging form of ambiguity occurs at the discourse level. A defining characteristic of MSA is the prevalence of discourse markers to connect and subordinate words and phrases  . Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as \ufffd wa and \ufffd fa to link new elements to both preceding clauses and the text as a whole.", "Recently ##CITATION##  demonstrated the effectiveness of lattice-parsing for parsing Arabic", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "Recently ##CITATION##  demonstrated the effectiveness of lattice-parsing for parsing Arabic", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217.", "Recently ##CITATION##  demonstrated the effectiveness of lattice-parsing for parsing Arabic", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (\u00a74) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits.", "The data was pre-processed with packages from the Stanford Arabic parser ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Various verbal (e.g., \ufffd, .::) and adjectival. suffixes (e.g., \ufffd=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters.", "The data was pre-processed with packages from the Stanford Arabic parser ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser. F1 85 Berkeley 80 Stanford. Bikel 75 training trees 5000 10000 15000 Figure 3: Dev set learning curves for sentence lengths \u2264 70.", "The data was pre-processed with packages from the Stanford Arabic parser ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.", "The data was pre-processed with packages from the Stanford Arabic parser ##CITATION##", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "PAPER"], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "One can either select a segmentation path prior to parsing or as has been recently argued one can let the parser pick a segmentation jointly with decoding Tsarfaty Cohen and Smith Goldberg and Tsarfaty ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["input token, the segmentation is then performed deterministically given the 1-best analysis. Since guess and gold trees may now have different yields, the question of evaluation is complex. Cohen and Smith   chose a metric like SParseval   that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.", "One can either select a segmentation path prior to parsing or as has been recently argued one can let the parser pick a segmentation jointly with decoding Tsarfaty Cohen and Smith Goldberg and Tsarfaty ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented. The ATB segmentation scheme is one of many alternatives.", "One can either select a segmentation path prior to parsing or as has been recently argued one can let the parser pick a segmentation jointly with decoding Tsarfaty Cohen and Smith Goldberg and Tsarfaty ##CITATION##", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "One can either select a segmentation path prior to parsing or as has been recently argued one can let the parser pick a segmentation jointly with decoding Tsarfaty Cohen and Smith Goldberg and Tsarfaty ##CITATION##", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty   Examples for similar phenomena in Arabic may be found in ##CITATION##   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty   Examples for similar phenomena in Arabic may be found in ##CITATION##   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (\u03bc Constituents / \u03bc Length). Evalb, the standard parsing metric, is biased toward such corpora  . Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic. In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance. 3.2 Inter-annotator Agreement.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty   Examples for similar phenomena in Arabic may be found in ##CITATION##   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty   Examples for similar phenomena in Arabic may be found in ##CITATION##   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "PAPER"], ["In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).", "In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context   or jointly with parsing   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions. The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship. This paper is based on work supported in part by DARPA through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.", "In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context   or jointly with parsing   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context   or jointly with parsing   ", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (\u00a75). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.", "In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context   or jointly with parsing   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.", "In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context   or jointly with parsing   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I \u2208 L, O \u2208/ L, each word automaton accepts the language I\u2217(O + I)I\u2217.", "In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context   or jointly with parsing   ", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives  . The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.", "Following ##CITATION##  and others sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["To our knowledge, ours is the first analysis of this kind for Arabic parsing. Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics. Particles are uninflected. Word Head Of Complement POS 1 '01 inna \u201cIndeed, truly\u201d VP Noun VBP 2 '01 anna \u201cThat\u201d SBAR Noun IN 3 01 in \u201cIf\u201d SBAR Verb IN 4 01 an \u201cto\u201d SBAR Verb IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.", "Following ##CITATION##  and others sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1). Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues. Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness. Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably  .", "Following ##CITATION##  and others sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Syntactic Ambiguity in Arabic."], ["All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al.  . Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP \u2192 NP.", "Following ##CITATION##  and others sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).", "Curiously while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems   ##CITATION##   obtain the opposite result in their Arabic parsing experiments with the lattice parser underperforming the pipeline system by over 3 points 7601 F1 vs 7917 F1", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Since these are distinct syntactic units, they are typically segmented. The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsing\u2014including the experiments in the previous section\u2014have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.", "Curiously while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems   ##CITATION##   obtain the opposite result in their Arabic parsing experiments with the lattice parser underperforming the pipeline system by over 3 points 7601 F1 vs 7917 F1", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization  , insensitivity to morphology  , and the effect of variable word order  . Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "Curiously while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems   ##CITATION##   obtain the opposite result in their Arabic parsing experiments with the lattice parser underperforming the pipeline system by over 3 points 7601 F1 vs 7917 F1", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "Curiously while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems   ##CITATION##   obtain the opposite result in their Arabic parsing experiments with the lattice parser underperforming the pipeline system by over 3 points 7601 F1 vs 7917 F1", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Cohen and Smith   chose a metric like SParseval   that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries.", "Qian and Liu   train independent models for word segmentation POS tagging and 13 ##CITATION##   find that using automatic tokenization provided by MADA   instead of gold tokenization results in a 192% F score drop in their constituent parsing work", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1. It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization  , insensitivity to morphology  , and the effect of variable word order  . Certainly these linguistic factors increase the difficulty of syntactic disambiguation.", "Qian and Liu   train independent models for word segmentation POS tagging and 13 ##CITATION##   find that using automatic tokenization provided by MADA   instead of gold tokenization results in a 192% F score drop in their constituent parsing work", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input. In Figure 4 we show an example of variation between the parsing models. We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing.", "Qian and Liu   train independent models for word segmentation POS tagging and 13 ##CITATION##   find that using automatic tokenization provided by MADA   instead of gold tokenization results in a 192% F score drop in their constituent parsing work", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["But we follow the more direct adaptation of Evalb suggested by Tsarfaty  , who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.", "Qian and Liu   train independent models for word segmentation POS tagging and 13 ##CITATION##   find that using automatic tokenization provided by MADA   instead of gold tokenization results in a 192% F score drop in their constituent parsing work", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "##other##"], ["All experiments use ATB parts 1\u20133 divided according to the canonical split suggested by Chiang et al.  . Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP \u2192 NP.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art we provide the constituency data set with most of the pre-processing steps of ##CITATION##  as they were shown to improve baseline performance on the PATB parsing considerably12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford we first discard all trees dominated by X which indicates errors and non-linguistic text", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art we provide the constituency data set with most of the pre-processing steps of ##CITATION##  as they were shown to improve baseline performance on the PATB parsing considerably12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford we first discard all trees dominated by X which indicates errors and non-linguistic text", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus. Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art we provide the constituency data set with most of the pre-processing steps of ##CITATION##  as they were shown to improve baseline performance on the PATB parsing considerably12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford we first discard all trees dominated by X which indicates errors and non-linguistic text", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Conclusion."], ["We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine). To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic  , but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art we provide the constituency data set with most of the pre-processing steps of ##CITATION##  as they were shown to improve baseline performance on the PATB parsing considerably12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford we first discard all trees dominated by X which indicates errors and non-linguistic text", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic  , but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (-) is an elongation character used in Arabic script to justify text.", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art we provide the constituency data set with most of the pre-processing steps of ##CITATION##  as they were shown to improve baseline performance on the PATB parsing considerably12 To convert the original PATB to preprocessed phrase-structure trees \u00e1 la Stanford we first discard all trees dominated by X which indicates errors and non-linguistic text", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["If we remove this sample from the evaluation, then the ATB type-level error rises to only 37.4% while the n-gram error rate increases to 6.24%. The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions. 7 Unlike Dickinson  , we strip traces and only con-. Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).", "We finally remove all traces but unlike ##CITATION##   we keep all function tags", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["We also collapse unary chains withidentical basic categories like NP \u2192 NP. The pre terminal morphological analyses are mapped to the shortened \u201cBies\u201d tags provided with the tree- bank. Finally, we add \u201cDT\u201d to the tags for definite nouns and adjectives  . The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markers\u2014which are consistent only in the vocalized section of the treebank\u2014to differentiate between e.g. \ufffd \u201cthey\u201d and \ufffd + \u201ctheir.\u201d Because we use the vocalized section, we must remove null pronoun markers.", "We finally remove all traces but unlike ##CITATION##   we keep all function tags", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4). The ATB is disadvantaged by having fewer trees with longer average 5 LDC A-E catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1). We map the ATB morphological analyses to the shortened \u201cBies\u201d tags for all experiments. yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (\u03bc Constituents / \u03bc Length). Evalb, the standard parsing metric, is biased toward such corpora  .", "We finally remove all traces but unlike ##CITATION##   we keep all function tags", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], [" . Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP \u2192 NP. The pre terminal morphological analyses are mapped to the shortened \u201cBies\u201d tags provided with the tree- bank.", "We finally remove all traces but unlike ##CITATION##   we keep all function tags", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Standard Parsing Experiments."], ["8 We use head-finding rules specified by a native speaker. of Arabic. This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses. termined by the category of the word that follows it. Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks PATB Maamouri et al:11 the Columbia Arabic Treebank   a dependency treebank and the Stanford version of the PATB   a phrase- structure treebank", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Grammar Development."], ["In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks PATB Maamouri et al:11 the Columbia Arabic Treebank   a dependency treebank and the Stanford version of the PATB   a phrase- structure treebank", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "ABSTRACT"], ["NN \ufffd .e NP NNP NP DTNNP NN \ufffd .e NP NP NNP NP Table 5: Evaluation of 100 randomly sampled variation nuclei types. The samples from each corpus were independently evaluated. The ATB has a much higher fraction of nuclei per tree, and a higher type-level error rate. summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add \u2217n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity. Human evaluation is one way to distinguish between the two cases.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks PATB Maamouri et al:11 the Columbia Arabic Treebank   a dependency treebank and the Stanford version of the PATB   a phrase- structure treebank", 0, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Treebank Comparison."], ["1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations  . To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank   were purposefully borrowed without major modification from English  . Further, Maamouri and Bies   argued that the English guidelines generalize well to other languages. But Arabic contains a variety of linguistic phenomena unseen in English.", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks PATB Maamouri et al:11 the Columbia Arabic Treebank   a dependency treebank and the Stanford version of the PATB   a phrase- structure treebank", 1, "Better Arabic Parsing: Baselines, Evaluations, and Analysis", "Introduction"], ["The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier. Such a classification can be seen as a not-always-correct summary of global features. The secondary classifier in   uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document. We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre.", "In such cases neither global features ##CITATION## nor aggregated contexts ##CITATION## can help", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["  did make use of information from the whole document. However, their system is a hybrid of hand-coded rules and machine learning methods. Another attempt at using global information can be found in  . He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution. Reference resolution involves finding words that co-refer to the same entity.", "In such cases neither global features ##CITATION## nor aggregated contexts ##CITATION## can help", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Related Work."], ["If any of the tokens from to is in Person-Prefix- List, then another feature Person-Prefix is set to 1. Note that we check for , the word preceding the consecutive sequence of initCaps tokens, since person prefixes like Mr., Dr., etc are not part of person names, whereas corporate suffixes like Corp., Inc., etc are part of corporate names. 4.2 Global Features. Context from the whole document can be important in classifying a named entity. A name already mentioned previously in a document may appear in abbreviated form when it is mentioned again later.", "In such cases neither global features ##CITATION## nor aggregated contexts ##CITATION## can help", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "In such cases neither global features ##CITATION## nor aggregated contexts ##CITATION## can help", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.", "In statistical methods the most popular models are Hidden Markov Models HMM Rabiner Maximum Entropy Models ME Chieu et al and Conditional Random Fields CRF Lafferty et al", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borth- wick   successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.", "In statistical methods the most popular models are Hidden Markov Models HMM Rabiner Maximum Entropy Models ME Chieu et al and Conditional Random Fields CRF Lafferty et al", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears. As we will see from Table 3, not much improvement is derived from this feature. The baseline system in Table 3 refers to the maximum entropy system that uses only local features. As each global feature group is added to the list of features, we see improvements to both MUC6 and MUC6 MUC7 Baseline 90.75% 85.22% + ICOC 91.50% 86.24% + CSPP 92.89% 86.96% + ACRO 93.04% 86.99% + SOIC 93.25% 87.22% + UNIQ 93.27% 87.24% Table 3: F-measure after successive addition of each global feature group Table 5: Comparison of results for MUC6 Systems MUC6 MUC7 No. of Articles No.", "In statistical methods the most popular models are Hidden Markov Models HMM Rabiner Maximum Entropy Models ME Chieu et al and Conditional Random Fields CRF Lafferty et al", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "In statistical methods the most popular models are Hidden Markov Models HMM Rabiner Maximum Entropy Models ME Chieu et al and Conditional Random Fields CRF Lafferty et al", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borth- wick   successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree Sekine et al Pailouras et al Hidden Markov Model Zhang et al Zhao maximum entropy ##CITATION## Bender et al and support vector machines Isozaki and Kazawa Takeuchi and Collier MayfieldFrom the linguistic perspective NIL expres sions are rather different from named entities in nature", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent   Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information  . We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).", "The latter is currently dominating in NER amongst which the most popular methods are decision tree Sekine et al Pailouras et al Hidden Markov Model Zhang et al Zhao maximum entropy ##CITATION## Bender et al and support vector machines Isozaki and Kazawa Takeuchi and Collier MayfieldFrom the linguistic perspective NIL expres sions are rather different from named entities in nature", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree Sekine et al Pailouras et al Hidden Markov Model Zhang et al Zhao maximum entropy ##CITATION## Bender et al and support vector machines Isozaki and Kazawa Takeuchi and Collier MayfieldFrom the linguistic perspective NIL expres sions are rather different from named entities in nature", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "The latter is currently dominating in NER amongst which the most popular methods are decision tree Sekine et al Pailouras et al Hidden Markov Model Zhang et al Zhao maximum entropy ##CITATION## Bender et al and support vector machines Isozaki and Kazawa Takeuchi and Collier MayfieldFrom the linguistic perspective NIL expres sions are rather different from named entities in nature", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique). To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise. The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy Zhao Jian Hai Leong Chieu", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy Zhao Jian Hai Leong Chieu", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed. Such constraints are derived from training data, expressing some relationship between features and outcome. The probability distribution that satisfies the above property is the one with the highest entropy. It is unique, agrees with the maximum-likelihood distribution, and has the exponential form  : where refers to the outcome, the history (or context), and is a normalization function. In addition, each feature function is a binary function.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy Zhao Jian Hai Leong Chieu", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["  have also used a maximum entropy classifier that uses already tagged entities to help tag other entities. The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules. We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models  . Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs.", "model\u2019s conditional probability is defined as Another model is Maximum Entropy Zhao Jian Hai Leong Chieu", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1. This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter). Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1. Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task. The importance of dictionaries in NERs has been investigated in the literature  .", "More details of this mixed case NER and its performance are given in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1. 4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.", "More details of this mixed case NER and its performance are given in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document. needs to be in initCaps to be considered for this feature. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.", "More details of this mixed case NER and its performance are given in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "More details of this mixed case NER and its performance are given in ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  .", "They are automatically derived based on the correlation metric value used in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  . This might be because our features are more comprehensive than those used by Borthwick.", "They are automatically derived based on the correlation metric value used in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling  . This is an iterative method that improves the estimation of the parameters at each iteration. We have used the Java-based opennlp maximum entropy package1. In Section 5, we try to compare results of MENE, IdentiFinder, and MENERGI. However, 1 http://maxent.sourceforge.net 3.2 Testing.", "They are automatically derived based on the correlation metric value used in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent   Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information  .", "They are automatically derived based on the correlation metric value used in ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent   Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information  . We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).", "Soder- land   and ##CITATION##  attempted machine learning approaches for a scaled-down version of the ST task where it was assumed that the information needed to fill one template came from one sentence only", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent   Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information  . We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.", "Soder- land   and ##CITATION##  attempted machine learning approaches for a scaled-down version of the ST task where it was assumed that the information needed to fill one template came from one sentence only", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["Mikheev et al.   have also used a maximum entropy classifier that uses already tagged entities to help tag other entities. The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules. We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models  .", "Soder- land   and ##CITATION##  attempted machine learning approaches for a scaled-down version of the ST task where it was assumed that the information needed to fill one template came from one sentence only", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.", "Soder- land   and ##CITATION##  attempted machine learning approaches for a scaled-down version of the ST task where it was assumed that the information needed to fill one template came from one sentence only", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC).", "Soder- land   and ##CITATION##  attempted machine learning approaches for a scaled-down version of the ST task where it was assumed that the information needed to fill one template came from one sentence only", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["Otherwise, it is set to 0. Lexicon Feature: The string of the token is used as a feature. This group contains a large number of features (one for each token string present in the training data). At most one feature in this group will be set to 1. If is seen infrequently during training (less than a small count), then will not be selected as a feature and all features in this group are set to 0.", "##CITATION##  propose a solution to this problem: for each token they define additional features taken from other occurrences of the same token in the document", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1. 4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.", "##CITATION##  propose a solution to this problem: for each token they define additional features taken from other occurrences of the same token in the document", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["On the other hand, if it is seen as McCann Pte. Ltd., then organization will be more probable. With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1. Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.", "##CITATION##  propose a solution to this problem: for each token they define additional features taken from other occurrences of the same token in the document", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "##CITATION##  propose a solution to this problem: for each token they define additional features taken from other occurrences of the same token in the document", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "Borthwick made a second tagging pass which uses information on token sequences tagged in the first pass; ##CITATION## used as features information about features assigned to other instances of the same tokenRecently in Ji and Grishman we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence and used coreference rules to correct and recover some names", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["On the other hand, if it is seen as McCann Pte. Ltd., then organization will be more probable. With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1. Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.", "Borthwick made a second tagging pass which uses information on token sequences tagged in the first pass; ##CITATION## used as features information about features assigned to other instances of the same tokenRecently in Ji and Grishman we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence and used coreference rules to correct and recover some names", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["At most one feature in this group will be set to 1. If is seen infrequently during training (less than a small count), then will not be selected as a feature and all features in this group are set to 0. Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1. If is not initCaps, then (not-initCaps, ) is set to 1. Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1.", "Borthwick made a second tagging pass which uses information on token sequences tagged in the first pass; ##CITATION## used as features information about features assigned to other instances of the same tokenRecently in Ji and Grishman we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence and used coreference rules to correct and recover some names", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "Borthwick made a second tagging pass which uses information on token sequences tagged in the first pass; ##CITATION## used as features information about features assigned to other instances of the same tokenRecently in Ji and Grishman we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence and used coreference rules to correct and recover some names", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "Such global features enhance the performance of NER ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier. Such a classification can be seen as a not-always-correct summary of global features. The secondary classifier in   uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document. We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre.", "Such global features enhance the performance of NER ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["Sequence of Initial Caps (SOIC): In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name. However, it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even. This group of features attempts to capture such information. For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1.", "Such global features enhance the performance of NER ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "Such global features enhance the performance of NER ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["For corporate suffixes, a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data. Frequency is calculated by counting the number of distinct previous tokens that each token has (e.g., if Electric Corp. is seen 3 times, and Manufacturing Corp. is seen 5 times during training, and Corp. is not seen with any other preceding tokens, then the \u201cfrequency\u201d of Corp. is 2). The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List. A Person-Prefix-List is compiled in an analogous way. For MUC6, for example, Corporate- Suffix-List is made up of ltd., associates, inc., co, corp, ltd, inc, committee, institute, commission, university, plc, airlines, co., corp. and Person-Prefix- List is made up of succeeding, mr., rep., mrs., secretary, sen., says, minister, dr., chairman, ms. . For a token that is in a consecutive sequence of init then a feature Corporate-Suffix is set to 1.", "Useful Unigrams UNI For each name class words that precede the name class are ranked using correlation metric ##CITATION## and the top 20 are compiled into a list", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["If they are found in a list, then a feature for that list will be set to 1. For example, if Barry is not in commonWords and is found in the list of person first names, then the feature PersonFirstName will be set to 1. Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1. For example, if is found in the list of person first names, the feature PersonFirstName is set to 1. Month Names, Days of the Week, and Numbers: If is initCaps and is one of January, February, . . .", "Useful Unigrams UNI For each name class words that precede the name class are ranked using correlation metric ##CITATION## and the top 20 are compiled into a list", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens. This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training. Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones). The zone to which a token belongs is used as a feature.", "Useful Unigrams UNI For each name class words that precede the name class are ranked using correlation metric ##CITATION## and the top 20 are compiled into a list", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The importance of dictionaries in NERs has been investigated in the literature  . The sources of our dictionaries are listed in Table 2. For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.", "Useful Unigrams UNI For each name class words that precede the name class are ranked using correlation metric ##CITATION## and the top 20 are compiled into a list", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The sources of our dictionaries are listed in Table 2. For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. A list of words occurring more than 10 times in the training data is also collected (commonWords).", "Useful Unigrams UNI For each name class words that precede the name class are ranked using correlation metric ##CITATION## and the top 20 are compiled into a list", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. A list of words occurring more than 10 times in the training data is also collected (commonWords). Only tokens with initCaps not found in commonWords are tested against each list in Table 2.", "Useful Unigrams UNI For each name class words that precede the name class are ranked using correlation metric ##CITATION## and the top 20 are compiled into a list", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used. We group the features used into feature groups.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used. We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borth- wick   successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Conclusion."], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  .", "Token Information These features are based on the string w such as contains-digits contains-dollar-sign etc ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens. This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training. Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones). The zone to which a token belongs is used as a feature.", "Token Information These features are based on the string w such as contains-digits contains-dollar-sign etc ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["If the token is the first word of a sentence, then this feature is set to 1. Otherwise, it is set to 0. Lexicon Feature: The string of the token is used as a feature. This group contains a large number of features (one for each token string present in the training data). At most one feature in this group will be set to 1.", "Token Information These features are based on the string w such as contains-digits contains-dollar-sign etc ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "Token Information These features are based on the string w such as contains-digits contains-dollar-sign etc ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  . This might be because our features are more comprehensive than those used by Borthwick.", "In this paper w\u2212i refers to the ith word before w and w+i refers to the ith word after w The features used are similar to those used in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["  do have some statistics that show how IdentiFinder performs when the training data is reduced. Our results show that MENERGI performs as well as IdentiFinder when trained on comparable amount of training data. The system described in this paper is similar to the MENE system of  . It uses a maximum entropy framework and classifies each word given its features. Each name class is subdivided into 4 sub-classes, i.e., N begin, N continue, N end, and N unique.", "In this paper w\u2212i refers to the ith word before w and w+i refers to the ith word after w The features used are similar to those used in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document. needs to be in initCaps to be considered for this feature. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.", "In this paper w\u2212i refers to the ith word before w and w+i refers to the ith word after w The features used are similar to those used in ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.", "In this paper w\u2212i refers to the ith word before w and w+i refers to the ith word after w The features used are similar to those used in ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  .", "In this paper w\u2212i refers to the ith word before w and w+i refers to the ith word after w The features used are similar to those used in ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "In this paper w\u2212i refers to the ith word before w and w+i refers to the ith word after w The features used are similar to those used in ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution  . Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results. The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier. Such a classification can be seen as a not-always-correct summary of global features.", "##CITATION## used global information such as the occurrence of the same word with other capitalisation in the same document ##CITATION## and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Experimental Results."], ["By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework. The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors). These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task  .", "##CITATION## used global information such as the occurrence of the same word with other capitalisation in the same document ##CITATION## and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent   Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information  . We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.", "##CITATION## used global information such as the occurrence of the same word with other capitalisation in the same document ##CITATION## and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all ##CITATION##", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["Sentence (2) and (3) help to disambiguate one way or the other. If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided. The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. For example, in the sentence that starts with \u201cBush put a freeze on . . .", "##CITATION## used global information such as the occurrence of the same word with other capitalisation in the same document ##CITATION## and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided. The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. For example, in the sentence that starts with \u201cBush put a freeze on . . . \u201d, because Bush is the first word, the initial caps might be due to its position (as in \u201cThey put a freeze on . . .", "##CITATION## used global information such as the occurrence of the same word with other capitalisation in the same document ##CITATION## and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all ##CITATION##", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "A wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Bikel et al Maximum Entropy methods Borthwick et al ##CITATION## Decision Trees Sekine et al Conditional Random Fields McCallum and Li Class-based Language Model Sun et al Agent-based Approach Ye et al and Support Vector Machines", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["Mikheev et al.   did make use of information from the whole document. However, their system is a hybrid of hand-coded rules and machine learning methods. Another attempt at using global information can be found in  . He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution.", "A wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Bikel et al Maximum Entropy methods Borthwick et al ##CITATION## Decision Trees Sekine et al Conditional Random Fields McCallum and Li Class-based Language Model Sun et al Agent-based Approach Ye et al and Support Vector Machines", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Related Work."], ["This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.", "A wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Bikel et al Maximum Entropy methods Borthwick et al ##CITATION## Decision Trees Sekine et al Conditional Random Fields McCallum and Li Class-based Language Model Sun et al Agent-based Approach Ye et al and Support Vector Machines", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework. The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors).", "A wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Bikel et al Maximum Entropy methods Borthwick et al ##CITATION## Decision Trees Sekine et al Conditional Random Fields McCallum and Li Class-based Language Model Sun et al Agent-based Approach Ye et al and Support Vector Machines", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Introduction"], ["A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass   or have used as features information about features assigned to other instances of the same token  ", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training. Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones). The zone to which a token belongs is used as a feature. For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD). Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass   or have used as features information about features assigned to other instances of the same token  ", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["On the other hand, if it is seen as McCann Pte. Ltd., then organization will be more probable. With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1. Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass   or have used as features information about features assigned to other instances of the same token  ", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder   or MENE  . However, to classify a token , while Borthwick uses tokens from to  .", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass   or have used as features information about features assigned to other instances of the same token  ", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["The zone to which a token belongs is used as a feature. For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD). Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0. Case and Zone: If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1. If it is made up of all capital letters, then (allCaps, zone) is set to 1.", "To develop UWI there are three approaches: 1 Statistical approach researchers use common statistical features such as maximum entropy Chieu et al association strength mutual information ambiguous matching and multi-statistical features for unknown word detection and extraction; 2 Linguistic approach three major types of linguistic rules knowledge: morphology syntax and semantics are used to identify unknown words; and 3 Hybrid approach recently one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique). To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise. The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability. The features we used can be divided into 2 classes: local and global.", "To develop UWI there are three approaches: 1 Statistical approach researchers use common statistical features such as maximum entropy Chieu et al association strength mutual information ambiguous matching and multi-statistical features for unknown word detection and extraction; 2 Linguistic approach three major types of linguistic rules knowledge: morphology syntax and semantics are used to identify unknown words; and 3 Hybrid approach recently one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "System Description."], ["This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1. Otherwise, it is set to 0.", "To develop UWI there are three approaches: 1 Statistical approach researchers use common statistical features such as maximum entropy Chieu et al association strength mutual information ambiguous matching and multi-statistical features for unknown word detection and extraction; 2 Linguistic approach three major types of linguistic rules knowledge: morphology syntax and semantics are used to identify unknown words; and 3 Hybrid approach recently one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches", 0, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "Feature Description."], ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.", "To develop UWI there are three approaches: 1 Statistical approach researchers use common statistical features such as maximum entropy Chieu et al association strength mutual information ambiguous matching and multi-statistical features for unknown word detection and extraction; 2 Linguistic approach three major types of linguistic rules knowledge: morphology syntax and semantics are used to identify unknown words; and 3 Hybrid approach recently one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches", 1, "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "ABSTRACT"], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation. Most natural languages construct words by concatenating morphemes together in strict orders.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In other applications, it may be useful to design the lexical strings to contain the traditional dictionary citation form, together with linguist-selected \u201ctag\u201d sym Analysis Strings Regular Expression Compiler F S T Word Strings Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages bols like +Noun, +Verb, +SG, +PL, that convey category, person, number, tense, mood, case, etc. Thus the lexical string representing paie, the first-person singular, present indicative form of the French verb payer (\u201cto pay\u201d), might be spelled payer+IndP+SG+P1+Verb. The tag symbols are stored and manipulated just like alphabetic symbols, but they have multicharacter print names. If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally. Following convention, we will often refer to the upper projection of the fst, representing analyses, as the lexical language, a set of lexical strings; and we will refer to the lower projection as the surface language, consisting of surface strings. There are compelling advantages to computing with such finite-state machines, including mathematical elegance, flexibility, and for most natural-language applications, high efficiency and data-compaction.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 1, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "Thus we employ the com\u00ad pile-replace feature in xfst ##CITATION## This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication ", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine. It expects to find delimited regular expression substrings on a given side (upper or After the special treatment of the regular- expression path is finished, normal processing is resumed in the destination state of the closing ^] arc. For example, the result shown in Figure 9 represents the crossproduct of the two networks shown in Figure 10. a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced. In the linguistic applications presented Lexical: b a g i +Noun +Plural Surface: ^[ { b a g i } ^ 2 ^] Lexical: p e l a b u h a n +Noun +Plural Surface: ^[ { p e l a b u h a n } ^ 2 ^] Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation in the next sections, the two sides of a regular- expression path contain different strings. The upper side contains morphological information; the regular-expression operators appear only on the lower side and are not present in the final result.", "Thus we employ the com\u00ad pile-replace feature in xfst ##CITATION## This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication ", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["However, the order in which the merge operations are performed is irrelevant in this case because the two filler languages do not provide competing instantiations for the same class symbols. 3.2.3 Merging Roots and Vocalizations with Templates Following the tradition, we can represent the lexical forms of Arabic stems as consisting of three components, a consonantal root, a CV template and a vocalization, possibly preceded and followed by additional affixes. In contrast to McCarthy, Kay, and Kiraz, we combine the three components into a single projection. In a sense, McCarthy\u2019s three tiers are conflated into a single one with three distinct parts. In our opinion, there is no substantive difference from a computational point of view.", "Thus we employ the com\u00ad pile-replace feature in xfst ##CITATION## This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication ", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics. The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific", "Thus we employ the com\u00ad pile-replace feature in xfst ##CITATION## This feature allows the repetition of arbi\u00ad trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re\u00ad duplication ", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation. Most natural languages construct words by concatenating morphemes together in strict orders.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm. The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions. The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation. To take a simple non-linguistic example, Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular- expression substring.Figure 9: After the Application of Compile Replace lower) of the network. Until an opening delimiter ^[ is encountered, the algorithm constructs a copy of the path it is following.", "##CITATION##  describe a technique called compile-replace for constructing FSTs which involves reapplying the regular-expression compiler to its own output", 1, "Finite-State Non-Concatenative Morphotactics", "Compile-Replace."], ["To take a simple non-linguistic example, Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular- expression substring.Figure 9: After the Application of Compile Replace lower) of the network. Until an opening delimiter ^[ is encountered, the algorithm constructs a copy of the path it is following. If the network contains no regular-expression substrings, the result will be a copy of the original network. When a ^[ is encountered, the algorithm looks for a closing ^] and extracts the path between the delimiters to be handled in a special way: 1. The symbols along the indicated side of the.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in ##CITATION## ", 0, "Finite-State Non-Concatenative Morphotactics", "Compile-Replace."], ["For example, if A is a regular expression denoting a language or a relation, A* denotes zero or more and A+ denotes one or more concatenations of A with itself. There are also operators that express a fixed number of concatenations. In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings. The reduplication of any string w can then be notated as {w}^2, and we start by defining a network where the lower-side strings are built by simple concatenation of a prefix ^[, a root enclosed in braces, and an overt-plural suffix ^2 followed by the closing ^]. Figure 11 shows the paths for two Malay plurals in the initial network.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in ##CITATION## ", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In Arabic, for example, prefixes and suffixes attach to stems in the usual concatenative way, but stems themselves are formed by a process known informally as interdigitation; while in Malay, noun plurals are formed by a process known as full-stem reduplication. Although Arabic and Malay also include prefixation and suffixation that are modeled straightforwardly by concatenation, a complete lexicon cannot be a a:0 *:a *:0 *:0 0:a obtained without non-concatenative processes. We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm. The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions. The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in ##CITATION## ", 0, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["We will show a simple and elegant way to do this with strictly finite-state operations. To understand the general solution to full- stem reduplication using the compile-replace algorithm requires a bit of background. In the regular expression calculus there are several operators that involve concatenation. For example, if A is a regular expression denoting a language or a relation, A* denotes zero or more and A+ denotes one or more concatenations of A with itself. There are also operators that express a fixed number of concatenations.", "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as de\ufb01ned in ##CITATION## ", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In effect, Condition 2 preserves any template arc that does not find a match. In that case, the path in the template network advances to a new state while the path in the filler network stays at the current state. We use the networks in Figure 13 to illustrate the effect of the merge algorithm. Figure 13 shows a linear template network and two filler networks, one of which is cyclic. C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm ##CITATION##11 and the result is shown in Figure 7", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["resenting the origin and the destination of 0:^[ a * 0:^] the regular-expression path. Figure 8: A Network with a Regular-Expression Substring on the Lower Side The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation. The network created by the operation is shown in Figure 9. When applied in the \u201cupward\u201d direction, the transducer in Figure 9 maps any string of the infinite a* language into the regular expression from which the language was compiled. The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm ##CITATION##11 and the result is shown in Figure 7", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We use the networks in Figure 13 to illustrate the effect of the merge algorithm. Figure 13 shows a linear template network and two filler networks, one of which is cyclic. C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14. The three symbols of the filler string are instantiated in the three consonant slots in the CVVCVC template. d V V r V s Figure 14: Intermediate Result.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm ##CITATION##11 and the result is shown in Figure 7", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Following the lines of Kataja and Koskenniemi  , we could define intermediate networks with regular-expression substrings that indicate the intersection of suitably encoded roots, templates, and vocalizations (for a formal description of what such regular-expression substrings would look like, see Beesley (1998c; 1998b)). However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge. 3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one. The strings of the filler language consist of ordinary symbols such as d, r, s, u, i. The template expressions may contain special class symbols such as C (= consonant) or V (= vowel) that represent a predefined set of ordinary symbols. The objective of the merge operation is to align the template strings with the filler strings and to instantiate the class symbols of the template as the matching filler symbols.", "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm ##CITATION##11 and the result is shown in Figure 7", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation. Most natural languages construct words by concatenating morphemes together in strict orders.", "##CITATION##  described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.", "##CITATION##  described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["We shall argue that the morphotactic limitations of the traditional implementations are the direct result of relying solely on the concatenation operation in morphotactic description. We describe a technique, within the Xerox implementation of finite-state morphology, that corrects the limitations at the source, going beyond concatenation to allow the full range of finite-state operations to be used in morphotac- tic description. Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network. This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below. Before illustrating these applications, we will first outline our general approach to finite-state morphology.", "##CITATION##  described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output", 0, "Finite-State Non-Concatenative Morphotactics", "Introduction"], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "##CITATION##  described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator ##CITATION## Beesley", 0, "Finite-State Non-Concatenative Morphotactics", "ABSTRACT"], ["Similarly, the root ktb can combine with template CVVCVC and ui to produce kuutib, the root drs can combine with CVCVC and ui to form duris, and so forth. tiers of McCarthy   as projections of a multi-level transducer and wrote a small Prolog- based prototype that handled the interdigitation of roots, CV-templates and vocalizations into abstract Arabic stems; this general approach, with multi-tape transducers, has been explored and extended by Kiraz in several papers (1994a; 1996; 1994b; 2000) with respect to Syriac and Arabic. The implementation is described in Kiraz and GrimleyEvans  . In work more directly related to the current solution, it was Kataja and Koskenniemi   who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages. Thus Akkadian words were formalized as consisting of morphemes, some of which were combined together by intersection and others of which were combined via concatenation.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator ##CITATION## Beesley", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Lexical: uta+ma+naka+p+xa+samacha-i+wa Surface: uta ma n ka p xa samach i wa uta = house (root) +ma = 2nd person possessive +na = in -ka = (locative, verbalizer) +p = plural +xa = perfect aspect +samacha = \"apparently\" -i = 3rd person +wa = topic marker Figure 1: Aymara: utamankapxasamachiwa = \u201dit appears that they are in your house\u201d Lexical: Paris+mut+nngau+juma+niraq+lauq+sima+nngit+junga Surface: Pari mu nngau juma nira lauq sima nngit tunga Paris = (root = Paris) +mut = terminalis case ending +nngau = go (verbalizer) +juma = want +niraq = declare (that) +lauq = past +sima = (added to -lauq- indicates \"distant past\") +nngit = negative +junga = 1st person sing. present indic (nonspecific) Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = \u201cI never said I wanted to go to Paris\u201d 2.1 Analysis and Generation. In the most theory- and implementation-neutral form, morphological analysis and generation of written words can be modeled as a relation between the words themselves and analyses of those words. Computationally, as shown in Figure 3, a black-box module maps from words to analyses to effect Analysis, and from analyses to words to effect Generation. ANALYSES ANALYZER/ GENERATOR WORDS Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words The basic claim or hope of the finite-state approach to natural-language morphology is that relations like that represented in Figure 3 are in fact regular relations, i.e. relations between two regular languages.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator ##CITATION## Beesley", 0, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["This was the key insight: morphotactic description could employ various finite-state operations, not just concatenation; and languages that required only concatenation were just special cases. By extension, the widely noticed limitations of early finite-state implementations in dealing with non-concatenative morphotactics could be traced to their dependence on the concatenation operation in morphotactic descriptions. This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime  , and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime. Kay   reformalized the autosegmental 5 These patterns combine what McCarthy  . issue here due to lack of space.", "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator ##CITATION## Beesley", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.", "In the last decade finite-state approaches to phonology Gildea and Jurafsky ##CITATION## have effectively brought theoretical linguistic work on rewrite rules into the computational realm", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The corresponding initial surface form is a regular-expression substring, containing two merge operators, that will be compiled and replaced by the interdigitated surface form. The application of the compile-replace operation to the lower side of the initial lexicon yields a transducer that maps the Arabic interdigitated forms directly into their corresponding tripartite analyses and vice versa, as illustrated in Figure 17. Alternation rules are subsequently composed on the lower side of the result to map the in- terdigitated, but still morphophonemic, strings into real surface strings. Although many Arabic templates are widely considered to be pure CV-patterns, it has been argued that certain templates also contain Lexical: k t b =Root C V C V C =Template a + =Voc Surface: k a t a b Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: k u t i b Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: d u u r i s Figure 17: After Applying Compile-Replace to the Lower Side \u201chard-wired\u201d specific vowels and consonants.6 For example, the so-called \u201cFormVIII\u201d template is considered, by some linguists, to contain an embedded t: CtVCVC. The presence of ordinary symbols in the template does not pose any problem for the analysis adopted here.", "In the last decade finite-state approaches to phonology Gildea and Jurafsky ##CITATION## have effectively brought theoretical linguistic work on rewrite rules into the computational realm", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation. An example of interdigitation occurs with the Arabic stem katab, which means \u201cwrote\u201d. According to an influential autosegmental analysis  , this stem consists of an all-consonant root ktb whose general meaning has to do with writing, an abstract consonant-vowel template CVCVC, and a voweling or vocalization that he symbolized simply as a, signifying perfect aspect and active voice.", "In the last decade finite-state approaches to phonology Gildea and Jurafsky ##CITATION## have effectively brought theoretical linguistic work on rewrite rules into the computational realm", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["In the standard notation, the path in Figure 7 is labeled as b i g 0:g +Adj:0 0:e +Comp:r. Lexical transducers are more efficient for analysis and generation than the classical two- level systems   because the morphotactics and the morphological alternations have been precompiled and need not be consulted at runtime. But it would be possible in principle, and perhaps advantageous for some purposes, to view the regular expressions defining the morphology of a language as an un- compiled \u201cvirtual network\u201d. All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime. Most languages build words by simply stringing morphemes (prefixes, roots and suffixes) b i g b i g 0 +Adj g 0 0 +Comp e r together in strict orders. The morphotactic (word building) processes of prefixation and suffixation can be straightforwardly Surface side: Figure 7: A Path in a Transducer for English the epenthetical e in the surface form bigger result from the composition of the original lexicon fst with the rule fst representing the regular morphological alternations in English.", "In the last decade finite-state approaches to phonology Gildea and Jurafsky ##CITATION## have effectively brought theoretical linguistic work on rewrite rules into the computational realm", 1, "Finite-State Non-Concatenative Morphotactics", "Finite-State  Morphology."], ["would call templates and vocalizations. The 1996 algorithm that intersected roots and patterns into stems, and substituted the original roots and patterns on just the lower side with the intersected stem, was admittedly rather ad hoc and computationally intensive, taking over two hours to handle about 90,000 stems on a SUN Ultra workstation. The compile-replace algorithm is a vast improvement in both generality and efficiency, producing the same result in a few minutes. Following the lines of Kataja and Koskenniemi  , we could define intermediate networks with regular-expression substrings that indicate the intersection of suitably encoded roots, templates, and vocalizations (for a formal description of what such regular-expression substrings would look like, see Beesley (1998c; 1998b)). However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge.", "The interdigitation is handled using a compile-replace process using the replace operator Karttunen and Beesley Karttunen", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["The network created by the operation is shown in Figure 9. When applied in the \u201cupward\u201d direction, the transducer in Figure 9 maps any string of the infinite a* language into the regular expression from which the language was compiled. The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine. It expects to find delimited regular expression substrings on a given side (upper or After the special treatment of the regular- expression path is finished, normal processing is resumed in the destination state of the closing ^] arc. For example, the result shown in Figure 9 represents the crossproduct of the two networks shown in Figure 10. a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced.", "The interdigitation is handled using a compile-replace process using the replace operator Karttunen and Beesley Karttunen", 0, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["This can be done in three steps. We first apply the technique used for Malay reduplication. That is, we redefine L as \"^[\" \"[\" L XX \"]\" \"^\" 2 \"^]\", and apply the compile-replace operation. At this point the lower-side of L contains strings such as dogXXdogXX and madamXXmadamXX where XX is a specially introduced symbol to mark the middle (and the end) of each string. The next, and somewhat delicate, step is to replace the XX markers by the desired operators, intersection and reverse, and to wrap the special regular expression delimiters ^[ and ^] around the whole lexicon.", "The interdigitation is handled using a compile-replace process using the replace operator Karttunen and Beesley Karttunen", 0, "Finite-State Non-Concatenative Morphotactics", "See Beesley (1998c) for a discussion of this contro-."], ["Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics. The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific versial issue. 7 http://www.x rce.xerox.com /research/mlt t/arabic/ morphotactic problems we have discussed.", "The interdigitation is handled using a compile-replace process using the replace operator Karttunen and Beesley Karttunen", 1, "Finite-State Non-Concatenative Morphotactics", "The result is spliced between the states rep-."], ["Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept. cor e\u00a3. y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect. par t. y e s y e s y e s y e s y e s ye s ye s ye s y e s y e s y e s y e s y e s y e s y e s ind. acti on loc at. inv ers.", "Levin's classification has been extended by other NLP researchers Dorr and Jones ##CITATION## <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete. We will be using re\u00ad sources such as dictionaries and online corpora to investigate potential additional members of our classes. Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept. cor e\u00a3. y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect.", "Levin's classification has been extended by other NLP researchers Dorr and Jones ##CITATION## <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "Levin's classification has been extended by other NLP researchers Dorr and Jones ##CITATION## <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "Levin's classification has been extended by other NLP researchers Dorr and Jones ##CITATION## <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.", "##CITATION##   modify it by adding new classes which remove the overlap between classes from the original scheme ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..\u2022 ,c} such that S C S' and v E ci n ... n dm (subset criterion). Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to.", "##CITATION##   modify it by adding new classes which remove the overlap between classes from the original scheme ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["For in\u00ad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\u00ad tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "##CITATION##   modify it by adding new classes which remove the overlap between classes from the original scheme ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "##CITATION##   modify it by adding new classes which remove the overlap between classes from the original scheme ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  . Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch).", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by Dang ct al", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["\"Meander Verbs\" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes. The Portuguese verbs we examined behaved much more similarly to their English counter\u00ad parts than we expected. Many of the verbs participate in alternations that are direct trans\u00ad lations of the English alternations. However, there are some interesting differences in which sense extensions are allowed. 4.1 Similar sense extensions.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by Dang ct al", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["For example, flutuar does not take a direct object, so some of the alterna\u00ad tions that are related to its transitive meaning are not present. For these verbs, we have the in\u00ad duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat). As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative. The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by Dang ct al", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "\\tVe think that many cases of am\u00ad biguous classification of verb types can be ad\u00ad dressed with the notion of intersedive sets in\u00ad troduced by Dang ct al", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily. However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut.", "Many verbs are listed in multiple classes some of which have conflicting sets of syntactic frames Dang ct al  showed that multiple listings could in some cases be interpreted as regular sense extensions and defined intcrsectivc Levin classes which are a more fine-grained syntactically and semantically coher\u00ad ent refinement of basic Levin classes  ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  .", "Many verbs are listed in multiple classes some of which have conflicting sets of syntactic frames Dang ct al  showed that multiple listings could in some cases be interpreted as regular sense extensions and defined intcrsectivc Levin classes which are a more fine-grained syntactically and semantically coher\u00ad ent refinement of basic Levin classes  ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "Many verbs are listed in multiple classes some of which have conflicting sets of syntactic frames Dang ct al  showed that multiple listings could in some cases be interpreted as regular sense extensions and defined intcrsectivc Levin classes which are a more fine-grained syntactically and semantically coher\u00ad ent refinement of basic Levin classes  ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.", "Many verbs are listed in multiple classes some of which have conflicting sets of syntactic frames Dang ct al  showed that multiple listings could in some cases be interpreted as regular sense extensions and defined intcrsectivc Levin classes which are a more fine-grained syntactically and semantically coher\u00ad ent refinement of basic Levin classes  ", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "Many verbs are listed in multiple classes some of which have conflicting sets of syntactic frames Dang ct al  showed that multiple listings could in some cases be interpreted as regular sense extensions and defined intcrsectivc Levin classes which are a more fine-grained syntactically and semantically coher\u00ad ent refinement of basic Levin classes  ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "VN is built on a refinement of the Levin classes the intersective Levin classes ##CITATION## aimed at achieving more coherent classes both semantically and syntactically", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Even though the Levin verb classes are defined by their syntactic behavior, many reflect seman\u00ad tic distinctions made by WordNet, a classifica\u00ad tion hierarchy defined in terms of purely se\u00ad mantic word relations (synonyms, hypernyms, etc.). When examining in detail the intersec\u00ad tive classes just described, which emphasize not only the individual classes, but also their rela\u00ad tion to other classes, we see a rich semantic lat\u00ad tice much like WordNet. This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs. The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs. WordNet distinguishes two subclasses of cut, differentiated by the type of result: 1.", "VN is built on a refinement of the Levin classes the intersective Levin classes ##CITATION## aimed at achieving more coherent classes both semantically and syntactically", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut. In addition, cut verbs can oc\u00ad cur in the conative, John valiantly cut/hacked at the frozen loaf, but his knife was too dull to make a dent in it, whereas break verbs cannot, *John broke at the window. The explanation given is that cut describes a series of actions di\u00ad rected at achieving the goal of separating some object into pieces. It is possible for these ac\u00ad tions to be performed without the end result being achieved, but where the cutting manner can still be recognized, i.e., John cut at the loaf. Where break is concerned, the only thing speci\u00ad fied is the resulting change of state where the object becomes separated into pieces.", "VN is built on a refinement of the Levin classes the intersective Levin classes ##CITATION## aimed at achieving more coherent classes both semantically and syntactically", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.", "VN is built on a refinement of the Levin classes the intersective Levin classes ##CITATION## aimed at achieving more coherent classes both semantically and syntactically", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  .", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes [##CITATION## 98 00 Kipper et al 00]", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily. However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes [##CITATION## 98 00 Kipper et al 00]", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes [##CITATION## 98 00 Kipper et al 00]", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy  ,  ,  . However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap be\u00ad tween Levin and WordNet. The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.", "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes [##CITATION## 98 00 Kipper et al 00]", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["For example, flutuar does not take a direct object, so some of the alterna\u00ad tions that are related to its transitive meaning are not present. For these verbs, we have the in\u00ad duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat). As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative. The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Where break is concerned, the only thing speci\u00ad fied is the resulting change of state where the object becomes separated into pieces. If the result is not achieved, there are no attempted breaking actions that can still be recognized. 2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy  ,  ,  . However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by ##CITATION##", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "We think that many cases of ambigu\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by ##CITATION##", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.", "Palmer  and ##CITATION##  argue that syntactic frames and verb classes are useful for developing principled classifications of verbs ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Palmer  and ##CITATION##  argue that syntactic frames and verb classes are useful for developing principled classifications of verbs ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Verbs in the slide/roll/run intersec\u00ad tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actu\u00ad ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley). These verbs are listed in the intersective classes with meander verbs of existence. \"Meander Verbs\" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes. The Portuguese verbs we examined behaved much more similarly to their English counter\u00ad parts than we expected. Many of the verbs participate in alternations that are direct trans\u00ad lations of the English alternations.", "Palmer  and ##CITATION##  argue that syntactic frames and verb classes are useful for developing principled classifications of verbs ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy  ,  ,  . However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap be\u00ad tween Levin and WordNet. The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.", "Palmer  and ##CITATION##  argue that syntactic frames and verb classes are useful for developing principled classifications of verbs ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..\u2022 ,c} such that S C S' and v E ci n ... n dm (subset criterion). Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to.", "In many cases the additional information that VerbNet provides for each class has caused it to subdivide or use intersections of Levin\u2019s original classes adding an additional level to the hierarchy ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["For in\u00ad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\u00ad tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "In many cases the additional information that VerbNet provides for each class has caused it to subdivide or use intersections of Levin\u2019s original classes adding an additional level to the hierarchy ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["\u2022 Although kick is not listed as a verb of exerting force, it displays all the alternations that define this class. Sim\u00ad ilarly, draw and yank can be viewed as carry verbs al\u00ad though they are not listed as such. The list of members for each Levin verb class is not always complete, so to check if a particular verb belongs to a class it is better to check that the verb exhibits all the alternations that de\u00ad fine the class. Since intersective classes were built using membership lists rather than the set of defining alterna\u00ad tions, they were similarly incomplete. This is an obvious shortcoming of the current implementation of intersec\u00ad tive classes, and might affect the choice of 3 as a relevance cutoff in later implementations.", "In many cases the additional information that VerbNet provides for each class has caused it to subdivide or use intersections of Levin\u2019s original classes adding an additional level to the hierarchy ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  .", "In many cases the additional information that VerbNet provides for each class has caused it to subdivide or use intersections of Levin\u2019s original classes adding an additional level to the hierarchy ##CITATION##", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  . Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch). The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise. These fringe split verbs appear in several other inter\u00ad sective classes that highlight the force aspect of their meaning.", "participants \u2014 causation change of state and others \u2014 are central not only in theoretical work on lexical semantics but in computational approaches to the lexicon as well eg Pustejovsky Dorr Wu and Palmer ##CITATION## ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes. The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection. In addition, only one verb (pull) has a WordNet sense cor\u00ad responding to the change of state - separation semantic component associated with the split class. The fact that the split sense for these verbs does not appear explicitly in WordNet is not surprising since it is only an extended sense of the verbs, and separation is inferred only when the verb occurs with an appropriate adjunct, such as apart. However, apart can also be used with other classes of verbs, including many verbs of motion.", "participants \u2014 causation change of state and others \u2014 are central not only in theoretical work on lexical semantics but in computational approaches to the lexicon as well eg Pustejovsky Dorr Wu and Palmer ##CITATION## ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "participants \u2014 causation change of state and others \u2014 are central not only in theoretical work on lexical semantics but in computational approaches to the lexicon as well eg Pustejovsky Dorr Wu and Palmer ##CITATION## ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular syn\u00ad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch). The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise. These fringe split verbs appear in several other inter\u00ad sective classes that highlight the force aspect of their meaning. Figure 2 depicts the intersection of split, carry and push/pull.", "participants \u2014 causation change of state and others \u2014 are central not only in theoretical work on lexical semantics but in computational approaches to the lexicon as well eg Pustejovsky Dorr Wu and Palmer ##CITATION## ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["However the simultaneous potential of mutually exclusive extensions is not a problem. It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative. The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion. Thus they cannot take the conative al\u00ad ternation. 3.2 Comparisons to WordNet.", "participants \u2014 causation change of state and others \u2014 are central not only in theoretical work on lexical semantics but in computational approaches to the lexicon as well eg Pustejovsky Dorr Wu and Palmer ##CITATION## ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics eg Jack- endoff Levin Pinker ##CITATION## Dorr Merlo and Stevenson ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics eg Jack- endoff Levin Pinker ##CITATION## Dorr Merlo and Stevenson ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["In the absence of a goal or path adjunct they do not specify any direction of motion, and in some cases (e.g., float, bounce) require the adjunct to explicitly specify any dis\u00ad placement at all. The two classes differ in that roll verbs relate to manners of motion charac\u00ad teristic of inanimate entities, while run verbs describe manners in which animate entities can move. Some manner of motion verbs allow a transitive alternation in addition to the basic in\u00ad transitive. When a roll verb occurs in the tran\u00ad sitive (Bill moved the box across the room), the subject physically causes the object to move, whereas the subject of a transitive run verb merely induces the object to move (the coach ran the athlete around the track). Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics eg Jack- endoff Levin Pinker ##CITATION## Dorr Merlo and Stevenson ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics eg Jack- endoff Levin Pinker ##CITATION## Dorr Merlo and Stevenson ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in\u00ad strument as an immediate hypernym. This distinction appears in the second-order Levin classes as membership vs. nonmember\u00ad ship in the intersective class with split. Levin verb classes are based on an underlying lat\u00ad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations. Whereas high level semantic relations (syn\u00ad onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class. However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification called Intersective Levin classes ILCs ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification called Intersective Levin classes ILCs ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Figure 2 depicts the intersection of split, carry and push/pull. Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo\u00ad nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion. Depending on the par\u00ad ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo\u00ad nent Levin classes. 1. Nora pushed the package to Pamela..", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification called Intersective Levin classes ILCs ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to ho\u00ad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\u00ad volving coherent sets of verbs.", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u2019s classification called Intersective Levin classes ILCs ##CITATION##", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in\u00ad strument as an immediate hypernym. This distinction appears in the second-order Levin classes as membership vs. nonmember\u00ad ship in the intersective class with split. Levin verb classes are based on an underlying lat\u00ad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations. Whereas high level semantic relations (syn\u00ad onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class. However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation Dorr and Jones machine transla\u00ad tion ##CITATION## and automatic lexical ac\u00ad quisition McCarthy and Korhonen Schulte im Walde", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation Dorr and Jones machine transla\u00ad tion ##CITATION## and automatic lexical ac\u00ad quisition McCarthy and Korhonen Schulte im Walde", 0, "Investigating regular sense extensions based on intersective Levin classes", "PAPER"], ["This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation. First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation Dorr and Jones machine transla\u00ad tion ##CITATION## and automatic lexical ac\u00ad quisition McCarthy and Korhonen Schulte im Walde", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation. First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete. We will be using re\u00ad sources such as dictionaries and online corpora to investigate potential additional members of our classes. Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.", "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00ad biguation Dorr and Jones machine transla\u00ad tion ##CITATION## and automatic lexical ac\u00ad quisition McCarthy and Korhonen Schulte im Walde", 1, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation. First, since our experi\u00ad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete. We will be using re\u00ad sources such as dictionaries and online corpora to investigate potential additional members of our classes. Second, since the translation map\u00ad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve \u2022 c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.", "We created a hierarchy of actions exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs such as motion verbs or verbs of contact to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["For example, flutuar does not take a direct object, so some of the alterna\u00ad tions that are related to its transitive meaning are not present. For these verbs, we have the in\u00ad duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat). As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\u00ad lar properties with the English verbs, including the causative/inchoative. The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class.", "We created a hierarchy of actions exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs such as motion verbs or verbs of contact to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["The fact that the split sense for these verbs does not appear explicitly in WordNet is not surprising since it is only an extended sense of the verbs, and separation is inferred only when the verb occurs with an appropriate adjunct, such as apart. However, apart can also be used with other classes of verbs, including many verbs of motion. To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be gener\u00ad ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb. WordNet does not currently provide a consistent treatment of regular sense exten\u00ad sion (some are listed as separate senses, others are not mentioned at all). It would be straight\u00ad forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and in\u00ad clude corresponding syntactic information.", "We created a hierarchy of actions exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs such as motion verbs or verbs of contact to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.", "We created a hierarchy of actions exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs such as motion verbs or verbs of contact to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share ##CITATION##", 1, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. The difficulty of achieving adequate hand\u00ad crafted semantic representations has limited the field of natural language processing to applica\u00ad tions that can be contained within well-defined subdomains.", "##CITATION##  for example have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class ", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["For in\u00ad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\u00ad tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.", "##CITATION##  for example have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["We also investigated the Portuguese translation of some intersective classes of motion verbs. We selected the slide/roll/run, meander/roll and roll/run intersective classes. Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes. The elements of the slide/roll/run class are rebater (bounce), flutuar (float), rolar ( rolQ and deslizar (slide). The resultative in Portuguese cannot be expressed in the same way as in En\u00ad glish.", "##CITATION##  for example have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  .", "##CITATION##  for example have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["These fringe split verbs appear in several other inter\u00ad sective classes that highlight the force aspect of their meaning. Figure 2 depicts the intersection of split, carry and push/pull. Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo\u00ad nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion. Depending on the par\u00ad ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo\u00ad nent Levin classes. 1.", "archy is composed of a set of members linked to their WordNet synsets and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described 3 004 lemmas within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes ##CITATION## refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic message as the direct object Agent Verb Topic as in \"John explained trigonometry\" and a frame for Topic and Recipient Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame Agent Verb Recipient Topic in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion contact transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory duringE  culmination endE  or consequent resultE  stage of an event in a tripartite event structure is similar to that of Moens and Steedman  which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 22 PropBank", 0, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["*Nora pushed at the package to Pamela. Although the Levin classes that make up an intersective class may have conflicting alterna\u00ad tions {e.g., verbs of exerting force can take the conative alternation, while carry verbs cannot), this does not invalidate the semantic regularity of the intersective class. As a verb of exerting force, push can appear in the conative alterna\u00ad tion, which emphasizes its force semantic com\u00ad ponent and ability to express an \"attempted\" action where any result that might be associ\u00ad ated- with the verb (e.g., motion) is not nec\u00ad essarily achieved; as a carry verb (used with a goal or directional phrase), push cannot take the conative alternation, which would conflict with the core meaning of the carry verb class (i.e., causation of motion). The critical point is that, while the verb's meaning can be extended to either \"attempted\" action or directed motion, these two extensions cannot co-occur - they are mutually exclusive. However the simultaneous potential of mutually exclusive extensions is not a problem.", "archy is composed of a set of members linked to their WordNet synsets and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described 3 004 lemmas within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes ##CITATION## refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic message as the direct object Agent Verb Topic as in \"John explained trigonometry\" and a frame for Topic and Recipient Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame Agent Verb Recipient Topic in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion contact transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory duringE  culmination endE  or consequent resultE  stage of an event in a tripartite event structure is similar to that of Moens and Steedman  which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 22 PropBank", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes. The slide class parti\u00ad tions this roll/run intersection into verbs that can take the transitive alternation and verbs that cannot (drift and glide cannot be causative, because they are not typically externally con\u00ad trollable). Verbs in the slide/roll/run intersec\u00ad tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actu\u00ad ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley). These verbs are listed in the intersective classes with meander verbs of existence. \"Meander Verbs\" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.", "archy is composed of a set of members linked to their WordNet synsets and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described 3 004 lemmas within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes ##CITATION## refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic message as the direct object Agent Verb Topic as in \"John explained trigonometry\" and a frame for Topic and Recipient Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame Agent Verb Recipient Topic in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion contact transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory duringE  culmination endE  or consequent resultE  stage of an event in a tripartite event structure is similar to that of Moens and Steedman  which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 22 PropBank", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes. We also have begun to ex\u00ad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.", "archy is composed of a set of members linked to their WordNet synsets and a set of syntactic frames and semantic information for each frame Currently VerbNet has over 4 000 verb senses described 3 004 lemmas within 191 first level classes VerbNet has a hierarchical structure with the first level classes constituted by the original Levin classes In order to ensure that each class is coherent so that all its members share a common set of thematic roles syntactic frames and semantic predicates some restructuring of the classes was required This reorganization which was facilitated by the use of inter- sective Levin classes ##CITATION## refined the classes to account for semantic and syntactic differences within a class A child subclass inherits all the information from its parent class and adds information to it which can be in terms of imposing further restrictions on the roles or adding syntactic frames or semantic predicates to the subclass The hierarchical organization of VerbNet is illustrated in Figure 1 The Transfer of a Message verb class is subdivided into three levels At the top level are thematic roles syntactic frames and semantic predicates shared by all members of the class In this particular case there is a transitive frame with the Topic message as the direct object Agent Verb Topic as in \"John explained trigonometry\" and a frame for Topic and Recipient Agent Verb Topic to the transfer of information event but in the first case the Recipient is underspecified Some of the verbs in this class are able to participate in other syntactic frames as well Verbs at the second level can take the ditransitive frame Agent Verb Recipient Topic in addition to the frames and predicates inherited from the parent class VerbNet uses a fiat semantic representation in which the semantics of each syntactic frame is captured by a conjunction of predicates 1 such as motion contact transfer info which can be negated or not These predicates can take arguments over the verb complements as well as over implicit existentially quantified event variables Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory duringE  culmination endE  or consequent resultE  stage of an event in a tripartite event structure is similar to that of Moens and Steedman  which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure 22 PropBank", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\u00ad eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics eg Pinker Jackendoff Levin Dorr ##CITATION## Merlo and Stevenson ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics eg Pinker Jackendoff Levin Dorr ##CITATION## Merlo and Stevenson ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on a fine-grained variation on Levin classes, inter\u00ad sective Levin classes, as a source of semantic components associated with specific adjuncts. We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them. Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible  ,  ,  .", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics eg Pinker Jackendoff Levin Dorr ##CITATION## Merlo and Stevenson ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics eg Pinker Jackendoff Levin Dorr ##CITATION## Merlo and Stevenson ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single great\u00ad est limitation on the general application of nat\u00ad ural language processing techniques. In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on a fine-grained variation on Levin classes, inter\u00ad sective Levin classes, as a source of semantic components associated with specific adjuncts. We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multi\u00ad plied when more than one language is involved and attempts are made to map between them.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\u00ad ternations of a class. This might be approxi\u00ad mated by automatically extracting the syntac\u00ad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes ##CITATION##", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  .", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes ##CITATION##", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose. For in\u00ad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\u00ad tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best.", "In explormg these quest1ons we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge hnowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn Dorr and document clClbsrficatwn Klavans and Kan :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task LevmMJller et al  ##CITATION##  To address these 1ssues we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes.", "In explormg these quest1ons we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge hnowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn Dorr and document clClbsrficatwn Klavans and Kan :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task LevmMJller et al  ##CITATION##  To address these 1ssues we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm ", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["What constitutes a clear sepa\u00ad ration into senses for any one verb, and how can these senses be computationally characterized and distinguished? The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single great\u00ad est limitation on the general application of nat\u00ad ural language processing techniques. In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on a fine-grained variation on Levin classes, inter\u00ad sective Levin classes, as a source of semantic components associated with specific adjuncts. We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components.", "In explormg these quest1ons we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge hnowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn Dorr and document clClbsrficatwn Klavans and Kan :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task LevmMJller et al  ##CITATION##  To address these 1ssues we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Introduction"], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes.", "In explormg these quest1ons we focus on verb clas\u00ad Sificatwn for several reasons Verbs are very Impor\u00ad tant sources of knowledge m many language eng1neermg tasks and the relat10nsh1ps among verbs ap\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge hnowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn Dorr and document clClbsrficatwn Klavans and Kan :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task LevmMJller et al  ##CITATION##  To address these 1ssues we suggest that one can tram an automatic classifier for verbs on the bast'S of stattst1cal apprm ", 1, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["WordNet was de\u00ad signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.", "Palmer  and ##CITATION##  argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\u00ad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.", "Palmer  and ##CITATION##  argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\u00ad tic frames that are in some sense meaning pre\u00ad serving  . The distribution of syntactic frames in which a verb can appear determines its class member\u00ad ship. The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\u00ad derlying semantic components that constrain al\u00ad lowable arguments.", "Palmer  and ##CITATION##  argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\u00ad tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb  .", "Palmer  and ##CITATION##  argue that the use of syntactic frames and verb classes can sim\u00ad plify the definition of different verb senses ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Intersective Levin classes."], ["The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\u00ad nar (glide) in the closely related roll/run class. We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accord\u00ad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more co\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet   and intersective Levin classes  ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Discussion."], ["In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making gen\u00ad eralizations about regular extensions of mean\u00ad ing. Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersec\u00ad tive sets, which are a more fine-grained clas\u00ad sification and have more coherent sets of syn\u00ad tactic frames and associated semantic compo\u00ad nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\u00ad inal Levin classes.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet   and intersective Levin classes  ", 0, "Investigating regular sense extensions based on intersective Levin classes", "ABSTRACT"], ["Levin verb classes are based on an underlying lat\u00ad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations. Whereas high level semantic relations (syn\u00ad onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class. However, other intersective classes, such as the split/push/carry class, are no more con\u00ad sistent with WordNet than the original Levin classes. The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection. In addition, only one verb (pull) has a WordNet sense cor\u00ad responding to the change of state - separation semantic component associated with the split class.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet   and intersective Levin classes  ", 0, "Investigating regular sense extensions based on intersective Levin classes", "Nora  pushed  the  package.."], ["Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible  ,  ,  . However, we have found interesting par\u00ad allels in how Portuguese and English treat reg\u00ad ular sense extensions. Two current approaches to English verb classi\u00ad fications are WordNet   and Levin classes  . WordNet is an on\u00ad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\u00ad resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.", "We also plan to experiment with different classification schemes for verb semantics such as WordNet   and intersective Levin classes  ", 1, "Investigating regular sense extensions based on intersective Levin classes", "Classifying verbs."], ["After inserting the expression letter bomb twice (because it occurs twice in the original topic), and tv channel that were in dictionary D1 used by the CN index, the relevant document is scored higher and as a consequence is returned in the first position of the ranking(Table 8) . The MAP of this topic has increased 75 percentage points, from 0.2500 in Baseline to 1.000 in the CN index. We see also that the document that was in first position in the Baseline ranking, has its score decreased and was ranked in fourth position in the ranking given by the CN. This document contained information on a \u201csmall bomb located outside the of the Russian embassy\u201d and has is not relevant to topic 141, being properly relegated to a lower position. An interesting fact about this topic is that only the MWE letter bomb influences the result. This was verified as in the index BCN, whose dictionary does not have this MWE, the topic was changed only because of the MWE tv channel and there was no gain or loss for the result.", "The task of identifying MWEs is relevant not only to lexical semantics applications but also machine translation Koehn et al Ren et al Pal et al information retrieval Xu et al ##CITATION## and syntactic parsing Sag et al", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Worst CN (WCN) - with 17,328 MWEs of D3.."], ["Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The term \u201cmultiword expression\u201d has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.", "The task of identifying MWEs is relevant not only to lexical semantics applications but also machine translation Koehn et al Ren et al Pal et al information retrieval Xu et al ##CITATION## and syntactic parsing Sag et al", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word.", "The task of identifying MWEs is relevant not only to lexical semantics applications but also machine translation Koehn et al Ren et al Pal et al information retrieval Xu et al ##CITATION## and syntactic parsing Sag et al", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "PAPER"], ["38 04 71 Table 8: Ranking for Topic #141CN In sum, the MWEs insertion seems to improve retrieval bringing more relevant documents, due to a more precise indexing of specific terms. However, the use of these expressions also brought a negative impact for some cases, because some topics require a semantic analysis to return relevant documents (as for example topic 130, which requires relevant documents to mention the causes of the death of Kurt Cobain \u2014 documents which mention his death without mentioning the causes were not considered relevant). This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions. MWEs are found in all genres of texts and their appropriate use is being targeted for study, both in linguistics and computing, due to the different characteristic variations of this type of expression, which ends up causing problems for the success of computational methods that aim their processing. In this work we aimed at achieving a better understanding of several important points associated with the use of Multiword Expressions in IR systems.", "The task of identifying MWEs is relevant not only to lexical semantics applications but also machine translation Koehn et al Ren et al Pal et al information retrieval Xu et al ##CITATION## and syntactic parsing Sag et al", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Conclusions and Future  Work."], ["For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way  . It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words  . However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs. In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing NPL tasks as shown for example by ##CITATION##  who utilized MWEs in Information Re\u00ad trieval IR ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In addition, it contains the form in which the term appeared in the text   as SYNSET SCORE and CODE, both not used for <TERM ID=\"GH950102000000-126\" LEMA=\"underworld\" POS=\"NN\"> <WF>underworld</WF> <SYNSET SCORE=\"0.5\" CODE=\"06120171-n\"/> <SYNSET SCORE=\"0.5\" CODE=\"06327598-n\"/> </TERM> Figure 1: Structure of a term in the original documents In this paper, we extracted the terms located in the LEMA attribute, in other words, in their canonical form (e.g. letter bomb for letter bombs). The use of lemmas and not the words (e.g. write for wrote, written, etc.) to the formation of the corpus, avoids linguistic variations that can affect the results of the experiments. As a results, our documents were formed only by lemmas and the next step is the indexing of documents using an IR system. For this task we used a tool called Zettair  , which is a compact textual search engine that can be used both for the indexing and for querying text collections. Porter\u2019s Stemmer   as implemented in Zettair was also used.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing NPL tasks as shown for example by ##CITATION##  who utilized MWEs in Information Re\u00ad trieval IR ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio   for verb-particle constructions, Pearce   and Evert and Krenn   for collocations, Nicholson and Baldwin   for compound nouns and many others. For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy  , dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus). Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment. The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents. In this section we describe the different resources and methods used in the experiments.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing NPL tasks as shown for example by ##CITATION##  who utilized MWEs in Information Re\u00ad trieval IR ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.", "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing NPL tasks as shown for example by ##CITATION##  who utilized MWEs in Information Re\u00ad trieval IR ", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The term \u201cmultiword expression\u201d has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit the accuracy of the system improves on multiword queries ##CITATION##", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit the accuracy of the system improves on multiword queries ##CITATION##", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit the accuracy of the system improves on multiword queries ##CITATION##", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["In an ideal system, the index terms should correspond to the concepts found in the documents. If indexing is performed only with the atomic terms, there may be a loss of semantic content of the documents. For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World  , pages 101\u2013109, Portland, Oregon, USA, 23 June 2011. Qc 2011 Association for Computational Linguistics return instead irrelevant documents about celestial bodies or carbonated drinks. In order to investigate the effects of indexing of MWEs for IR, the results of queries are analyzed using IR quality metrics.", "\u2022 Information retrieval: when MWEs like pop star are indexed as a unit the accuracy of the system improves on multiword queries ##CITATION##", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using \u201c \u201d (e.g. letter bomb as letter bomb). Figure 2 exemplifies this concatenation. Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes. The rationale was that documents containing specific MWEs can be indexed more adequately than those containing the words of the expression separately. As a result, retrieval quality should increase.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval ##CITATION## or Machine Translation Carpuat and Diab", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["We finish with some conclusions and future work. The concept of Multiword Expression has been widely viewed as a sequence of words that acts as a single unit at some level of linguistic analysis  , or as Idiosyncratic interpretations that cross word boundaries  . One of the great challenges of NLP is the identification of such expressions, \u201chidden\u201d in texts of various genres. The difficulties encountered for identifying Multiword Expressions arise for reasons like: \u2022 the difficulty to find the boundaries of a multi- word, because the number of component words may vary, or they may not always occur in a canonical sequence (e.g. rock the boat, rock the seemingly intransigent boat and the bourgeois boat was rocked); \u2022 even some of the core components of an MWE may present some variation (e.g. throw NP to the lions/wolves/dogs/?birds/?butterflies); \u2022 in a multilingual perspective, MWEs of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guardachuva in Portuguese as umbrella in English and not as ?store rain). The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval ##CITATION## or Machine Translation Carpuat and Diab", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval ##CITATION## or Machine Translation Carpuat and Diab", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "PAPER"], ["In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data they are interested  . Although language processing is not vital to modern IR systems, it may be convenient   and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system. The selection of appropriate indexing terms is a key factor for the quality of IR systems. In an ideal system, the index terms should correspond to the concepts found in the documents.", "Biemann\u2019s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval ##CITATION## or Machine Translation Carpuat and Diab", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data they are interested  . Although language processing is not vital to modern IR systems, it may be convenient   and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system. The selection of appropriate indexing terms is a key factor for the quality of IR systems. In an ideal system, the index terms should correspond to the concepts found in the documents.", " Baldwin and Kim  has been shown to be useful in various NLP applications Ramisch recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval \u201cIR\u201d: ##CITATION##  and machine translation \u201cMT\u201d: Weller et al  Carpuat and Diab  and Venkatapathy and Joshi   ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In terms of practical MWE identification systems, a well known approach is that of Smadja  , who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. This approach is implemented in a lexico- graphic tool called Xtract. More recently there has been the release of the mwetoolkit   for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio   for verb-particle constructions, Pearce   and Evert and Krenn   for collocations, Nicholson and Baldwin   for compound nouns and many others.", " Baldwin and Kim  has been shown to be useful in various NLP applications Ramisch recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval \u201cIR\u201d: ##CITATION##  and machine translation \u201cMT\u201d: Weller et al  Carpuat and Diab  and Venkatapathy and Joshi   ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.", " Baldwin and Kim  has been shown to be useful in various NLP applications Ramisch recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval \u201cIR\u201d: ##CITATION##  and machine translation \u201cMT\u201d: Weller et al  Carpuat and Diab  and Venkatapathy and Joshi   ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "ABSTRACT"], ["One of the great challenges of NLP is the identification of such expressions, \u201chidden\u201d in texts of various genres. The difficulties encountered for identifying Multiword Expressions arise for reasons like: \u2022 the difficulty to find the boundaries of a multi- word, because the number of component words may vary, or they may not always occur in a canonical sequence (e.g. rock the boat, rock the seemingly intransigent boat and the bourgeois boat was rocked); \u2022 even some of the core components of an MWE may present some variation (e.g. throw NP to the lions/wolves/dogs/?birds/?butterflies); \u2022 in a multilingual perspective, MWEs of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guardachuva in Portuguese as umbrella in English and not as ?store rain). The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years. With the recent increase in efficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing, these can become an aid in improving the performance of MWE detection techniques. In terms of practical MWE identification systems, a well known approach is that of Smadja  , who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora.", " Baldwin and Kim  has been shown to be useful in various NLP applications Ramisch recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval \u201cIR\u201d: ##CITATION##  and machine translation \u201cMT\u201d: Weller et al  Carpuat and Diab  and Venkatapathy and Joshi   ", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Multiword Expressions."], ["  defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis. The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way  . It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words  .", "For instance ##CITATION##   showed that by considering non-compositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab   showed that by adding compositionality scores to the Moses SMT system   they could improve translation quality ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data they are interested  . Although language processing is not vital to modern IR systems, it may be convenient   and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system. The selection of appropriate indexing terms is a key factor for the quality of IR systems. In an ideal system, the index terms should correspond to the concepts found in the documents.", "For instance ##CITATION##   showed that by considering non-compositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab   showed that by adding compositionality scores to the Moses SMT system   they could improve translation quality ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Introduction"], ["3.2 Multiword Expression as Single Terms. In this work, we focused on MWEs composed of exactly two words (i.e. bigrams). In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using \u201c \u201d (e.g. letter bomb as letter bomb). Figure 2 exemplifies this concatenation. Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes.", "For instance ##CITATION##   showed that by considering non-compositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab   showed that by adding compositionality scores to the Moses SMT system   they could improve translation quality ", 0, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["The test collection has a total of 310 query topics. The judgment of whether a document is relevant to a query was assigned according to a list of relevant documents, manually prepared and supplied with the material provided by CLEF. We used Zettair to generate the ranked list of documents retrieved in response to each query. For each query topic, the 1,000 top scoring documents were selected. We used the cosine metric to calculate the scores and rank the documents.", "For instance ##CITATION##   showed that by considering non-compositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab   showed that by adding compositionality scores to the Moses SMT system   they could improve translation quality ", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["We used Zettair to generate the ranked list of documents retrieved in response to each query. For each query topic, the 1,000 top scoring documents were selected. We used the cosine metric to calculate the scores and rank the documents. Finally, to calculate the retrieval evaluation metrics (detailed in Section 3.5) we used the tool trec eval. This tool compares the list of retrieved documents (obtained from Zettair) against the list of relevant documents (provided by CLEF).", "For instance ##CITATION##   showed that by considering non-compositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab   showed that by adding compositionality scores to the Moses SMT system   they could improve translation quality ", 1, "Identification and Treatment of Multiword Expressions applied to Information Retrieval", "Materials and Methods."], ["The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech.", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["Neural networks are worth investigating since they offer potential advantages over decision trees. They can learn decision surfaces that lie at an angle to the axes of the input feature space, unlike standard CART trees, which always split continuous features on one dimension at a time. The response function of neural networks is continuous (smooth) at the decision boundaries, allowing them to avoid hard decisions and the complete fragmentation of data associated with decision tree questions. Most important, however, related work (Ries 1999a) indicated that similarly structured networks are superior classifiers if the input features are words and are therefore a plugin replacement for the language model classifiers described in this paper. Neural networks are therefore a good candidate for a jointly optimized classifier of prosodic and word-level information since one can show that they are a generalization of the integration approach used here.", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The approach relies on the intuition that different utterance types are characterized by different intonational \"tunes\"  , and has been successfully applied to the classification of move types in the DCIEM Map Task corpus  . The system detects sequences of distinctive pitch patterns by training one continuous- density HMM for each DA type. Unfortunately, the event classification accuracy on the Switchboard corpus was considerably poorer than in the Map Task domain, and DA recognition results when coupled with a discourse grammar were substantially worse than with decision trees. The approach could prove valuable in the future, however, if the intonation event detector can be made more robust to corpora like OURS. A1 Ai An T 1 1 Wl Wi W,, T T t <start> -, 0\"1 , ...---* U/ , ...", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The n-gram-based discourse grammars we used have this property. As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model  . The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure  .", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["5.2.4 Neural Network Classifiers. Although we chose to use decision trees as prosodic classifiers for their relative ease of inspection, we might have used any suitable probabilistic classifier, i.e., any model that estimates the posterior probabilities of DAs given the prosodic features. We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here. Neural networks are worth investigating since they offer potential advantages over decision trees. They can learn decision surfaces that lie at an angle to the axes of the input feature space, unlike standard CART trees, which always split continuous features on one dimension at a time.", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The test data was split roughly in half (without speaker overlap), each half was used to separately optimize the parameters, and the best values were then tested on the respective other half. The reported results are from the aggregate outcome on the two test set halves. Table 9 Combined utterance classification accuracies (chance = 35%). The first two columns correspond to Tables 7 and 6, respectively. Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["Table 9 Combined utterance classification accuracies (chance = 35%). The first two columns correspond to Tables 7 and 6, respectively. Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%). Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody only 76.0 76.0 words only 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody only 72.9 72.9 words only 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results. In this experiment we combined the acoustic n-best likelihoods based on recognized words with the Top-5 tree classifier mentioned in Section 5.2.3.", "##CITATION## use HMMs for dialogue modelling where sequences of observations correspond to sequences of dialogue act types They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The Map Task corpus   consists of conversations between two speakers with slightly different maps of an imaginary territory. Their task is to help one speaker reproduce a route drawn only on the other speaker's map, all without being able to see each other's maps. Of the DA modeling algorithms described below, Taylor et al.   and Wright   were based on Map Task. The VERBMOBIL corpus consists of two-party scheduling dialogues. A number of the DA m6deling algorithms described below were developed for VERBMOBIL, including those of Mast et al.  , Warnke et al.  , Reithinger et al.  , Reithinger and Klesen  , and Samuel, Carberry, and VijayShanker  .", "To date the majority of work on dialogue act modeling has addressed spoken dialogue Samuel et al ##CITATION## Surendran and Levow Bangalore et al Sridhar et al Di Eugenio et al", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["Table 14 shows the approximate size of the corpora, the tag set, and tag estimation accuracy rates for various recent models of DA prediction. The results summarized in the table also illustrate the differences in inherent difficulty of the tasks. For example, the task of Warnke et al.   was to simultaneously segment and tag DAs, whereas the other results rely on a prior manual segmentation. Similarly, the task in Wright   and in our study was to determine DA types from speech input, whereas work by others is based on hand-transcribed textual input. Table 13 Dialogue act tag sets used in three other extensively studied corpora.", "To date the majority of work on dialogue act modeling has addressed spoken dialogue Samuel et al ##CITATION## Surendran and Levow Bangalore et al Sridhar et al Di Eugenio et al", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.", "To date the majority of work on dialogue act modeling has addressed spoken dialogue Samuel et al ##CITATION## Surendran and Levow Bangalore et al Sridhar et al Di Eugenio et al", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "PAPER"], ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality.", "To date the majority of work on dialogue act modeling has addressed spoken dialogue Samuel et al ##CITATION## Surendran and Levow Bangalore et al Sridhar et al Di Eugenio et al", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality. Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech. These results, besides validating the methods described, are of interest for several reasons. For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14).", "To date the majority of work on dialogue act modeling has addressed spoken dialogue Samuel et al ##CITATION## Surendran and Levow Bangalore et al Sridhar et al Di Eugenio et al", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.", "Previous research has leveraged prosodic cues   and facial expressions   for automatic dialogue act classification but other types of nonverbal cues remain unexplored", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "PAPER"], ["For the speech recognition task, the generalized model gives a clean probabilistic framework for conditioning word probabilities on the conversation context via the underlying DA structure. Unlike previous models that did not address speech recognition or relied only on an intuitive 1-best approximation, our model allows computation of the optimum word sequence by effectively summing over all possible DA sequences as well as all recognition hypotheses throughout the conversation, using evidence from both past and future. Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs. We made a number of significant simplifications to arrive at a computationally and statistically tractable formulation. In this formulation, DAs serve as the hinges that join the various model components, but also decouple these components through statistical independence assumptions.", "Previous research has leveraged prosodic cues   and facial expressions   for automatic dialogue act classification but other types of nonverbal cues remain unexplored", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Discussion and Issues for Future Research."], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram.", "Previous research has leveraged prosodic cues   and facial expressions   for automatic dialogue act classification but other types of nonverbal cues remain unexplored", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["This can be done using familiar techniques from language modeling for speech recognition, although the sequenced objects in this case are DA labels rather than words; discourse grammars will be discussed in detail in Section 4. 3.1 Dialogue Act Likelihoods. The computation of likelihoods P(EIU ) depends on the types of evidence used. In our experiments we used the following sources of evidence, either alone or in combination: Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W refers to the true (hand-transcribed) words spoken in a conversation. Recognized words: The evidence consists of recognizer acoustics A, and we seek to compute P(A I U).", "Previous research has leveraged prosodic cues   and facial expressions   for automatic dialogue act classification but other types of nonverbal cues remain unexplored", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["Recognized words: The evidence consists of recognizer acoustics A, and we seek to compute P(A I U). As described later, this involves considering multiple alternative recognized word sequences. Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U). For ease of reference, all random variables used here are summarized in Table 4. The same variables are used with subscripts to refer to individual utterances.", "Previous research has leveraged prosodic cues   and facial expressions   for automatic dialogue act classification but other types of nonverbal cues remain unexplored", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech ##CITATION## and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus Shawar and Atwell", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "PAPER"], ["To enable such research, we need fairly large, standardized corpora that allow comparisons over time and across approaches. Despite its shortcomings, the Switchboard domain could serve this purpose. We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus. The approach combines models for lexical and prosodic realizations of DAs, as well as a statistical discourse 10 The inadequacy of n-gram models for nested discourse structures is pointed out by ChuCarroll  , although the suggested solution is a modified n-gram approach.. grammar.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech ##CITATION## and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus Shawar and Atwell", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Conclusions."], ["Reithinger et al.  , for example, used deleted interpolation to smooth the dialogue n-grams. ChuCarroll   uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition. The idea caught on very quickly: Suhm and Waibel  , Mast et aL  , Warnke et al.  , Reithinger and Klesen  , and Taylor et al.   all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods. Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger   propose using automatically learned stochastic context-free grammars.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech ##CITATION## and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus Shawar and Atwell", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["<.1% You are what kind of buff? <.1% I'm sorry. <.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7).", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech ##CITATION## and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus Shawar and Atwell", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality.", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech ##CITATION## and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus Shawar and Atwell", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["To expedite the DA labeling task and remain consistent with other Switchboard-based research efforts, we made use of a version of the corpus that had been hand-segmented into sentence-level units prior to our own work and independently of our DA labeling system  . We refer to the units of this segmentation as utterances. The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance). Each utterance unit was identified with one DA, and was annotated with a single DA label. The DA labeling system had special provisions for rare cases where utterances seemed to combine aspects of several DA types.", "Conversational feedback is mostly performedthrough short utterances such as yeah mh okaynot produced by the main speaker but by one ofthe other participants of a conversation Such utterances are among the most frequent in conversational data ##CITATION## ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["This means that the likelihood given a complete conversation can be factored into likelihoods given the individual utterances. We use Ui for the ith DA label in the sequence U, i.e., U = (U1 ..... Ui,..., Un), where n is the number of utterances in a conversation. In addition, we use Ei for that portion of the evidence that corresponds to the ith utterance, e.g., the words or the prosody of the ith utterance. Decomposability of the likelihood means that P(EIU) = P(E11 U1).....", "Conversational feedback is mostly performedthrough short utterances such as yeah mh okaynot produced by the main speaker but by one ofthe other participants of a conversation Such utterances are among the most frequent in conversational data ##CITATION## ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy. Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department. 36% Uh-huh.", "Conversational feedback is mostly performedthrough short utterances such as yeah mh okaynot produced by the main speaker but by one ofthe other participants of a conversation Such utterances are among the most frequent in conversational data ##CITATION## ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["WH-QUESTION Well, how old are you? Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively  . We expect recognition of backchannels to be useful because of their discourse-structuring role (knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative) and because they seem to occur at certain kinds of syntactic boundaries; detecting a backchannel may thus help in predicting utterance boundaries and surrounding lexical material.", "Conversational feedback is mostly performedthrough short utterances such as yeah mh okaynot produced by the main speaker but by one ofthe other participants of a conversation Such utterances are among the most frequent in conversational data ##CITATION## ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively  . We expect recognition of backchannels to be useful because of their discourse-structuring role (knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative) and because they seem to occur at certain kinds of syntactic boundaries; detecting a backchannel may thus help in predicting utterance boundaries and surrounding lexical material. For an intuition about what backchannels look like, Table 3 shows the most common realizations of the approximately 300 types (35,827 tokens) of backchannel in our Switchboard subset.", "Conversational feedback is mostly performedthrough short utterances such as yeah mh okaynot produced by the main speaker but by one ofthe other participants of a conversation Such utterances are among the most frequent in conversational data ##CITATION## ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively  . We expect recognition of backchannels to be useful because of their discourse-structuring role (knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative) and because they seem to occur at certain kinds of syntactic boundaries; detecting a backchannel may thus help in predicting utterance boundaries and surrounding lexical material. For an intuition about what backchannels look like, Table 3 shows the most common realizations of the approximately 300 types (35,827 tokens) of backchannel in our Switchboard subset.", "These are represented as layers in the DAMSL system Core & Allen", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy. Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department. 36% Uh-huh.", "These are represented as layers in the DAMSL system Core & Allen", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Researchers using this corpus include Nagata  , Nagata and Morimoto (1993, 1994), and Kita et al.  . Table 13 shows the most commonly used versions of the tag sets from those three tasks. As discussed earlier, these domains differ from the Switchboard corpus in being task-oriented. Their tag sets are also generally smaller, but some of the same problems of balance occur. For example, in the Map Task domain, 33% of the words occur in 1 of the 12 DAs 0NSTRUCT).", "These are represented as layers in the DAMSL system Core & Allen", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department. 36% Uh-huh. 19% I think it's great 13% So, -/ 6% That's exactly it.", "These are represented as layers in the DAMSL system Core & Allen", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "Dialog act DA annotations and tagging inspiredby the speech act theory of Austin  and Searle have been used in the NLP community to understand and model dialog Initial work was done onspoken interactions see for example ##CITATION## ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Reithinger et al.  , for example, used deleted interpolation to smooth the dialogue n-grams. ChuCarroll   uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition. The idea caught on very quickly: Suhm and Waibel  , Mast et aL  , Warnke et al.  , Reithinger and Klesen  , and Taylor et al.   all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods. Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger   propose using automatically learned stochastic context-free grammars.", "Dialog act DA annotations and tagging inspiredby the speech act theory of Austin  and Searle have been used in the NLP community to understand and model dialog Initial work was done onspoken interactions see for example ##CITATION## ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Prior and Related Work."], ["Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech. These results, besides validating the methods described, are of interest for several reasons. For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14). To keep the presentation interesting and concrete, we will alternate between the description of general methods and empirical results. Section 2 describes the task and our data in detail.", "Dialog act DA annotations and tagging inspiredby the speech act theory of Austin  and Searle have been used in the NLP community to understand and model dialog Initial work was done onspoken interactions see for example ##CITATION## ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["A SIGNAL-NoN-UNDERSTANDING What did you say? B STATEMENT N C State. The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). A DA represents the meaning of an utterance at the level of illocutionary force  .", "Dialog act DA annotations and tagging inspiredby the speech act theory of Austin  and Searle have been used in the NLP community to understand and model dialog Initial work was done onspoken interactions see for example ##CITATION## ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["B STATEMENT N C State. The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). A DA represents the meaning of an utterance at the level of illocutionary force  . Thus, a DA is approximately the equivalent of the speech act of Searle  , the conversational game move of Power  , or the adjacency pair part of Schegloff   and Saks, Schegloff, and Jefferson  .", "Dialog act DA annotations and tagging inspiredby the speech act theory of Austin  and Searle have been used in the NLP community to understand and model dialog Initial work was done onspoken interactions see for example ##CITATION## ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Table 1 shows a sample of the kind of discourse structure in which we are interested. Each utterance is assigned a unique DA label (shown in column 2), drawn from a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria. The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture  . While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications.", "Dialog act DA annotations and tagging inspiredby the speech act theory of Austin  and Searle have been used in the NLP community to understand and model dialog Initial work was done onspoken interactions see for example ##CITATION## ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.", "dialogue acts such as statements questions backchannels  are detected using a language model based detector trained on Switchboard similar to ##CITATION##  5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["5.3.2 Focused Classifications. To gain a better understanding of the potential for prosodic DA classification independent of the effects of discourse grammar and the skewed DA distribution in Switchboard, we examined several binary DA classification tasks. The choice of tasks was motivated by an analysis of confusions committed by a purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, and BACKCHANNELS for AGREEMENTS (and vice versa). We tested a prosodic classifier, a word-based classifier (with both transcribed and recognized words), and a combined classifier on these two tasks, downsampling the DA distribution to equate the class sizes in each case. Chance performance in all experiments is therefore 50%.", "dialogue acts such as statements questions backchannels  are detected using a language model based detector trained on Switchboard similar to ##CITATION##  5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.  Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.", "dialogue acts such as statements questions backchannels  are detected using a language model based detector trained on Switchboard similar to ##CITATION##  5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["<.1% You are what kind of buff? <.1% I'm sorry. <.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7).", "dialogue acts such as statements questions backchannels  are detected using a language model based detector trained on Switchboard similar to ##CITATION##  5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set.", "dialogue acts such as statements questions backchannels  are detected using a language model based detector trained on Switchboard similar to ##CITATION##  5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Prior and related work is summarized in Section 7. Further issues and open problems are addressed in Section 8, followed by concluding remarks in Section 9. The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech   distributed by the Linguistic Data Consortium. Each conversation involved two randomly selected strangers who had been charged with talking informally about one of several, self- selected general-interest topics. To train our statistical models on this corpus, we combined an extensive effort in human hand-coding of DAs for each utterance, with a variety of automatic and semiautomatic tools.", "dialogue acts such as statements questions backchannels  are detected using a language model based detector trained on Switchboard similar to ##CITATION##  5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "The Dialogue Act Labeling Task."], ["This leads to the following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ The last step in Equation 12 is justified because, as shown in Figures 1 and 4, the evidence E (acoustics, prosody, words) pertaining to utterances other than i can affect the current utterance only through its DA type Ui. We call this the mixture-of-posteriorsapproach, because it amounts to a mixture of the posterior distributions obtained from DA-specific speech recognizers (Equation 11), using the DA posteriors as weights. This approach is quite expensive, however, as it requires multiple full recognizer or rescoring passes of the input, one for each DA type. A more efficient, though mathematically less accurate, solution can be obtained by combining guesses about the correct DA types directly at the level of the LM. We estimate the distribution of likely DA types for a given utterance using the entire conversation E as evidence, and then use a sentence-level mixture   of DA-specific LMs in a single recognizer run.", "The HMM has been widely used in many tagging problems ##CITATION## ##CITATION## used it for dialog act classification where each utterance or dialog act is used as the observation  ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Speech Recognition."], ["We observe about a 21% relative increase in classification error when using recognizer words; this is remarkably small considering that the speech recognizer used had a word error rate of 41% on the test set. We also compared the n-best DA classification approach to the more straightforward 1-best approach. In this experiment, only the single best recognizer hypothesis is used, effectively treating it as the true word string. The 1-best method increased classification error by about 7% relative to the n-best algorithm (61.5% accuracy with a bigram discourse grammar). 5.2 Dialogue Act Classification Using Prosody.", "The HMM has been widely used in many tagging problems ##CITATION## ##CITATION## used it for dialog act classification where each utterance or dialog act is used as the observation  ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["noise ratio [SNR]), speaking rate (based on the \"enrate\" measure of Morgan, Fosler, and Mirghafori [1997]), and gender (of both speaker and listener). In the case of utterance duration, the measure correlates both with length in words and with overall speaking rate. The gender feature that classified speakers as either male or female was used to test for potential inadequacies in F0 normalizations. Where appropriate, we included both raw features and values normalized by utterance and/or conversation. We also included features that are the output of the pitch accent and boundary tone event detector of Taylor   (e.g., the number of pitch accents in the utterance).", "The HMM has been widely used in many tagging problems ##CITATION## ##CITATION## used it for dialog act classification where each utterance or dialog act is used as the observation  ", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Dialogue Act Classification."], ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel   (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "The HMM has been widely used in many tagging problems ##CITATION## ##CITATION## used it for dialog act classification where each utterance or dialog act is used as the observation  ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["The n-gram-based discourse grammars we used have this property. As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model  . The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure  .", "The HMM has been widely used in many tagging problems ##CITATION## ##CITATION## used it for dialog act classification where each utterance or dialog act is used as the observation  ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model  . The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure  . Figure 1 shows the variables in the resulting HMM with directed edges representing conditional dependence.", "The HMM has been widely used in many tagging problems ##CITATION## ##CITATION## used it for dialog act classification where each utterance or dialog act is used as the observation  ", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Hidden Markov Modeling of Dialogue."], ["While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games  , a slightly higher level unit that comprises a small number of DAs. Interactional dominance   might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.", "By representing a higher level intention of utterancesduring human conversation dialogue act labels arebeing used to enrich the information provided byspoken words ##CITATION##", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Another important role of DA information could be feedback to lower-level processing. For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy. Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department.", "By representing a higher level intention of utterancesduring human conversation dialogue act labels arebeing used to enrich the information provided byspoken words ##CITATION##", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.", "By representing a higher level intention of utterancesduring human conversation dialogue act labels arebeing used to enrich the information provided byspoken words ##CITATION##", 0, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "ABSTRACT"], ["For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games  , a slightly higher level unit that comprises a small number of DAs. Interactional dominance   might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing.", "By representing a higher level intention of utterancesduring human conversation dialogue act labels arebeing used to enrich the information provided byspoken words ##CITATION##", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["In related work DAs are used as a first processing step to infer dialogue games  , a slightly higher level unit that comprises a small number of DAs. Interactional dominance   might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing. For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy.", "By representing a higher level intention of utterancesduring human conversation dialogue act labels arebeing used to enrich the information provided byspoken words ##CITATION##", 1, "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "Introduction"], ["(Condition on the first production: with probability q the tree terminates and with probability p it produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.) It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi.", "##CITATION##  proved that any PCFG estimated from a treebank with the relative frequency estimator is tight", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms. Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse. For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol. Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one. The reason is that the derivation tree may have probability greater than zero of never terminating--some mass can be lost to infinity.", "##CITATION##  proved that any PCFG estimated from a treebank with the relative frequency estimator is tight", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi. Consequently: f(A ---+AA;wi) < l[f(A ~ AA;~i) + f(A ~ a;wi)] for each wi, and ~ < . The maximum-likelihood probability is tight. If only the yields (left-to-right sequence of terminals) Y(o;1), Y(w2)..... Y(wn) are available, the EM algorithm can be used to iteratively \"climb\" the likelihood surface (see Section 2).", "##CITATION##  proved that any PCFG estimated from a treebank with the relative frequency estimator is tight", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2..", "##CITATION##  proved that any PCFG estimated from a treebank with the relative frequency estimator is tight", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["The reason is that the derivation tree may have probability greater than zero of never terminating--some mass can be lost to infinity. This phenomenon is well known and well understood, and there are tests for \"tightness\" (by which we mean total probability mass equal to one) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules (see for example Booth and Thompson [1973], Grenander [1976], and Harris [1963]). What if the production probabilities are estimated from data? Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar. It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees.", "##CITATION##  proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["(Condition on the first production: with probability q the tree terminates and with probability p it produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.) It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi.", "##CITATION##  proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["What if the production probabilities are estimated from data? Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar. It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees. For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight?", "##CITATION##  proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["(2) AEV a s.t. i=1 The function p : R ~ [0,1] subject to (1) that maximizes (2) satisfies: 6 AAp(A ~ a) + f(A ~ a;cai)logp(A ~ a) = 0 AEV   i=1 (A~o~)ER V(B ~/3) E R where {AA}AEV are Lagrange multipliers. Denote the maximum-likelihood estimator by fi: n B AB q- ~i=lf( --+ /3;ca;) = 0 V(S ~ /3) E R f,(B +/3) Since ~ fi(B+/3)=l) fl sA. (8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator. Suppose B E V is unobserved among the parse trees cabc02,-..,can. Then we can assign fi(B --+ fl) arbitrarily, requiring only that (1) be respected.", "##CITATION##  proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus", 1, "Estimation of Probabilistic Context-Free Grammars", "Maximum-Likelihood Estimation."], ["Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar. It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees. For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "When a PCFG probability distribution is estimated from training data in our case the Penn tree-bank PCFGs dene a tight summing to one probability distribution over strings  thus making them appropriate for language models", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["We show here that estimated production probabilities always yield proper distributions. Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms. Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse. For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol. Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one.", "When a PCFG probability distribution is estimated from training data in our case the Penn tree-bank PCFGs dene a tight summing to one probability distribution over strings  thus making them appropriate for language models", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one. The condition for proper assignment is rather subtle. Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper.", "When a PCFG probability distribution is estimated from training data in our case the Penn tree-bank PCFGs dene a tight summing to one probability distribution over strings  thus making them appropriate for language models", 0, "Estimation of Probabilistic Context-Free Grammars", "ABSTRACT"], ["Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2..", "When a PCFG probability distribution is estimated from training data in our case the Penn tree-bank PCFGs dene a tight summing to one probability distribution over strings  thus making them appropriate for language models", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production \".\" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi. Consequently: f(A ---+AA;wi) < l[f(A ~ AA;~i) + f(A ~ a;wi)] for each wi, and ~ < .", "##CITATION## studied the question for Maximum Likelihood ML estimation and showed that ML estimates are always tight for both the supervisedcase where the input consists of parse trees andthe unsupervised case where the input consists ofyields or terminal strings", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Evidently the likelihood is unaffected by the particular assignment of fi(B --~ fl). Furthermore, it is not hard to see that any such B has probability zero of arising in any derivation that is based upon the maximum-likelihood probabilitiesg--~ence the issue of tightness is independent of this assignment. We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1. 3 Consider any sequence of productions that leads from S to B. If the parent (antecedent) of B arose in. the sample, then the last production has ~ probability zero and hence the sequence has probability zero.", "##CITATION## studied the question for Maximum Likelihood ML estimation and showed that ML estimates are always tight for both the supervisedcase where the input consists of parse trees andthe unsupervised case where the input consists ofyields or terminal strings", 0, "Estimation of Probabilistic Context-Free Grammars", "Maximum-Likelihood Estimation."], ["2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2.. Computational Linguistics Volume 24, Number 2 Wetherell   has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight. (Wetherell and others use the designation \"consistent\" instead of \"tight,\" but in statistics, consistency refers to the asymptotic correctness of an estimator.) A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol.", "##CITATION## studied the question for Maximum Likelihood ML estimation and showed that ML estimates are always tight for both the supervisedcase where the input consists of parse trees andthe unsupervised case where the input consists ofyields or terminal strings", 0, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]).", "##CITATION## studied the question for Maximum Likelihood ML estimation and showed that ML estimates are always tight for both the supervisedcase where the input consists of parse trees andthe unsupervised case where the input consists ofyields or terminal strings", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2..", "##CITATION## studied the question for Maximum Likelihood ML estimation and showed that ML estimates are always tight for both the supervisedcase where the input consists of parse trees andthe unsupervised case where the input consists ofyields or terminal strings", 1, "Estimation of Probabilistic Context-Free Grammars", "Introduction"], ["There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders  . 2.1 Combinatory Categorial Grammar. As a preliminary to semantics, we need syntax. Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG  . CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression).", "We use Boxer semantic analyzer ##CITATION## to extract semantic predicates such as EVENT or DATE", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types \u2014 and this must be the case to ensure the semantic composition process proceeds without type clashes \u2014 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.", "We use Boxer semantic analyzer ##CITATION## to extract semantic predicates such as EVENT or DATE", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Obviously, negated DRSs are translated as negated formulas, disjunctive DRSs as disjunctive formulas, and implicational DRSs as formulas with material implication. Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation. This level of representation is referred to as underspecified DRS, or UDRS for short. It is a small extension of the DRS language given in the previous section and is defined as follows: <expt > ::= <udrs> <udrs> ::= <drs> | (<expt >;<expt >) | (<expt >\u03b1<expt >) Note here that expressions of type t are redefined as UDRSs. UDRSs are either ordinarly DRSs, DRSs conjoined by the merge (for which we use the semicolon), or NP/N: A N/N: record N: date \u03bbq.\u03bbp.( x ;q@x;p@x) \u03bbp.\u03bbx.( y record(y) nn(y,x) ;p@x) \u03bbx. date(x) [fa] N: record date y \u03bbx.( record(y) nn(y,x) ; ) date(x) . . .", "We use Boxer semantic analyzer ##CITATION## to extract semantic predicates such as EVENT or DATE", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "We use Boxer semantic analyzer ##CITATION## to extract semantic predicates such as EVENT or DATE", 1, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["Good: The named entities were correctly recognised and classified as locations. The various cases of VP coordination all got properly analysed. The numerical and date expressions got correct representations. Bad: The occurrences of the third-person neuter pronouns were not resolved. The preposition \u201cAmid\u201d was not correctly analysed.", "We use Boxer semantic analyzer ##CITATION## to extract semantic predicates such as EVENT or DATE", 1, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["That has been known for some time and it has led to a vaccine that seems to prevent it. Researchers have been looking for other cancers that may be caused by viruses. The output of Boxer for this text is shown in Figure 3. Only the box format is shown here \u2014 Boxer is also able to output the DRSs in Prolog or XML encodings. It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here).", "This means that it is relatively straightforward to deterministically map parser output to a logical form as in the Boxer system ##CITATION##", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Researchers have been looking for other cancers that may be caused by viruses. The output of Boxer for this text is shown in Figure 3. Only the box format is shown here \u2014 Boxer is also able to output the DRSs in Prolog or XML encodings. It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here). As we can see from the example and Boxer\u2019s analysis various things go right and various things go wrong.", "This means that it is relatively straightforward to deterministically map parser output to a logical form as in the Boxer system ##CITATION##", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["For example, the category n/n can correspond to an adjective, a cardinal expression, or even common nouns and proper names (in the compound expressions). In the latter two cases the lexical entry introduces a new discourse referent, in the former two it does not. To account for this difference we also need to look at the part of speech that is assigned to a token. 3.3 Resolution. Boxer implements various presupposition triggers introduced by noun phrases, including personal pronouns, possessive pronouns, reflexive pronouns, emphasising pronouns, demonstrative pronouns, proper names, other-anaphora, definite descriptions.", "This means that it is relatively straightforward to deterministically map parser output to a logical form as in the Boxer system ##CITATION##", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG  . CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG  . We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "This means that it is relatively straightforward to deterministically map parser output to a logical form as in the Boxer system ##CITATION##", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["3.1 Preprocessing. The input text needs to be tokenised with one sentence per line. In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing  . The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents. An example of a CCG derivation is shown in Figure 2.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer ##CITATION## \u2013 in the context of the shared task on recognising negation in English texts Morante and Blanco", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Completed: 3 (100.00%). Figure 3: Boxer output for Shared Task Text 2 Here we discuss the output of Boxer on the Shared Task Texts  . Boxer was able to produce semantic representation for all text without any further modifications to the software. For each text we briefly say what was good and bad about Boxer\u2019s analysis.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer ##CITATION## \u2013 in the context of the shared task on recognising negation in English texts Morante and Blanco", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["Bad: The measure phrase \u201c125 m high\u201d got misinterpreted as noun-noun comn- pound. The definite description \u201cthe fall\u201d was not linked to the falling event mentioned before. Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions. Text 3: John went into a restaurant ... Good: The pronouns were correctly resolved to the proper name \u201cJohn\u201d rather than \u201cthe waiter\u201d, even though this is based on the simple strategy in Boxer to link third- person pronouns to named entities of type human.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer ##CITATION## \u2013 in the context of the shared task on recognising negation in English texts Morante and Blanco", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG  . CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG  . We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "In this paper we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer ##CITATION## \u2013 in the context of the shared task on recognising negation in English texts Morante and Blanco", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.", "Wide-coverage logic-based semantics Boxer ##CITATION## is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures Kamp and Reyle", 0, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["The units of measurement in the last two sentences were not recognised as such. The tricky time expression \u201cmid-80\u2019s\u201d only got a shallow interpretation. Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank   and robust parsers trained on it   make Boxer a state-of-the-art open- domain tool for deep semantic analysis.", "Wide-coverage logic-based semantics Boxer ##CITATION## is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures Kamp and Reyle", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "Wide-coverage logic-based semantics Boxer ##CITATION## is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures Kamp and Reyle", 0, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG  . CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG  . We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "Wide-coverage logic-based semantics Boxer ##CITATION## is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures Kamp and Reyle", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["277 Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory  , Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders  . 2.1 Combinatory Categorial Grammar.", "Wide-coverage logic-based semantics Boxer ##CITATION## is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures Kamp and Reyle", 1, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic.", "For the discursive analysis of texts DR metrics rely on the C&C Tools Curran et al specifically on the Boxer component ##CITATION##", 0, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["Completed: 3 (100.00%). Figure 3: Boxer output for Shared Task Text 2 Here we discuss the output of Boxer on the Shared Task Texts  . Boxer was able to produce semantic representation for all text without any further modifications to the software. For each text we briefly say what was good and bad about Boxer\u2019s analysis.", "For the discursive analysis of texts DR metrics rely on the C&C Tools Curran et al specifically on the Boxer component ##CITATION##", 0, "Wide-Coverage Semantic Analysis with Boxer", "Performance on Shared Task."], ["It was able to produce complete DRSs for all seven texts. Manually inspecting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren\u2019t; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C&C tools and freely available for research purposes. 277 Boxer is an open-domain tool for computing and reasoning with semantic representations.", "For the discursive analysis of texts DR metrics rely on the C&C Tools Curran et al specifically on the Boxer component ##CITATION##", 0, "Wide-Coverage Semantic Analysis with Boxer", "ABSTRACT"], ["277 Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory  , Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders  . 2.1 Combinatory Categorial Grammar.", "For the discursive analysis of texts DR metrics rely on the C&C Tools Curran et al specifically on the Boxer component ##CITATION##", 1, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["Finally, a comment on availability of Boxer. All sources of Boxer are available for download and free of noncommercial use. It is distributed with the C&C tools for natural language processing  , which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer", "For the discursive analysis of texts DR metrics rely on the C&C Tools Curran et al specifically on the Boxer component ##CITATION##", 1, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["<expe > ::= <ref> <expt > ::= <drs> <ref>\u2217 <drs> ::= <condition>\u2217 <condition> ::= <basic> | <complex> <basic> ::= <sym1 >(<expe >) | <sym2 >(<expe >,<expe >) | <named>(<expe >,<nam>,<sort>) <complex> ::= <expt > | <expt >\u21d2<expt > | <expt >\u2228<expt > | <ref>:<expt > DRSs are structures comprising two parts: 1) a set of discourse referents; and 2) a set of conditions constraining the interpretation of the discourse referents. Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs. The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics  , where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations. The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet  , and has some attractive formal properties  . There is only one way to state that an individual is participating in an event\u2014namely by relating it to the event using a binary relation expressing some thematic role.", "Computing logical forms as eg in ##CITATION##  and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers reshaping into generic present tense from other tenses and other issues that affect the quality of the statements", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types \u2014 and this must be the case to ensure the semantic composition process proceeds without type clashes \u2014 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.", "Computing logical forms as eg in ##CITATION##  and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers reshaping into generic present tense from other tenses and other issues that affect the quality of the statements", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["There is only one way to state that an individual is participating in an event\u2014namely by relating it to the event using a binary relation expressing some thematic role. Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear. Finally, it also allows us to characterize the meaning of thematic roles independently of the meaning of the verb that describes the event. We won\u2019t show the standard translation from DRS to FOL here  . Intuitively, translating DRSs into first-order formulas proceeds as follows: each discourse referent is translated as a first-order quantifier, and all DRS-conditions are translated into a conjunctive formula of FOL.", "Computing logical forms as eg in ##CITATION##  and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers reshaping into generic present tense from other tenses and other issues that affect the quality of the statements", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG  . CCG lends itself extremely well for this task because it is lexically driven and has only few \u201cgrammar\u201d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG  . We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.", "Computing logical forms as eg in ##CITATION##  and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers reshaping into generic present tense from other tenses and other issues that affect the quality of the statements", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations  . DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition. We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals  .In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1. Discourse Representation Structures (DRSs). 2.", "Computing logical forms as eg in ##CITATION##  and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers reshaping into generic present tense from other tenses and other issues that affect the quality of the statements", 1, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["The tricky time expression \u201cmid-80\u2019s\u201d only got a shallow interpretation. Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank   and robust parsers trained on it   make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays ##CITATION## Butler and Yoshimoto it would be natural to see generation as a reversed process and consider such semantic representations as input of a surface realization component", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank   and robust parsers trained on it   make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising. It was able to produce DRSs for all texts.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays ##CITATION## Butler and Yoshimoto it would be natural to see generation as a reversed process and consider such semantic representations as input of a surface realization component", 0, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations  . DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition. We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals  .In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1. Discourse Representation Structures (DRSs). 2.", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays ##CITATION## Butler and Yoshimoto it would be natural to see generation as a reversed process and consider such semantic representations as input of a surface realization component", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["Boxer is distributed with the C&C tools and freely available for research purposes. 277 Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory  , Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders  .", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays ##CITATION## Butler and Yoshimoto it would be natural to see generation as a reversed process and consider such semantic representations as input of a surface realization component", 1, "Wide-Coverage Semantic Analysis with Boxer", "Introduction"], ["Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types \u2014 and this must be the case to ensure the semantic composition process proceeds without type clashes \u2014 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.", "This line of research converts logical representations obtained from syntactic parses using ##CITATION##\u2019 Boxer ##CITATION## into Markov Logic Networks Richardson and Domingos and uses distributional semantics-based models such as that of Erk and Pado\u00b4  to deal with issues polysemy and ambiguity", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["3.1 Preprocessing. The input text needs to be tokenised with one sentence per line. In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing  . The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents. An example of a CCG derivation is shown in Figure 2.", "This line of research converts logical representations obtained from syntactic parses using ##CITATION##\u2019 Boxer ##CITATION## into Markov Logic Networks Richardson and Domingos and uses distributional semantics-based models such as that of Erk and Pado\u00b4  to deal with issues polysemy and ambiguity", 0, "Wide-Coverage Semantic Analysis with Boxer", "Practice."], ["Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs. The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics  , where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations. The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet  , and has some attractive formal properties  . There is only one way to state that an individual is participating in an event\u2014namely by relating it to the event using a binary relation expressing some thematic role. Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear.", "This line of research converts logical representations obtained from syntactic parses using ##CITATION##\u2019 Boxer ##CITATION## into Markov Logic Networks Richardson and Domingos and uses distributional semantics-based models such as that of Erk and Pado\u00b4  to deal with issues polysemy and ambiguity", 0, "Wide-Coverage Semantic Analysis with Boxer", "Theory."], ["The tricky time expression \u201cmid-80\u2019s\u201d only got a shallow interpretation. Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank   and robust parsers trained on it   make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxer\u2019s performance on the shared task for comparing semantic represtations was promising.", "This line of research converts logical representations obtained from syntactic parses using ##CITATION##\u2019 Boxer ##CITATION## into Markov Logic Networks Richardson and Domingos and uses distributional semantics-based models such as that of Erk and Pado\u00b4  to deal with issues polysemy and ambiguity", 1, "Wide-Coverage Semantic Analysis with Boxer", "Conclusion."], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "Additional German tags are obtained using the RFTagger 2 toolkit which annotates text with fine-grained part-of-speech tags ##CITATION## with a vocabulary of more than 700 tags containing rich morpho-syntactic information gender number case tense etc", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "Additional German tags are obtained using the RFTagger 2 toolkit which annotates text with fine-grained part-of-speech tags ##CITATION## with a vocabulary of more than 700 tags containing rich morpho-syntactic information gender number case tense etc", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Additional German tags are obtained using the RFTagger 2 toolkit which annotates text with fine-grained part-of-speech tags ##CITATION## with a vocabulary of more than 700 tags containing rich morpho-syntactic information gender number case tense etc", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "Additional German tags are obtained using the RFTagger 2 toolkit which annotates text with fine-grained part-of-speech tags ##CITATION## with a vocabulary of more than 700 tags containing rich morpho-syntactic information gender number case tense etc", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech. Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realita\u00a8 t?", "Additional German tags are obtained using the RFTagger 2 toolkit which annotates text with fine-grained part-of-speech tags ##CITATION## with a vocabulary of more than 700 tags containing rich morpho-syntactic information gender number case tense etc", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "Additional German tags are obtained using the RFTagger 2 toolkit which annotates text with fine-grained part-of-speech tags ##CITATION## with a vocabulary of more than 700 tags containing rich morpho-syntactic information gender number case tense etc", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German we obtain a tagging accuracy of 9724 which is close to the 9739 achieved by the RFTagger ##CITATION## which to our knowledge is the best tagger for German5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Table 3 shows the results of an evaluation based on the plain STTS tagset. The first result was obtained with TnT trained on Tiger data which was mapped to STTS before. The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS. The third row gives the corresponding figures for our tagger. 5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.", "For German we obtain a tagging accuracy of 9724 which is close to the 9739 achieved by the RFTagger ##CITATION## which to our knowledge is the best tagger for German5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["A supplementary lexicon was created by analyzing a word list which included all words from the fa\u03b1 pa\u03b1 (t) log pa\u03b1(t) < 1 training, development, and test data with a German computationa l morphology. The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger   and the SVMTool  , which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically.", "For German we obtain a tagging accuracy of 9724 which is close to the 9739 achieved by the RFTagger ##CITATION## which to our knowledge is the best tagger for German5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["With athreshold of 10\u22123 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.", "For German we obtain a tagging accuracy of 9724 which is close to the 9739 achieved by the RFTagger ##CITATION## which to our knowledge is the best tagger for German5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["With athreshold of 10\u22123 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.", "For German finally we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics  , pages 777\u2013784 Manchester, August 2008 context probability of the third POS tag is therefore 0.", "For German finally we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.", "For German finally we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["2 8 9 7. 1 7 97.26 97.51 9 7. 3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants   for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.", "For German finally we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data  ", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "We use the following baselines: SVMTool Gime\u00b4nez and Ma`rquez an SVM-based dis- criminative tagger; RFTagger ##CITATION## an n-gram Hidden Markov Model HMM tagger developed for POS+MORPH tagging; Morfette Chrupa\u0142a et al an averaged perceptron with beam search decoder; CRFSuite Okazaki a fast CRF implementation; and the Stanford Tagger Toutanova et al a bidirectional Maximum Entropy Markov Model", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Czech POS tagging has been extensively studied in the past  . Spoustov et al.   compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (Morc\u02c7e), and evaluated them on the Prague Dependency Treebank (PDT 2.0). Morc\u02c7e\u2019s tagging accuracy was 95.12%, 0.3% better than the n-gram tagger. A hybrid system based on four different tagging methods reached an accuracy of 95.68%.", "We use the following baselines: SVMTool Gime\u00b4nez and Ma`rquez an SVM-based dis- criminative tagger; RFTagger ##CITATION## an n-gram Hidden Markov Model HMM tagger developed for POS+MORPH tagging; Morfette Chrupa\u0142a et al an averaged perceptron with beam search decoder; CRFSuite Okazaki a fast CRF implementation; and the Stanford Tagger Toutanova et al a bidirectional Maximum Entropy Markov Model", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "We use the following baselines: SVMTool Gime\u00b4nez and Ma`rquez an SVM-based dis- criminative tagger; RFTagger ##CITATION## an n-gram Hidden Markov Model HMM tagger developed for POS+MORPH tagging; Morfette Chrupa\u0142a et al an averaged perceptron with beam search decoder; CRFSuite Okazaki a fast CRF implementation; and the Stanford Tagger Toutanova et al a bidirectional Maximum Entropy Markov Model", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "We use the following baselines: SVMTool Gime\u00b4nez and Ma`rquez an SVM-based dis- criminative tagger; RFTagger ##CITATION## an n-gram Hidden Markov Model HMM tagger developed for POS+MORPH tagging; Morfette Chrupa\u0142a et al an averaged perceptron with beam search decoder; CRFSuite Okazaki a fast CRF implementation; and the Stanford Tagger Toutanova et al a bidirectional Maximum Entropy Markov Model", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German we show results for RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes). The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold. A threshold of 6 consistently produced optimal or near optimal results for pre-pruning. Thus, pre-pruning with a threshold of 6 was used in the experiments. The tagger treats dots in POS tag labels as attribute separators.", "For German we show results for RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.)", "For German we show results for RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities. The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|ti\u22121 ) is internally computed as a product of attribute probabili ties. In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.", "For German we show results for RFTagger ##CITATION##", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category. The number of additional attributes is fixed for each main category. The additional attributes are category-specific. The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.", "The decisiontree uses different context features for the predic tion of different attributes ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Splitting of the POS Tags."], ["\u2022 All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases). By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) \u2217 p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) \u2217 p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) \u2217 p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) \u2217 p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the \u2217 p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) \u2217 p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1. Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context. These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.", "The decisiontree uses different context features for the predic tion of different attributes ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization. Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words? A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.", "The decisiontree uses different context features for the predic tion of different attributes ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class. In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data. Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not. The best test for each node is selected with the standard information gain criterion.", "The decisiontree uses different context features for the predic tion of different attributes ##CITATION##", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Ferri et al.   describe a more complex backoff smoothing method. Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0). Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes. These two-class trees can be pruned with a fixed pruning threshold.", "However we found that we achieved better accuracy by using RFTagger ##CITATION## which tags nouns with their morphological case", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "However we found that we achieved better accuracy by using RFTagger ##CITATION## which tags nouns with their morphological case", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Our tagger combines two ideas, the decomposition of the probability of complex POS tags into a product of feature probabilities, and the estimation of the conditional probabilities with decision trees. A similar idea was previously presented in Kempe  , but apparently never applied again. The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid   and Ma`rquez   used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.", "However we found that we achieved better accuracy by using RFTagger ##CITATION## which tags nouns with their morphological case", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class. In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data. Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not. The best test for each node is selected with the standard information gain criterion.", "However we found that we achieved better accuracy by using RFTagger ##CITATION## which tags nouns with their morphological case", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d. \u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not. The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k \u2265 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined.", "However we found that we achieved better accuracy by using RFTagger ##CITATION## which tags nouns with their morphological case", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "With respect to morphosyntactic annotations parts of speech pos and morphological annotations morph five Annotation Models for German are currently available: STTS Schiller et al pos TIGER Brants and Hansen morph Morphisto Zielinski and Simon pos morph RFTagger ##CITATION## pos morph Connexor Tapanainen and Ja\u00a8rvinen pos morph", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "With respect to morphosyntactic annotations parts of speech pos and morphological annotations morph five Annotation Models for German are currently available: STTS Schiller et al pos TIGER Brants and Hansen morph Morphisto Zielinski and Simon pos morph RFTagger ##CITATION## pos morph Connexor Tapanainen and Ja\u00a8rvinen pos morph", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["More precisely, if T\u03b1 is the set of POS tags that occurred with suffix \u03b1, |T | is the size of the set T , f\u03b1 is the frequency of suffix \u03b1, and p\u03b1(t) is the probability of POS tag t among the words with suffix \u03b1, then the following condition must hold: tion between definite and indefinite articles, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other symbols which the Tiger treebank annotates with \u201c$(\u201d. A supplementary lexicon was created by analyzing a word list which included all words from the fa\u03b1 pa\u03b1 (t) log pa\u03b1(t) < 1 training, development, and test data with a German computationa l morphology. The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger   and the SVMTool  , which is based on support vector machines.7 The training of the SVMTool took more than a day.", "With respect to morphosyntactic annotations parts of speech pos and morphological annotations morph five Annotation Models for German are currently available: STTS Schiller et al pos TIGER Brants and Hansen morph Morphisto Zielinski and Simon pos morph RFTagger ##CITATION## pos morph Connexor Tapanainen and Ja\u00a8rvinen pos morph", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "With respect to morphosyntactic annotations parts of speech pos and morphological annotations morph five Annotation Models for German are currently available: STTS Schiller et al pos TIGER Brants and Hansen morph Morphisto Zielinski and Simon pos morph RFTagger ##CITATION## pos morph Connexor Tapanainen and Ja\u00a8rvinen pos morph", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["The information gain is therefore 0.92 \u2212 (8/15 \u2217 1 \u2212 7/15 \u2217 0.59) = 0.11. The resulting score is 75 \u2217 0.11 = 8.25. Given a threshold of 6, the node is therefore not pruned. We experimented with pre-pruning (where a node is always pruned if the gain is below the in the two subsets. The weight of each subset is proportional to its size.", "4 rdf:type  given in 5 can be transformed into a description in terms of the OLiA Reference Model such as in 6", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold. A threshold of 6 consistently produced optimal or near optimal results for pre-pruning. Thus, pre-pruning with a threshold of 6 was used in the experiments. The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category.", "4 rdf:type  given in 5 can be transformed into a description in terms of the OLiA Reference Model such as in 6", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["The number of additional attributes is fixed for each main category. The additional attributes are category-specific. The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.", "4 rdf:type  given in 5 can be transformed into a description in terms of the OLiA Reference Model such as in 6", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Splitting of the POS Tags."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "4 rdf:type  given in 5 can be transformed into a description in terms of the OLiA Reference Model such as in 6", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["7 It was planned to include also the Stanford tagger.   in this comparison, but it was not possible to train it on the Tiger data. 8 In German, the genitive case of arguments is more and. more replaced by the dative. Table 2: Tagging accuracies on development data in percent.", "From the annotated document the plain tokenized text is extracted and analyzed by one or more of the following NLP tools:     and the Stanford Tagger       and the BerkeleyParser   and  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Decision trees   are normally used as classifiers, i.e. they assign classes to objects which are represented as attribute vectors. The non-terminal nodes are labeled with attribute tests, the edges with the possible outcomes of a test, and the terminal nodes are labeled with classes. An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating the test of the daughter node, and so on until a terminal node is reached whose class is assigned to the object. Decision Trees are turned into probability estimation trees by storing a probability for each possible class at the terminal nodes instead of a single result class. Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.", "From the annotated document the plain tokenized text is extracted and analyzed by one or more of the following NLP tools:     and the Stanford Tagger       and the BerkeleyParser   and  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger   and the SVMTool  , which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0   and compared to the TnT tag- ger.", "From the annotated document the plain tokenized text is extracted and analyzed by one or more of the following NLP tools:     and the Stanford Tagger       and the BerkeleyParser   and  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "From the annotated document the plain tokenized text is extracted and analyzed by one or more of the following NLP tools:     and the Stanford Tagger       and the BerkeleyParser   and  ", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "From the annotated document the plain tokenized text is extracted and analyzed by one or more of the following NLP tools:     and the Stanford Tagger       and the BerkeleyParser   and  ", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context. These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute. Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method. The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger ##CITATION## which uses a tagset containing approximately 800 tags", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger ##CITATION## which uses a tagset containing approximately 800 tags", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger ##CITATION## which uses a tagset containing approximately 800 tags", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger ##CITATION## which uses a tagset containing approximately 800 tags", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.", "For German we used morphologically rich tags from RFTagger ##CITATION## that contains morphological information such as case number and gender for nouns and tense for verbs", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "For German we used morphologically rich tags from RFTagger ##CITATION## that contains morphological information such as case number and gender for nouns and tense for verbs", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc). Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in). The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or). For evaluation purposes, the refined tags are mapped back to the original tags. This mapping is unambiguous.", "For German we used morphologically rich tags from RFTagger ##CITATION## that contains morphological information such as case number and gender for nouns and tense for verbs", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "For German we used morphologically rich tags from RFTagger ##CITATION## that contains morphological information such as case number and gender for nouns and tense for verbs", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German the fine-grained POS information used for pre-processing was computed by the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["A similar idea was previously presented in Kempe  , but apparently never applied again. The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid   and Ma`rquez   used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman   applied probabilistic decision trees to parsing, but not with a generative model.", "For German the fine-grained POS information used for pre-processing was computed by the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold. A threshold of 6 consistently produced optimal or near optimal results for pre-pruning. Thus, pre-pruning with a threshold of 6 was used in the experiments. The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category.", "For German the fine-grained POS information used for pre-processing was computed by the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "For German the fine-grained POS information used for pre-processing was computed by the RFTagger ##CITATION##", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["The training with a context size of 10 took about 4 minutes. 5.2 Czech Academic Corpus. We also evaluated our tagger on the Czech Academic corpus   which contains 652.131 tokens and about 1200 different POS tags. The data was divided into 80% training data, 10% development data and 10% test data. 89 88.9 88.8 Provost & Domingos   noted that well- known decision tree induction algorithms such as C4.5   or CART   fail to produce accurate probability estimates.", "The part-of-speeches were generated using the TreeTagger and the RFTagger ##CITATION## which produces more fine-grained tags that include also person gender and case information While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus the RFTagger produces 756 different fine-grained tags on the same corpus ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon. 5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank, although this information is important for the disambiguation of the case of the next noun phrase. In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc). Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in). The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or).", "The part-of-speeches were generated using the TreeTagger and the RFTagger ##CITATION## which produces more fine-grained tags that include also person gender and case information While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus the RFTagger produces 756 different fine-grained tags on the same corpus ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["7 It was planned to include also the Stanford tagger.   in this comparison, but it was not possible to train it on the Tiger data. 8 In German, the genitive case of arguments is more and. more replaced by the dative. Table 2: Tagging accuracies on development data in percent.", "The part-of-speeches were generated using the TreeTagger and the RFTagger ##CITATION## which produces more fine-grained tags that include also person gender and case information While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus the RFTagger produces 756 different fine-grained tags on the same corpus ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "The part-of-speeches were generated using the TreeTagger and the RFTagger ##CITATION## which produces more fine-grained tags that include also person gender and case information While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus the RFTagger produces 756 different fine-grained tags on the same corpus ", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "The part-of-speeches were generated using the TreeTagger and the RFTagger ##CITATION## which produces more fine-grained tags that include also person gender and case information While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus the RFTagger produces 756 different fine-grained tags on the same corpus ", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "For German the POS and morphological tags were obtained from RFTagger ##CITATION## which provides morphological information such as case number and gender for nouns and tense for verbs", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data. Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23. Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here.", "For German the POS and morphological tags were obtained from RFTagger ##CITATION## which provides morphological information such as case number and gender for nouns and tense for verbs", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.", "For German the POS and morphological tags were obtained from RFTagger ##CITATION## which provides morphological information such as case number and gender for nouns and tense for verbs", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "For German the POS and morphological tags were obtained from RFTagger ##CITATION## which provides morphological information such as case number and gender for nouns and tense for verbs", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech.", "For German the POS and morphological tags were obtained from RFTagger ##CITATION## which provides morphological information such as case number and gender for nouns and tense for verbs", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["To understand why, assume that there are three trees for the gender attributes. Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d. \u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not.", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "The POS tags are generated with the RFTagger ##CITATION## for German which produces fine-grained tags that include person gender and case information", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "The POS tags are generated with the RFTagger ##CITATION## for German which produces fine-grained tags that include person gender and case information", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "The POS tags are generated with the RFTagger ##CITATION## for German which produces fine-grained tags that include person gender and case information", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.", "The POS tags are generated with the RFTagger ##CITATION## for German which produces fine-grained tags that include person gender and case information", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger   and the SVMTool  , which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0   and compared to the TnT tag- ger.", "Morphological information is annotated using RFTagger ##CITATION## a state-of-the-art morphological tagger based on decision trees and a large context window which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "Morphological information is annotated using RFTagger ##CITATION## a state-of-the-art morphological tagger based on decision trees and a large context window which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities. The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|ti\u22121 ) is internally computed as a product of attribute probabili ties. In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.", "Morphological information is annotated using RFTagger ##CITATION## a state-of-the-art morphological tagger based on decision trees and a large context window which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The results were compared to those obtained with the TnT tagger   and the SVMTool  , which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0   and compared to the TnT tag- ger. 5.1 Tiger Corpus. The German Tiger treebank   contains over 888,000 tokens.", "Morphological information is annotated using RFTagger ##CITATION## a state-of-the-art morphological tagger based on decision trees and a large context window which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Morphological information is annotated using RFTagger ##CITATION## a state-of-the-art morphological tagger based on decision trees and a large context window which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["To understand why, assume that there are three trees for the gender attributes. Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is \u201c1:ART.Nom\u201d which states that the preceding tag is an article with the attribute \u201cNom\u201d. \u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not.", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "All parallel corpora were POS-tagged with the TreeTagger Schmid; in addition for German fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger ##CITATION##", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["The analyses gener |Ta\u03b1| t\u2208Ta\u03b1 p\u03b1(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing. Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger   and the SVMTool  , which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0   and compared to the TnT tag- ger.", "In order to train the POS-based reordering model probabilistic rules were learned based on the POS tags from the TreeTagger ##CITATION## of the training corpus and the alignment", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "In order to train the POS-based reordering model probabilistic rules were learned based on the POS tags from the TreeTagger ##CITATION## of the training corpus and the alignment", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics  , pages 777\u2013784 Manchester, August 2008 context probability of the third POS tag is therefore 0.", "In order to train the POS-based reordering model probabilistic rules were learned based on the POS tags from the TreeTagger ##CITATION## of the training corpus and the alignment", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "In order to train the POS-based reordering model probabilistic rules were learned based on the POS tags from the TreeTagger ##CITATION## of the training corpus and the alignment", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.)", "The POS tags are generated using the RFTagger   for German", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in). The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or). For evaluation purposes, the refined tags are mapped back to the original tags. This mapping is unambiguous. 7 It was planned to include also the Stanford tagger.", "The POS tags are generated using the RFTagger   for German", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["88.7 88.6 88.5 \u2019 c o n t e x t d a t a 2 \u2019 2 3 4 5 6 7 8 9 10 A n ope n que stio n is wh eth er the SV MT ool (or oth er dis cri min ativ ely trai ned tag ger s) cou ld out - perf orm the pre sen ted tag ger if the sa me dec om positi on of PO S tag s and the sa me con text size wasFigure 3: Accuracy on development data depend ing on context size The best accuracy of our tagger on the development set was 88.9% obtained with a context of 4 preceding POS tags. The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5. The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag- ger. The difference is significant.", "The POS tags are generated using the RFTagger   for German", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1. This problem 2:N.Reg p=0.999 0:N.Name 0:N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 .... is solved by renormalizing the probabilities. The probability of an attribute (such as \u201cNom\u201d) is always conditioned on the respective base POS (such as \u201cN\u201d) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns. The test 1:ART.Nom checks if the preceding word is a nominative article. assigned to the top node.", "The POS tags are generated using the RFTagger   for German", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "We lemmatized German articles adjectives only positive form for some pronouns and for nouns in order to remove the lexical redundancy eg Bildes as Bild by using the fine- grained part-of-speech tags generated by RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "We lemmatized German articles adjectives only positive form for some pronouns and for nouns in order to remove the lexical redundancy eg Bildes as Bild by using the fine- grained part-of-speech tags generated by RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "We lemmatized German articles adjectives only positive form for some pronouns and for nouns in order to remove the lexical redundancy eg Bildes as Bild by using the fine- grained part-of-speech tags generated by RFTagger ##CITATION##", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.", "We lemmatized German articles adjectives only positive form for some pronouns and for nouns in order to remove the lexical redundancy eg Bildes as Bild by using the fine- grained part-of-speech tags generated by RFTagger ##CITATION##", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "PAPER"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "611 POS Tagging We use RFTagger ##CITATION## for POS tagging", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["\u201c1:ART\u201d is also a valid attribute specification, but \u201c1:Nom\u201d is not. The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k \u2265 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined. This restriction improved the tagging accuracy for large contexts. 2.2 Pruning Criterion. The tagger applies3 the critical-value pruning strategy proposed by  .", "611 POS Tagging We use RFTagger ##CITATION## for POS tagging", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Decision Trees."], ["A hybrid system based on four different tagging methods reached an accuracy of 95.68%. Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.", "611 POS Tagging We use RFTagger ##CITATION## for POS tagging", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "611 POS Tagging We use RFTagger ##CITATION## for POS tagging", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.", "In the second step the normalized training data is annotated with Part-of-Speech tags PoS-tags and word lemmas using RFTagger ##CITATION## which was trained on the French tree- bank Abeille\u00b4 et al", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "In the second step the normalized training data is annotated with Part-of-Speech tags PoS-tags and word lemmas using RFTagger ##CITATION## which was trained on the French tree- bank Abeille\u00b4 et al", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u02c6N = t\u02c61, ..., t\u02c6N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "In the second step the normalized training data is annotated with Part-of-Speech tags PoS-tags and word lemmas using RFTagger ##CITATION## which was trained on the French tree- bank Abeille\u00b4 et al", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "In the second step the normalized training data is annotated with Part-of-Speech tags PoS-tags and word lemmas using RFTagger ##CITATION## which was trained on the French tree- bank Abeille\u00b4 et al", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger which is designed for annotating fine-grained morphological tags  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["With athreshold of 10\u22123 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger which is designed for annotating fine-grained morphological tags  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Our Tagger."], ["The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realita\u00a8 t (feminine), the article reading must be wrong. The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (\u201cThe to be taxed income decreases\u201d; The t\u02c6N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger which is designed for annotating fine-grained morphological tags  ", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger which is designed for annotating fine-grained morphological tags  ", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "Tagging and tagging errors For tagging we use a version of RFTagger   2 Pairwise bootstrap resampling withsamples", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["\u2022 All adjectives appearing after a singular article and a particle are singular (32 of 32 cases). \u2022 All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases). By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) \u2217 p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) \u2217 p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) \u2217 p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) \u2217 p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the \u2217 p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) \u2217 p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1. Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context.", "Tagging and tagging errors For tagging we use a version of RFTagger   2 Pairwise bootstrap resampling withsamples", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Introduction"], ["The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid   and Ma`rquez   used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman   applied probabilistic decision trees to parsing, but not with a generative model. used.", "Tagging and tagging errors For tagging we use a version of RFTagger   2 Pairwise bootstrap resampling withsamples", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "Tagging and tagging errors For tagging we use a version of RFTagger   2 Pairwise bootstrap resampling withsamples", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Summary."], ["This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization. Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words? A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.", "The results presented here were achieved using the RFTagger ##CITATION## with an increased context size of 10 which we found to perform best on average on our data", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["Since German is a verb-final language, these tests clearly make sense. Table 4 shows the performance on the test data. Our tagger was used with a context size of 10. The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon. These values were optimal on the development data.", "The results presented here were achieved using the RFTagger ##CITATION## with an increased context size of 10 which we found to perform best on average on our data", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants   for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization. Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words?", "The results presented here were achieved using the RFTagger ##CITATION## with an increased context size of 10 which we found to perform best on average on our data", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "The results presented here were achieved using the RFTagger ##CITATION## with an increased context size of 10 which we found to perform best on average on our data", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["2 8 9 7. 1 7 97.26 97.51 9 7. 3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants   for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.", "The results presented here were achieved using the RFTagger ##CITATION## with an increased context size of 10 which we found to perform best on average on our data", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["4 5 84.11 89.14 8 5. 0 0 85.92 91.07 Table 4: Tagging accuracies on test data. By far the most frequent tagging error was the confusion of nominative and accusative case. If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger. The resulting score of a binomial test is below 0.001.", "So far the Complex Concept Builder implements tokenization Schmid lemmatisation Schmid part-of-speech tagging ##CITATION## named entity detection Faruqui and Pado\u00b4  syntactical parsing Bohnet coreference analysis for German Lappin and Leass Stuckardt relation extraction Blessing et al and sentiment analysis for English Taboada et al", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Evaluation."], ["The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid   and Ma`rquez   used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman   applied probabilistic decision trees to parsing, but not with a generative model. used.", "So far the Complex Concept Builder implements tokenization Schmid lemmatisation Schmid part-of-speech tagging ##CITATION## named entity detection Faruqui and Pado\u00b4  syntactical parsing Bohnet coreference analysis for German Lappin and Leass Stuckardt relation extraction Blessing et al and sentiment analysis for English Taboada et al", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Morc\u02c7e e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset. We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).", "So far the Complex Concept Builder implements tokenization Schmid lemmatisation Schmid part-of-speech tagging ##CITATION## named entity detection Faruqui and Pado\u00b4  syntactical parsing Bohnet coreference analysis for German Lappin and Leass Stuckardt relation extraction Blessing et al and sentiment analysis for English Taboada et al", 0, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "Discussion."], ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.", "So far the Complex Concept Builder implements tokenization Schmid lemmatisation Schmid part-of-speech tagging ##CITATION## named entity detection Faruqui and Pado\u00b4  syntactical parsing Bohnet coreference analysis for German Lappin and Leass Stuckardt relation extraction Blessing et al and sentiment analysis for English Taboada et al", 1, "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging", "ABSTRACT"], ["8 of the 43 words are translated to English multi-word phrases (denoted as \u201cphrase\u201d in Table 3). Since our method currently only considers unigram English words, we are not able to find translations for these words. But it is not difficult to extend our method to handle this problem. We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases. The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d).", "To our knowledge this association measure has not been used yet in translation spotting It is computed as: O11 + 1 O22 + 1  scribed in ##CITATION## ", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.", "To our knowledge this association measure has not been used yet in translation spotting It is computed as: O11 + 1 O22 + 1  scribed in ##CITATION## ", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["ranked list of candidate words, associating with each candidate a score estimated by the particular method. If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general. Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work. To estimate P . We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 ,..., ek that ap query Q is generated by this model: pear in top M positions in both lists.", "To our knowledge this association measure has not been used yet in translation spotting It is computed as: O11 + 1 O22 + 1  scribed in ##CITATION## ", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["/ \u220ft ct ! can be omitted as this part depends on the query only and thus is the same for all the documents. in the English target corpus. In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. So our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.", "To our knowledge this association measure has not been used yet in translation spotting It is computed as: O11 + 1 O22 + 1  scribed in ##CITATION## ", 1, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping. ble of pinyin, api is the ith sylla li is the English letter sequence estimation. We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = \u03b1 \u22c5 Pml (tc | Tc (C (e))) + (1 \u2212\u03b1 ) \u22c5 Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem. We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) \u2211dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t\u2208Tc (C ( e )) lables to English letter sequences. To reduce the search space, we limit the number of English letters that each pinyin syllable can map to as 0, where Pml (\u2022) are the maximu m likelihood esti 1, or 2.", "To our knowledge this association measure has not been used yet in translation spotting It is computed as: O11 + 1 O22 + 1  scribed in ##CITATION## ", 1, "Mining New Word Translations from Comparable Corpora", "Translation by context."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "At present the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary transliteration or parallel corpus to the intermediate pattern based on comparable corpus Lee et al ##CITATION## Virga and Khudanpur andColing Poster Volume pages1443 Beijing Augustthen become a new pattern based on Web mining Fang et al Sproat et al", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. New words such as person names, organization names, technical terms, etc. appear frequently.", "At present the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary transliteration or parallel corpus to the intermediate pattern based on comparable corpus Lee et al ##CITATION## Virga and Khudanpur andColing Poster Volume pages1443 Beijing Augustthen become a new pattern based on Web mining Fang et al Sproat et al", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus. \u03b1 is set to 0.6 in our experiments. must precede the pinyin syllable mapped to e2 . Our method differs from   and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP  and (Al ing in the English corpus. Another difference is Onaizan and Knight, 2002b). Knight and Graehl   proposed a probabilistic model for machine transliteration.", "At present the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary transliteration or parallel corpus to the intermediate pattern based on comparable corpus Lee et al ##CITATION## Virga and Khudanpur andColing Poster Volume pages1443 Beijing Augustthen become a new pattern based on Web mining Fang et al Sproat et al", 0, "Mining New Word Translations from Comparable Corpora", "Translation by transliteration."], ["Precision and recall for different values of M The past research of   utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs. Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus. As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus. This list of 43 words is shown in Table 3.", "At present the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary transliteration or parallel corpus to the intermediate pattern based on comparable corpus Lee et al ##CITATION## Virga and Khudanpur andColing Poster Volume pages1443 Beijing Augustthen become a new pattern based on Web mining Fang et al Sproat et al", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.", "Using parallel corpora Kupiec Feng et al eg bilingual Wikipedia entries on the same person renders high accuracy but suffers from high scarcity To alleviate such scarcity Fung and Yee ##CITATION## explore a more vast resource of comparable corpora which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages eg news articles on the same event ", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.", "Using parallel corpora Kupiec Feng et al eg bilingual Wikipedia entries on the same person renders high accuracy but suffers from high scarcity To alleviate such scarcity Fung and Yee ##CITATION## explore a more vast resource of comparable corpora which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages eg news articles on the same event ", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Using parallel corpora Kupiec Feng et al eg bilingual Wikipedia entries on the same person renders high accuracy but suffers from high scarcity To alleviate such scarcity Fung and Yee ##CITATION## explore a more vast resource of comparable corpora which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages eg news articles on the same event ", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["Much research has been done on using parallel corpora to learn bilingual lexicons  . But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.", "Using parallel corpora Kupiec Feng et al eg bilingual Wikipedia entries on the same person renders high accuracy but suffers from high scarcity To alleviate such scarcity Fung and Yee ##CITATION## explore a more vast resource of comparable corpora which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages eg news articles on the same event ", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include  .", "Using parallel corpora Kupiec Feng et al eg bilingual Wikipedia entries on the same person renders high accuracy but suffers from high scarcity To alleviate such scarcity Fung and Yee ##CITATION## explore a more vast resource of comparable corpora which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages eg news articles on the same event ", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["\u2018phrase\u2019 means the correct translation contains multiple English words. As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position. And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position. On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position. If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except \u53f6\u739b\u65af,\u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.", "Various correlation measures have been used: log-likelihood ratio Rapp Chiao and Zweigenbaum tfidf Fung and Yee pointwise mutual information PMI Andrade et al context heterogeneity Fung etc ##CITATION##  represented contexts using language models", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["\u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words. As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position. And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position. On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.", "Various correlation measures have been used: log-likelihood ratio Rapp Chiao and Zweigenbaum tfidf Fung and Yee pointwise mutual information PMI Andrade et al context heterogeneity Fung etc ##CITATION##  represented contexts using language models", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["Precision and recall for different values of M The past research of   utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs. Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus. As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus. This list of 43 words is shown in Table 3.", "Various correlation measures have been used: log-likelihood ratio Rapp Chiao and Zweigenbaum tfidf Fung and Yee pointwise mutual information PMI Andrade et al context heterogeneity Fung etc ##CITATION##  represented contexts using language models", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. The English word e document. We employ the language modeling approach   for corresponding to that document translation of c . C (e* ) is the this retrieval problem. More details are given in Section 3. On the other hand, when we only look at the word w itself, we can rely on the pronunciation of w to locate its translation.", "Various correlation measures have been used: log-likelihood ratio Rapp Chiao and Zweigenbaum tfidf Fung and Yee pointwise mutual information PMI Andrade et al context heterogeneity Fung etc ##CITATION##  represented contexts using language models", 1, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus   co-occurrence models generated from aligned documents   and transliteration information  ", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. So our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection. \u220f P(tc tc\u2208C ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word. q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus   co-occurrence models generated from aligned documents   and transliteration information  ", 0, "Mining New Word Translations from Comparable Corpora", "Translation by context."], ["\u2018phrase\u2019 means the correct translation contains multiple English words. As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position. And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position. On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position. If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except \u53f6\u739b\u65af,\u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus   co-occurrence models generated from aligned documents   and transliteration information  ", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus   co-occurrence models generated from aligned documents   and transliteration information  ", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.", "The other is multilingual parallel and comparable corpora eg Wikipedia1 wherein features such as co- occurrence frequency and context are popularly employed Cheng et al ##CITATION## Cao et al Lin et al", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus. In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = \u2211t ct is the total number of terms in query Q . For ranking purpose, the first fraction n! / \u220ft ct ! can be omitted as this part depends on the query only and thus is the same for all the documents. in the English target corpus.", "The other is multilingual parallel and comparable corpora eg Wikipedia1 wherein features such as co- occurrence frequency and context are popularly employed Cheng et al ##CITATION## Cao et al Lin et al", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["That is, Chinese is the source language and English is the target language. We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. The work of   noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem.", "The other is multilingual parallel and comparable corpora eg Wikipedia1 wherein features such as co- occurrence frequency and context are popularly employed Cheng et al ##CITATION## Cao et al Lin et al", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "The other is multilingual parallel and comparable corpora eg Wikipedia1 wherein features such as co- occurrence frequency and context are popularly employed Cheng et al ##CITATION## Cao et al Lin et al", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "##CITATION##   presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies combining both transliteration and context information", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.", "##CITATION##   presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies combining both transliteration and context information", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates. English Service. The size of the English corpus from Jul to Dec The context C(c)of a Chinese word c was col 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes. We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list.", "##CITATION##   presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies combining both transliteration and context information", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "##CITATION##   presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies combining both transliteration and context information", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include  . When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.", "Much of the work involving comparable corpora has focused on extracting word translations Fung and Yee Rapp Diab and Finch Koehn and Knight Gaussier et al ##CITATION## Shinyama and Sekine", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.", "Much of the work involving comparable corpora has focused on extracting word translations Fung and Yee Rapp Diab and Finch Koehn and Knight Gaussier et al ##CITATION## Shinyama and Sekine", 0, "Mining New Word Translations from Comparable Corpora", "PAPER"], ["Previous research efforts on acquiring translations from comparable corpora include  . When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.", "Much of the work involving comparable corpora has focused on extracting word translations Fung and Yee Rapp Diab and Finch Koehn and Knight Gaussier et al ##CITATION## Shinyama and Sekine", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Much of the work involving comparable corpora has focused on extracting word translations Fung and Yee Rapp Diab and Finch Koehn and Knight Gaussier et al ##CITATION## Shinyama and Sekine", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["That is, Chinese is the source language and English is the target language. We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. The work of   noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem.", "Some recent research used comparable corpora to mine name translation pairs Feng et al Kutsumi et al Udupa et al Ji Fung and Yee Rapp ##CITATION## Lu and Zhao Hassan et al", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["Much research has been done on using parallel corpora to learn bilingual lexicons  . But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.", "Some recent research used comparable corpora to mine name translation pairs Feng et al Kutsumi et al Udupa et al Ji Fung and Yee Rapp ##CITATION## Lu and Zhao Hassan et al", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Some recent research used comparable corpora to mine name translation pairs Feng et al Kutsumi et al Udupa et al Ji Fung and Yee Rapp ##CITATION## Lu and Zhao Hassan et al", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm. lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected.", "Some recent research used comparable corpora to mine name translation pairs Feng et al Kutsumi et al Udupa et al Ji Fung and Yee Rapp ##CITATION## Lu and Zhao Hassan et al", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. The work of   noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching West on a bipartite graph GB = VB = Si  Sj  EB by Xinhua news who publishes news in E C both English and Chinese which were also used with edge weights that are defined by TS  The maximum bipartite matching finds a subset of by Kim et al  and ##CITATION##  ", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm. lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected. The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching West on a bipartite graph GB = VB = Si  Sj  EB by Xinhua news who publishes news in E C both English and Chinese which were also used with edge weights that are defined by TS  The maximum bipartite matching finds a subset of by Kim et al  and ##CITATION##  ", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["Each of the two individual methods provides a P(Q | D) is the one that best matches the query. ranked list of candidate words, associating with each candidate a score estimated by the particular method. If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general. Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work. To estimate P .", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching West on a bipartite graph GB = VB = Si  Sj  EB by Xinhua news who publishes news in E C both English and Chinese which were also used with edge weights that are defined by TS  The maximum bipartite matching finds a subset of by Kim et al  and ##CITATION##  ", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. The English word e document. We employ the language modeling approach   for corresponding to that document translation of c . C (e* ) is the this retrieval problem.", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching West on a bipartite graph GB = VB = Si  Sj  EB by Xinhua news who publishes news in E C both English and Chinese which were also used with edge weights that are defined by TS  The maximum bipartite matching finds a subset of by Kim et al  and ##CITATION##  ", 1, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["And among those cases where more than one English words appeared within the top M positions for both lists, many were multiple translations of a Chinese word. This happened for example when a Chinese word was a non-English person name. The name could have multiple translations in English. For example, \u7c73 \u6d1b\u897f\u5a1c was a Russian name. Mirochina and Miroshina both appeared in top 10 positions of both lists.", "Recently holistic approaches combining such similarities have been studied ##CITATION## You et al Kim et al", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available.", "Recently holistic approaches combining such similarities have been studied ##CITATION## You et al Kim et al", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons  .", "Recently holistic approaches combining such similarities have been studied ##CITATION## You et al Kim et al", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Recently holistic approaches combining such similarities have been studied ##CITATION## You et al Kim et al", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["We then n! rank these words e1 , e2 ,..., ek according to the P (Q | D ) = \u220f P (t | D ) c t average of their rank positions in the two lists. \u220f t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output. If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.", "  rank translation candidates using PH and CX independently and return results with the highest average rank", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P . More details are is estimated. The document with the highest given in Section 4. Each of the two individual methods provides a P(Q | D) is the one that best matches the query. ranked list of candidate words, associating with each candidate a score estimated by the particular method.", "  rank translation candidates using PH and CX independently and return results with the highest average rank", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 \u2013 Jul 15, period 2 is Jul 16 \u2013 Jul 31, \u2026, period 12 is Dec 16 \u2013 Dec 31. #c is the total number of new Chinese source words in the period. #e is the total number of English translation candidates in the period. #o is the total number of output English translations. #Cor is the number of correct English translations output.", "  rank translation candidates using PH and CX independently and return results with the highest average rank", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["We considered these English words as potential translations of the Chinese source words. We call Section 4), which was used to rank the English candidate words based on transliteration. Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2). If no words appear within the top M positions in both ranked lists, then no translation is output. Note that for many Chinese words, only one English word e appeared within the top M positions for both lists.", "  rank translation candidates using PH and CX independently and return results with the highest average rank", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.", "Some recent research used comparable corpora to re-score name transliterations Sproat et al Klementiev and Roth or mine new word translations Fung and Yee Rapp ##CITATION## Tao and Zhai Hassan et al Udupa et al Ji", 0, "Mining New Word Translations from Comparable Corpora", "Conclusion."], ["New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.", "Some recent research used comparable corpora to re-score name transliterations Sproat et al Klementiev and Roth or mine new word translations Fung and Yee Rapp ##CITATION## Tao and Zhai Hassan et al Udupa et al Ji", 0, "Mining New Word Translations from Comparable Corpora", "PAPER"], ["As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. New words such as person names, organization names, technical terms, etc. appear frequently.", "Some recent research used comparable corpora to re-score name transliterations Sproat et al Klementiev and Roth or mine new word translations Fung and Yee Rapp ##CITATION## Tao and Zhai Hassan et al Udupa et al Ji", 0, "Mining New Word Translations from Comparable Corpora", "ABSTRACT"], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "Some recent research used comparable corpora to re-score name transliterations Sproat et al Klementiev and Roth or mine new word translations Fung and Yee Rapp ##CITATION## Tao and Zhai Hassan et al Udupa et al Ji", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.", "The traditional approch to translation extraction from comparable corpora and most of its extensions Fung Rapp ##CITATION## Otero Yu and Tsujii Marsi and Krahmer presuppose the availability of a bilingual lexicon for translating source vectors into the target language", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of   used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora.", "The traditional approch to translation extraction from comparable corpora and most of its extensions Fung Rapp ##CITATION## Otero Yu and Tsujii Marsi and Krahmer presuppose the availability of a bilingual lexicon for translating source vectors into the target language", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. On the other hand, the work of   used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight   attempted to combine multiple clues, including similar context and spelling.", "The traditional approch to translation extraction from comparable corpora and most of its extensions Fung Rapp ##CITATION## Otero Yu and Tsujii Marsi and Krahmer presuppose the availability of a bilingual lexicon for translating source vectors into the target language", 0, "Mining New Word Translations from Comparable Corpora", "Related work."], ["We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons  . But parallel corpora are scarce resources, especially for uncommon lan guage pairs.", "The traditional approch to translation extraction from comparable corpora and most of its extensions Fung Rapp ##CITATION## Otero Yu and Tsujii Marsi and Krahmer presuppose the availability of a bilingual lexicon for translating source vectors into the target language", 1, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["Since our method currently only considers unigram English words, we are not able to find translations for these words. But it is not difficult to extend our method to handle this problem. We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases. The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d). Our method is not able to find 43 (329 + 205) \u00d7 4499 = 362words in all 12 pe these translations.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages ##CITATION## Feng et al Lu and Zhao", 0, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["We then n! rank these words e1 , e2 ,..., ek according to the P (Q | D ) = \u220f P (t | D ) c t average of their rank positions in the two lists. \u220f t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output. If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages ##CITATION## Feng et al Lu and Zhao", 0, "Mining New Word Translations from Comparable Corpora", "Our approach."], ["But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include  .", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages ##CITATION## Feng et al Lu and Zhao", 0, "Mining New Word Translations from Comparable Corpora", "Introduction"], ["Precision and recall for different values of M The past research of   utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs. Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus. As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus. This list of 43 words is shown in Table 3.", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages ##CITATION## Feng et al Lu and Zhao", 1, "Mining New Word Translations from Comparable Corpora", "Experiment."], ["It consists of 97 documents (~50k words) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set. It shows that the \u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified.", "However detailed research ##CITATION## shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "However detailed research ##CITATION## shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN: \u2022 ML12: combination of mention levels 4.4 Overlap. This category of features includes: \u2022 #MB: number of other mentions in between \u2022 #WB: number of words in between \u2022 M1>M2 or M1<M2: flag indicating whether M2/M1is included in M1/M2. Normally, the above overlap features are too general to be effective alone. Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking.", "However detailed research ##CITATION## shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "However detailed research ##CITATION## shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition. For example, the head word of the name mention \u201cUniversity of Michigan\u201d is \u201cUniversity\u201d. \u2022 WM2: bag-of-words in M2 \u2022 HM2: head word of M2 \u2022 HM12: combination of HM1 and HM2 \u2022 WBNULL: when no word in between \u2022 WBFL: the only word in between when only one word in between \u2022 WBF: first word in between when at least two words in between \u2022 WBL: last word in between when at least two words in between \u2022 WBO: other words in between except first and last words when at least three words in between \u2022 BM1F: first word before M1 \u2022 BM1L: second word before M1 \u2022 AM2F: first word after M2 \u2022 AM2L: second word after M2 4.2 Entity Type.", "Furthermore when the UPST   into a composite one via polynomial interpolation in a setting similar to ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Mentions have three levels: names, nomial expressions or pronouns. The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task. For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Furthermore when the UPST   into a composite one via polynomial interpolation in a setting similar to ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition.", "Furthermore when the UPST   into a composite one via polynomial interpolation in a setting similar to ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  .", "Furthermore when the UPST   into a composite one via polynomial interpolation in a setting similar to ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight.", "Furthermore when the UPST   into a composite one via polynomial interpolation in a setting similar to ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.", "Most of the features used in our system are based on the work in ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Most of the features used in our system are based on the work in ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations. During development, 155 of 674 documents in the training set are set aside for fine-tuning the system. The testing set is held out only for final evaluation. It consists of 97 documents (~50k words) and 1386 instances of relations.", "Most of the features used in our system are based on the work in ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Most of the features used in our system are based on the work in ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "Due to space limitations we only describe the collocation features and refer the reader to   for the rest of the features", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention.", "Due to space limitations we only describe the collocation features and refer the reader to   for the rest of the features", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation. This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention.", "Due to space limitations we only describe the collocation features and refer the reader to   for the rest of the features", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Due to space limitations we only describe the collocation features and refer the reader to   for the rest of the features", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M.", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M.  .", "Among them feature-based methods Kambhatla ##CITATION## achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity- related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information ##CITATION## which is critical for further performance improvement in relation extraction", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74. 1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33. 3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4. 8 66 .7 74 .7 77 .2 60 .7 68 .0 6 3. 1 4 9.", "Composite Kernel In this paper a composite kernel via polynomial interpolation as described Zhang et al   is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel   7 : P per formance by ~12 in F-measure6  This suggests the K1    a K L    1 a  K C    5 usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition.", "Composite Kernel In this paper a composite kernel via polynomial interpolation as described Zhang et al   is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel   7 : P per formance by ~12 in F-measure6  This suggests the K1    a K L    1 a  K C    5 usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Composite Kernel In this paper a composite kernel via polynomial interpolation as described Zhang et al   is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel   7 : P per formance by ~12 in F-measure6  This suggests the K1    a K L    1 a  K C    5 usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  .", "Composite Kernel In this paper a composite kernel via polynomial interpolation as described Zhang et al   is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel   7 : P per formance by ~12 in F-measure6  This suggests the K1    a K L    1 a  K C    5 usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight.", "Composite Kernel In this paper a composite kernel via polynomial interpolation as described Zhang et al   is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel   7 : P per formance by ~12 in F-measure6  This suggests the K1    a K L    1 a  K C    5 usefulness of extending the tree span beyond SPT for the \u201cpredicate-linked\u201d tree span category", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "7 Here we use the same set of flat features ie word entity type mention level overlap base phrase chunk- ing dependency tree parse tree and semantic information as ##CITATION## 20 05 ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP. \u2022 ET1DW1: combination of the entity type and the dependent word for M1 \u2022 H1DW1: combination of the head word and the dependent word for M1 \u2022 ET2DW2: combination of the entity type and the dependent word for M2 \u2022 H2DW2: combination of the head word and the dependent word for M2 \u2022 ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP \u2022 ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP \u2022 ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree. This category of features concerns about the information inherent only in the full parse tree. \u2022 PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree \u2022 PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path.", "7 Here we use the same set of flat features ie word entity type mention level overlap base phrase chunk- ing dependency tree parse tree and semantic information as ##CITATION## 20 05 ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition. For example, the head word of the name mention \u201cUniversity of Michigan\u201d is \u201cUniversity\u201d.", "7 Here we use the same set of flat features ie word entity type mention level overlap base phrase chunk- ing dependency tree parse tree and semantic information as ##CITATION## 20 05 ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "7 Here we use the same set of flat features ie word entity type mention level overlap base phrase chunk- ing dependency tree parse tree and semantic information as ##CITATION## 20 05 ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla   by about 3 F-measure on extracting 24 ACE relation subtypes. It also shows that our system outperforms tree kernel-based systems   by over 20 F-measure on extracting 5 ACE relation types. Support Vector Machines  . Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.", "Finally they also Ours: composite kernel Zhang et al  : composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al  : convolution tree ke rnel Bunescu et al  : shortest path dependency kernel Culotta et al  : dependency kernel ##CITATION##  : feature-based Kambhatla  : feature-based 808 652 773 649 801 634 761 624 655 - 671 - 772 631 - 635 684 549  656 512 638 519  626 485 438 - 350 - 607 495 - 452 741 596 709 572 710 571 687 546 525 - 458 - 680 555 - 528 show that our composite kernel-based system outper forms other composite ke rnel-based systems ", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Finally they also Ours: composite kernel Zhang et al  : composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al  : convolution tree ke rnel Bunescu et al  : shortest path dependency kernel Culotta et al  : dependency kernel ##CITATION##  : feature-based Kambhatla  : feature-based 808 652 773 649 801 634 761 624 655 - 671 - 772 631 - 635 684 549  656 512 638 519  626 485 438 - 350 - 607 495 - 452 741 596 709 572 710 571 687 546 525 - 458 - 680 555 - 528 show that our composite kernel-based system outper forms other composite ke rnel-based systems ", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.", "Finally they also Ours: composite kernel Zhang et al  : composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al  : convolution tree ke rnel Bunescu et al  : shortest path dependency kernel Culotta et al  : dependency kernel ##CITATION##  : feature-based Kambhatla  : feature-based 808 652 773 649 801 634 761 624 655 - 671 - 772 631 - 635 684 549  656 512 638 519  626 485 438 - 350 - 607 495 - 452 741 596 709 572 710 571 687 546 525 - 458 - 680 555 - 528 show that our composite kernel-based system outper forms other composite ke rnel-based systems ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Finally they also Ours: composite kernel Zhang et al  : composite kernel Ours: context -sensitive convolution tree ke rnel Zhang et al  : convolution tree ke rnel Bunescu et al  : shortest path dependency kernel Culotta et al  : dependency kernel ##CITATION##  : feature-based Kambhatla  : feature-based 808 652 773 649 801 634 761 624 655 - 671 - 772 631 - 635 684 549  656 512 638 519  626 485 438 - 350 - 607 495 - 452 741 596 709 572 710 571 687 546 525 - 458 - 680 555 - 528 show that our composite kernel-based system outper forms other composite ke rnel-based systems ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "For each pair of entity mentions we extract and compute various lexical and syntactic features as employed in a state-of-the-art relation extraction system ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "For each pair of entity mentions we extract and compute various lexical and syntactic features as employed in a state-of-the-art relation extraction system ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "For each pair of entity mentions we extract and compute various lexical and syntactic features as employed in a state-of-the-art relation extraction system ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "For each pair of entity mentions we extract and compute various lexical and syntactic features as employed in a state-of-the-art relation extraction system ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "For each pair of entity mentions we extract and compute various lexical and syntactic features as employed in a state-of-the-art relation extraction system ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking. It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced   as shown in Table 1 for the ACE RDCcorpus", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype. It is not surprising that the performance on the relation type \u201cNEAR\u201d is low because it occurs rarely in both the training and testing data. Others like \u201cPART.Subsidary\u201d and \u201cSOCIAL.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced   as shown in Table 1 for the ACE RDCcorpus", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The testing set is held out only for final evaluation. It consists of 97 documents (~50k words) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set. It shows that the \u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced   as shown in Table 1 for the ACE RDCcorpus", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["\u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE  General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced   as shown in Table 1 for the ACE RDCcorpus", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE  General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1.", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced   as shown in Table 1 for the ACE RDCcorpus", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features ##CITATION## towards a higher absolute score but rather to serve as a point of comparison to the models which rely on syntactic information", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes. We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships. Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features ##CITATION## towards a higher absolute score but rather to serve as a point of comparison to the models which rely on syntactic information", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["The words between the two mentions are classified into three bins: the first word in between, the last word in between and other words in between. Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention. Since a pronominal mention (especially neutral pronoun such as \u2018it\u2019 and \u2018its\u2019) contains little information about the sense of the mention, the co- reference chain is used to decide its sense. This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features ##CITATION## towards a higher absolute score but rather to serve as a point of comparison to the models which rely on syntactic information", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features ##CITATION## towards a higher absolute score but rather to serve as a point of comparison to the models which rely on syntactic information", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Recent work has begun to address relation and event extraction through trainable means chiefly SVM classification Zelenko et al ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text. Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities.", "Recent work has begun to address relation and event extraction through trainable means chiefly SVM classification Zelenko et al ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Recent work has begun to address relation and event extraction through trainable means chiefly SVM classification Zelenko et al ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Recent work has begun to address relation and event extraction through trainable means chiefly SVM classification Zelenko et al ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M.", "For the choice of features we use the full set of features from ##CITATION##  since it is reported to have a state-of-the-art performance Sun et al ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2. For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important. The words between the two mentions are classified into three bins: the first word in between, the last word in between and other words in between. Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention.", "For the choice of features we use the full set of features from ##CITATION##  since it is reported to have a state-of-the-art performance Sun et al ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser   is employed for full parsing.", "For the choice of features we use the full set of features from ##CITATION##  since it is reported to have a state-of-the-art performance Sun et al ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "For the choice of features we use the full set of features from ##CITATION##  since it is reported to have a state-of-the-art performance Sun et al ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "For the choice of features we use the full set of features from ##CITATION##  since it is reported to have a state-of-the-art performance Sun et al ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight.", "We use a state-of-the-art feature space   to represent examples   and outputs probabilities associated with each prediction", 0, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla   by about 3 F-measure on extracting 24 ACE relation subtypes.", "We use a state-of-the-art feature space   to represent examples   and outputs probabilities associated with each prediction", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "We use a state-of-the-art feature space   to represent examples   and outputs probabilities associated with each prediction", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "We use a state-of-the-art feature space   to represent examples   and outputs probabilities associated with each prediction", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "We use a state-of-the-art feature space   to represent examples   and outputs probabilities associated with each prediction", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations. During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.", "We use SVM as our learning algorithm with the full feature set from ##CITATION##   ", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition. For example, the head word of the name mention \u201cUniversity of Michigan\u201d is \u201cUniversity\u201d.", "We use SVM as our learning algorithm with the full feature set from ##CITATION##   ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser   is employed for full parsing.", "We use SVM as our learning algorithm with the full feature set from ##CITATION##   ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "We use SVM as our learning algorithm with the full feature set from ##CITATION##   ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Indeed such feature-based methods have been widely applied in parsing Collins Charniak semantic role labeling Pradhan et al semantic relation extraction ##CITATION## and co-reference resolution Lapin and Leass Aone and Bennett Mitkov Yang et al Luo and Zitouni Bergsma and Lin", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "Indeed such feature-based methods have been widely applied in parsing Collins Charniak semantic role labeling Pradhan et al semantic relation extraction ##CITATION## and co-reference resolution Lapin and Leass Aone and Bennett Mitkov Yang et al Luo and Zitouni Bergsma and Lin", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], [" . Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "Indeed such feature-based methods have been widely applied in parsing Collins Charniak semantic role labeling Pradhan et al semantic relation extraction ##CITATION## and co-reference resolution Lapin and Leass Aone and Bennett Mitkov Yang et al Luo and Zitouni Bergsma and Lin", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Indeed such feature-based methods have been widely applied in parsing Collins Charniak semantic role labeling Pradhan et al semantic relation extraction ##CITATION## and co-reference resolution Lapin and Leass Aone and Bennett Mitkov Yang et al Luo and Zitouni Bergsma and Lin", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla   by about 3 F-measure on extracting 24 ACE relation subtypes.", "However it is reported ##CITATION## Kambhatla that hierarchical structured syntactic features contributes less to performance improvement", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "However it is reported ##CITATION## Kambhatla that hierarchical structured syntactic features contributes less to performance improvement", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "However it is reported ##CITATION## Kambhatla that hierarchical structured syntactic features contributes less to performance improvement", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "However it is reported ##CITATION## Kambhatla that hierarchical structured syntactic features contributes less to performance improvement", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.", "##CITATION##   explore various features in relation extraction using SVM ", 0, "Exploring Various Knowledge in Relation Extraction", "PAPER"], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "##CITATION##   explore various features in relation extraction using SVM ", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "##CITATION##   explore various features in relation extraction using SVM ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "##CITATION##   explore various features in relation extraction using SVM ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "The features used in Kambhatla   and ##CITATION##   have to be selected and carefully calibrated manually ", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN: \u2022 ML12: combination of mention levels 4.4 Overlap. This category of features includes: \u2022 #MB: number of other mentions in between \u2022 #WB: number of words in between \u2022 M1>M2 or M1<M2: flag indicating whether M2/M1is included in M1/M2. Normally, the above overlap features are too general to be effective alone. Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking.", "The features used in Kambhatla   and ##CITATION##   have to be selected and carefully calibrated manually ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], [" . Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "The features used in Kambhatla   and ##CITATION##   have to be selected and carefully calibrated manually ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "The features used in Kambhatla   and ##CITATION##   have to be selected and carefully calibrated manually ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "The features used in Kambhatla   and ##CITATION##   have to be selected and carefully calibrated manually ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["\u2022 Chunking features are very useful. It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.", "Besides ##CITATION##   introduce additional chunking features to enhance the parse tree features ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also includes flags indicating whether the two mentions are in the same NP/PP/VP. \u2022 ET1DW1: combination of the entity type and the dependent word for M1 \u2022 H1DW1: combination of the head word and the dependent word for M1 \u2022 ET2DW2: combination of the entity type and the dependent word for M2 \u2022 H2DW2: combination of the head word and the dependent word for M2 \u2022 ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP \u2022 ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP \u2022 ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree. This category of features concerns about the information inherent only in the full parse tree. \u2022 PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree \u2022 PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path. 4.8 Semantic Resources.", "Besides ##CITATION##   introduce additional chunking features to enhance the parse tree features ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN: \u2022 ML12: combination of mention levels 4.4 Overlap. This category of features includes: \u2022 #MB: number of other mentions in between \u2022 #WB: number of words in between \u2022 M1>M2 or M1<M2: flag indicating whether M2/M1is included in M1/M2. Normally, the above overlap features are too general to be effective alone. Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking.", "Besides ##CITATION##   introduce additional chunking features to enhance the parse tree features ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "Besides ##CITATION##   introduce additional chunking features to enhance the parse tree features ", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),.", "we call the features used in ##CITATION##   and Kambhatla   flat feature set ", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "we call the features used in ##CITATION##   and Kambhatla   flat feature set ", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], [" . Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "we call the features used in ##CITATION##   and Kambhatla   flat feature set ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "we call the features used in ##CITATION##   and Kambhatla   flat feature set ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "we call the features used in ##CITATION##   and Kambhatla   flat feature set ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.", "  our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation. This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention.", "  our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships. Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document. 427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.", "  our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC.", "  our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers", 1, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis from part-of-speech POS tagging to full parsing and dependency parsing Kambhatla Zhao and Grishman ##CITATION##1", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser   is employed for full parsing.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis from part-of-speech POS tagging to full parsing and dependency parsing Kambhatla Zhao and Grishman ##CITATION##1", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis from part-of-speech POS tagging to full parsing and dependency parsing Kambhatla Zhao and Grishman ##CITATION##1", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.", "The first utilizes a set of carefully selected features obtained from different levels of text analysis from part-of-speech POS tagging to full parsing and dependency parsing Kambhatla Zhao and Grishman ##CITATION##1", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.", "Zhao and Grishman   and ##CITATION##   explored a large set of features that are potentially useful for relation extraction ", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "Zhao and Grishman   and ##CITATION##   explored a large set of features that are potentially useful for relation extraction ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types. This suggests that relation detection is critical for relation extraction. # of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68. 1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.", "Zhao and Grishman   and ##CITATION##   explored a large set of features that are potentially useful for relation extraction ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Zhao and Grishman   and ##CITATION##   explored a large set of features that are potentially useful for relation extraction ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure. \u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.", "Bag-of-Words: These features have also been explore by Zhao and Grishman   and ##CITATION##    ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.", "Bag-of-Words: These features have also been explore by Zhao and Grishman   and ##CITATION##    ", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["During development, 155 of 674 documents in the training set are set aside for fine-tuning the system. The testing set is held out only for final evaluation. It consists of 97 documents (~50k words) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set. It shows that the", "Bag-of-Words: These features have also been explore by Zhao and Grishman   and ##CITATION##    ", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention. Since a pronominal mention (especially neutral pronoun such as \u2018it\u2019 and \u2018its\u2019) contains little information about the sense of the mention, the co- reference chain is used to decide its sense. This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.", "Bag-of-Words: These features have also been explore by Zhao and Grishman   and ##CITATION##    ", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Since a pronominal mention (especially neutral pronoun such as \u2018it\u2019 and \u2018its\u2019) contains little information about the sense of the mention, the co- reference chain is used to decide its sense. This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation. This has an effect of choosing the base phrase contained in the extent annotation.", "Bag-of-Words: These features have also been explore by Zhao and Grishman   and ##CITATION##    ", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney  Zhao and Grishman   and ##CITATION##    ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["\u2022 Chunking features are very useful. It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney  Zhao and Grishman   and ##CITATION##    ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney  Zhao and Grishman   and ##CITATION##    ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney  Zhao and Grishman   and ##CITATION##    ", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Although in order to achieve the best performance it is necessary to use a proper combination of these features ##CITATION## in this paper we will concentrate on how to better capture the syntactic features for relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Although in order to achieve the best performance it is necessary to use a proper combination of these features ##CITATION## in this paper we will concentrate on how to better capture the syntactic features for relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "Although in order to achieve the best performance it is necessary to use a proper combination of these features ##CITATION## in this paper we will concentrate on how to better capture the syntactic features for relation extraction", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Although in order to achieve the best performance it is necessary to use a proper combination of these features ##CITATION## in this paper we will concentrate on how to better capture the syntactic features for relation extraction", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Type Subtype Freq Residence 308 Other 6 ROLE  General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results.", "These experiments are done using ##CITATION##  TPWF kernel SL kernel different versions of proposed KH F kernel and KH ybrid kernel ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "These experiments are done using ##CITATION##  TPWF kernel SL kernel different versions of proposed KH F kernel and KH ybrid kernel ", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Residence\u201d by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype. It is not surprising that the performance on the relation type \u201cNEAR\u201d is low because it occurs rarely in both the training and testing data.", "These experiments are done using ##CITATION##  TPWF kernel SL kernel different versions of proposed KH F kernel and KH ybrid kernel ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  .", "These experiments are done using ##CITATION##  TPWF kernel SL kernel different versions of proposed KH F kernel and KH ybrid kernel ", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure. \u2022 Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase. \u2022 The usefulness of mention level features is quite limited. It only improves the F-measure by 0.8 due to the recall increase.", "We also performed 5-fold cross validation experiments by combining the Stage 1 classifier with each of the ##CITATION##   TPWF kernel SL kernel PET kernel KH F kernel and KH ybrid kernel separately only the results of KH ybrid are reported in Table 1 due to space limitation ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.", "We also performed 5-fold cross validation experiments by combining the Stage 1 classifier with each of the ##CITATION##   TPWF kernel SL kernel PET kernel KH F kernel and KH ybrid kernel separately only the results of KH ybrid are reported in Table 1 due to space limitation ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods.", "We also performed 5-fold cross validation experiments by combining the Stage 1 classifier with each of the ##CITATION##   TPWF kernel SL kernel PET kernel KH F kernel and KH ybrid kernel separately only the results of KH ybrid are reported in Table 1 due to space limitation ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight. for multi-class classification.", "We also performed 5-fold cross validation experiments by combining the Stage 1 classifier with each of the ##CITATION##   TPWF kernel SL kernel PET kernel KH F kernel and KH ybrid kernel separately only the results of KH ybrid are reported in Table 1 due to space limitation ", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "One major challenge in relation extraction is due to the data sparseness problem ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "One major challenge in relation extraction is due to the data sparseness problem ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["\u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.", "One major challenge in relation extraction is due to the data sparseness problem ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["\u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE  General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "One major challenge in relation extraction is due to the data sparseness problem ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "While various machine learning approaches such as generative modeling   maximum entropy   and support vector machines   have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "While various machine learning approaches such as generative modeling   maximum entropy   and support vector machines   have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "While various machine learning approaches such as generative modeling   maximum entropy   and support vector machines   have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight. for multi-class classification.", "While various machine learning approaches such as generative modeling   maximum entropy   and support vector machines   have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["\u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE  General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.", "While various machine learning approaches such as generative modeling   maximum entropy   and support vector machines   have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Zhao and Grisman   2 combined various kinds of knowledge from tokenization sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 701 on the 7 relation types of the ACE RDCcorpus3  ##CITATION##   further systematically explored diverse lexical syntactic and semantic features through support vector machines and achieved F- measure of 681 and 555 on the 5 relation types and the 24 relation subtypes in the ACE RDCcorpus respectively", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text. Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.", "Zhao and Grisman   2 combined various kinds of knowledge from tokenization sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 701 on the 7 relation types of the ACE RDCcorpus3  ##CITATION##   further systematically explored diverse lexical syntactic and semantic features through support vector machines and achieved F- measure of 681 and 555 on the 5 relation types and the 24 relation subtypes in the ACE RDCcorpus respectively", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Zhao and Grisman   2 combined various kinds of knowledge from tokenization sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 701 on the 7 relation types of the ACE RDCcorpus3  ##CITATION##   further systematically explored diverse lexical syntactic and semantic features through support vector machines and achieved F- measure of 681 and 555 on the 5 relation types and the 24 relation subtypes in the ACE RDCcorpus respectively", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.", "Zhao and Grisman   2 combined various kinds of knowledge from tokenization sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 701 on the 7 relation types of the ACE RDCcorpus3  ##CITATION##   further systematically explored diverse lexical syntactic and semantic features through support vector machines and achieved F- measure of 681 and 555 on the 5 relation types and the 24 relation subtypes in the ACE RDCcorpus respectively", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE  General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d.", "Same as ##CITATION##   we only model explicit relations and explicitly model the argument order of the two mentions involved", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Same as ##CITATION##   we only model explicit relations and explicitly model the argument order of the two mentions involved", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation. This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention.", "Same as ##CITATION##   we only model explicit relations and explicitly model the argument order of the two mentions involved", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC.", "Same as ##CITATION##   we only model explicit relations and explicitly model the argument order of the two mentions involved", 1, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["However, this paper only uses the binary-class version. For details about SVMLight, please see http://svmlight.joachims.org/ The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "Same as ##CITATION##   we only model explicit relations and explicitly model the argument order of the two mentions involved", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["For details about SVMLight, please see http://svmlight.joachims.org/ The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words.", "Same as ##CITATION##   we only model explicit relations and explicitly model the argument order of the two mentions involved", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "In the future we would like to use more effective feature sets ##CITATION##  or kernel based similarity measure with LP for relation extraction ", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], [" . Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.", "In the future we would like to use more effective feature sets ##CITATION##  or kernel based similarity measure with LP for relation extraction ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.", "In the future we would like to use more effective feature sets ##CITATION##  or kernel based similarity measure with LP for relation extraction ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "In the future we would like to use more effective feature sets ##CITATION##  or kernel based similarity measure with LP for relation extraction ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.", "Feature-based methods Kambhatla ##CITATION## Zhao and Grishman 20052  for this task employ a large amount of diverse linguistic features such as lexical syntactic and semantic features", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Feature-based methods Kambhatla ##CITATION## Zhao and Grishman 20052  for this task employ a large amount of diverse linguistic features such as lexical syntactic and semantic features", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "Feature-based methods Kambhatla ##CITATION## Zhao and Grishman 20052  for this task employ a large amount of diverse linguistic features such as lexical syntactic and semantic features", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Feature-based methods Kambhatla ##CITATION## Zhao and Grishman 20052  for this task employ a large amount of diverse linguistic features such as lexical syntactic and semantic features", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.", "Based on his work ##CITATION##  89 Proceedings of ACL08: HLT Short Papers Companion Volume pages 89\u201392 Columbus Ohio USA June QcAssociation for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships. Country Name List This is to differentiate the relation subtype \u201cROLE.Citizen-Of\u201d, which defines the relationship between a person and the country of the person\u2019s citizenship, from other subtypes, especially \u201cROLE.Residence\u201d, where defines the relationship between a person and the location in which the person lives. Two features are defined to include this information: \u2022 ET1Country: the entity type of M1 when M2 is a country name \u2022 CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal. This trigger word list is first gathered from WordNet by checking whether a word has the semantic class \u201cperson|\u2026|relative\u201d. Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes.", "Based on his work ##CITATION##  89 Proceedings of ACL08: HLT Short Papers Companion Volume pages 89\u201392 Columbus Ohio USA June QcAssociation for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "Based on his work ##CITATION##  89 Proceedings of ACL08: HLT Short Papers Companion Volume pages 89\u201392 Columbus Ohio USA June QcAssociation for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "Based on his work ##CITATION##  89 Proceedings of ACL08: HLT Short Papers Companion Volume pages 89\u201392 Columbus Ohio USA June QcAssociation for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It also shows that our system performs best on the subtype \u201cSOCIAL.Parent\u201d and \u201cROLE. Citizen-Of\u201d. This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type \u201cAT\u201d although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes.", "Based on his work ##CITATION##  89 Proceedings of ACL08: HLT Short Papers Companion Volume pages 89\u201392 Columbus Ohio USA June QcAssociation for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list ", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla   by about 3 F-measure on extracting 24 ACE relation subtypes. It also shows that our system outperforms tree kernel-based systems   by over 20 F-measure on extracting 5 ACE relation types.", "While syntactic features are known to improve the performance of supervised IE at least using clean hand-labeled ACE data ##CITATION## ##CITATION## we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla   by about 3 F-measure on extracting 24 ACE relation subtypes.", "While syntactic features are known to improve the performance of supervised IE at least using clean hand-labeled ACE data ##CITATION## ##CITATION## we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.", "While syntactic features are known to improve the performance of supervised IE at least using clean hand-labeled ACE data ##CITATION## ##CITATION## we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "While syntactic features are known to improve the performance of supervised IE at least using clean hand-labeled ACE data ##CITATION## ##CITATION## we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences including work exploiting syntactic dependencies by Lin and Pantel   and Snow et al   and work in the ACE paradigm such as ##CITATION##   and ##CITATION##     ", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences including work exploiting syntactic dependencies by Lin and Pantel   and Snow et al   and work in the ACE paradigm such as ##CITATION##   and ##CITATION##     ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences including work exploiting syntactic dependencies by Lin and Pantel   and Snow et al   and work in the ACE paradigm such as ##CITATION##   and ##CITATION##     ", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "More recent approaches have used deeper syntactic information derived from parses of the input sentences including work exploiting syntactic dependencies by Lin and Pantel   and Snow et al   and work in the ACE paradigm such as ##CITATION##   and ##CITATION##     ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods ##CITATION##  and Zhao and Grishman  studied various features and feature combinations for relation extraction  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods ##CITATION##  and Zhao and Grishman  studied various features and feature combinations for relation extraction  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods ##CITATION##  and Zhao and Grishman  studied various features and feature combinations for relation extraction  ", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods ##CITATION##  and Zhao and Grishman  studied various features and feature combinations for relation extraction  ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem  ", 0, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem  ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem  ", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Then the feature based method explicitly extracts a variety of lexical syntactic and semantic features for statistical learning either generative or discriminative Miller et al Kambhatla Boschee et al Grishman et al ##CITATION## Jiang and Zhai", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "Then the feature based method explicitly extracts a variety of lexical syntactic and semantic features for statistical learning either generative or discriminative Miller et al Kambhatla Boschee et al Grishman et al ##CITATION## Jiang and Zhai", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "Then the feature based method explicitly extracts a variety of lexical syntactic and semantic features for statistical learning either generative or discriminative Miller et al Kambhatla Boschee et al Grishman et al ##CITATION## Jiang and Zhai", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "Then the feature based method explicitly extracts a variety of lexical syntactic and semantic features for statistical learning either generative or discriminative Miller et al Kambhatla Boschee et al Grishman et al ##CITATION## Jiang and Zhai", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.", "We first adopted the full feature set from ##CITATION##   a state-of-the-art feature based relation extraction system ", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "We first adopted the full feature set from ##CITATION##   a state-of-the-art feature based relation extraction system ", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["Table 4 also indicates the low performance on the relation type \u201cAT\u201d although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes. Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.", "We first adopted the full feature set from ##CITATION##   a state-of-the-art feature based relation extraction system ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "We first adopted the full feature set from ##CITATION##   a state-of-the-art feature based relation extraction system ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight.", "We first adopted the full feature set from ##CITATION##   a state-of-the-art feature based relation extraction system ", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes. Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.", "Following most of the 6 http://nlpstanfordedu/software/lex-parsershtml 7 http://ilkuvtnl/team/sabine/chunklink/READMEhtml 8 http://opennlpsourceforgenet/ 9 ##CITATION##   tested their system on the ACEdata; ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88. 9 2 2 . 9 3 6 . 4 P A R T 1 6 4 1 0 6 3 9 73. 1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70. 4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37. 8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84.", "Following most of the 6 http://nlpstanfordedu/software/lex-parsershtml 7 http://ilkuvtnl/team/sabine/chunklink/READMEhtml 8 http://opennlpsourceforgenet/ 9 ##CITATION##   tested their system on the ACEdata; ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37. 8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84. 4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75. 8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71. 1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.", "Following most of the 6 http://nlpstanfordedu/software/lex-parsershtml 7 http://ilkuvtnl/team/sabine/chunklink/READMEhtml 8 http://opennlpsourceforgenet/ 9 ##CITATION##   tested their system on the ACEdata; ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "Following most of the 6 http://nlpstanfordedu/software/lex-parsershtml 7 http://ilkuvtnl/team/sabine/chunklink/READMEhtml 8 http://opennlpsourceforgenet/ 9 ##CITATION##   tested their system on the ACEdata; ", 1, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. \u2022 To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.", "However most approaches to RE have assumed that the relations\u2019 arguments are given as input Chan and Roth Jiang and Zhai Jiang ##CITATION## and therefore offer only a partial solution to the problem", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It shows that the \u201cGrandParent\u201d, \u201cSpouse\u201d and \u201cSibling\u201d are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate.", "However most approaches to RE have assumed that the relations\u2019 arguments are given as input Chan and Roth Jiang and Zhai Jiang ##CITATION## and therefore offer only a partial solution to the problem", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.", "However most approaches to RE have assumed that the relations\u2019 arguments are given as input Chan and Roth Jiang and Zhai Jiang ##CITATION## and therefore offer only a partial solution to the problem", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For details about SVMLight, please see http://svmlight.joachims.org/ The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words.", "However most approaches to RE have assumed that the relations\u2019 arguments are given as input Chan and Roth Jiang and Zhai Jiang ##CITATION## and therefore offer only a partial solution to the problem", 1, "Exploring Various Knowledge in Relation Extraction", "Features."], ["2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities. Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.). Mentions have three levels: names, nomial expressions or pronouns.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input  ", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input  ", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "We used ##CITATION##\u2019s lexical features ##CITATION## as the basis for the features of our system similar to what other researchers have done Chan and Roth", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs.", "We used ##CITATION##\u2019s lexical features ##CITATION## as the basis for the features of our system similar to what other researchers have done Chan and Roth", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships.", "We used ##CITATION##\u2019s lexical features ##CITATION## as the basis for the features of our system similar to what other researchers have done Chan and Roth", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "We used ##CITATION##\u2019s lexical features ##CITATION## as the basis for the features of our system similar to what other researchers have done Chan and Roth", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes. We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships. Two features are defined to include this information: \u2022 ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. \u2022 SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.", "Although a bit lower than ##CITATION##\u2019s result of 555   we attribute the difference to our use of a different tokenizer different parser and having not used the semantic information features", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collins\u2019 parser used in our system represents the state-of-the-art in full parsing. \u2022 Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype \u201cROLE.Citizen-Of\u201d from \u201cROLE. Residence\u201d by distinguishing country GPEs from other GPEs.", "Although a bit lower than ##CITATION##\u2019s result of 555   we attribute the difference to our use of a different tokenizer different parser and having not used the semantic information features", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "Although a bit lower than ##CITATION##\u2019s result of 555   we attribute the difference to our use of a different tokenizer different parser and having not used the semantic information features", 0, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data \u2022 Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.", "Although a bit lower than ##CITATION##\u2019s result of 555   we attribute the difference to our use of a different tokenizer different parser and having not used the semantic information features", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors.", "This is slightly behind that of Zhang ; the reason might be threefold: i different data partitioning; ii different pre-processing; iii they incorporate features from additional sources ie a phrase chunker dependency parser and semantic resources ##CITATION## we have on average 9 features/instance they use 40", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "This is slightly behind that of Zhang ; the reason might be threefold: i different data partitioning; ii different pre-processing; iii they incorporate features from additional sources ie a phrase chunker dependency parser and semantic resources ##CITATION## we have on average 9 features/instance they use 40", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["It also shows that our system performs best on the subtype \u201cSOCIAL.Parent\u201d and \u201cROLE. Citizen-Of\u201d. This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type \u201cAT\u201d although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type \u201cAT\u201d and its subtypes.", "This is slightly behind that of Zhang ; the reason might be threefold: i different data partitioning; ii different pre-processing; iii they incorporate features from additional sources ie a phrase chunker dependency parser and semantic resources ##CITATION## we have on average 9 features/instance they use 40", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \u201cWho is the president of the United States?\u201d. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.", "This is slightly behind that of Zhang ; the reason might be threefold: i different data partitioning; ii different pre-processing; iii they incorporate features from additional sources ie a phrase chunker dependency parser and semantic resources ##CITATION## we have on average 9 features/instance they use 40", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: \u201cRelative- Location\u201d, \u201cAssociate\u201d, \u201cOther-Relative\u201d, \u201cOther- Professional\u201d, \u201cSibling\u201d, and \u201cSpouse\u201d. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.", "This is slightly behind that of Zhang ; the reason might be threefold: i different data partitioning; ii different pre-processing; iii they incorporate features from additional sources ie a phrase chunker dependency parser and semantic resources ##CITATION## we have on average 9 features/instance they use 40", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "Techniques based on machine learning ##CITATION## Hao et al Bunescu and Mooney are expected to alleviate this problem in manually crafted IE", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.", "Techniques based on machine learning ##CITATION## Hao et al Bunescu and Mooney are expected to alleviate this problem in manually crafted IE", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes.", "Techniques based on machine learning ##CITATION## Hao et al Bunescu and Mooney are expected to alleviate this problem in manually crafted IE", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims  . 2 Joachims has just released a new version of SVMLight. for multi-class classification.", "Techniques based on machine learning ##CITATION## Hao et al Bunescu and Mooney are expected to alleviate this problem in manually crafted IE", 1, "Exploring Various Knowledge in Relation Extraction", "Support Vector Machines."], ["This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M.", "A technique that many previous approaches have used is shallow parsing  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types. This suggests that relation detection is critical for relation extraction. # of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68. 1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.", "A technique that many previous approaches have used is shallow parsing  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking. It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing.", "A technique that many previous approaches have used is shallow parsing  ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.", "A technique that many previous approaches have used is shallow parsing  ", 1, "Exploring Various Knowledge in Relation Extraction", "ABSTRACT"], ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.", "Especially although we did not concern the dependency tree and full parse tree information as other supervised methods Miller et al Culotta and Soresen Kambhatla ##CITATION## the incorporation of simple features such as words and chunking information still can provide complement information for capturing the characteristics of entity pairs", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Moreover, we also consider the phrase path in between. \u2022 CPHBNULL when no phrase in between \u2022 CPHBFL: the only phrase head when only one phrase in between \u2022 CPHBF: first phrase head in between when at least two phrases in between \u2022 CPHBL: last phrase head in between when at least two phrase heads in between \u2022 CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between \u2022 CPHBM1F: first phrase head before M1 \u2022 CPHBM1L: second phrase head before M1 \u2022 CPHAM2F: first phrase head after M2 \u2022 CPHAM2F: second phrase head after M2 \u2022 CPP: path of phrase labels connecting the two mentions in the chunking \u2022 CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins\u2019 parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP.", "Especially although we did not concern the dependency tree and full parse tree information as other supervised methods Miller et al Culotta and Soresen Kambhatla ##CITATION## the incorporation of simple features such as words and chunking information still can provide complement information for capturing the characteristics of entity pairs", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "Especially although we did not concern the dependency tree and full parse tree information as other supervised methods Miller et al Culotta and Soresen Kambhatla ##CITATION## the incorporation of simple features such as words and chunking information still can provide complement information for capturing the characteristics of entity pairs", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.", "Especially although we did not concern the dependency tree and full parse tree information as other supervised methods Miller et al Culotta and Soresen Kambhatla ##CITATION## the incorporation of simple features such as words and chunking information still can provide complement information for capturing the characteristics of entity pairs", 1, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser   is employed for full parsing.", "This follows on from the success of these methods in general NLP see for example ##CITATION## ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427\u2013434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 and Section 4 describe our approach and various features employed respectively.", "This follows on from the success of these methods in general NLP see for example ##CITATION## ", 0, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase \u201cthe former CEO of McDonald\u201d has the head annotation of \u201cCEO\u201d and the extent annotation of \u201cthe former CEO of McDonald\u201d, we only consider \u201cthe former CEO\u201d in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention.", "This follows on from the success of these methods in general NLP see for example ##CITATION## ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Section 3 and Section 4 describe our approach and various features employed respectively. Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees.", "This follows on from the success of these methods in general NLP see for example ##CITATION## ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins\u2019 parser   is employed for full parsing.", "We use features developed in part from those described in ##CITATION##   and Wang et al  ", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations. During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.", "We use features developed in part from those described in ##CITATION##   and Wang et al  ", 0, "Exploring Various Knowledge in Relation Extraction", "Experimentation."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "We use features developed in part from those described in ##CITATION##   and Wang et al  ", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "We use features developed in part from those described in ##CITATION##   and Wang et al  ", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al   proposed extracting relations by computing kernel functions between parse trees. Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.", "A variety of features have been explored for ERD in previous research ##CITATION## ##CITATION## Jiang and Zhai Miller et al", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", "A variety of features have been explored for ERD in previous research ##CITATION## ##CITATION## Jiang and Zhai Miller et al", 0, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.", "A variety of features have been explored for ERD in previous research ##CITATION## ##CITATION## Jiang and Zhai Miller et al", 0, "Exploring Various Knowledge in Relation Extraction", "Features."], ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al   which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.", "A variety of features have been explored for ERD in previous research ##CITATION## ##CITATION## Jiang and Zhai Miller et al", 1, "Exploring Various Knowledge in Relation Extraction", "Related Work."], ["5 55 .5 Ka mb hat la (20 04) :fe ature bas ed 6 3. 5 4 5. 2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges Kim et al and geographical locations ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["Advances in Neural Information Processing Systems 14. Cambridge, MA. Culotta A. and Sorensen J.  . Dependency tree th parse trees.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges Kim et al and geographical locations ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges Kim et al and geographical locations ##CITATION##", 0, "Exploring Various Knowledge in Relation Extraction", "Those words that have the semantic classes \u201cParent\u201d,."], ["The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities. Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.). Mentions have three levels: names, nomial expressions or pronouns. The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges Kim et al and geographical locations ##CITATION##", 1, "Exploring Various Knowledge in Relation Extraction", "Introduction"], ["Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "While an improvement over simple destructive unification Tomabechi's approach still suffers from what ##CITATION## ##CITATION## calls redundant copying", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["These methods take two DGs as their inputs and give a unification result DG. Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures.", "While an improvement over simple destructive unification Tomabechi's approach still suffers from what ##CITATION## ##CITATION## calls redundant copying", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.", "While an improvement over simple destructive unification Tomabechi's approach still suffers from what ##CITATION## ##CITATION## calls redundant copying", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying.", "While an improvement over simple destructive unification Tomabechi's approach still suffers from what ##CITATION## ##CITATION## calls redundant copying", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts.", "While an improvement over simple destructive unification Tomabechi's approach still suffers from what ##CITATION## ##CITATION## calls redundant copying", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "While an improvement over simple destructive unification Tomabechi's approach still suffers from what ##CITATION## ##CITATION## calls redundant copying", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Method In a system where FS unification is applied, there are features whose values fail relatively often in unification with other values and there are features whose values do not fail so often. For example, in Japanese sentence analysis, unification of features for conjugation forms, case markers, and semantic selectional restrictions tends to fail but unification of features for semantic representations does not fail. In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails. For example, when unification of features for case markers does fail, treating these features first avoids treating features for senmntic representations. The SING unification method uses this failure tendency infornmtion.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification  Thus treatments such as strategic unification  have been developed ", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["(2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small. (3) Unevenness of FS unification failure tendency: in extreme cases, if every feature has the same unification failure tendency, this method has no advantage. However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints. In such cases, the SING unification method obtains efl]ciency gains. The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification  Thus treatments such as strategic unification  have been developed ", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system. That is, an FS unification method is proposed that introduces a strategy called the e_arly failure \u00a3inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures. This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method). These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method). Section 2 explains typed feature structures (TFSs) and unification on them.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification  Thus treatments such as strategic unification  have been developed ", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification  Thus treatments such as strategic unification  have been developed ", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy. 'the method treats features tending to fail in unification first.", "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification  Thus treatments such as strategic unification  have been developed ", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "This observation is the basis for a reordering method proposed by ##CITATION## ", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.", "This observation is the basis for a reordering method proposed by ##CITATION## ", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.", "This observation is the basis for a reordering method proposed by ##CITATION## ", 0, "Strategic Lazy Incremental Copy Graph Unification", "ABSTRACT"], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation. The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing systems mainly based on I.yped feature structure unification such as natural language analysis and generation sysl~ems. Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.", "This observation is the basis for a reordering method proposed by ##CITATION## ", 1, "Strategic Lazy Incremental Copy Graph Unification", "ABSTRACT"], ["Unification fails in treating arcs with common labels more often than in treating arcs with unique labels. Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes see also [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test. Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method. The efficiency of the LING unification method depends on the proportion of newly created structures in the unification result structures. Two worst eases can be considered: (t) If there are no arcs whose labels are unique to an input node witlh respect to each other, the procedure in LING unification method behaves in the same way as the procedure in the Wroblewski's method.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes see also [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["The whole subgraph rooted by 6 l/<a c> is then copied. This is because such subgraphs can be modified later. For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated. Incremental Copy Graph Unification PROCEDURE Unify(node1, node2) node1 = Dereference(nodel). node2 = Dereferencelnode2).", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes see also [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes see also [##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction. As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes see also [##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction. As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag. This is because the type symbol of a 'rFS represents salient information on the whole TFS.", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes see also [##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["It substitutes arcs with newly copied nodes for existing arcs. That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.", "The lazy copying approach [##CITATION## and [Emele for lazy copying in TFS with historical backtracking copies only overlapping parts of the structure", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem. The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS.", "The lazy copying approach [##CITATION## and [Emele for lazy copying in TFS with historical backtracking copies only overlapping parts of the structure", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Next, the procedure treats arcs obtained by ComplementArcs. Each arc value is copied and an arc with the same label and the copied value is added to the output node. For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig. 5. The unification procedure is applied recursively to feature a values of the input nodes.", "The lazy copying approach [##CITATION## and [Emele for lazy copying in TFS with historical backtracking copies only overlapping parts of the structure", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG. Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).", "The lazy copying approach [##CITATION## and [Emele for lazy copying in TFS with historical backtracking copies only overlapping parts of the structure", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.", "The lazy copying approach [##CITATION## and [Emele for lazy copying in TFS with historical backtracking copies only overlapping parts of the structure", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["U_ Bottom Figure 1: Exainple of a type symbol lattice --2-- peSymb\u00b010 eaturel TypeSymboll ] ]] I feature2 TypeSymbol2 I feature3 ?Tag T ypeSymbol3 ] ]feature4 TypeSymbol4 L [.feature5 TypeSymbol5 TIeature3 7Tag (a) feature-value matrix notation \"?\" i~ the prefix for a tag and TFSs with the same tag are token-identical. TypeSym bol/~ feo~.,o/ I TypeSymboll ~ [. TypeSymbol2 4\u00a2\" '~\u00b0~'~/.~ypeSymbol3 featury \"X~ature5 TypeSymbol4 4r \"~TypeSymbol5 (b) directed graph notation Figure 2: TFS notations Phrase [sub(at ?X2 SignList ] dtrs CHconst Sign U Syn i'oo I syn I head ?Xl . ] ubcat NonEmptySignLIst | ['first ]1 ?\u00d73 Lrest ?X2 J j Phrase -dtrs CHconst hdtr LexicalSignsyn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign ,11 yn Synead Head L~,os N] Irest EmptySignkist Phrase \"syn Syn head ?X1 Head Fpos P Lform Ga ] Lsubcat ?X2 Empl.ySignList dtrs CHconst ccltr ?X3 Sign syn iyn head Head _ [pos N hdtr LexicalSign l-syn Syn l I F head :x~ 7/ Lsubcat [ NonEinptySignList l l P\"\" ~\u00d7~ llll Lrest ?X2 JJjJ Figure 3: Example of TFS unification Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet. A unification example is shown in Fig. 3.", "The lazy copying approach [##CITATION## and [Emele for lazy copying in TFS with historical backtracking copies only overlapping parts of the structure", 1, "Strategic Lazy Incremental Copy Graph Unification", "Typed Feature Structures."], ["I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem. The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS.", " Data-Structure Sharing: Two or more distinct graphs share the same subgralh by converging nil the same node the notim nf", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not. The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", " Data-Structure Sharing: Two or more distinct graphs share the same subgralh by converging nil the same node the notim nf", 0, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation. The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing systems mainly based on I.yped feature structure unification such as natural language analysis and generation sysl~ems.", " Data-Structure Sharing: Two or more distinct graphs share the same subgralh by converging nil the same node the notim nf", 0, "Strategic Lazy Incremental Copy Graph Unification", "ABSTRACT"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", " Data-Structure Sharing: Two or more distinct graphs share the same subgralh by converging nil the same node the notim nf", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy.", " Data-Structure Sharing: Two or more distinct graphs share the same subgralh by converging nil the same node the notim nf", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan. Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification. Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871. These methods uses rooted directed graphs (DGs) to represent FSs.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR sometimes 98 percent of the elapsed time is devoted to graph unification [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["In contrast, in a procedure-based system which uses IF-TItEN style rules (i.e., consisting of explicit test and structure-building operations), it is possible to construct the semantic representation (TIIEN par'g) after checking the agreement (IF part). Such a system has the advantage of processing efficiency but the disadvantage of lacking multidirectionality. In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system. That is, an FS unification method is proposed that introduces a strategy called the e_arly failure \u00a3inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures. This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR sometimes 98 percent of the elapsed time is devoted to graph unification [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig. 5. The unification procedure is applied recursively to feature a values of the input nodes. The node specified by the feature path <a> fi'om input graph G1 (Gl/<a>) has an arc with the label c and the corresponding node of input graph G2 does not. The whole subgraph rooted by 6 l/<a c> is then copied.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR sometimes 98 percent of the elapsed time is devoted to graph unification [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy.", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR sometimes 98 percent of the elapsed time is devoted to graph unification [##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["its arguments and gives one list of arcs whose labels are unique to one input list. The unification procedure first treats arc pairs obtained by SharedArcs. The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom. Next, the procedure treats arcs obtained by ComplementArcs. Each arc value is copied and an arc with the same label and the copied value is added to the output node.", "With the simplicity of our algorithm and the ease of implementing it compared to both incremental copying schemes and lazy schemes combined with the demonstrated speed of the algorithm the algorithm could be a viable alternative to existing unification algorithms used in current ~That is unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The slot has pairs consisting of nodes and arcs as its value. The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig. 7): (1) if nodel ', the dereference result of node/, is current, then CopyNode returns node l\" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1\" and if it returns ,~;everal arc copies, CopyNode creates a new copy node. It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1\" and returns Nil_. ,',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results.", "With the simplicity of our algorithm and the ease of implementing it compared to both incremental copying schemes and lazy schemes combined with the demonstrated speed of the algorithm the algorithm could be a viable alternative to existing unification algorithms used in current ~That is unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["It substitutes arcs with newly copied nodes for existing arcs. That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.", "With the simplicity of our algorithm and the ease of implementing it compared to both incremental copying schemes and lazy schemes combined with the demonstrated speed of the algorithm the algorithm could be a viable alternative to existing unification algorithms used in current ~That is unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest [##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["The efficiency of systems based on ..~uch formalisms, such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies. Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO[Pollard and Sag 861 and JPSG[Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs. For example, a spoken Present. affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan.", "With the simplicity of our algorithm and the ease of implementing it compared to both incremental copying schemes and lazy schemes combined with the demonstrated speed of the algorithm the algorithm could be a viable alternative to existing unification algorithms used in current ~That is unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest [##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan. Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification. Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871. These methods uses rooted directed graphs (DGs) to represent FSs.", "With the simplicity of our algorithm and the ease of implementing it compared to both incremental copying schemes and lazy schemes combined with the demonstrated speed of the algorithm the algorithm could be a viable alternative to existing unification algorithms used in current ~That is unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest [##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["ENDIF ENDIE ENDPROCEDURE Figure 6: Incremental copy graph unification procedure The problem with Wroblewski's method is that tile whole result DG is created by using only newly created structures. In the example in Fig. 5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels. This order is related to the unification failure tendency. Unification fails in treating arcs with common labels more often than in treating arcs with unique labels.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["I\"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly. In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts. Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency. I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS. Therefore, Pereira's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification. However, Pereira's method can create skeleton-enviromnent structures that are deeply embedded, for example, in reeursively constructing large phrase structure fl'om their parts.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not. The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Conclusion."], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "A more eNcient unification algorithm would avoid this redundant copying copying structures that can be shared by the input and resultant graphs ##CITATION##", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden   proposed a unification algorithm that uses active data structures ##CITATION##   proposed a lazy incremental copy graph   proposed a lazy-incremental copying LIC unification that uses chronological dereference", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden   proposed a unification algorithm that uses active data structures ##CITATION##   proposed a lazy incremental copy graph   proposed a lazy-incremental copying LIC unification that uses chronological dereference", 0, "Strategic Lazy Incremental Copy Graph Unification", "ABSTRACT"], ["The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden   proposed a unification algorithm that uses active data structures ##CITATION##   proposed a lazy incremental copy graph   proposed a lazy-incremental copying LIC unification that uses chronological dereference", 0, "Strategic Lazy Incremental Copy Graph Unification", "PAPER"], ["Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden   proposed a unification algorithm that uses active data structures ##CITATION##   proposed a lazy incremental copy graph   proposed a lazy-incremental copying LIC unification that uses chronological dereference", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden   proposed a unification algorithm that uses active data structures ##CITATION##   proposed a lazy incremental copy graph   proposed a lazy-incremental copying LIC unification that uses chronological dereference", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "Recent lazy copying unification algorithms are based on Wroblewski's or Tomabeehi's schema: Godden   proposed a unification algorithm that uses active data structures ##CITATION##   proposed a lazy incremental copy graph   proposed a lazy-incremental copying LIC unification that uses chronological dereference", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["I\"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly. In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts. Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency. I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem.", "A better method would avoid eliminate such redundant copying as it is called by [##CITATION## 90]", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.", "A better method would avoid eliminate such redundant copying as it is called by [##CITATION## 90]", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Strategic Incremental Copy Graph Unification."], ["In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>).", "A better method would avoid eliminate such redundant copying as it is called by [##CITATION## 90]", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "A better method would avoid eliminate such redundant copying as it is called by [##CITATION## 90]", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "A better method would avoid eliminate such redundant copying as it is called by [##CITATION## 90]", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["In TFS unification based on Wrobtewski's method, a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively, as shown in Fig. 4. The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY. When a NODE's GENERATION value is equal to the global value specifying the current unit]cation process, the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE's two different slots, FORWARD and COPY, for representing forwarding relationships. A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship.", "As it has been noticed by [Godden 90] and [##CITATION## 90] the key idea of avoiding \"redundant copying\" is to do copying lazily Copying of nodes will be delayed until a destructive change is about to take place Godden uses active data structures Lisp closures to implement lazy evaluation of copying and ##CITATION## uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying Similarly in ##CITATION##'s approach not all redundant copying is avoided in cases where there exists a feature path a sequence of nodes connected by arcs to a node that needs to be copied   ", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig. 7): (1) if nodel ', the dereference result of node/, is current, then CopyNode returns node l\" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1\" and if it returns ,~;everal arc copies, CopyNode creates a new copy node. It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1\" and returns Nil_. ,',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results. When a new copy of a node is needed later, the LING unification procedure will actually copy structures using the COPY-DEPENDENCY slot value of the node (in GetOutNode procedure in lJ'ig.", "As it has been noticed by [Godden 90] and [##CITATION## 90] the key idea of avoiding \"redundant copying\" is to do copying lazily Copying of nodes will be delayed until a destructive change is about to take place Godden uses active data structures Lisp closures to implement lazy evaluation of copying and ##CITATION## uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying Similarly in ##CITATION##'s approach not all redundant copying is avoided in cases where there exists a feature path a sequence of nodes connected by arcs to a node that needs to be copied   ", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["The procedure assumes the existence of two procedures, namely, SharedArcs and ComplementArcs. The SharedArcs procedure takes two lists of arcs as its arguments and gives two lists of arcs each of which contains arcs whose labels exists in both lists with the same arc label order. The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: \"<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski's method Input graph GI Input graph 62 \u00a2 .......'77 ........ i : Sobg,'aphs not required to be copied L ........................................... Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted. its arguments and gives one list of arcs whose labels are unique to one input list.", "As it has been noticed by [Godden 90] and [##CITATION## 90] the key idea of avoiding \"redundant copying\" is to do copying lazily Copying of nodes will be delayed until a destructive change is about to take place Godden uses active data structures Lisp closures to implement lazy evaluation of copying and ##CITATION## uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying Similarly in ##CITATION##'s approach not all redundant copying is avoided in cases where there exists a feature path a sequence of nodes connected by arcs to a node that needs to be copied   ", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths.", "As it has been noticed by [Godden 90] and [##CITATION## 90] the key idea of avoiding \"redundant copying\" is to do copying lazily Copying of nodes will be delayed until a destructive change is about to take place Godden uses active data structures Lisp closures to implement lazy evaluation of copying and ##CITATION## uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying Similarly in ##CITATION##'s approach not all redundant copying is avoided in cases where there exists a feature path a sequence of nodes connected by arcs to a node that needs to be copied   ", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>).", "As it has been noticed by [Godden 90] and [##CITATION## 90] the key idea of avoiding \"redundant copying\" is to do copying lazily Copying of nodes will be delayed until a destructive change is about to take place Godden uses active data structures Lisp closures to implement lazy evaluation of copying and ##CITATION## uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying Similarly in ##CITATION##'s approach not all redundant copying is avoided in cases where there exists a feature path a sequence of nodes connected by arcs to a node that needs to be copied   ", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>). To achieve this, I, he LING unification method, which uses copy dependency information, was developed.", "As it has been noticed by [Godden 90] and [##CITATION## 90] the key idea of avoiding \"redundant copying\" is to do copying lazily Copying of nodes will be delayed until a destructive change is about to take place Godden uses active data structures Lisp closures to implement lazy evaluation of copying and ##CITATION## uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying Similarly in ##CITATION##'s approach not all redundant copying is avoided in cases where there exists a feature path a sequence of nodes connected by arcs to a node that needs to be copied   ", 1, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test. Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata Wroblewski ##CITATION##;  provide a better match between the characteristics of the unifiers and those of the linguistic processors", 0, "Strategic Lazy Incremental Copy Graph Unification", "The Lazy Incremental Copy Graph Unification Method."], ["These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method). Section 2 explains typed feature structures (TFSs) and unification on them. Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method. The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method. Section 3 and 4 introduce the LING method and the SING method, respectively.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata Wroblewski ##CITATION##;  provide a better match between the characteristics of the unifiers and those of the linguistic processors", 0, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Unification fails in treating arcs with common labels more often than in treating arcs with unique labels. Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed. In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata Wroblewski ##CITATION##;  provide a better match between the characteristics of the unifiers and those of the linguistic processors", 0, "Strategic Lazy Incremental Copy Graph Unification", "Wroblewski's Incremental Copy Graph Unifitation Method and Its Problems."], ["This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous.", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata Wroblewski ##CITATION##;  provide a better match between the characteristics of the unifiers and those of the linguistic processors", 1, "Strategic Lazy Incremental Copy Graph Unification", "Introduction"], ["Hence, this approach produced lower F-scores. However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters.", "CRF + Rule system represents a combination of CRF model and rule based model presented in ##CITATION##  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Another advantage of the word-based IOB tagging over the character-based is its speed. The subword-based approach is faster because fewer words than characters were labeled. We found a speed up both in training and test. The idea of using the confidence measure has appeared in  , where it was used to recognize the OOVs. In this work we used it more delicately.", "CRF + Rule system represents a combination of CRF model and rule based model presented in ##CITATION##  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["71 0 0. 71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging. The upper numbers are of the character- based and the lower ones, the subword-based. using the FMM, and then labeled with \u201cIOB\u201d tags by the CRFs. The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.", "CRF + Rule system represents a combination of CRF model and rule based model presented in ##CITATION##  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "CRF + Rule system represents a combination of CRF model and rule based model presented in ##CITATION##  ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach.", "CRF + Rule system represents a combination of CRF model and rule based model presented in ##CITATION##  ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv). For detailed info. of the corpora and these scores, refer to  . For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.", "Consequently many strategies are proposed to balance the IV and OOV performance Goh et al ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv. The authors appreciate the reviewers\u2019 effort and good advice for improving the paper.", "Consequently many strategies are proposed to balance the IV and OOV performance Goh et al ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only. Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv). For detailed info. of the corpora and these scores, refer to  .", "Consequently many strategies are proposed to balance the IV and OOV performance Goh et al ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  .", "Consequently many strategies are proposed to balance the IV and OOV performance Goh et al ##CITATION##", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["However, neither was perfect. The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.", "Among these strategies the confidence measure used to combine the results of CT and DS is a straightforward one which is introduced in ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["3.2 Effect of the confidence measure. In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation. The effect of the confidence measure is shown in Table 3, where we used \u03b1 = 0.7 and confidence threshold t = 0.8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based. We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.", "Among these strategies the confidence measure used to combine the results of CT and DS is a straightforward one which is introduced in ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure. In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation. The effect of the confidence measure is shown in Table 3, where we used \u03b1 = 0.7 and confidence threshold t = 0.8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based.", "Among these strategies the confidence measure used to combine the results of CT and DS is a straightforward one which is introduced in ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.", "Among these strategies the confidence measure used to combine the results of CT and DS is a straightforward one which is introduced in ##CITATION##", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.", "Among these strategies the confidence measure used to combine the results of CT and DS is a straightforward one which is introduced in ##CITATION##", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging. Each subword is given a prior IOB tag, tw . C Miob (t|w), a \uf8eb M \uf8eb\uf8f6\uf8f6 confidence probability derived in the process of IOB tag exp \uf8ec)' \uf8ec)' \u03bbk fk (ti\u22121 , ti , W ) + )' \u00b5k gk (ti , W )\uf8f7\uf8f7 /Z, \uf8ec\uf8ed i=1 \uf8ec\uf8ed k k \uf8f7\uf8f8 \uf8f7\uf8f8 (1) ging, is defined as Z = )' T =t0 t1 \u00b7\u00b7\u00b7tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 \u00b7\u00b7\u00b7tM ,ti =t P(T |W, wi ) T =t 0 t1 \u00b7\u00b7\u00b7 tM P ( T | W ) where we call fk (ti\u22121 , ti , W ) bigram feature functions because the features trigger the previous observation ti\u22121 where the numerator is a sum of all the observation sequences with word wi labeled as t. \u03b4(tw , tiob )ng denotes the contribution of the dictionary- based segmentation. It is a Kronecker delta function defined as \u03b4(tw , tiob )ng = { 1 if tw = tiob 0 otherwise In Eq. 2, \u03b1 is a weighting between the IOB tagging and the dictionary-based word segmentation.", "After we get word-based segmentation result we use it to revise the CRF tagging result similar to ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2. Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tagging was very effective. The IOB tagging approach adopted in this work is not a new idea. It was first used in Chinese word segmentation by  , where maximum entropy methods were used.", "After we get word-based segmentation result we use it to revise the CRF tagging result similar to ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "After we get word-based segmentation result we use it to revise the CRF tagging result similar to ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach.", "After we get word-based segmentation result we use it to revise the CRF tagging result similar to ##CITATION##", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure. In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation. The effect of the confidence measure is shown in Table 3, where we used \u03b1 = 0.7 and confidence threshold t = 0.8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based.", "If the confidence of a character is lower than the threshold the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation R ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Qc 2006 Association for Computational Linguistics input \u5498\u38c5\u1bf9\u0523\u0cfc\u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation \u5498 \u38c5 \u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output \u5498\u38c5\u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data. We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.", "If the confidence of a character is lower than the threshold the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation R ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.", "If the confidence of a character is lower than the threshold the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation R ##CITATION##", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found the value 0.7 for \u03b1, empirically. By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created.", "If the confidence of a character is lower than the threshold the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation R ##CITATION##", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created. For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.", "If the confidence of a character is lower than the threshold the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation R ##CITATION##", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We will give a detailed description of this approach in Section 2. \u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005  , the work of  , using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired.", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  .", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["However, neither was perfect. The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found the value 0.7 for \u03b1, empirically. By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created.", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created. For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach.", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure.", "According to the results reported in   CRF performs relatively better on Out-of-Vocabulary OOV words while Maximum Probability performs well on IV words so a model combining the advantages of these two methods is appealing", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["For detailed info. of the corpora and these scores, refer to  . For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation.", "We chose the three models that achieved at least one best score in the closed tests from Emerson  as well as the sub-word-based model of Zhang Kikui and Sumita  for comparison", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["using the FMM, and then labeled with \u201cIOB\u201d tags by the CRFs. The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based. We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.", "We chose the three models that achieved at least one best score in the closed tests from Emerson  as well as the sub-word-based model of Zhang Kikui and Sumita  for comparison", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  . Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d.", "We chose the three models that achieved at least one best score in the closed tests from Emerson  as well as the sub-word-based model of Zhang Kikui and Sumita  for comparison", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "We chose the three models that achieved at least one best score in the closed tests from Emerson  as well as the sub-word-based model of Zhang Kikui and Sumita  for comparison", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.", " denoted as G07 with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models respectively ##CITATION##; ZC07 represents the word-based perceptron model in Zhang and Clark ; T05 represents the CRF model in Tseng et al", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["The idea of using the confidence measure has appeared in  , where it was used to recognize the OOVs. In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.", " denoted as G07 with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models respectively ##CITATION##; ZC07 represents the word-based perceptron model in Zhang and Clark ; T05 represents the CRF model in Tseng et al", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  . Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", " denoted as G07 with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models respectively ##CITATION##; ZC07 represents the word-based perceptron model in Zhang and Clark ; T05 represents the CRF model in Tseng et al", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", " denoted as G07 with an improved performance; Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models respectively ##CITATION##; ZC07 represents the word-based perceptron model in Zhang and Clark ; T05 represents the CRF model in Tseng et al", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv. The authors appreciate the reviewers\u2019 effort and good advice for improving the paper.", "One existing method that is based on sub-word information ##CITATION##  combines a C R F and a rule-based model ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, \u201c (Beijing-city)\u201d can be segmented as \u201c (Beijing-city)/O,\u201d or \u201c (Beijing)/B (city)/I,\u201d or \u201d (north)/B (capital)/I (city)/I.\u201d In this work we used forward maximal match (FMM) for disambiguation.", "One existing method that is based on sub-word information ##CITATION##  combines a C R F and a rule-based model ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions. However, keep in mind that the dictionary-based approach can produce a higher R-iv rate. We will use this advantage in the confidence measure approach.", "One existing method that is based on sub-word information ##CITATION##  combines a C R F and a rule-based model ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "One existing method that is based on sub-word information ##CITATION##  combines a C R F and a rule-based model ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["For detailed info. of the corpora and these scores, refer to  . For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation.", "We chose the three models that achieved at least one best score in the closed tests from Emerson   as well as the sub-word-based model of ##CITATION##   for comparison ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["9 5 1 0. 97 1 0. 95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2. Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tagging was very effective.", "We chose the three models that achieved at least one best score in the closed tests from Emerson   as well as the sub-word-based model of ##CITATION##   for comparison ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known. For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d. Our approaches produced wrong segmentations for labeling inconsistency. Another advantage of the word-based IOB tagging over the character-based is its speed.", "We chose the three models that achieved at least one best score in the closed tests from Emerson   as well as the sub-word-based model of ##CITATION##   for comparison ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "We chose the three models that achieved at least one best score in the closed tests from Emerson   as well as the sub-word-based model of ##CITATION##   for comparison ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models presented in ##CITATION##  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  . Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models presented in ##CITATION##  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models presented in ##CITATION##  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models presented in ##CITATION##  ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "Also the CRF model using maximum subword-based tagging ##CITATION## and the CRF model using minimum subword-based tagging both of which are statistical methods are used individually to solve the Figure 1: Outline of the segmentation process 21 Forward Maximum Matching", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The upper numbers are of the character- based and the lower ones, the subword-based. using the FMM, and then labeled with \u201cIOB\u201d tags by the CRFs. The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based. We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved.", "Also the CRF model using maximum subword-based tagging ##CITATION## and the CRF model using minimum subword-based tagging both of which are statistical methods are used individually to solve the Figure 1: Outline of the segmentation process 21 Forward Maximum Matching", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding. 2.2 Confidence-dependent word segmentation. Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging. However, neither was perfect. The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.", "Also the CRF model using maximum subword-based tagging ##CITATION## and the CRF model using minimum subword-based tagging both of which are statistical methods are used individually to solve the Figure 1: Outline of the segmentation process 21 Forward Maximum Matching", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Also the CRF model using maximum subword-based tagging ##CITATION## and the CRF model using minimum subword-based tagging both of which are statistical methods are used individually to solve the Figure 1: Outline of the segmentation process 21 Forward Maximum Matching", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "Recently   proposed a maximum subword-based IOB tagger for Chinese word segmentation and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "Recently   proposed a maximum subword-based IOB tagger for Chinese word segmentation and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  . Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "Recently   proposed a maximum subword-based IOB tagger for Chinese word segmentation and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Recently   proposed a maximum subword-based IOB tagger for Chinese word segmentation and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov and R-iv. The authors appreciate the reviewers\u2019 effort and good advice for improving the paper.", "Recently   proposed a maximum subword-based IOB tagger for Chinese word segmentation and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold. In Section 3.2 we will present the experimental segmentation results of the confidence measure approach. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections. The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.", "Recently   proposed a maximum subword-based IOB tagger for Chinese word segmentation and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = \u03b1C Miob (tiob |w) + (1 \u2212 \u03b1)\u03b4(tw , tiob )ng (2) where tiob is the word w\u2019s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.", "Part of the work using this tool was described by ##CITATION## The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The IOB tagging approach adopted in this work is not a new idea. It was first used in Chinese word segmentation by  , where maximum entropy methods were used. Later, this approach was implemented by the CRF-based method  , which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem  . Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based. We proved the new approach enhanced the word segmentation significantly.", "Part of the work using this tool was described by ##CITATION## The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["In the results of the closed test in Bakeoff 2005  , the work of  , using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness. By this approach we can change R-oovs and R-ivs and find an optimal tradeoff. This approach will be described in Section 2.2.", "Part of the work using this tool was described by ##CITATION## The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "Part of the work using this tool was described by ##CITATION## The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold. In Section 3.2 we will present the experimental segmentation results of the confidence measure approach. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections. The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.", "Part of the work using this tool was described by ##CITATION## The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["The subscripts are position indicators. 0 means the current word; \u22121, \u22122, the first or second word to the left; 1, 2, the first or second word to the right. For the bigram features, we only used the previous and the current observations, t\u22121 t0 . As to feature selection, we simply used absolute counts for each feature in the training data. We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff. A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS like the \u201cdict-hybrid\u201d   We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known. For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d. Our approaches produced wrong segmentations for labeling inconsistency. Another advantage of the word-based IOB tagging over the character-based is its speed.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS like the \u201cdict-hybrid\u201d   We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["Of course, backward maximal match (BMM) or other approaches are also applicable. We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach. In the third step, we used the CRFs approach to train the IOB tagger   on the training data. We downloaded and used the package \u201cCRF++\u201d from the site \u201chttp://www.chasen.org/\u02dctaku/software.\u201d According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 \u00b7 \u00b7 \u00b7 tM , given the word sequence, W = w0 w1 \u00b7 \u00b7 \u00b7 wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . \u03bbk and \u00b5k are the model parameters corresponding to feature functions fk and gk respectively. The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS like the \u201cdict-hybrid\u201d   We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["For detailed info. of the corpora and these scores, refer to  . For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS like the \u201cdict-hybrid\u201d   We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["of the corpora and these scores, refer to  . For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation. Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS like the \u201cdict-hybrid\u201d   We used the \u201cdict-hybrid\u201d to segment the SMT training corpus and test data", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193\u2013196, New York, June 2006. Qc 2006 Association for Computational Linguistics input \u5498\u38c5\u1bf9\u0523\u0cfc\u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation \u5498 \u38c5 \u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation \u5498/% \u38c5/, \u1bf9/, \u0523/2 \u0cfc/2 \u08eb\u0480/% \u13d6/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output \u5498\u38c5\u1bf9 \u0523 \u0cfc \u08eb\u0480\u13d6 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data. We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging.", "Some previous work Peng et al Tseng et al Low et al illustrated the effectiveness of using characters as tagging units while literatures ##CITATION## Zhao and Kit; Zhang and Clark focus on employing lexical words or subwords as tagging units", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation.", "Some previous work Peng et al Tseng et al Low et al illustrated the effectiveness of using characters as tagging units while literatures ##CITATION## Zhao and Kit; Zhang and Clark focus on employing lexical words or subwords as tagging units", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["The idea of using the confidence measure has appeared in  , where it was used to recognize the OOVs. In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.", "Some previous work Peng et al Tseng et al Low et al illustrated the effectiveness of using characters as tagging units while literatures ##CITATION## Zhao and Kit; Zhang and Clark focus on employing lexical words or subwords as tagging units", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Some previous work Peng et al Tseng et al Low et al illustrated the effectiveness of using characters as tagging units while literatures ##CITATION## Zhao and Kit; Zhang and Clark focus on employing lexical words or subwords as tagging units", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "For this purpose our system is based on a combination of subword-based tagging method ##CITATION## and accessor variety-based new word recognition method Feng et al", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters. For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.", "For this purpose our system is based on a combination of subword-based tagging method ##CITATION## and accessor variety-based new word recognition method Feng et al", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "For this purpose our system is based on a combination of subword-based tagging method ##CITATION## and accessor variety-based new word recognition method Feng et al", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "For this purpose our system is based on a combination of subword-based tagging method ##CITATION## and accessor variety-based new word recognition method Feng et al", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.", "For this purpose our system is based on a combination of subword-based tagging method ##CITATION## and accessor variety-based new word recognition method Feng et al", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "ABSTRACT"], ["We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit. We will give a detailed description of this approach in Section 2.", "Feature Template Description f instr subword-list is str in subword list g instr confident-word-list is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "Feature Template Description f instr subword-list is str in subword list g instr confident-word-list is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters. For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.", "Feature Template Description f instr subword-list is str in subword list g instr confident-word-list is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Experiments."], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "Feature Template Description f instr subword-list is str in subword list g instr confident-word-list is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in  ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. The character-based \u201cIOB\u201d tagging approach has been widely used in Chinese word segmentation recently  . Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging.", "See the details of subword-based Chinese word segmentation in   22 OOV Recognition with Accessor Variety", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "See the details of subword-based Chinese word segmentation in   22 OOV Recognition with Accessor Variety", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Conclusions."], ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.", "See the details of subword-based Chinese word segmentation in   22 OOV Recognition with Accessor Variety", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "PAPER"], ["Under the scheme, each character of a word is labeled as \u2018B\u2019 if it is the first character of a multiple-character word, or \u2018O\u2019 if the character functions as an independent word, or \u2018I\u2019 otherwise.\u201d For example, \u201d (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (north)/B (capital)/I (city)/I\u201d. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit.", "See the details of subword-based Chinese word segmentation in   22 OOV Recognition with Accessor Variety", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, \u201c (whole) (Beijing city)\u201d is labeled as \u201d (whole)/O (Beijing)/B (city)/I\u201d in the subword-based tagging, where \u201d (Beijing)/B\u201d is labeled as one unit. We will give a detailed description of this approach in Section 2.", "See the details of subword-based Chinese word segmentation in   22 OOV Recognition with Accessor Variety", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known. For AS corpus, \u201cAdam Smith\u201d are two words in the training but become a one- word in the test, \u201cAdamSmith\u201d. Our approaches produced wrong segmentations for labeling inconsistency. Another advantage of the word-based IOB tagging over the character-based is its speed.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH\u201d However when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH\u201d \u201cENQUIRIES\u201d does not occur but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may Thus the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear therefore corresponding solutions such as those of ##CITATION##  were proposed  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Discussion and Related works."], ["\u2217 Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005  , the work of  , using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH\u201d However when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH\u201d \u201cENQUIRIES\u201d does not occur but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may Thus the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear therefore corresponding solutions such as those of ##CITATION##  were proposed  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Introduction"], ["Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each step\u2019s results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH\u201d However when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH\u201d \u201cENQUIRIES\u201d does not occur but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may Thus the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear therefore corresponding solutions such as those of ##CITATION##  were proposed  ", 0, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, \u201c (Beijing-city)\u201d can be segmented as \u201c (Beijing-city)/O,\u201d or \u201c (Beijing)/B (city)/I,\u201d or \u201d (north)/B (capital)/I (city)/I.\u201d In this work we used forward maximal match (FMM) for disambiguation.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH\u201d However when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH\u201d \u201cENQUIRIES\u201d does not occur but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may Thus the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear therefore corresponding solutions such as those of ##CITATION##  were proposed  ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, \u201c (Beijing-city)\u201d can be segmented as \u201c (Beijing-city)/O,\u201d or \u201c (Beijing)/B (city)/I,\u201d or \u201d (north)/B (capital)/I (city)/I.\u201d In this work we used forward maximal match (FMM) for disambiguation. Of course, backward maximal match (BMM) or other approaches are also applicable.", "\u201cENQUIRIES\u201d follows \u201cRAIL\u201d with a very high probability when it is preceded by \u201cBRITISH\u201d However when \u201cRAIL\u201d is preceded by words other than \u201cBRITISH\u201d \u201cENQUIRIES\u201d does not occur but words like \u201cTICKET\u201d or \u201cJOURNEY\u201d may Thus the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear therefore corresponding solutions such as those of ##CITATION##  were proposed  ", 1, "Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation", "Our Chinese word segmentation process."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.  , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.  ; here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "We explore the different options for context and feature se 1 See Chan et al for the relevance of word sense disambiguation and ##CITATION## for the role of prepositions in MT 454 Coling Poster Volume pages 454\u2013462 Beijing Augustlection the influence of different preprocessing methods and different levels of sense granularity", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["This minimization is performed by a variant of sequential minimal optimization  . Following Chiang et al.  , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "We explore the different options for context and feature se 1 See Chan et al for the relevance of word sense disambiguation and ##CITATION## for the role of prepositions in MT 454 Coling Poster Volume pages 454\u2013462 Beijing Augustlection the influence of different preprocessing methods and different levels of sense granularity", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall B\uf76c\uf765\uf775 improvement. 6.2 Word context features. In Table 6 are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with translations of dates and bylines. Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, \u2019s, said, parentheses, and quotes).", "We explore the different options for context and feature se 1 See Chan et al for the relevance of word sense disambiguation and ##CITATION## for the role of prepositions in MT 454 Coling Poster Volume pages 454\u2013462 Beijing Augustlection the influence of different preprocessing methods and different levels of sense granularity", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research   tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "We explore the different options for context and feature se 1 See Chan et al for the relevance of word sense disambiguation and ##CITATION## for the role of prepositions in MT 454 Coling Poster Volume pages 454\u2013462 Beijing Augustlection the influence of different preprocessing methods and different levels of sense granularity", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al.  , did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features Liang et al Tillmann and Zhang Watanabe et al Blunsom et al ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking  .", "Recent work have shown that SMT benefits a lot from exploiting large amount of features Liang et al Tillmann and Zhang Watanabe et al Blunsom et al ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features Liang et al Tillmann and Zhang Watanabe et al Blunsom et al ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "Recent work have shown that SMT benefits a lot from exploiting large amount of features Liang et al Tillmann and Zhang Watanabe et al Blunsom et al ##CITATION##", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al.  , did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.", "Overfitting problem often comes when training many features on a small data Watanabe et al 880 ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.  , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.  ; here, we are incorporating some of its features directly into the translation model.", "Overfitting problem often comes when training many features on a small data Watanabe et al 880 ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al.   introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already.", "Overfitting problem often comes when training many features on a small data Watanabe et al 880 ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research   tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "Overfitting problem often comes when training many features on a small data Watanabe et al 880 ##CITATION##", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["e\u2217 = arg max (B\uf76c\uf765\uf775(e) + h(e) \u00b7 w) (2) e Let \u2206hi j = h(e\u2217) \u2212 h(ei j). \u2022 For each ei j, compute the loss fi j = B\uf76c\uf765\uf775(e\u2217) \u2212 B\uf76c\uf765\uf775(ei j) (3) \u2022 Update w to the value of wt that minimizes: m Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems. For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position. If the second rule supplies the wrong preposition, a bad translation results. The IN node here is an overlap point between rules.", "3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3  Generally this is done by deleting a node X01  ing to constructing the oracle reference   which is nontrivial for SMT and needs to be determined experimentally", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The system rewards insertion of forms of be; examples \u22120.16 PRN \u22120.15 NPB \u22120.13 RB \u22120.12 SBAR-C \u22120.12 VP-C-BAR\u22120.11RRB . +0.14 NML +0.13 comma +0.12 VBD +0.12 NNPS +0.12 PRP +0.11 SG . 1\u20133 in Figure 1 show typical improved translations that result. Among determiners, inserting a is rewarded, while inserting the is punished. This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules. Inserting the outside Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories. these fixed phrases is a risk that the generative model is too inclined to take.", "3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3  Generally this is done by deleting a node X01  ing to constructing the oracle reference   which is nontrivial for SMT and needs to be determined experimentally", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We analyze the impact of the new features and the performance of the learning algorithm. What linguistic features can improve statistical machine translation (MT)? This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have. Further: \u2022 Do syntax-based translation systems have unique and effective levers to pull when designing new features? \u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?", "3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3  Generally this is done by deleting a node X01  ing to constructing the oracle reference   which is nontrivial for SMT and needs to be determined experimentally", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3  Generally this is done by deleting a node X01  ing to constructing the oracle reference   which is nontrivial for SMT and needs to be determined experimentally", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.", "Following   we only use 100 most frequent words for word context feature", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall B\uf76c\uf765\uf775 improvement. 6.2 Word context features. In Table 6 are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with translations of dates and bylines. Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, \u2019s, said, parentheses, and quotes).", "Following   we only use 100 most frequent words for word context feature", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik  . In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al.", "Following   we only use 100 most frequent words for word context feature", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning  . 3.2 Syntax-based system.", "Following   we only use 100 most frequent words for word context feature", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning  . 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees.", "Following   we only use 100 most frequent words for word context feature", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules. Inserting the outside Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories. these fixed phrases is a risk that the generative model is too inclined to take. We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data. Table 4 shows weights for rule-overlap features.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable   gradient descent  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["e\u2217 = arg max (B\uf76c\uf765\uf775(e) + h(e) \u00b7 w) (2) e Let \u2206hi j = h(e\u2217) \u2212 h(ei j). \u2022 For each ei j, compute the loss fi j = B\uf76c\uf765\uf775(e\u2217) \u2212 B\uf76c\uf765\uf775(ei j) (3) \u2022 Update w to the value of wt that minimizes: m Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems. For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position. If the second rule supplies the wrong preposition, a bad translation results. The IN node here is an overlap point between rules.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable   gradient descent  ", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable   gradient descent  ", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero   is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT.", "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable   gradient descent  ", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["6.2 Word context features. In Table 6 are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with translations of dates and bylines. Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, \u2019s, said, parentheses, and quotes). Finally, we note that several of the features (the third- and eighth-ranked reward and twelfth- ranked penalty) shape the translation of shuo \u2018said\u2019, preferring translations with an overt complementizer that and without a comma.", "The MIRA technique of ##CITATION## has been shown to perform well on large-scale tasks with hundreds or thousands of features ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "The MIRA technique of ##CITATION## has been shown to perform well on large-scale tasks with hundreds or thousands of features ", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Finally, we note that several of the features (the third- and eighth-ranked reward and twelfth- ranked penalty) shape the translation of shuo \u2018said\u2019, preferring translations with an overt complementizer that and without a comma. Thus these features work together to attack a frequent problem that our target- syntax features also addressed. Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time. The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data. This seems in line with the finding of Watanabe et al.", "The MIRA technique of ##CITATION## has been shown to perform well on large-scale tasks with hundreds or thousands of features ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "The MIRA technique of ##CITATION## has been shown to perform well on large-scale tasks with hundreds or thousands of features ", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  .", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al   and ##CITATION## ;  ", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking  .", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al   and ##CITATION## ;  ", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.  , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.  ; here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al   and ##CITATION## ;  ", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al   and ##CITATION## ;  ", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins cf ##CITATION##   Section 41 \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only cf  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall B\uf76c\uf765\uf775 improvement. 6.2 Word context features. In Table 6 are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with translations of dates and bylines. Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, \u2019s, said, parentheses, and quotes).", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins cf ##CITATION##   Section 41 \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only cf  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["How did the various new features improve the translation quality of our two systems? We begin by examining the discount features. For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights. We see in both cases that one-count rules are strongly penalized, as expected. Reward \u22120.42 a \u22120.13 are \u22120.09 at \u22120.09 on \u22120.05 was \u22120.05 from \u22120.04 \u2019s \u22120.04 by \u22120.04 is \u22120.03 it \u22120.03 its . Penalty +0.67 of +0.56 the +0.47 comma +0.13 period +0.11 in +0.08 for +0.06 to +0.05 will +0.04 and +0.02 as +0.02 have . Bonus \u22120.50 period \u22120.39 VP-C \u22120.36 VB \u22120.31 SG-C \u22120.30 MD \u22120.26 VBG \u22120.25 ADJP\u22120.22LRB \u22120.21 VP-BAR \u22120.20 NPB-BAR \u22120.16 FRAG P en alt y +0.93 IN +0.57 NNP +0.44 NN +0.41 DT +0.34 JJ +0.24 right double quote +0.20 VBZ +0.19 NP +0.16 TO +0.15 ADJP-BAR +0.14 PRN-BAR Table 3: Weights learned for inserting target English words with rules that lack Chinese words.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins cf ##CITATION##   Section 41 \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only cf  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins cf ##CITATION##   Section 41 \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only cf  ", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "##CITATION##   Section 41:10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["ations of all learners to decode the test set. The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1.5 B\uf76c\uf765\uf775 improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B\uf76c\uf765\uf775 improvement. The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes. 6 Analysis.", "##CITATION##   Section 41:10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["minimal rules. These larger rules have been shown to substantially improve translation accuracy  . We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero   is a hierarchical, string-to- string translation system.", "##CITATION##   Section 41:10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word ", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "##CITATION##   Section 41:10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word ", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["This minimization is performed by a variant of sequential minimal optimization  . Following Chiang et al.  , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "541 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by ##CITATION##  12 but instead of using the 10-best of each of the best hw  hw +g and hw -g we use the 30-best according to hw 13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as ##CITATION## ;  ", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [", fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.", "541 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by ##CITATION##  12 but instead of using the 10-best of each of the best hw  hw +g and hw -g we use the 30-best according to hw 13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as ##CITATION## ;  ", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [" , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder. In this section, we describe the new features introduced on top of our baseline systems. Discount features Both of our systems calculate several features based on observed counts of rules in the training data.", "541 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by ##CITATION##  12 but instead of using the 10-best of each of the best hw  hw +g and hw -g we use the 30-best according to hw 13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as ##CITATION## ;  ", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "541 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by ##CITATION##  12 but instead of using the 10-best of each of the best hw  hw +g and hw -g we use the 30-best according to hw 13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as ##CITATION## ;  ", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "Additionally we used 50000 sparse binary-valued source and target features based on ##CITATION##  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking  . Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "Additionally we used 50000 sparse binary-valued source and target features based on ##CITATION##  ", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["ations of all learners to decode the test set. The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1.5 B\uf76c\uf765\uf775 improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B\uf76c\uf765\uf775 improvement. The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes. 6 Analysis.", "Additionally we used 50000 sparse binary-valued source and target features based on ##CITATION##  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "Additionally we used 50000 sparse binary-valued source and target features based on ##CITATION##  ", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [" , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder. In this section, we describe the new features introduced on top of our baseline systems. Discount features Both of our systems calculate several features based on observed counts of rules in the training data.", "Alternatively by using the large- margin optimizer in ##CITATION## and moving it into the for-each loop lines 49 one can get an online algorithm such PMOMIRA", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model  . Then we use a CKY-style parser   with cube pruning to decode new sentences. We include two other techniques in our baseline.", "Alternatively by using the large- margin optimizer in ##CITATION## and moving it into the for-each loop lines 49 one can get an online algorithm such PMOMIRA", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We also notice that the-insertion rules sometimes have a good effect, as in the translation \u201cin the bloom of youth,\u201d but other times have a bad effect, as in \u201cpeople seek areas of the conspiracy.\u201d Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk. There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk. We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features. We now turn to features that make use of source-side context.", "Alternatively by using the large- margin optimizer in ##CITATION## and moving it into the for-each loop lines 49 one can get an online algorithm such PMOMIRA", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "Alternatively by using the large- margin optimizer in ##CITATION## and moving it into the for-each loop lines 49 one can get an online algorithm such PMOMIRA", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.  , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.  ; here, we are incorporating some of its features directly into the translation model.", "Online large-margin algorithms such as MIRA have also gained prominence in SMT thanks to their ability to learn models in high-dimensional feature spaces Watanabe et al ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time. The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data. This seems in line with the finding of Watanabe et al.   that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data. Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest.", "Online large-margin algorithms such as MIRA have also gained prominence in SMT thanks to their ability to learn models in high-dimensional feature spaces Watanabe et al ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero. Modified KneserNey smoothing   was applied to all language models. The language models are represented using randomized data structures similar to those of Talbot et al.  . Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level).", "Online large-margin algorithms such as MIRA have also gained prominence in SMT thanks to their ability to learn models in high-dimensional feature spaces Watanabe et al ##CITATION##", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "Online large-margin algorithms such as MIRA have also gained prominence in SMT thanks to their ability to learn models in high-dimensional feature spaces Watanabe et al ##CITATION##", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We draw three conclusions from this study. First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al.  , did observe overfitting, yet saw improvements on new data.", "task Corpus Sentences Tokens En Zh/Ar MT0535k 33k training 1M 237M 228M tune MT0655k 49k baselines: hypergraph-based MERT   k-best variants of MIRA   PRO   and RAMPION  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality.", "task Corpus Sentences Tokens En Zh/Ar MT0535k 33k training 1M 237M 228M tune MT0655k 49k baselines: hypergraph-based MERT   k-best variants of MIRA   PRO   and RAMPION  ", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["\u2217 or \u2217\u2217 = significantly better than MERT baseline ( p < 0.05 or 0.01, respectively). the syntax-based system, we ran a reimplementation of the Collins parser   on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.", "task Corpus Sentences Tokens En Zh/Ar MT0535k 33k training 1M 237M 228M tune MT0655k 49k baselines: hypergraph-based MERT   k-best variants of MIRA   PRO   and RAMPION  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "task Corpus Sentences Tokens En Zh/Ar MT0535k 33k training 1M 237M 228M tune MT0655k 49k baselines: hypergraph-based MERT   k-best variants of MIRA   PRO   and RAMPION  ", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["This minimization is performed by a variant of sequential minimal optimization  . Following Chiang et al.  , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.", "The bound constraint B was set to 14 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to   namely in the context of previous 1-best translations of the tuning set", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [", fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.", "The bound constraint B was set to 14 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to   namely in the context of previous 1-best translations of the tuning set", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features.", "The bound constraint B was set to 14 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to   namely in the context of previous 1-best translations of the tuning set", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Considering that 1 2 2 \\lw \u2212 w\\l + C i=1 max (fi j \u2212 \u2206hi j \u00b7 wt) (4) 1\u2264 j\u2264n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01. This minimization is performed by a variant of sequential minimal optimization  . Following Chiang et al.  , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.", "The bound constraint B was set to 14 The approximate sentence-level BLEU cost \u2206i is computed in a manner similar to   namely in the context of previous 1-best translations of the tuning set", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We also notice that the-insertion rules sometimes have a good effect, as in the translation \u201cin the bloom of youth,\u201d but other times have a bad effect, as in \u201cpeople seek areas of the conspiracy.\u201d Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk. There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk. We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features. We now turn to features that make use of source-side context.", "For experiments with a larger feature set we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature ##CITATION## Watan 4 We also conducted an investigation into the setting of the B parameter", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set: PP \u2192 VBN NP-C PP-BAR \u2192 NP-C IN VP \u2192 NP-C PP CONJP \u2192 RB IN Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category. For example, there may be an tendency to generate too many determiners or past-tense verbs. We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols. For a rule like NPB(NNP(us) NNP(president) x0:NNP) \u2194 meiguo zongtong x0 the feature node-count-NPB gets value 1, node- count-NNP gets value 2, and all others get 0. Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.", "For experiments with a larger feature set we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature ##CITATION## Watan 4 We also conducted an investigation into the setting of the B parameter", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["From this data, we use the the GHKM minimal-rule extraction algorithm of   to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) \u2194 x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al.   in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words.", "For experiments with a larger feature set we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature ##CITATION## Watan 4 We also conducted an investigation into the setting of the B parameter", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research   tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "For experiments with a larger feature set we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature ##CITATION## Watan 4 We also conducted an investigation into the setting of the B parameter", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["However, when we apply MIRA with the features already listed, these translation errors all disappear, as demon 38.5 38 37.5 37 36.5 36 35.5 35 Tune Test 0 5 10 15 20 25 Epoch strated by examples 4\u20135 in Figure 1. Why does this happen? It turns out that in translation hypotheses that move \u201cX said\u201d or \u201cX asked\u201d away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear. Therefore, the new features work to discourage these hypotheses. Example 6 shows additionally that commas next to speaking verbs are now correctly deleted.", "These challenges have prompted some researchers to move away from MERT in favor of linearly decomposable approximations of the evaluation metric ##CITATION## Hopkins and May Cherry and Foster which correspond to easier optimization problems and which naturally incorporate regularization", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Following Chiang et al.  , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple\u2014for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update\u2014it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder. In this section, we describe the new features introduced on top of our baseline systems.", "These challenges have prompted some researchers to move away from MERT in favor of linearly decomposable approximations of the evaluation metric ##CITATION## Hopkins and May Cherry and Foster which correspond to easier optimization problems and which naturally incorporate regularization", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Inserting the outside Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories. these fixed phrases is a risk that the generative model is too inclined to take. We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data. Table 4 shows weights for rule-overlap features. MIRA punishes the case where rules overlap with an IN (preposition) node.", "These challenges have prompted some researchers to move away from MERT in favor of linearly decomposable approximations of the evaluation metric ##CITATION## Hopkins and May Cherry and Foster which correspond to easier optimization problems and which naturally incorporate regularization", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "These challenges have prompted some researchers to move away from MERT in favor of linearly decomposable approximations of the evaluation metric ##CITATION## Hopkins and May Cherry and Foster which correspond to easier optimization problems and which naturally incorporate regularization", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality. We draw three conclusions from this study. First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al.", "In particular recent work   has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  .", "In particular recent work   has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model  . Then we use a CKY-style parser   with cube pruning to decode new sentences.", "In particular recent work   has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "In particular recent work   has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["  that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data. Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest. Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights. Early stopping would have given +0.2 B\uf76c\uf765\uf775 over the results reported in Table 1.1 7 Conclusion. We have described a variety of features for statistical machine translation and applied them to syntax- based and hierarchical systems.", "This observation con\ufb01rms previous \ufb01ndings ##CITATION## regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations. \u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features.", "This observation con\ufb01rms previous \ufb01ndings ##CITATION## regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "This observation con\ufb01rms previous \ufb01ndings ##CITATION## regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "This observation con\ufb01rms previous \ufb01ndings ##CITATION## regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper. 1 MERT: the united states pending israeli clarification on golan settlement plan. MIRA: the united states is waiting for israeli clarification on golan settlement plan 2 MERT: . . . the average life expectancy of only 18 months , canada \u2019s minority goverment will . . .. MIRA: . . .", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it. On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S. Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished. The combined effect of all weights is subtle. To interpret them further, it helps to look at gross Bonus \u22120.73 SBAR-C \u22120.54 VBZ \u22120.54 IN \u22120.52 NN \u22120.51 PP-C \u22120.47 right double quote \u22120.39 ADJP \u22120.34 POS \u22120.31 ADVP \u22120.30 RP \u22120.29 PRT \u22120.27 SG-C \u22120.22 S-C \u22120.21 NNPS \u22120.21 VP-BAR \u22120.20 PRP \u22120.20 NPB-BAR . Penalty +1.30 comma +0.80 DT +0.58 PP +0.44 TO +0.33 NNP +0.30 NNS +0.30 NML +0.22 CD +0.18 PRN +0.16 SYM +0.15 ADJP-BAR +0.15 NP +0.15 MD +0.15 HYPH +0.14 PRN-BAR +0.14 NP-C +0.11 ADJP-C . changes in the system\u2019s behavior. For example, a major error in the baseline system is to move \u201cX said\u201d or \u201cX asked\u201d from the beginning of the Chinese input to the middle or end of the English trans Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings  ", 0, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["Considering that 1 2 2 \\lw \u2212 w\\l + C i=1 max (fi j \u2212 \u2206hi j \u00b7 wt) (4) 1\u2264 j\u2264n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01. This minimization is performed by a variant of sequential minimal optimization  . Following Chiang et al.  , we calculate the sentence B\uf76c\uf765\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.", "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings  ", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Inserting the outside Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories. these fixed phrases is a risk that the generative model is too inclined to take. We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data. Table 4 shows weights for rule-overlap features. MIRA punishes the case where rules overlap with an IN (preposition) node.", "We would also like to look into alternative tuning techniques especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features. Negative weights indicate bonuses; positive weights indicate penalties.", "We would also like to look into alternative tuning techniques especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings  ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "We would also like to look into alternative tuning techniques especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings  ", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "We would also like to look into alternative tuning techniques especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings  ", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["These features are somewhat similar to features used by Watanabe et al.  , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.  ; here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm   to yield better-quality alignments.", "Chiang et wX \u2192 \u03b1 \u03b3 \u223c  = \u03bbi\u03c6i 2 i al  define new translational features using neighbouring word contexts of the source phrase which are directly integrated into the translation model of Hiero system ", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "Chiang et wX \u2192 \u03b1 \u03b3 \u223c  = \u03bbi\u03c6i 2 i al  define new translational features using neighbouring word contexts of the source phrase which are directly integrated into the translation model of Hiero system ", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f\u22121) with f\u22121 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al.  , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.  ; here, we are incorporating some of its features directly into the translation model. For our experiments, we used a 260 million word Chinese/English bitext.", "Chiang et wX \u2192 \u03b1 \u03b3 \u223c  = \u03bbi\u03c6i 2 i al  define new translational features using neighbouring word contexts of the source phrase which are directly integrated into the translation model of Hiero system ", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero   is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT.", "Chiang et wX \u2192 \u03b1 \u03b3 \u223c  = \u03bbi\u03c6i 2 i al  define new translational features using neighbouring word contexts of the source phrase which are directly integrated into the translation model of Hiero system ", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["\u2022 For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) \u00b7 w B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2212B\uf76c\uf765\uf775(e) + h(e) \u00b7 w \u2022 For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees. e\u2217 = arg max (B\uf76c\uf765\uf775(e) + h(e) \u00b7 w) (2) e Let \u2206hi j = h(e\u2217) \u2212 h(ei j).", "For example ##CITATION##  designed many target-side syntax features to improve the string-to-tree translation", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], [" . Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "For example ##CITATION##  designed many target-side syntax features to improve the string-to-tree translation", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the B\uf76c\uf765\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3\u20135 \u22120.73 3 +0.54 6\u201310 \u22120.64 4 +0.29 5+ \u22120.02 Table 2: Weights learned for discount features.", "For example ##CITATION##  designed many target-side syntax features to improve the string-to-tree translation", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], [" . 3.3 MIRA training. We incorporate all our new features into a linear model   and train them using MIRA  , following previous work  . Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: \u2022 Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations.", "For example ##CITATION##  designed many target-side syntax features to improve the string-to-tree translation", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al.   introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules.", "Moreover to establish a strong baseline we also include the discount feature used by ##CITATION##  in our baseline string-to-tree model", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model  . Then we use a CKY-style parser   with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectation- maximization  , and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al.  .", "Moreover to establish a strong baseline we also include the discount feature used by ##CITATION##  in our baseline string-to-tree model", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT  , we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer  . It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of   to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) \u2194 x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English).", "Moreover to establish a strong baseline we also include the discount feature used by ##CITATION##  in our baseline string-to-tree model", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero   is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT.", "Moreover to establish a strong baseline we also include the discount feature used by ##CITATION##  in our baseline string-to-tree model", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  . Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. The work of Och et al   is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking  .", "When the number of features is too large even popular reranking algorithms such as SVM Shen et al and MIRA Watanabe et al ##CITATION## may fail", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "When the number of features is too large even popular reranking algorithms such as SVM Shen et al and MIRA Watanabe et al ##CITATION## may fail", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al.  , did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.", "When the number of features is too large even popular reranking algorithms such as SVM Shen et al and MIRA Watanabe et al ##CITATION## may fail", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research   tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "When the number of features is too large even popular reranking algorithms such as SVM Shen et al and MIRA Watanabe et al ##CITATION## may fail", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking  . Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.\u2019s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009.", "Recent work by   describes new features for hierarchical phrase-based MT while   describes features for parsing", 0, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively.", "Recent work by   describes new features for hierarchical phrase-based MT while   describes features for parsing", 0, "11,001 New Features for Statistical Machine Translation", "PAPER"], ["Why does this happen? It turns out that in translation hypotheses that move \u201cX said\u201d or \u201cX asked\u201d away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear. Therefore, the new features work to discourage these hypotheses. Example 6 shows additionally that commas next to speaking verbs are now correctly deleted. Examples 7\u20138 in Figure 1 show other kinds of unanticipated improvements.", "Recent work by   describes new features for hierarchical phrase-based MT while   describes features for parsing", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning  . 3.2 Syntax-based system.", "Recent work by   describes new features for hierarchical phrase-based MT while   describes features for parsing", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning  . 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees.", "Recent work by   describes new features for hierarchical phrase-based MT while   describes features for parsing", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights. Early stopping would have given +0.2 B\uf76c\uf765\uf775 over the results reported in Table 1.1 7 Conclusion. We have described a variety of features for statistical machine translation and applied them to syntax- based and hierarchical systems. We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality. We draw three conclusions from this study.", "##CITATION##  added thousands of linguistically-motivated features to hierarchical and syntax systems however the source syntax features are derived from the research above ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], [" . Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.", "##CITATION##  added thousands of linguistically-motivated features to hierarchical and syntax systems however the source syntax features are derived from the research above ", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\uf76c\uf765\uf775 and+1.1 B\uf76c\uf765\uf775, respectively. We analyze the impact of the new features and the performance of the learning algorithm.", "##CITATION##  added thousands of linguistically-motivated features to hierarchical and syntax systems however the source syntax features are derived from the research above ", 0, "11,001 New Features for Statistical Machine Translation", "ABSTRACT"], ["\u2022 Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT system\u2014already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track\u2014by +1.1 B\uf76c\uf765\uf775. We also add more than 10,000 features to Hiero   and obtain a +1.5 B\uf76c\uf765\uf775 improvement. \u2217This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.", "##CITATION##  added thousands of linguistically-motivated features to hierarchical and syntax systems however the source syntax features are derived from the research above ", 1, "11,001 New Features for Statistical Machine Translation", "Introduction"], ["P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero   is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls i i] Terminal Symbol X \u2192 \u03b1Ls \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X j Rjk ] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i Rjk ] [X \u2192 \u03b1 \u2022 Ls i Rij ] [Fij = Ls] the log function hm = log hm [X \u2192 \u03b1Ls\u2022 i Rij ] log pt|s = \u03bbm hmt s 3 m Goal [X \u2192 \u03b1Ls\u2022 0 |V | \u2212 1] An advanta ge of our model over Marto n and Resnik ##CITATION## is the number of feature function s remains the sameThis model allows translation rules to take ad vantage of both syntactic label and word context", 0, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9\u2217 36. 38. 4 37.6\u2217\u2217 Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8\u2217 38 .7 39.9\u2217 38. 39. 6 40.6\u2217\u2217 Table 1: Adding new features with MIRA significantly improves translation accuracy.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls i i] Terminal Symbol X \u2192 \u03b1Ls \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X j Rjk ] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i Rjk ] [X \u2192 \u03b1 \u2022 Ls i Rij ] [Fij = Ls] the log function hm = log hm [X \u2192 \u03b1Ls\u2022 i Rij ] log pt|s = \u03bbm hmt s 3 m Goal [X \u2192 \u03b1Ls\u2022 0 |V | \u2212 1] An advanta ge of our model over Marto n and Resnik ##CITATION## is the number of feature function s remains the sameThis model allows translation rules to take ad vantage of both syntactic label and word context", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system\u2019s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al.   introduce a structural distortion model, which we include in our experiment.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls i i] Terminal Symbol X \u2192 \u03b1Ls \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X j Rjk ] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i Rjk ] [X \u2192 \u03b1 \u2022 Ls i Rij ] [Fij = Ls] the log function hm = log hm [X \u2192 \u03b1Ls\u2022 i Rij ] log pt|s = \u03bbm hmt s 3 m Goal [X \u2192 \u03b1Ls\u2022 0 |V | \u2212 1] An advanta ge of our model over Marto n and Resnik ##CITATION## is the number of feature function s remains the sameThis model allows translation rules to take ad vantage of both syntactic label and word context", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Others have introduced alternative discriminative training methods  , in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218\u2013226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research   tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules.", "A non-exhaustive sample is given below: [X \u2192 \u2022\u03b1Ls i i] Terminal Symbol X \u2192 \u03b1Ls \u2208 G X \u2192 ADJ A1 Akt # X1 Act N P \u2192 N E1 X2 # X1 X2 T OP \u2192 N E1 letzter X2 # X1 Last X2 [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i j + 1] Non-Terminal Symbol [X \u2192 \u03b1 \u2022 Fjk \u03b2Ls i j] [X j Rjk ] [X \u2192 \u03b1Fjk \u2022 \u03b2Ls i Rjk ] [X \u2192 \u03b1 \u2022 Ls i Rij ] [Fij = Ls] the log function hm = log hm [X \u2192 \u03b1Ls\u2022 i Rij ] log pt|s = \u03bbm hmt s 3 m Goal [X \u2192 \u03b1Ls\u2022 0 |V | \u2212 1] An advanta ge of our model over Marto n and Resnik ##CITATION## is the number of feature function s remains the sameThis model allows translation rules to take ad vantage of both syntactic label and word context", 1, "11,001 New Features for Statistical Machine Translation", "Related Work."], ["Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik  . In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separately- tunable features for each syntactic category.", "The model also differs from Marton and Resnik ##CITATION## by adding informative labels to rule non-terminals and requiring them to match the source span label", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["ations of all learners to decode the test set. The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1.5 B\uf76c\uf765\uf775 improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B\uf76c\uf765\uf775 improvement. The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes. 6 Analysis.", "The model also differs from Marton and Resnik ##CITATION## by adding informative labels to rule non-terminals and requiring them to match the source span label", 0, "11,001 New Features for Statistical Machine Translation", "##other##"], ["Sample syntax- based insertion rules are: NPB(DT(the) x0:NN) \u2194 x0 S(x0:NP-C VP(VBZ(is) x1:VP-C)) \u2194 x0 x1 We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source. We also notice that the-insertion rules sometimes have a good effect, as in the translation \u201cin the bloom of youth,\u201d but other times have a bad effect, as in \u201cpeople seek areas of the conspiracy.\u201d Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk. There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk. We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features.", "The model also differs from Marton and Resnik ##CITATION## by adding informative labels to rule non-terminals and requiring them to match the source span label", 0, "11,001 New Features for Statistical Machine Translation", "Features."], ["Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X \u2192 X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  . The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning  . 3.2 Syntax-based system.", "The model also differs from Marton and Resnik ##CITATION## by adding informative labels to rule non-terminals and requiring them to match the source span label", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning  . 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees.", "The model also differs from Marton and Resnik ##CITATION## by adding informative labels to rule non-terminals and requiring them to match the source span label", 1, "11,001 New Features for Statistical Machine Translation", "Systems Used."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.", "Two examples of such corpora are the RST Tree Corpus by Marcu et al for English and the Potsdam Commentary Corpus ##CITATION## for German", 0, "The Potsdam Commentary Corpus", "PAPER"], ["Like in the co-reference annotation, G\u00a8otze\u2019s proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet. We use MMAX for this annotation as well. Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).", "Two examples of such corpora are the RST Tree Corpus by Marcu et al for English and the Potsdam Commentary Corpus ##CITATION## for German", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide.", "Two examples of such corpora are the RST Tree Corpus by Marcu et al for English and the Potsdam Commentary Corpus ##CITATION## for German", 0, "The Potsdam Commentary Corpus", "Introduction"], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Two examples of such corpora are the RST Tree Corpus by Marcu et al for English and the Potsdam Commentary Corpus ##CITATION## for German", 1, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "However when we trained two experienced students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus   and upon completion of the task asked them about their experiences a very different picture emerged", 0, "The Potsdam Commentary Corpus", "ABSTRACT"], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "However when we trained two experienced students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus   and upon completion of the task asked them about their experiences a very different picture emerged", 0, "The Potsdam Commentary Corpus", "Introduction"], ["The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD. This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.", "However when we trained two experienced students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus   and upon completion of the task asked them about their experiences a very different picture emerged", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "However when we trained two experienced students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus   and upon completion of the task asked them about their experiences a very different picture emerged", 1, "The Potsdam Commentary Corpus", "Introduction"], ["When finished, the whole material is written into an XML-structured annotation file. 2.6 Co-reference. We developed a first version of annotation guidelines for co-reference in PCC  , which served as basis for annotating the core corpus but have not been empirically evaluated for inter-annotator agreement yet. The tool we use is MMAX8, which has been specifically designed for marking co-reference. Upon identifying an anaphoric expression (currently restricted to: pronouns, prepositional adverbs, definite noun phrases), the an- notator first marks the antecedent expression (currently restricted to: various kinds of noun phrases, prepositional phrases, verb phrases, sentences) and then establishes the link between the two.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements which is not the case in our approach  ", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Like in the co-reference annotation, G\u00a8otze\u2019s proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet. We use MMAX for this annotation as well. Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements which is not the case in our approach  ", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Figure 2 shows a screenshot (which is of somewhat limited value, though, as color plays a major role in signalling the different statuses of the information). In the small window on the left, search queries can be entered, here one for an NP that has been annotated on the co-reference layer as bridging. The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) \u2022 the full text, \u2022 the annotation values for the activated annotation set (co-reference), \u2022 the actual annotation tiers, and \u2022 the portion of text currently \u2018in focus\u2019 (which also appears underlined in the full text). Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels. Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements which is not the case in our approach  ", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future. Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements which is not the case in our approach  ", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements which is not the case in our approach  ", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) \u2022 the full text, \u2022 the annotation values for the activated annotation set (co-reference), \u2022 the actual annotation tiers, and \u2022 the portion of text currently \u2018in focus\u2019 (which also appears underlined in the full text). Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels. Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase. 3.2 Stochastic rhetorical analysis. In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by   as a training corpus for statistical classification with Support Vector Machines.", "\u2022 words/phrases as preposed predicate complements \u2022 pre- and post-modified connectives \u2022 co-occurring connectives \u2022 single and multiple clauses/sentences as arguments of connectives \u2022 annotation of discontinuous connective arguments Annotators have to also make syntactic judgements which is not the case in our approach  ", 1, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English. For effectively annotating connectives/scopes, we found that existing annotation tools were not well-suited, for two reasons: \u2022 Some tools are dedicated to modes of annotation (e.g., tiers), which could only quite un-intuitively be used for connectives and scopes. \u2022 Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them. 5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java  , which provides specifically the functionality needed for our purpose.", "Discourse structures cannot always be described completely either because they are ambiguous ##CITATION## or because a discourse parser fails to analyse them completely", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["In   we went a different way and suggested URML5, an XML format for underspecifying rhetorical structure: a number of relations can be assigned instead of a single one, competing analyses can be represented with shared forests. The rhetorical structure annotations of PCC have all been converted to URML. There are still some open issues to be resolved with the format, but it represents a first step. What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes.", "Discourse structures cannot always be described completely either because they are ambiguous ##CITATION## or because a discourse parser fails to analyse them completely", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["The significant drop in number of pupils will begin in the fall of 2003. The government has to make a decision, and do it quickly. Either save money at any cost - or give priority to education. Figure 1: Translation of PCC sample commentary (STTS)2. 2.2 Syntactic structure.", "Discourse structures cannot always be described completely either because they are ambiguous ##CITATION## or because a discourse parser fails to analyse them completely", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.", "Discourse structures cannot always be described completely either because they are ambiguous ##CITATION## or because a discourse parser fails to analyse them completely", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "For the purpose of language engineering and linguistic investigation we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus Carlson et al ##CITATION##", 0, "The Potsdam Commentary Corpus", "Introduction"], ["We use MMAX for this annotation as well. Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive). They are also labelled for their topicality (yes / no), and this annotation is accompanied by a confidence value assigned by the annotator (since it is a more subjective matter).", "For the purpose of language engineering and linguistic investigation we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus Carlson et al ##CITATION##", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide.", "For the purpose of language engineering and linguistic investigation we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus Carlson et al ##CITATION##", 0, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "For the purpose of language engineering and linguistic investigation we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus Carlson et al ##CITATION##", 1, "The Potsdam Commentary Corpus", "Introduction"], ["2.5 Connectives with scopes. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes. This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation Taboada and Mann  outline this and further applications But discourse structures cannot always be described completely either due to genuine ambiguity ##CITATION## or to the limitations of a discourse parser ", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD. This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation Taboada and Mann  outline this and further applications But discourse structures cannot always be described completely either due to genuine ambiguity ##CITATION## or to the limitations of a discourse parser ", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see  . 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter  , which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation Taboada and Mann  outline this and further applications But discourse structures cannot always be described completely either due to genuine ambiguity ##CITATION## or to the limitations of a discourse parser ", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  .", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation Taboada and Mann  outline this and further applications But discourse structures cannot always be described completely either due to genuine ambiguity ##CITATION## or to the limitations of a discourse parser ", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["3.5 Improved models of discourse. structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure. One key issue here is to seek a discourse-based model of information structure. Since Dane\u02c7s\u2019 proposals of \u2018thematic development patterns\u2019, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts.  , for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST).", "Following annotation schemes like the one of ##CITATION##   we model discourse structures by binary trees", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  .", "Following annotation schemes like the one of ##CITATION##   we model discourse structures by binary trees", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  . Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised. Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.", "Following annotation schemes like the one of ##CITATION##   we model discourse structures by binary trees", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Thus we are interested not in extraction, but actual generation from representations that may be developed to different degrees of granularity. In order to evaluate and advance this approach, it helps to feed into the knowledge base data that is already enriched with some of the desired information \u2014 as in PCC. That is, we can use the discourse parser on PCC texts, emulating for instance a \u201cco-reference oracle\u201d that adds the information from our co-reference annotations. The knowledge base then can be tested for its relation-inference capabilities on the basis of full-blown co-reference information. Conversely, we can use the full rhetorical tree from the annotations and tune the co-reference module.", "Following annotation schemes like the one of ##CITATION##   we model discourse structures by binary trees", 1, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  . Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised. Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.", "Discourse studies The Potsdam Commentary Corpus PCC ##CITATION## consists of 173 newspaper commentaries annotated for morphosyn- tax coreference discourse structure according to Rhetorical Structure Theory and information structure", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "Discourse studies The Potsdam Commentary Corpus PCC ##CITATION## consists of 173 newspaper commentaries annotated for morphosyn- tax coreference discourse structure according to Rhetorical Structure Theory and information structure", 0, "The Potsdam Commentary Corpus", "ABSTRACT"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "Discourse studies The Potsdam Commentary Corpus PCC ##CITATION## consists of 173 newspaper commentaries annotated for morphosyn- tax coreference discourse structure according to Rhetorical Structure Theory and information structure", 0, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.", "Discourse studies The Potsdam Commentary Corpus PCC ##CITATION## consists of 173 newspaper commentaries annotated for morphosyn- tax coreference discourse structure according to Rhetorical Structure Theory and information structure", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Discourse studies The Potsdam Commentary Corpus PCC ##CITATION## consists of 173 newspaper commentaries annotated for morphosyn- tax coreference discourse structure according to Rhetorical Structure Theory and information structure", 1, "The Potsdam Commentary Corpus", "Introduction"], ["This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide.", "Discourse studies The Potsdam Commentary Corpus PCC ##CITATION## consists of 173 newspaper commentaries annotated for morphosyn- tax coreference discourse structure according to Rhetorical Structure Theory and information structure", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "The original annotation guidelines were drafted inby the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries PCC ##CITATION## and the RST Discourse Treebank of Wall Street Journal articles WSJ Carlson et al", 0, "The Potsdam Commentary Corpus", "Introduction"], ["2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  .", "The original annotation guidelines were drafted inby the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries PCC ##CITATION## and the RST Discourse Treebank of Wall Street Journal articles WSJ Carlson et al", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Figure 1: Translation of PCC sample commentary (STTS)2. 2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure.", "The original annotation guidelines were drafted inby the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries PCC ##CITATION## and the RST Discourse Treebank of Wall Street Journal articles WSJ Carlson et al", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "The original annotation guidelines were drafted inby the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries PCC ##CITATION## and the RST Discourse Treebank of Wall Street Journal articles WSJ Carlson et al", 1, "The Potsdam Commentary Corpus", "Introduction"], ["5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java  , which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then \u201cclick away\u201d those words that are here not used as connectives (such as the conjunction und (\u2018and\u2019) used in lists, or many adverbials that are ambiguous between connective and discourse particle). Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like \u2018for sub- junctor, mark all words up to the next comma as the first segment\u2019), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.", "The PCC176 ##CITATION## is a sub-corpus that is available upon request for research purposes It consists of 176 relatively short commentaries 12 15 sentences with 33000 tokens in total ", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "The PCC176 ##CITATION## is a sub-corpus that is available upon request for research purposes It consists of 176 relatively short commentaries 12 15 sentences with 33000 tokens in total ", 0, "The Potsdam Commentary Corpus", "Introduction"], ["As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers Su\u00a8ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence. The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences. For illustration, an English translation of one of the commentaries is given in Figure 1. The paper is organized as follows: Section 2 explains the different layers of annotation that have been produced or are being produced. Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future.", "The PCC176 ##CITATION## is a sub-corpus that is available upon request for research purposes It consists of 176 relatively short commentaries 12 15 sentences with 33000 tokens in total ", 0, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "The PCC176 ##CITATION## is a sub-corpus that is available upon request for research purposes It consists of 176 relatively short commentaries 12 15 sentences with 33000 tokens in total ", 1, "The Potsdam Commentary Corpus", "Introduction"], ["The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily.", "The PCC176 ##CITATION## is a sub-corpus that is available upon request for research purposes It consists of 176 relatively short commentaries 12 15 sentences with 33000 tokens in total ", 1, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.", "Manfred ##CITATION## Potsdam Construction of the Potsdam Commentary Corpus PCC began inand is still ongoing", 0, "The Potsdam Commentary Corpus", "PAPER"], [" ). Again, the idea is that having a picture of syntax, co-reference, and sentence-internal information structure at one\u2019s disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported. The PCC is not the result of a funded project. Instead, the designs of the various annotation layers and the actual annotation work are results of a series of diploma theses, of students\u2019 work in course projects, and to some extent of paid assistentships. This means that the PCC cannot grow particularly quickly.", "Manfred ##CITATION## Potsdam Construction of the Potsdam Commentary Corpus PCC began inand is still ongoing", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["The government has to make a decision, and do it quickly. Either save money at any cost - or give priority to education. Figure 1: Translation of PCC sample commentary (STTS)2. 2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun.", "Manfred ##CITATION## Potsdam Construction of the Potsdam Commentary Corpus PCC began inand is still ongoing", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization).", "Manfred ##CITATION## Potsdam Construction of the Potsdam Commentary Corpus PCC began inand is still ongoing", 1, "The Potsdam Commentary Corpus", "ABSTRACT"], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Manfred ##CITATION## Potsdam Construction of the Potsdam Commentary Corpus PCC began inand is still ongoing", 1, "The Potsdam Commentary Corpus", "Introduction"], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 0, "The Potsdam Commentary Corpus", "ABSTRACT"], ["5 \u2018Underspecified Rhetorical Markup Language\u2019 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java  , which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then \u201cclick away\u201d those words that are here not used as connectives (such as the conjunction und (\u2018and\u2019) used in lists, or many adverbials that are ambiguous between connective and discourse particle). Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like \u2018for sub- junctor, mark all words up to the next comma as the first segment\u2019), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 0, "The Potsdam Commentary Corpus", "Introduction"], ["Two aspects of the corpus have been presented in previous papers   on underspecified rhetorical structure;   on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 1, "The Potsdam Commentary Corpus", "Introduction"], ["Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future. Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Section 4 draws some conclusions from the present state of the effort. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.", "Another well-known corpus is the Potsdam Commentary Corpus for German ##CITATION## Reitter and ##CITATION## This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung It contains 32962 words and 2195 sentences It is annotated with several data: morphology syntax rhetorical structure connectors correference and informative structure This corpus has several advantages: it is annotated at different levels the annotation of connectors is especially interesting; all the texts were annotated by two people with a previous RST training phase; it is free for research purposes and there is a tool for searching over the corpus although it is not available online The disadvantages are: the genre and domain of all the texts are the same the methodology of annotation was quite intuitive without a manual or specific criteria and the inter-annotator agreement is not given     ", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by   as a training corpus for statistical classification with Support Vector Machines. Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method. For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%. Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based.", "For discourse relations annotated in the RST framework there is the RST Discourse TreeBank of English text Carlson et al available through the Linguistic Data Consortium LDC as well as similarly annotated corpora in Spanish da Cunha et al Portugese Pardo et al and German ##CITATION##", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a \u2018core corpus\u2019 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD. This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.", "For discourse relations annotated in the RST framework there is the RST Discourse TreeBank of English text Carlson et al available through the Linguistic Data Consortium LDC as well as similarly annotated corpora in Spanish da Cunha et al Portugese Pardo et al and German ##CITATION##", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a \u2018core corpus\u2019, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see  . 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter  , which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.", "For discourse relations annotated in the RST framework there is the RST Discourse TreeBank of English text Carlson et al available through the Linguistic Data Consortium LDC as well as similarly annotated corpora in Spanish da Cunha et al Portugese Pardo et al and German ##CITATION##", 0, "The Potsdam Commentary Corpus", "Conclusions."], ["We follow the guidelines developed in the TIGER project   for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  . Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised. Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.", "For discourse relations annotated in the RST framework there is the RST Discourse TreeBank of English text Carlson et al available through the Linguistic Data Consortium LDC as well as similarly annotated corpora in Spanish da Cunha et al Portugese Pardo et al and German ##CITATION##", 1, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Having explained the various layers of annotation in PCC, we now turn to the question what all this might be good for. This concerns on the one hand the basic question of retrieval, i.e. searching for information across the annotation layers (see 3.1). On the other hand, we are interested in the application of rhetorical analysis or \u2018discourse parsing\u2019 (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5). basically complete, yet some improvements and extensions are still under way. The web-based Annis imports data in a variety of XML formats and tagsets and displays it in a tier-orientedway (optionally, trees can be drawn more ele gantly in a separate window).", "We first extracted opinionated and objective texts from DeReKo corpus ##CITATION## Kupietz Figure 4: 10 most used verbs lemma in indirect speech", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide. (Again, the goal of also in structural features. As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers Su\u00a8ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence. The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences. For illustration, an English translation of one of the commentaries is given in Figure 1.", "We first extracted opinionated and objective texts from DeReKo corpus ##CITATION## Kupietz Figure 4: 10 most used verbs lemma in indirect speech", 0, "The Potsdam Commentary Corpus", "Introduction"], ["One key issue here is to seek a discourse-based model of information structure. Since Dane\u02c7s\u2019 proposals of \u2018thematic development patterns\u2019, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts.  , for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST).   extended these ideas toward a conception of kommunikative Gewichtung (\u2018communicative-weight assignment\u2019). A different notion of information structure, is used in work such as that of (?), who tried to characterize felicitous constituent ordering (theme choice, in particular) that leads to texts presenting information in a natural, \u201cflowing\u201d way rather than with abrupt shifts of attention.", "We first extracted opinionated and objective texts from DeReKo corpus ##CITATION## Kupietz Figure 4: 10 most used verbs lemma in indirect speech", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from Ma\u00a8rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide. (Again, the goal of also in structural features.", "We first extracted opinionated and objective texts from DeReKo corpus ##CITATION## Kupietz Figure 4: 10 most used verbs lemma in indirect speech", 1, "The Potsdam Commentary Corpus", "Introduction"], ["In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by   as a training corpus for statistical classification with Support Vector Machines. Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method. For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%. Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based.", "For discourse relations and DCs especially more and more annotated resources have become available in several languages such as English Prasad et al French Pe\u00b4ryWoodley et al Danlos et al German ##CITATION## Arabic AlSaif Chinese Zhou and and the discourse relation or sense signaled is Xue and Czech Mladova\u00b4 et al", 0, "The Potsdam Commentary Corpus", "Past, Present, Future Applications."], ["Like in the co-reference annotation, G\u00a8otze\u2019s proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet. We use MMAX for this annotation as well. Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).", "For discourse relations and DCs especially more and more annotated resources have become available in several languages such as English Prasad et al French Pe\u00b4ryWoodley et al Danlos et al German ##CITATION## Arabic AlSaif Chinese Zhou and and the discourse relation or sense signaled is Xue and Czech Mladova\u00b4 et al", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive). They are also labelled for their topicality (yes / no), and this annotation is accompanied by a confidence value assigned by the annotator (since it is a more subjective matter). Finally, the focus/background partition is annotated, together with the focus question that elicits the corresponding answer.", "For discourse relations and DCs especially more and more annotated resources have become available in several languages such as English Prasad et al French Pe\u00b4ryWoodley et al Danlos et al German ##CITATION## Arabic AlSaif Chinese Zhou and and the discourse relation or sense signaled is Xue and Czech Mladova\u00b4 et al", 0, "The Potsdam Commentary Corpus", "Layers of Annotation."], ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "For discourse relations and DCs especially more and more annotated resources have become available in several languages such as English Prasad et al French Pe\u00b4ryWoodley et al Danlos et al German ##CITATION## Arabic AlSaif Chinese Zhou and and the discourse relation or sense signaled is Xue and Czech Mladova\u00b4 et al", 1, "The Potsdam Commentary Corpus", "ABSTRACT"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "Current approaches have used clustering ##CITATION## Klapaftis and Manandhar or statistical graph models Klapaftis and Manandhar to identify sense-specific subgraphs", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  . This paper is organised as follows.", "Current approaches have used clustering ##CITATION## Klapaftis and Manandhar or statistical graph models Klapaftis and Manandhar to identify sense-specific subgraphs", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted. They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora. The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus. We prepare an evaluation of our algorithm as applied to the collocation relationships (cf. section 2), and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly.", "Current approaches have used clustering ##CITATION## Klapaftis and Manandhar or statistical graph models Klapaftis and Manandhar to identify sense-specific subgraphs", 0, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen  .", "Current approaches have used clustering ##CITATION## Klapaftis and Manandhar or statistical graph models Klapaftis and Manandhar to identify sense-specific subgraphs", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm. We used the simple graph model based on co-occurrences of nouns in lists (cf. section 2) for our experiment. We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry).", "##CITATION##  use the BNC to build a cooccurrencegraph for nouns based on a co-occurrence frequency threshold They perform Markov clustering on this graph ", 0, "Discovering Corpus-Specific Word Senses", "Experimental Results."], ["In section 2, we present the graph model from which we discover word senses. Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering  . The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.", "##CITATION##  use the BNC to build a cooccurrencegraph for nouns based on a co-occurrence frequency threshold They perform Markov clustering on this graph ", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "##CITATION##  use the BNC to build a cooccurrencegraph for nouns based on a co-occurrence frequency threshold They perform Markov clustering on this graph ", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "##CITATION##  use the BNC to build a cooccurrencegraph for nouns based on a co-occurrence frequency threshold They perform Markov clustering on this graph ", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen  .", "##CITATION##  use the BNC to build a cooccurrencegraph for nouns based on a co-occurrence frequency threshold They perform Markov clustering on this graph ", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "The algorithm in ##CITATION## represented target noun word its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times Then senses of target word were iteratively learned by clustering the local graph of similar words around target word Their algorithm required a threshold as input which controlled the number of senses  ", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty. The algorithm consists of the following steps: 1. Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6. 2. Recursively remove all nodes of degree one. Then remove the node corresponding with w from G. 3.", "The algorithm in ##CITATION## represented target noun word its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times Then senses of target word were iteratively learned by clustering the local graph of similar words around target word Their algorithm required a threshold as input which controlled the number of senses  ", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "The algorithm in ##CITATION## represented target noun word its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times Then senses of target word were iteratively learned by clustering the local graph of similar words around target word Their algorithm required a threshold as input which controlled the number of senses  ", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "The algorithm in ##CITATION## represented target noun word its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times Then senses of target word were iteratively learned by clustering the local graph of similar words around target word Their algorithm required a threshold as input which controlled the number of senses  ", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted. The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose. The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label.", "Another graph-based method is presented in##CITATION## They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word Additionally theyextract second-order co-occurrences  ", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm. We used the simple graph model based on co-occurrences of nouns in lists (cf. section 2) for our experiment. We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry).", "Another graph-based method is presented in##CITATION## They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word Additionally theyextract second-order co-occurrences  ", 0, "Discovering Corpus-Specific Word Senses", "Experimental Results."], ["The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning. This approach to disambiguation combines the benefits of both Yarowsky's   and Schtitze's   approaches. Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used. Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted. They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora.", "Another graph-based method is presented in##CITATION## They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word Additionally theyextract second-order co-occurrences  ", 0, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "Another graph-based method is presented in##CITATION## They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word Additionally theyextract second-order co-occurrences  ", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity. This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.", "The last trend explored by Vronis ##CITATION## and Rapp starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  . This paper is organised as follows. In section 2, we present the graph model from which we discover word senses.", "The last trend explored by Vronis ##CITATION## and Rapp starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "The last trend explored by Vronis ##CITATION## and Rapp starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen  .", "The last trend explored by Vronis ##CITATION## and Rapp starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another. In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise.", "This method as the ones presented in     and   relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "This method as the ones presented in     and   relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  . This paper is organised as follows. In section 2, we present the graph model from which we discover word senses.", "This method as the ones presented in     and   relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "This method as the ones presented in     and   relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "This method as the ones presented in     and   relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen  .", "As they rely on the detection of high-density areas in a network of cooccurrences   and   are the closest methods to ours", 0, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["41=0 441=P .4161. sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph of the word mouse Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse. However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense. There are, of course, many more types of polysemy (cf.", "As they rely on the detection of high-density areas in a network of cooccurrences   and   are the closest methods to ours", 0, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["In section 2, we present the graph model from which we discover word senses. Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering  . The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.", "As they rely on the detection of high-density areas in a network of cooccurrences   and   are the closest methods to ours", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning. This approach to disambiguation combines the benefits of both Yarowsky's   and Schtitze's   approaches. Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used. Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted. They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora.", "As they rely on the detection of high-density areas in a network of cooccurrences   and   are the closest methods to ours", 1, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  . This paper is organised as follows. In section 2, we present the graph model from which we discover word senses.", "In our case we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence our situation is comparable to the one of   and  ; but in the same framework we can also take into account other kinds of similarity relations such as the second order cooccurrences", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "In our case we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence our situation is comparable to the one of   and  ; but in the same framework we can also take into account other kinds of similarity relations such as the second order cooccurrences", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph of the word mouse Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse. However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense. There are, of course, many more types of polysemy (cf. e.g.", "In our case we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence our situation is comparable to the one of   and  ; but in the same framework we can also take into account other kinds of similarity relations such as the second order cooccurrences", 0, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "In our case we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence our situation is comparable to the one of   and  ; but in the same framework we can also take into account other kinds of similarity relations such as the second order cooccurrences", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.", "From a global viewpoint these two differences lead   and   to build finer senses than ours", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features. 1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words.", "From a global viewpoint these two differences lead   and   to build finer senses than ours", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity. This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.", "From a global viewpoint these two differences lead   and   to build finer senses than ours", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Finally, section 6 sketches applications of the algorithm and discusses future work. The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words.", "From a global viewpoint these two differences lead   and   to build finer senses than ours", 1, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty. The algorithm consists of the following steps: 1. Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6. 2. Recursively remove all nodes of degree one. Then remove the node corresponding with w from G. 3.", "The methodology of ##CITATION##  was adopted: for the focus word obtain its graph neighborhood all vertices that are connected via edges to the focus word vertex and edges between these", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Apply MCL to Gw with a fairly big inflation parameter r which is fixed. 4. Take the \"best\" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5. Go back to 1 with the reduced/devalued set of features F. 6. Go through the final list of clusters L and assign a name to each cluster using a broad-coverage taxonomy (see below).", "The methodology of ##CITATION##  was adopted: for the focus word obtain its graph neighborhood all vertices that are connected via edges to the focus word vertex and edges between these", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "The methodology of ##CITATION##  was adopted: for the focus word obtain its graph neighborhood all vertices that are connected via edges to the focus word vertex and edges between these", 0, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen  .", "The methodology of ##CITATION##  was adopted: for the focus word obtain its graph neighborhood all vertices that are connected via edges to the focus word vertex and edges between these", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another. In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory such as those for medicine or law ##CITATION##", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory such as those for medicine or law ##CITATION##", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  . This paper is organised as follows. In section 2, we present the graph model from which we discover word senses.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory such as those for medicine or law ##CITATION##", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["7. Output the list of class-labels which best represent the different senses of w in the corpus. The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory such as those for medicine or law ##CITATION##", 1, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\". Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work  , we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features. 1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words.", "We follow Pantel and Lin   and ##CITATION##   using the sentence as contexts and all words with a dependency path of length 3 or less with the last word and its relation as a feature", 0, "Discovering Corpus-Specific Word Senses", "Building a Graph of Similar Words."], ["The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity. Section 5 describes the experiment and presents a sample of the results. Finally, section 6 sketches applications of the algorithm and discusses future work.", "We follow Pantel and Lin   and ##CITATION##   using the sentence as contexts and all words with a dependency path of length 3 or less with the last word and its relation as a feature", 0, "Discovering Corpus-Specific Word Senses", "Introduction"], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "We follow Pantel and Lin   and ##CITATION##   using the sentence as contexts and all words with a dependency path of length 3 or less with the last word and its relation as a feature", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["Here we only mention a few direct results of our work. Our algorithm does not only recognise ambiguity, but can also be used to resolve it, because the features shared by the members of each sense cluster provide strong indication of which reading of an ambiguous word is appropriate given a certain context. This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated. The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning. This approach to disambiguation combines the benefits of both Yarowsky's   and Schtitze's   approaches.", "We follow Pantel and Lin   and ##CITATION##   using the sentence as contexts and all words with a dependency path of length 3 or less with the last word and its relation as a feature", 1, "Discovering Corpus-Specific Word Senses", "Applications and future research."], ["Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty.", "Similar to the approach as presented in ##CITATION## we construct a word graph ##CITATION## construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w without w and clustering it with MCL ", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["7. Output the list of class-labels which best represent the different senses of w in the corpus. The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.", "Similar to the approach as presented in ##CITATION## we construct a word graph ##CITATION## construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w without w and clustering it with MCL ", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise. Below, we outline an algorithm which circumvents the problem of choosing the right parameters.", "Similar to the approach as presented in ##CITATION## we construct a word graph ##CITATION## construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w without w and clustering it with MCL ", 0, "Discovering Corpus-Specific Word Senses", "Word Sense Clustering Algorithm."], ["The same happens with wing \"part of a building\" and wing \"political group\" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen  .", "Similar to the approach as presented in ##CITATION## we construct a word graph ##CITATION## construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w without w and clustering it with MCL ", 1, "Discovering Corpus-Specific Word Senses", "Markov Clustering."], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses.", "Similar to the approach as presented in ##CITATION## we construct a word graph ##CITATION## construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w without w and clustering it with MCL ", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.", "Similar to the approach as presented in ##CITATION## we construct a word graph ##CITATION## construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w without w and clustering it with MCL ", 1, "Discovering Corpus-Specific Word Senses", "ABSTRACT"], ["Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).", "Under this constraint many researchers had contributed algorithms and associated pruning strategies such as Berger et al  Och et al  Wang and Waibel  ##CITATION##  GarciaVarea and Casacuberta  and Germann et al  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated. 3) A tight coupling with the speech recognizer output. This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology and as part of the Eutrans project (ESPRIT project number 30268) by the European Community. Table 6: Example Translations for the Verbmobil task. Input: Ja , wunderbar . K\u007fonnen wir machen . MonS: Yes, wonderful.", "Under this constraint many researchers had contributed algorithms and associated pruning strategies such as Berger et al  Och et al  Wang and Waibel  ##CITATION##  GarciaVarea and Casacuberta  and Germann et al  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Acknowledgements"], ["For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0. The negative logarithm of t0 is reported. The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.", "Under this constraint many researchers had contributed algorithms and associated pruning strategies such as Berger et al  Och et al  Wang and Waibel  ##CITATION##  GarciaVarea and Casacuberta  and Germann et al  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.", "Under this constraint many researchers had contributed algorithms and associated pruning strategies such as Berger et al  Och et al  Wang and Waibel  ##CITATION##  GarciaVarea and Casacuberta  and Germann et al  ", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.", "There exists stack decoding algorithm   A* search algorithm   and dynamic-programming algorithms   and all translate a given input string word-by-word and render the translation in left-to-right with pruning technologies assuming almost linearly aligned translation source and target texts", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.", "There exists stack decoding algorithm   A* search algorithm   and dynamic-programming algorithms   and all translate a given input string word-by-word and render the translation in left-to-right with pruning technologies assuming almost linearly aligned translation source and target texts", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["Here, we process only full-form words within the translation procedure. the number of permutations carried out for the word reordering is given. During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet. Only one of the first n positions which are not already aligned in a partial hypothesis may be chosen, where n is set to 4. The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.", "There exists stack decoding algorithm   A* search algorithm   and dynamic-programming algorithms   and all translate a given input string word-by-word and render the translation in left-to-right with pruning technologies assuming almost linearly aligned translation source and target texts", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "There exists stack decoding algorithm   A* search algorithm   and dynamic-programming algorithms   and all translate a given input string word-by-word and render the translation in left-to-right with pruning technologies assuming almost linearly aligned translation source and target texts", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup. A typical situation is shown in Figure 1. When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.", "There exists stack decoding algorithm   A* search algorithm   and dynamic-programming algorithms   and all translate a given input string word-by-word and render the translation in left-to-right with pruning technologies assuming almost linearly aligned translation source and target texts", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily as presented in ##CITATION##   and Och et al", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The German finite verbs 'bin' (second example) and 'k\u007fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable. In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily as presented in ##CITATION##   and Och et al", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task. In this section, we brie y review our translation approach. In Eq.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily as presented in ##CITATION##   and Och et al", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city. Subsets C of increasing cardinality c are processed. The algorithm works due to the fact that not all permutations of cities have to be considered explicitly. For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored. This algorithm can be applied to statistical machine translation.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily as presented in ##CITATION##   and Och et al", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Subsets C of increasing cardinality c are processed. The algorithm works due to the fact that not all permutations of cities have to be considered explicitly. For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored. This algorithm can be applied to statistical machine translation. Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.", "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily as presented in ##CITATION##   and Och et al", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["= p(fj je) max \u00c6;e00 j02Cnfjg np(jjj0; J) p(\u00c6) p\u00c6(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j). The resulting algorithm is depicted in Table 1. The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary. 3.1 Word ReOrdering with Verbgroup. Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.", "The computational complexity for the left-to-right and right-to-left is the same O|E|3m22m  as reported by ##CITATION##   in which |E| is the size of the vocabulary for output sentences 3", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "The computational complexity for the left-to-right and right-to-left is the same O|E|3m22m  as reported by ##CITATION##   in which |E| is the size of the vocabulary for output sentences 3", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions. To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered. Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.", "The computational complexity for the left-to-right and right-to-left is the same O|E|3m22m  as reported by ##CITATION##   in which |E| is the size of the vocabulary for output sentences 3", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup. A typical situation is shown in Figure 1. When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.", "The computational complexity for the left-to-right and right-to-left is the same O|E|3m22m  as reported by ##CITATION##   in which |E| is the size of the vocabulary for output sentences 3", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg. This approach leads to a search procedure with complexity O(E3 J4). The proof is given in  . 4.1 The Task and the Corpus.", "The computational complexity for the left-to-right and right-to-left is the same O|E|3m22m  as reported by ##CITATION##   in which |E| is the size of the vocabulary for output sentences 3", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O l3m4 \u2248 O m7 algo rithm for French-English translation Tillman and Ney", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities. The resulting algorithm has a complexity of O(n!). However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp. The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O l3m4 \u2248 O m7 algo rithm for French-English translation Tillman and Ney", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task. The goal of machine translation is the translation of a text given in some source language into a target language.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O l3m4 \u2248 O m7 algo rithm for French-English translation Tillman and Ney", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["In the following, we assume that this word joining has been carried out. Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup. In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem  . The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.", "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O l3m4 \u2248 O m7 algo rithm for French-English translation Tillman and Ney", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The details are given in  . For each extension a new position is added to the coverage set. Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $). Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence. The search starts in the hypothesis (I; f;g; 0).", "\u2022 Conditional Probability Given the model parameters and a sentence pair f  e compute P f |e Tillman Och et al Germann et al Udupa et al ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001   and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.", "\u2022 Conditional Probability Given the model parameters and a sentence pair f  e compute P f |e Tillman Och et al Germann et al Udupa et al ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["For \u00c6 = 1, a new target language word is generated using the trigram language model p(eje0; e00). For \u00c6 = 0, no new target word is generated, while an additional source sentence position is covered. A modified language model probability p\u00c6(eje0; e00) is defined as follows: p\u00c6(eje0; e00) =  1:0 if \u00c6 = 0 p(eje0; e00) if \u00c6 = 1 : We associate a distribution p(\u00c6) with the two cases \u00c6 = 0 and \u00c6 = 1 and set p(\u00c6 = 1) = 0:7. The above auxiliary quantity satisfies the following recursive DP equation: Qe0 (e; C; j) = Initial Skip Verb Final 1. In.", "\u2022 Conditional Probability Given the model parameters and a sentence pair f  e compute P f |e Tillman Och et al Germann et al Udupa et al ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task. The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words.", "\u2022 Conditional Probability Given the model parameters and a sentence pair f  e compute P f |e Tillman Och et al Germann et al Udupa et al ", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "To summarize these experimental tests we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach ; \u2022 alignment template approach ; \u2022 cascaded transducer approach :unlike the other two-approaches this approach re quires a semiautomatic training procedure in which the structure of the finite state transducers is designed manually", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["The German finite verbs 'bin' (second example) and 'k\u007fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable. In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task.", "To summarize these experimental tests we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach ; \u2022 alignment template approach ; \u2022 cascaded transducer approach :unlike the other two-approaches this approach re quires a semiautomatic training procedure in which the structure of the finite state transducers is designed manually", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "To summarize these experimental tests we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach ; \u2022 alignment template approach ; \u2022 cascaded transducer approach :unlike the other two-approaches this approach re quires a semiautomatic training procedure in which the structure of the finite state transducers is designed manually", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches.", "To summarize these experimental tests we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach ; \u2022 alignment template approach ; \u2022 cascaded transducer approach :unlike the other two-approaches this approach re quires a semiautomatic training procedure in which the structure of the finite state transducers is designed manually", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The German finite verbs 'bin' (second example) and 'k\u007fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable. In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task.", "This article will present a DP-based beam search decoder for the IBM4 translation model A preliminary version of the work presented here was published in ##CITATION##  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "This article will present a DP-based beam search decoder for the IBM4 translation model A preliminary version of the work presented here was published in ##CITATION##  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach  . A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in  , a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.", "This article will present a DP-based beam search decoder for the IBM4 translation model A preliminary version of the work presented here was published in ##CITATION##  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "This article will present a DP-based beam search decoder for the IBM4 translation model A preliminary version of the work presented here was published in ##CITATION##  ", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in  . An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) max bI 1 I Yi=1 [p(bijbi\udbc0\udc001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\udbc0\udc001 i\udbc0\udc002) p(bijbi\udbc0\udc001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\udbc0\udc001 i\udbc0\udc002) is the trigram language model probability.", "Many existing systems for statistical machine translation 1 1 Garc\u00b4\u0131a-Varea and Casacuberta Germann et al Nie\u00dfen et al Och ##CITATION## implement models presented by Brown Della Pietra Della Pietra and Mercer : The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["The advantage is that we can recombine search hypotheses by dynamic programming. The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation. input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max \u00c6;e00 j02Cnfjg fp(jjj0; J) p(\u00c6) p\u00c6(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once. Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed. For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).", "Many existing systems for statistical machine translation 1 1 Garc\u00b4\u0131a-Varea and Casacuberta Germann et al Nie\u00dfen et al Och ##CITATION## implement models presented by Brown Della Pietra Della Pietra and Mercer : The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed. For a trigram language model, the partial hypotheses are of the form (e0; e; C; j). e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited. Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities. The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei\udbc0\udc001 = e0.", "Many existing systems for statistical machine translation 1 1 Garc\u00b4\u0131a-Varea and Casacuberta Germann et al Nie\u00dfen et al Och ##CITATION## implement models presented by Brown Della Pietra Della Pietra and Mercer : The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In Eq. (1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001   and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.", "Many existing systems for statistical machine translation 1 1 Garc\u00b4\u0131a-Varea and Casacuberta Germann et al Nie\u00dfen et al Och ##CITATION## implement models presented by Brown Della Pietra Della Pietra and Mercer : The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) ! (S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account. To be short, we omit the target words e; e0 in the formulation of the search hypotheses. There are 13 types of extensions needed to describe the verbgroup reordering. The details are given in  .", "We call this selection of highly probable words observation pruning ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["For the inverted alignment probability p(bijbi\udbc0\udc001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining. The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment. We use a solution to this problem similar to the one presented in  , where target words are joined during training. The word joining is done on the basis of a likelihood criterion. An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.", "We call this selection of highly probable words observation pruning ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["On average, 6 reference translations per automatic translation are available. The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken. This measure has the advantage of being completely automatic. SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person. For the error counts, a range from 0:0 to 1:0 is used.", "We call this selection of highly probable words observation pruning ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.", "We call this selection of highly probable words observation pruning ##CITATION##", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "Och et al report word error rates of 6868% for optimal search based on a variant of the A* algorithm and 6965% for the most restricted version of a decoder that combines dynamic programming with a beam search ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.", "Och et al report word error rates of 6868% for optimal search based on a variant of the A* algorithm and 6965% for the most restricted version of a decoder that combines dynamic programming with a beam search ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["Here, the pruning threshold t0 = 10:0 is used. Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER). The monotone search performs worst in terms of both error rates mWER and SSER. The computing time is low, since no reordering is carried out. The quasi-monotone search performs best in terms of both error rates mWER and SSER.", "Och et al report word error rates of 6868% for optimal search based on a variant of the A* algorithm and 6965% for the most restricted version of a decoder that combines dynamic programming with a beam search ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction. Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction. A dynamic programming recursion similar to the one in Eq. 2 is evaluated. In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.", "Och et al report word error rates of 6868% for optimal search based on a variant of the A* algorithm and 6965% for the most restricted version of a decoder that combines dynamic programming with a beam search ##CITATION##", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "Och et al report word error rates of 6868% for optimal search based on a variant of the A* algorithm and 6965% for the most restricted version of a decoder that combines dynamic programming with a beam search ##CITATION##", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.", "We use the top-10 list of hypothesis provided by the translation system described in ##CITATION## for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions. To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered. Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.", "We use the top-10 list of hypothesis provided by the translation system described in ##CITATION## for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task. Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word. 2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated. 3) A tight coupling with the speech recognizer output.", "We use the top-10 list of hypothesis provided by the translation system described in ##CITATION## for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches.", "We use the top-10 list of hypothesis provided by the translation system described in ##CITATION## for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "The decoding algorithm employed for this chunk + weight \u00d7 j f reqEA j  J j  based statistical translation is based on the beam search algorithm for word alignment statistical in which PtmJ|E and Plm E are translationmodel and language model probability respec translation presented in ##CITATION## tively1  f reqEA j  J j  is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.", "The decoding algorithm employed for this chunk + weight \u00d7 j f reqEA j  J j  based statistical translation is based on the beam search algorithm for word alignment statistical in which PtmJ|E and Plm E are translationmodel and language model probability respec translation presented in ##CITATION## tively1  f reqEA j  J j  is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["The German finite verbs 'bin' (second example) and 'k\u007fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable. In this paper, we have presented a new, e\u00c6cient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task.", "The decoding algorithm employed for this chunk + weight \u00d7 j f reqEA j  J j  based statistical translation is based on the beam search algorithm for word alignment statistical in which PtmJ|E and Plm E are translationmodel and language model probability respec translation presented in ##CITATION## tively1  f reqEA j  J j  is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Conclusion."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "The decoding algorithm employed for this chunk + weight \u00d7 j f reqEA j  J j  based statistical translation is based on the beam search algorithm for word alignment statistical in which PtmJ|E and Plm E are translationmodel and language model probability respec translation presented in ##CITATION## tively1  f reqEA j  J j  is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["We use a solution to this problem similar to the one presented in  , where target words are joined during training. The word joining is done on the basis of a likelihood criterion. An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account. E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment. In the following, we assume that this word joining has been carried out.", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words. The model is often further restricted so that each source word is assigned to exactly one target word  .", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["The word joining is done on the basis of a likelihood criterion. An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account. E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment. In the following, we assume that this word joining has been carried out. Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup.", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings  ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities. The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei\udbc0\udc001 = e0. The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words. For \u00c6 = 1, a new target language word is generated using the trigram language model p(eje0; e00). For \u00c6 = 0, no new target word is generated, while an additional source sentence position is covered.", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings  ", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities. The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei\udbc0\udc001 = e0. The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words. For \u00c6 = 1, a new target language word is generated using the trigram language model p(eje0; e00). For \u00c6 = 0, no new target word is generated, while an additional source sentence position is covered.", "Some of these studies have concentrated on finite-state or extended finite-state machinery such as Vilar and others others have chosen models closer to context-free grammars and context-free transduction such as Alshawi et al Watanabe et al Yamamoto and Matsumoto and yet other studies cannot be comfortably assigned to either of these two frameworks such as Brown and others and ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions. To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered. Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.", "Some of these studies have concentrated on finite-state or extended finite-state machinery such as Vilar and others others have chosen models closer to context-free grammars and context-free transduction such as Alshawi et al Watanabe et al Yamamoto and Matsumoto and yet other studies cannot be comfortably assigned to either of these two frameworks such as Brown and others and ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Our approach uses word-to-word dependencies between source and target words. The model is often further restricted so that each source word is assigned to exactly one target word  . These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition. The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns. A simple extension will be used to handle this problem.", "Some of these studies have concentrated on finite-state or extended finite-state machinery such as Vilar and others others have chosen models closer to context-free grammars and context-free transduction such as Alshawi et al Watanabe et al Yamamoto and Matsumoto and yet other studies cannot be comfortably assigned to either of these two frameworks such as Brown and others and ##CITATION##", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["On average, 6 reference translations per automatic translation are available. The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken. This measure has the advantage of being completely automatic. SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person. For the error counts, a range from 0:0 to 1:0 is used.", "Some of these studies have concentrated on finite-state or extended finite-state machinery such as Vilar and others others have chosen models closer to context-free grammars and context-free transduction such as Alshawi et al Watanabe et al Yamamoto and Matsumoto and yet other studies cannot be comfortably assigned to either of these two frameworks such as Brown and others and ##CITATION##", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "no  of se nt en ce s 2 0 0 no  of ru nn in g w or ds 2 05 5 no  of w or d for ms no  of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in ##CITATION## and compared to other approaches in Ney et al", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).", "no  of se nt en ce s 2 0 0 no  of ru nn in g w or ds 2 05 5 no  of w or d for ms no  of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in ##CITATION## and compared to other approaches in Ney et al", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment. In the following, we assume that this word joining has been carried out. Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup. In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem  . The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1.", "no  of se nt en ce s 2 0 0 no  of ru nn in g w or ds 2 05 5 no  of w or d for ms no  of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in ##CITATION## and compared to other approaches in Ney et al", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words. We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches.", "no  of se nt en ce s 2 0 0 no  of ru nn in g w or ds 2 05 5 no  of w or d for ms no  of un kn o w n w or d for ms 3 8 5 2 5 We used a translation system called \u201csingle- word based approach\u201d described in ##CITATION## and compared to other approaches in Ney et al", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach  . A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in  , a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm BS: Tillmann ##CITATION## In this algorithm the search space is explored in a breadth-first manner", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction. A dynamic programming recursion similar to the one in Eq. 2 is evaluated. In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg. This approach leads to a search procedure with complexity O(E3 J4).", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm BS: Tillmann ##CITATION## In this algorithm the search space is explored in a breadth-first manner", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this section, we brie y review our translation approach. In Eq. (1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\udbc0\udc001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\udbc0\udc001   and lexicon probabilities p(fj jeaj ).", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm BS: Tillmann ##CITATION## In this algorithm the search space is explored in a breadth-first manner", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Basic Approach."], ["4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "Search algorithms We evaluate the following two search algorithms: \u2022 beam search algorithm BS: Tillmann ##CITATION## In this algorithm the search space is explored in a breadth-first manner", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Experimental Results."], ["A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in  . In Section 4, we present the performance measures used and give translation results on the Verbmobil task.", "It is faster because the search problem for noisy- channel models is NP-complete Knight and even the fastest dynamic-programming heuristics used in statistical MT Niessen et al Till- mann and Ney are polynomial in J \u2014for in pv1 w2     wm\u22121 um|h s = stance OmJ 4V 3 in ##CITATION## ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "Introduction"], ["A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in  , a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search. Here, we process only full-form words within the translation procedure. the number of permutations carried out for the word reordering is given.", "It is faster because the search problem for noisy- channel models is NP-complete Knight and even the fastest dynamic-programming heuristics used in statistical MT Niessen et al Till- mann and Ney are polynomial in J \u2014for in pv1 w2     wm\u22121 um|h s = stance OmJ 4V 3 in ##CITATION## ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup. In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem  . The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities. The resulting algorithm has a complexity of O(n!).", "It is faster because the search problem for noisy- channel models is NP-complete Knight and even the fastest dynamic-programming heuristics used in statistical MT Niessen et al Till- mann and Ney are polynomial in J \u2014for in pv1 w2     wm\u22121 um|h s = stance OmJ 4V 3 in ##CITATION## ", 0, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "DP Algorithm for Statistical."], ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A search restriction especially useful for the translation direction from German to English is presented.", "It is faster because the search problem for noisy- channel models is NP-complete Knight and even the fastest dynamic-programming heuristics used in statistical MT Niessen et al Till- mann and Ney are polynomial in J \u2014for in pv1 w2     wm\u22121 um|h s = stance OmJ 4V 3 in ##CITATION## ", 1, "Word Re-ordering and DP-based Search in Statistical Machine Translation", "ABSTRACT"], ["This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features ##CITATION## This is quite similar to the bagged decision trees of bigrams B presented here except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams according to the loglikelihood ratio This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64% whereas the bagged decision tree presented here achieves an accuracy of 68% on that data  ", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "ABSTRACT"], ["Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient. The bigram must have occurred 5 or more times to be included as a feature. This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process. The training and test data are converted to feature vectors where each feature represents the occurrence of one of the bigrams that belong in the feature set. This representation of the training data is the actual input to the learning algorithms.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features ##CITATION## This is quite similar to the bagged decision trees of bigrams B presented here except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams according to the loglikelihood ratio This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64% whereas the bagged decision tree presented here achieves an accuracy of 68% on that data  ", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Method."], ["This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7. 1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coecient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner. If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features ##CITATION## This is quite similar to the bagged decision trees of bigrams B presented here except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams according to the loglikelihood ratio This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64% whereas the bagged decision tree presented here achieves an accuracy of 68% on that data  ", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Analysis of Experimental Results."], ["We use the Weka   implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classi?er. Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml. Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. Ten teams participated in the supervised learning portion of this event. Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarri?", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features ##CITATION## This is quite similar to the bagged decision trees of bigrams B presented here except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams according to the loglikelihood ratio This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64% whereas the bagged decision tree presented here achieves an accuracy of 68% on that data  ", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Data."], ["The following process is repeated for each task. Capitalization and punctuation are removed from the training and test data. Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient. The bigram must have occurred 5 or more times to be included as a feature. This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features ##CITATION## This is quite similar to the bagged decision trees of bigrams B presented here except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams according to the loglikelihood ratio This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64% whereas the bagged decision tree presented here achieves an accuracy of 68% on that data  ", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Method."], ["One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation   where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. While the accuracy of this approach was as good as any previously published results, the learned models were complex and dicult to interpret, in e?ect acting as very accurate black boxes. Our experience has been that variations in learning algorithms are far less signi?cant contributors to disambiguation accuracy than are variations in the feature set. In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features ##CITATION## This is quite similar to the bagged decision trees of bigrams B presented here except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams according to the loglikelihood ratio This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64% whereas the bagged decision tree presented here achieves an accuracy of 68% on that data  ", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Discussion."], ["Test instances are disambiguated by ?nding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features. An instance of an ambiguous word is dis- ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context. We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er. The majority classi?er assigns the most common sense in the training data to every instance in the test data. A decision stump is a one node decision tree  that is created by stopping the decision tree learner after the single most informative feature is added to the tree.", "We also obtain salient bigrams in the context with the methods and the software described in ##CITATION##", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Learning Decision Trees."], ["These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in  . We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.", "We also obtain salient bigrams in the context with the methods and the software described in ##CITATION##", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution. One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements. This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning. Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison. The experimental data is discussed, and then the empirical results are presented.", "We also obtain salient bigrams in the context with the methods and the software described in ##CITATION##", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources. These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy.", "We also obtain salient bigrams in the context with the methods and the software described in ##CITATION##", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy. It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution.", "We also obtain salient bigrams in the context with the methods and the software described in ##CITATION##", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in  . We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse. Decision trees are among the most widely used machine learning algorithms.", "We also obtain salient bigrams in the context with the methods and the software described in ##CITATION##", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram  ,  ,  ). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies  ,  ).", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact ##CITATION##  found that bigrams alone can be eective features for word sense disambiguation", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution. One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements. This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning. Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison. The experimental data is discussed, and then the empirical results are presented.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact ##CITATION##  found that bigrams alone can be eective features for word sense disambiguation", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases. Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram. These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in  .", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact ##CITATION##  found that bigrams alone can be eective features for word sense disambiguation", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text. We are optimistic that this is viable, since bigram features are easy to identify in raw text. This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words. The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.", "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact ##CITATION##  found that bigrams alone can be eective features for word sense disambiguation", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Conclusion."], ["Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram  ,  ,  ). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies  ,  ). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation ##CITATION##", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram  ,  ,  ). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies  ,  ).", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation ##CITATION##", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation  ,  ,  ). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over{trained.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation ##CITATION##", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Related Work."], ["In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text. We are optimistic that this is viable, since bigram features are easy to identify in raw text. This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words. The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation ##CITATION##", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Conclusion."], ["1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coecient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner. If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses. In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense.", "The convergence is quicker for X2 than G2\u201d In addition ##CITATION##  questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read  who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Analysis of Experimental Results."], ["We are optimistic that this is viable, since bigram features are easy to identify in raw text. This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words. The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota. We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.", "The convergence is quicker for X2 than G2\u201d In addition ##CITATION##  questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read  who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Conclusion."], ["The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks. In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice Coecient. The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither. The Dice Coecient is based strictly on the event where w 1 and w 2 occur together in a bigram. There are 6 tasks where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, and sanction-p.", "The convergence is quicker for X2 than G2\u201d In addition ##CITATION##  questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read  who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Experimental Results."], ["These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance.   argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions. However,   suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this,   presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.", "The convergence is quicker for X2 than G2\u201d In addition ##CITATION##  questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read  who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["However,   suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this,   presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of data. We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical signi?cance when the bi- gram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of ?t statistics. We perform tests using X 2 , G 2 , and Fisher's exact test for each bigram.", "The convergence is quicker for X2 than G2\u201d In addition ##CITATION##  questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read  who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["This was motivated by the success of decision stumps in performing disambiguation based on a single bigram feature. In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features. This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7. 1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coecient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores as described by ##CITATION## ", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Analysis of Experimental Results."], ["A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases. Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram. These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coecient is also discussed in  .", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores as described by ##CITATION## ", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["The value of n 12 shows how often bigrams occur where big is the ?rst word and cat is not the second. The counts in n +1 and n 1+ indicate how often words big and cat occur as the ?rst and second words of any bigram in the corpus. The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.   introduce the power divergence family of goodness of ?t statistics. A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores as described by ##CITATION## ", 0, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources. These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores as described by ##CITATION## ", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Introduction"], ["We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bigrams, which we de?ne to be two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation of Bigram Counts consecutive words that occur in a text. The distributional characteristics of bigrams are fairly consistent across corpora; a majority of them only occur one time. Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen. We explore two alternatives, the power divergence family of goodness of ?t statistics and the Dice Coecient, an information theoretic measure related to point- wise Mutual Information. Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 ? 2 contingency table.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores as described by ##CITATION## ", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.   introduce the power divergence family of goodness of ?t statistics. A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic. These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance.   argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions.", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores as described by ##CITATION## ", 1, "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", "Building a Feature Set of Bigrams."], ["The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "##CITATION##  showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and they're", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "##CITATION##  showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and they're", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "##CITATION##  showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and they're", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "##CITATION##  showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and they're", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "##CITATION##  showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and they're", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.", "A number of feature-based methods have been tried including Bayesian classifiers Gale Church and Yarowsky ##CITATION## decision lists Yarowsky and knowledge-based approaches McRoy", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context.", "A number of feature-based methods have been tried including Bayesian classifiers Gale Church and Yarowsky ##CITATION## decision lists Yarowsky and knowledge-based approaches McRoy", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.", "A number of feature-based methods have been tried including Bayesian classifiers Gale Church and Yarowsky ##CITATION## decision lists Yarowsky and knowledge-based approaches McRoy", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A number of feature-based methods have been tried including Bayesian classifiers Gale Church and Yarowsky ##CITATION## decision lists Yarowsky and knowledge-based approaches McRoy", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech.", "The results described in this section are based on the 18 confusion sets selected by ##CITATION##  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence. But as the reliability and U(xjy) metrics indicate, it is not completely clear how the metric should be defined. This problem is addressed in the next section. 3.5 Hybrid method 2: Bayesian classifiers. The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations.", "The results described in this section are based on the 18 confusion sets selected by ##CITATION##  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "The results described in this section are based on the 18 confusion sets selected by ##CITATION##  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "The results described in this section are based on the 18 confusion sets selected by ##CITATION##  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!) = max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid co\u00ad occurs 10 times with desert and 1 time with dessert in the training corpus.", "We have also selected a decision list classifier DL which is similar to the classifier used by Yarowsky for words having two senses and extended for more senses by ##CITATION##", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "We have also selected a decision list classifier DL which is similar to the classifier used by Yarowsky for words having two senses and extended for more senses by ##CITATION##", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.", "We have also selected a decision list classifier DL which is similar to the classifier used by Yarowsky for words having two senses and extended for more senses by ##CITATION##", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "We have also selected a decision list classifier DL which is similar to the classifier used by Yarowsky for words having two senses and extended for more senses by ##CITATION##", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "We have also selected a decision list classifier DL which is similar to the classifier used by Yarowsky for words having two senses and extended for more senses by ##CITATION##", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context.", "##CITATION##  builds a classifier based on a rich set of context features", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context. The methods handle multiple confusion sets by applying the same technique to each confusion set independently.", "##CITATION##  builds a classifier based on a rich set of context features", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence con\u00ad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence.", "##CITATION##  builds a classifier based on a rich set of context features", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "##CITATION##  builds a classifier based on a rich set of context features", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.", "##CITATION##  builds a classifier based on a rich set of context features", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields such as Bayesian classifiers ##CITATION## ##CITATION## and Roth Winnow-based learning ##CITATION## and Roth decision lists ##CITATION## transformation-based learning Mangu and Brill augmented mixture models Cucerzan and Yarowsky and maximum entropy classifiers Izumi et al Han et al Chodorow et al Tetreault and Chodorow Felice and Pulman", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields such as Bayesian classifiers ##CITATION## ##CITATION## and Roth Winnow-based learning ##CITATION## and Roth decision lists ##CITATION## transformation-based learning Mangu and Brill augmented mixture models Cucerzan and Yarowsky and maximum entropy classifiers Izumi et al Han et al Chodorow et al Tetreault and Chodorow Felice and Pulman", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["3.4 Hybrid method 1: Decision lists. Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\u00ad cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields such as Bayesian classifiers ##CITATION## ##CITATION## and Roth Winnow-based learning ##CITATION## and Roth decision lists ##CITATION## transformation-based learning Mangu and Brill augmented mixture models Cucerzan and Yarowsky and maximum entropy classifiers Izumi et al Han et al Chodorow et al Tetreault and Chodorow Felice and Pulman", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields such as Bayesian classifiers ##CITATION## ##CITATION## and Roth Winnow-based learning ##CITATION## and Roth decision lists ##CITATION## transformation-based learning Mangu and Brill augmented mixture models Cucerzan and Yarowsky and maximum entropy classifiers Izumi et al Han et al Chodorow et al Tetreault and Chodorow Felice and Pulman", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The main parameter to tune for the method of context words is k, the half-width of the context window. Previous work [Yarowsky, 1994] shows that sma.ller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities. \\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax. In the rest of this paper, this value of k will be used. 2 We are interpreting the condition \"c. occurs within a \u00b1k-word window of w;\" as a binary feature - either it happens, or it does not.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["Like the method of context words, the method of collocations has one main parameter to tune: f, the maximum number of syntactic elements in a collocation. Since the number of collocations grows exponentially with e, it was only practical to vary f from 1 to 3. We tried this on some practice confusion sets, and found that a.ll values of\u00a3 gave roughly comparable performance. We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3). Table 3 shows the results of varying\u00a3 for the usual confusion sets.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "The memory-based learner was tested using the 18 confusion word sets from ##CITATION##  on the WSJ section of the Penn Treebank and the Brown Corpus", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Take the case of context-sensitive spelling error detection 3 which is equivalent to the homophone problem For that problem some statistical methods have been applied and succeeded##CITATION## Gold\u00ad ing and Schabes ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.", "Take the case of context-sensitive spelling error detection 3 which is equivalent to the homophone problem For that problem some statistical methods have been applied and succeeded##CITATION## Gold\u00ad ing and Schabes ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "Take the case of context-sensitive spelling error detection 3 which is equivalent to the homophone problem For that problem some statistical methods have been applied and succeeded##CITATION## Gold\u00ad ing and Schabes ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "Take the case of context-sensitive spelling error detection 3 which is equivalent to the homophone problem For that problem some statistical methods have been applied and succeeded##CITATION## Gold\u00ad ing and Schabes ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Take the case of context-sensitive spelling error detection 3 which is equivalent to the homophone problem For that problem some statistical methods have been applied and succeeded##CITATION## Gold\u00ad ing and Schabes ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!) = max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid co\u00ad occurs 10 times with desert and 1 time with dessert in the training corpus.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with. There are several ways to obtain such a collection.", "The more recent set of techniques includes multiplicative weight-update algorithms  latent semantic analysis  transformation-based learning  differential grammars  decision lists  and a variety of Bayesian classifiers [235] In all of these papers the problem is formulated as follows: Given a specific confusion set eg {to two too} all occurrences of confusion set members in the test set are replaced by some marker Then everywhere the system sees this marker it must decide which member of the confusion set to choose  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj). The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework. The task is to pick the word Wi that is most probable, given the context words Cj observed within a \u00b1k-word window of the target word. The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The task is to pick the word Wi that is most probable, given the context words Cj observed within a \u00b1k-word window of the target word. The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], [", c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus.", "For each si the probability is computed with Bayes' rule: As ##CITATION##  points out the term pc_kf  Ck I si is difficult to estimate because of the sparse data problem but if we assume as is often done that the occurrence of each cue is independent of the others then this term can be replaced with: pc-b\u00b7\u00b7 ck I si =IT pcj I Si j=-k In TLC we have made this assumption and have estimated pcj I si from the training ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["The context-word and collocation methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Yarowsky  proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is ##CITATION##  proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["One clue about the identity of an ambiguous target word comes from the words around it. For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert. On the other hand, words such as chocolate and delicious in the context imply desser\u00b7t. This observation is the basis for the method of context words. The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow. C on fu si on se t No.", "Hybrid approach [3 12] combines the strengths of other techniques such as Bayesian classifier n-gram and decision list", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "Hybrid approach [3 12] combines the strengths of other techniques such as Bayesian classifier n-gram and decision list", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.", "Hybrid approach [3 12] combines the strengths of other techniques such as Bayesian classifier n-gram and decision list", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Hybrid approach [3 12] combines the strengths of other techniques such as Bayesian classifier n-gram and decision list", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Since the number of collocations grows exponentially with e, it was only practical to vary f from 1 to 3. We tried this on some practice confusion sets, and found that a.ll values of\u00a3 gave roughly comparable performance. We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3). Table 3 shows the results of varying\u00a3 for the usual confusion sets. There is no clear winner; each value of\u00a3 did best for certain confusion sets.", "In the experiment we classify the data into three group depending on types of text ambiguity according to section 2: CDSA CISA and Homograph and compare the results from different approaches; Winnow Bayseian hybrid  and POS trigram", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech.", "In the experiment we classify the data into three group depending on types of text ambiguity according to section 2: CDSA CISA and Homograph and compare the results from different approaches; Winnow Bayseian hybrid  and POS trigram", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "In the experiment we classify the data into three group depending on types of text ambiguity according to section 2: CDSA CISA and Homograph and compare the results from different approaches; Winnow Bayseian hybrid  and POS trigram", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "In the experiment we classify the data into three group depending on types of text ambiguity according to section 2: CDSA CISA and Homograph and compare the results from different approaches; Winnow Bayseian hybrid  and POS trigram", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.", "These include a variety of Bayesian classifi ers ##CITATION## ##CITATION## and Schabes decision lists ##CITATION## transformation-based learning Mangu and Brill Latent Semantic Analysis LSA Jones and Martin multiplicative weight update algorithms ##CITATION## and Roth and augmented mixture models Cucerzan and Yarowsky Despite their differences most approaches use two types of features: context words and collocations ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.", "These include a variety of Bayesian classifi ers ##CITATION## ##CITATION## and Schabes decision lists ##CITATION## transformation-based learning Mangu and Brill Latent Semantic Analysis LSA Jones and Martin multiplicative weight update algorithms ##CITATION## and Roth and augmented mixture models Cucerzan and Yarowsky Despite their differences most approaches use two types of features: context words and collocations ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "These include a variety of Bayesian classifi ers ##CITATION## ##CITATION## and Schabes decision lists ##CITATION## transformation-based learning Mangu and Brill Latent Semantic Analysis LSA Jones and Martin multiplicative weight update algorithms ##CITATION## and Roth and augmented mixture models Cucerzan and Yarowsky Despite their differences most approaches use two types of features: context words and collocations ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "These include a variety of Bayesian classifi ers ##CITATION## ##CITATION## and Schabes decision lists ##CITATION## transformation-based learning Mangu and Brill Latent Semantic Analysis LSA Jones and Martin multiplicative weight update algorithms ##CITATION## and Roth and augmented mixture models Cucerzan and Yarowsky Despite their differences most approaches use two types of features: context words and collocations ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.", "These include a variety of Bayesian classifi ers ##CITATION## ##CITATION## and Schabes decision lists ##CITATION## transformation-based learning Mangu and Brill Latent Semantic Analysis LSA Jones and Martin multiplicative weight update algorithms ##CITATION## and Roth and augmented mixture models Cucerzan and Yarowsky Despite their differences most approaches use two types of features: context words and collocations ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context. The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by ##CITATION##  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus. Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by ##CITATION##  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by ##CITATION##  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.", "All methods use either the full set or a subset of 18 confusion sets originally gathered by ##CITATION##  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["}, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f t  7298 7000 f w1  t  w2 / f t  8777 7633 f w1  t  8440 8302 f w1  w2  t / f t  8627 7447 f t  w1  8489 8274 f t  w2  w2 / f t  8494 7423 f w1  t  w2  8924#*7713 f w1  t  w2 / f w1  t  8070 7369 f t  w1  w2  8468 7508 f w1  w2  t / f w2  t  7211 6928 f w1  t / f t  8281 7784 f t  w1  w2 / f t  w1  7565 7257 f t  w1 / f t  7749 8071# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction   Model Accuracy Baseline BNC 7000 Baseline Altavista 7298 Best BNC 8071\u2020\u2021 ##CITATION##   8140 Jones and Martin   8426 Best Altavista 8924\u2020\u2021 ##CITATION## and Schabes   8982 Mangu and Brill   9279 Cucerzan and Yarowsky   9220 ##CITATION## and Roth   9423 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus using 80% for training and 20% for testing3 We devised a simple unsupervised method for performing spelling correction using web counts", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f t  7298 7000 f w1  t  w2 / f t  8777 7633 f w1  t  8440 8302 f w1  w2  t / f t  8627 7447 f t  w1  8489 8274 f t  w2  w2 / f t  8494 7423 f w1  t  w2  8924#*7713 f w1  t  w2 / f w1  t  8070 7369 f t  w1  w2  8468 7508 f w1  w2  t / f w2  t  7211 6928 f w1  t / f t  8281 7784 f t  w1  w2 / f t  w1  7565 7257 f t  w1 / f t  7749 8071# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction   Model Accuracy Baseline BNC 7000 Baseline Altavista 7298 Best BNC 8071\u2020\u2021 ##CITATION##   8140 Jones and Martin   8426 Best Altavista 8924\u2020\u2021 ##CITATION## and Schabes   8982 Mangu and Brill   9279 Cucerzan and Yarowsky   9220 ##CITATION## and Roth   9423 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus using 80% for training and 20% for testing3 We devised a simple unsupervised method for performing spelling correction using web counts", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["5 3 0 0.840 0.840 0.840 O . G 9. 0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0.. 538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction. 4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f t  7298 7000 f w1  t  w2 / f t  8777 7633 f w1  t  8440 8302 f w1  w2  t / f t  8627 7447 f t  w1  8489 8274 f t  w2  w2 / f t  8494 7423 f w1  t  w2  8924#*7713 f w1  t  w2 / f w1  t  8070 7369 f t  w1  w2  8468 7508 f w1  w2  t / f w2  t  7211 6928 f w1  t / f t  8281 7784 f t  w1  w2 / f t  w1  7565 7257 f t  w1 / f t  7749 8071# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction   Model Accuracy Baseline BNC 7000 Baseline Altavista 7298 Best BNC 8071\u2020\u2021 ##CITATION##   8140 Jones and Martin   8426 Best Altavista 8924\u2020\u2021 ##CITATION## and Schabes   8982 Mangu and Brill   9279 Cucerzan and Yarowsky   9220 ##CITATION## and Roth   9423 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus using 80% for training and 20% for testing3 We devised a simple unsupervised method for performing spelling correction using web counts", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f t  7298 7000 f w1  t  w2 / f t  8777 7633 f w1  t  8440 8302 f w1  w2  t / f t  8627 7447 f t  w1  8489 8274 f t  w2  w2 / f t  8494 7423 f w1  t  w2  8924#*7713 f w1  t  w2 / f w1  t  8070 7369 f t  w1  w2  8468 7508 f w1  w2  t / f w2  t  7211 6928 f w1  t / f t  8281 7784 f t  w1  w2 / f t  w1  7565 7257 f t  w1 / f t  7749 8071# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction   Model Accuracy Baseline BNC 7000 Baseline Altavista 7298 Best BNC 8071\u2020\u2021 ##CITATION##   8140 Jones and Martin   8426 Best Altavista 8924\u2020\u2021 ##CITATION## and Schabes   8982 Mangu and Brill   9279 Cucerzan and Yarowsky   9220 ##CITATION## and Roth   9423 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus using 80% for training and 20% for testing3 We devised a simple unsupervised method for performing spelling correction using web counts", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f t  7298 7000 f w1  t  w2 / f t  8777 7633 f w1  t  8440 8302 f w1  w2  t / f t  8627 7447 f t  w1  8489 8274 f t  w2  w2 / f t  8494 7423 f w1  t  w2  8924#*7713 f w1  t  w2 / f w1  t  8070 7369 f t  w1  w2  8468 7508 f w1  w2  t / f w2  t  7211 6928 f w1  t / f t  8281 7784 f t  w1  w2 / f t  w1  7565 7257 f t  w1 / f t  7749 8071# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction   Model Accuracy Baseline BNC 7000 Baseline Altavista 7298 Best BNC 8071\u2020\u2021 ##CITATION##   8140 Jones and Martin   8426 Best Altavista 8924\u2020\u2021 ##CITATION## and Schabes   8982 Mangu and Brill   9279 Cucerzan and Yarowsky   9220 ##CITATION## and Roth   9423 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus using 80% for training and 20% for testing3 We devised a simple unsupervised method for performing spelling correction using web counts", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We tried this on some practice confusion sets, and found that a.ll values of\u00a3 gave roughly comparable performance. We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3). Table 3 shows the results of varying\u00a3 for the usual confusion sets. There is no clear winner; each value of\u00a3 did best for certain confusion sets. Table 5 gives examples of the collocations learned for {peace, piece} with\u00a3= 2.", "Table 6 shows 3 An exception is ##CITATION##   who uses the entire Brown corpus for training 1M words and 3/4 of the Wall Street Journal corpus   for testing", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Table 5: Excerpts from the sorted list of 98 collocations learned for {peace, piece} with \u00a3 = 2. Each line gives a collocation, and the number of peace and piece occurrences it matched. The last line of the table gives the total number of occurrences of peace and piece in the training corpus. 49 population. Applying the U(xjy) metric to the arid example, the value returned now depends on the number of occurrences of deser\u00b7t and dessert in the training corpus.", "Table 6 shows 3 An exception is ##CITATION##   who uses the entire Brown corpus for training 1M words and 3/4 of the Wall Street Journal corpus   for testing", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This metric is therefore the one that will be used from here on. It was also used for all experiments involving the method of collocations. Table 6 shows the performance of decision lists with each metric for the usual confusion sets. As with the practice confusion sets, we see sometimes dramatic performance differences between the two metrics, and no clear winner. For instance, for {I, me}, the reliability metric did better than U(xjy) (0.980 versus 0.808); whereas for {between, among}, it did worse (0.659 versus 0.800).", "Table 6 shows 3 An exception is ##CITATION##   who uses the entire Brown corpus for training 1M words and 3/4 of the Wall Street Journal corpus   for testing", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "Table 6 shows 3 An exception is ##CITATION##   who uses the entire Brown corpus for training 1M words and 3/4 of the Wall Street Journal corpus   for testing", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus.", "Table 6 shows 3 An exception is ##CITATION##   who uses the entire Brown corpus for training 1M words and 3/4 of the Wall Street Journal corpus   for testing", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["where dessert was misspelled as desert. This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.", "A comparison with the literature shows that the best Altavista model outperforms ##CITATION##   Jones and Martin   highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Run time (1) Initialize the probability for each word in the confusion set to its prior probability. (2) Go through the sorted list of features that was saved during training. For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities. (3) Choose the word in the confusion set with the highest probability. Figure 2: Outline of the method of collocations.", "A comparison with the literature shows that the best Altavista model outperforms ##CITATION##   Jones and Martin   highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["One clue about the identity of an ambiguous target word comes from the words around it. For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert. On the other hand, words such as chocolate and delicious in the context imply desser\u00b7t. This observation is the basis for the method of context words. The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow. C on fu si on se t No.", "A comparison with the literature shows that the best Altavista model outperforms ##CITATION##   Jones and Martin   highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "A comparison with the literature shows that the best Altavista model outperforms ##CITATION##   Jones and Martin   highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help. By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time. To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set. If the observed association is not judged to be significant,3 then c is discarded. The significance level is currently set to 0.05.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context continuing the tradition established for contextual spelling correction by ##CITATION##  and ##CITATION## and Roth ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["In other words, c is ignored if it practically never occurs within the context of any w;, or if it practically always occurs within the context of every w;. In the former case, we have insufficient data to measure its presence; in the latter, its absence. Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set. For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help. By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context continuing the tradition established for contextual spelling correction by ##CITATION##  and ##CITATION## and Roth ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context continuing the tradition established for contextual spelling correction by ##CITATION##  and ##CITATION## and Roth ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deser\u00b7t, desser\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with. There are several ways to obtain such a collection.", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context continuing the tradition established for contextual spelling correction by ##CITATION##  and ##CITATION## and Roth ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "The more recent set of techniques includes mult iplicative weight- update algorithms ##CITATION## and Roth latent semantic analysis Jones and Martin transformation- based learning Mangu and Brill differential grammars Powers decision lists Yarowsky and a variety of Bayesian classifiers Gale et al ##CITATION## ##CITATION## and Schabes In all of these approaches the problem is formulated as follows: Given a specific confusion set eg {totwotoo} all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker it must decide which member of the confusion set to choose ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "The more recent set of techniques includes mult iplicative weight- update algorithms ##CITATION## and Roth latent semantic analysis Jones and Martin transformation- based learning Mangu and Brill differential grammars Powers decision lists Yarowsky and a variety of Bayesian classifiers Gale et al ##CITATION## ##CITATION## and Schabes In all of these approaches the problem is formulated as follows: Given a specific confusion set eg {totwotoo} all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker it must decide which member of the confusion set to choose ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research. We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.", "The more recent set of techniques includes mult iplicative weight- update algorithms ##CITATION## and Roth latent semantic analysis Jones and Martin transformation- based learning Mangu and Brill differential grammars Powers decision lists Yarowsky and a variety of Bayesian classifiers Gale et al ##CITATION## ##CITATION## and Schabes In all of these approaches the problem is formulated as follows: Given a specific confusion set eg {totwotoo} all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker it must decide which member of the confusion set to choose ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "The more recent set of techniques includes mult iplicative weight- update algorithms ##CITATION## and Roth latent semantic analysis Jones and Martin transformation- based learning Mangu and Brill differential grammars Powers decision lists Yarowsky and a variety of Bayesian classifiers Gale et al ##CITATION## ##CITATION## and Schabes In all of these approaches the problem is formulated as follows: Given a specific confusion set eg {totwotoo} all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker it must decide which member of the confusion set to choose ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ). The method of collocations was implemented in much the same way as the method of context words. The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;. An ambiguous target word is then classified by finding all collocations that match its context. Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's).", "The more recent set of techniques includes mult iplicative weight- update algorithms ##CITATION## and Roth latent semantic analysis Jones and Martin transformation- based learning Mangu and Brill differential grammars Powers decision lists Yarowsky and a variety of Bayesian classifiers Gale et al ##CITATION## ##CITATION## and Schabes In all of these approaches the problem is formulated as follows: Given a specific confusion set eg {totwotoo} all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker it must decide which member of the confusion set to choose ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The method of collocations was implemented in much the same way as the method of context words. The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;. An ambiguous target word is then classified by finding all collocations that match its context. Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's). 45 for each word in the confusion set.", "The more recent set of techniques includes mult iplicative weight- update algorithms ##CITATION## and Roth latent semantic analysis Jones and Martin transformation- based learning Mangu and Brill differential grammars Powers decision lists Yarowsky and a variety of Bayesian classifiers Gale et al ##CITATION## ##CITATION## and Schabes In all of these approaches the problem is formulated as follows: Given a specific confusion set eg {totwotoo} all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker it must decide which member of the confusion set to choose ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "Feature-based approaches such as Bayesian clas\u00ad sifiers Gale Church and Yarowsky deci\u00ad sion lists Yarowsky and Bayesian hybrids ##CITATION## have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "Feature-based approaches such as Bayesian clas\u00ad sifiers Gale Church and Yarowsky deci\u00ad sion lists Yarowsky and Bayesian hybrids ##CITATION## have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.", "Feature-based approaches such as Bayesian clas\u00ad sifiers Gale Church and Yarowsky deci\u00ad sion lists Yarowsky and Bayesian hybrids ##CITATION## have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "PAPER"], ["Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.", "Feature-based approaches such as Bayesian clas\u00ad sifiers Gale Church and Yarowsky deci\u00ad sion lists Yarowsky and Bayesian hybrids ##CITATION## have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.", "Feature-based approaches such as Bayesian clas\u00ad sifiers Gale Church and Yarowsky deci\u00ad sion lists Yarowsky and Bayesian hybrids ##CITATION## have had varying degrees of suc\u00ad cess for the problem of context-sensitive spelling correction", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "We consider an alternative method Bayes a Bayesian hybrid method   for the case where the words have the same part of speech", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity. Gale et al. interpolate between the two so as to minimize the overall inaccuracy. We have pursued an alternative approach to the problem of estimating the likelihood terms. We start with the observation that there is no need to use every word in the \u00b1k-word window to discriminate among the words in the confusion set. If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.", "We consider an alternative method Bayes a Bayesian hybrid method   for the case where the words have the same part of speech", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.", "We consider an alternative method Bayes a Bayesian hybrid method   for the case where the words have the same part of speech", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "PAPER"], ["We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech. In this case, trigrams can distinguish between the words only by their prior probabilities\u00ad this follows from the way the method calculates sentence probabilities. Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method.", "We consider an alternative method Bayes a Bayesian hybrid method   for the case where the words have the same part of speech", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["In this case, trigrams can distinguish between the words only by their prior probabilities\u00ad this follows from the way the method calculates sentence probabilities. Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method. In such cases, the Bayesian hybrid method is clearly better. On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence.", "We consider an alternative method Bayes a Bayesian hybrid method   for the case where the words have the same part of speech", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context.", "A number of feature-based methods have been proposed including Bayesian classifiers   decision lists   Bayesian hybrids   and more recently a method based on the Winnow multiplicative weight-updating algorithm  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["3.4 Hybrid method 1: Decision lists. Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\u00ad cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.", "A number of feature-based methods have been proposed including Bayesian classifiers   decision lists   Bayesian hybrids   and more recently a method based on the Winnow multiplicative weight-updating algorithm  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "A number of feature-based methods have been proposed including Bayesian classifiers   decision lists   Bayesian hybrids   and more recently a method based on the Winnow multiplicative weight-updating algorithm  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A number of feature-based methods have been proposed including Bayesian classifiers   decision lists   Bayesian hybrids   and more recently a method based on the Winnow multiplicative weight-updating algorithm  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\u00ad trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.", "We adopt the Bayesian hybrid method which we will call Bayes having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand This method has been described elsewhere   and so will only be briefly reviewed here; how\u00ad ever the version used here uses an improved smooth\u00ad ing technique which is mentioned briefly below ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.", "We adopt the Bayesian hybrid method which we will call Bayes having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand This method has been described elsewhere   and so will only be briefly reviewed here; how\u00ad ever the version used here uses an improved smooth\u00ad ing technique which is mentioned briefly below ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o\u00b7r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus. Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases.", "We adopt the Bayesian hybrid method which we will call Bayes having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand This method has been described elsewhere   and so will only be briefly reviewed here; how\u00ad ever the version used here uses an improved smooth\u00ad ing technique which is mentioned briefly below ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "We adopt the Bayesian hybrid method which we will call Bayes having experi\u00ad mented with each of the methods and found Bayes to be among the best-performing for the task at hand This method has been described elsewhere   and so will only be briefly reviewed here; how\u00ad ever the version used here uses an improved smooth\u00ad ing technique which is mentioned briefly below ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax.", "For English a number of methods have been proposed to cope with real-word errors in spelling correction ##CITATION## ##CITATION## and Roth ##CITATION## and Schabes Tong and Evans", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["One is based on finding words in the dictionary that are one typo away from each other [Mays et al., 1991).1 Another finds words that have the same or similar pronunciations. Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of \"\\Vords Commonly Confused\" in the back of the Random House unabridged dictionary [Fiexner, 1983]. A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error). We will make the simplifying assumption that both kinds of errors are equally bad. In practice, however, false negatives are much worse, as users get irritated by programs that badger them with bogus complaints.", "For English a number of methods have been proposed to cope with real-word errors in spelling correction ##CITATION## ##CITATION## and Roth ##CITATION## and Schabes Tong and Evans", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["}, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "For English a number of methods have been proposed to cope with real-word errors in spelling correction ##CITATION## ##CITATION## and Roth ##CITATION## and Schabes Tong and Evans", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "For English a number of methods have been proposed to cope with real-word errors in spelling correction ##CITATION## ##CITATION## and Roth ##CITATION## and Schabes Tong and Evans", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["In the work reported here, the method of collocations was used to capture order dependencies. A collocation expresses a pattern of syntactic elements around the target word. We allow two types of syntactic elements: words, and part-of-speech tags. Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords \u00b1 3 \u00b1 6 \u00b1 1 2 \u00b1 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 .. 5 7 5 0.575 0.585 0.498 0 . 7 5 9 0.697 0.671 0.586 0 . 5 3 0 0.530 0.521 0.557 0 . 6 9 5 0.526 0.516 0.558 0 . 7 5 4 0.705 0..574 0.574 0 . 7 2 6 0.623 0.557 0.466 0 . 2 9 0 0.290 0.290 0.435 0 . 4 5 5 0.2.50 0.364 0.318 A vg no.", "Following previous works   we have tried two types of features: context words and collocations", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Table 5 gives examples of the collocations learned for {peace, piece} with\u00a3= 2. A good deal of redundancy can be seen among the collocations. There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps). Many of the collocations a.t the end of the list appear to be overgeneral and irrelevant. 3.4 Hybrid method 1: Decision lists.", "Following previous works   we have tried two types of features: context words and collocations", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\u00ad cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making. An ambiguous target word is then classified by running down the list and matching each feature against the target context.", "Following previous works   we have tried two types of features: context words and collocations", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.", "Following previous works   we have tried two types of features: context words and collocations", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions Rab89 pos tagging Kup92; Sch95 word-sense ambiguation GCY93 and context\u00ad sensitive spelling correction Gol95", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions Rab89 pos tagging Kup92; Sch95 word-sense ambiguation GCY93 and context\u00ad sensitive spelling correction Gol95", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["}, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive. This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within \u00b1k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ...", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions Rab89 pos tagging Kup92; Sch95 word-sense ambiguation GCY93 and context\u00ad sensitive spelling correction Gol95", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions Rab89 pos tagging Kup92; Sch95 word-sense ambiguation GCY93 and context\u00ad sensitive spelling correction Gol95", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions Rab89 pos tagging Kup92; Sch95 word-sense ambiguation GCY93 and context\u00ad sensitive spelling correction Gol95", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "MBL by using long and very specialized conjunctions DBZ99 and decision lists due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature rather than a combination Gol95", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "MBL by using long and very specialized conjunctions DBZ99 and decision lists due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature rather than a combination Gol95", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["3.5 Hybrid method 2: Bayesian classifiers. The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations. In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers.", "MBL by using long and very specialized conjunctions DBZ99 and decision lists due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature rather than a combination Gol95", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction.", "MBL by using long and very specialized conjunctions DBZ99 and decision lists due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature rather than a combination Gol95", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "A partial list consists of Bayesian classifiers Gale et al decision lists Yarowsky Bayesian hybrids ##CITATION## HMMs Charniak inductive logic methods Zelle and Mooney memoryThis research is supported by NSF grants IIS9801638 IIS 0085836 and SBR987345", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "A partial list consists of Bayesian classifiers Gale et al decision lists Yarowsky Bayesian hybrids ##CITATION## HMMs Charniak inductive logic methods Zelle and Mooney memoryThis research is supported by NSF grants IIS9801638 IIS 0085836 and SBR987345", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "A partial list consists of Bayesian classifiers Gale et al decision lists Yarowsky Bayesian hybrids ##CITATION## HMMs Charniak inductive logic methods Zelle and Mooney memoryThis research is supported by NSF grants IIS9801638 IIS 0085836 and SBR987345", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.", "A partial list consists of Bayesian classifiers Gale et al decision lists Yarowsky Bayesian hybrids ##CITATION## HMMs Charniak inductive logic methods Zelle and Mooney memoryThis research is supported by NSF grants IIS9801638 IIS 0085836 and SBR987345", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models ##CITATION## Winnow ##CITATION## and Roth and Transformation-Based Learning Mangu and Brill", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models ##CITATION## Winnow ##CITATION## and Roth and Transformation-Based Learning Mangu and Brill", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models ##CITATION## Winnow ##CITATION## and Roth and Transformation-Based Learning Mangu and Brill", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models ##CITATION## Winnow ##CITATION## and Roth and Transformation-Based Learning Mangu and Brill", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models ##CITATION## Winnow ##CITATION## and Roth and Transformation-Based Learning Mangu and Brill", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within \u00b1k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj).", "For CSSC we tested our system on the identical data from the Brown corpus used by ##CITATION##   ##CITATION## and Roth   and Mangu and Brill  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["It can be thought of as the feature's reliability a.t picking out that w; from the others in the confusion set. 5Jn fact, we guarantee that this inequalit.y holds by performing smoothing before calculating strength. We smooth the data by adding 1 to the count. of how many times each feature was observed for each w;. 47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.", "For CSSC we tested our system on the identical data from the Brown corpus used by ##CITATION##   ##CITATION## and Roth   and Mangu and Brill  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Consider, for example, the following collocations for desert: PREP the in the the These collocations are highly interdependent- we will say they conflict. To deal with this problem, we invoke our earlier observation that there is no need to use all the evidence. If two pieces of evidence conflict, we simply eliminate one of them, and base our decision on the rest of the evidence. We identify conflicts by the heuristic that two collocations conflict iff they overlap. The overlapping portion is the factor they have in common, and thus represents their lack of independence.", "For CSSC we tested our system on the identical data from the Brown corpus used by ##CITATION##   ##CITATION## and Roth   and Mangu and Brill  ", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "For CSSC we tested our system on the identical data from the Brown corpus used by ##CITATION##   ##CITATION## and Roth   and Mangu and Brill  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method.", "For CSSC we tested our system on the identical data from the Brown corpus used by ##CITATION##   ##CITATION## and Roth   and Mangu and Brill  ", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions. Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.", "A different body of work eg ##CITATION## ##CITATION## and Roth Mangu and Brill focused on resolving a limited number of cognitive substitution errors in the framework of context sensitive spelling correction CSSC", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "A different body of work eg ##CITATION## ##CITATION## and Roth Mangu and Brill focused on resolving a limited number of cognitive substitution errors in the framework of context sensitive spelling correction CSSC", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "A different body of work eg ##CITATION## ##CITATION## and Roth Mangu and Brill focused on resolving a limited number of cognitive substitution errors in the framework of context sensitive spelling correction CSSC", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "A different body of work eg ##CITATION## ##CITATION## and Roth Mangu and Brill focused on resolving a limited number of cognitive substitution errors in the framework of context sensitive spelling correction CSSC", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["To compare the two strength metrics, we tried both on some practice confusion sets. Sometimes one metric did substantially better, sometimes the other. In the balance, the reliability metric seemed to give higher performance. This metric is therefore the one that will be used from here on. It was also used for all experiments involving the method of collocations.", "We use the metric described in Yarowsky ##CITATION##", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["We then ignore a context word c if: L m; < Tmin or L (Af;- m;) < Tmin l i n l5i n where m; and A{ are defined as above. In other words, c is ignored if it practically never occurs within the context of any w;, or if it practically always occurs within the context of every w;. In the former case, we have insufficient data to measure its presence; in the latter, its absence. Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set. For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help.", "We use the metric described in Yarowsky ##CITATION##", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Gale et al. interpolate between the two so as to minimize the overall inaccuracy. We have pursued an alternative approach to the problem of estimating the likelihood terms. We start with the observation that there is no need to use every word in the \u00b1k-word window to discriminate among the words in the confusion set. If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence. We implement this by introducing a \"minimum occurrences\" threshold, Tmin.", "We use the metric described in Yarowsky ##CITATION##", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This obviates the need for resolving conflicts between features. Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!)", "We use the metric described in Yarowsky ##CITATION##", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.", "More generally as a precursor to the above- mentioned work confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task Yarowsky ##CITATION## Mangu and Brill Huang and Powers", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "ABSTRACT"], ["Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right. Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one. Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength. This preserves the strongest non-conflicting evidence as the basis for our answer. The strength of a collocation reflects its reliability for decision-making; a further discussion of strength is deferred to Section 3.4.", "More generally as a precursor to the above- mentioned work confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task Yarowsky ##CITATION## Mangu and Brill Huang and Powers", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers.", "More generally as a precursor to the above- mentioned work confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task Yarowsky ##CITATION## Mangu and Brill Huang and Powers", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \\Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.", "More generally as a precursor to the above- mentioned work confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task Yarowsky ##CITATION## Mangu and Brill Huang and Powers", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Context-sensitive spelling correction."], ["This is a research direction we plan to pursue. The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented.", "There are also other studies Yarowsky ##CITATION##or ##CITATION## and Roth that report the application of decision lists and Bayesian classifiers for spell checking; however these models cannot be applied to grammar error detection", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["The overlapping portion is the factor they have in common, and thus represents their lack of independence. This is only a heuristic because we could imagine collocations that do not overlap, but still conflict. Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right. Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one. Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.", "There are also other studies Yarowsky ##CITATION##or ##CITATION## and Roth that report the application of decision lists and Bayesian classifiers for spell checking; however these models cannot be applied to grammar error detection", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features.", "There are also other studies Yarowsky ##CITATION##or ##CITATION## and Roth that report the application of decision lists and Bayesian classifiers for spell checking; however these models cannot be applied to grammar error detection", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "There are also other studies Yarowsky ##CITATION##or ##CITATION## and Roth that report the application of decision lists and Bayesian classifiers for spell checking; however these models cannot be applied to grammar error detection", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "There are also other studies Yarowsky ##CITATION##or ##CITATION## and Roth that report the application of decision lists and Bayesian classifiers for spell checking; however these models cannot be applied to grammar error detection", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.", "##CITATION##  has applied a hybrid Bayesian method for real-word error correction and ##CITATION## and Schabes  have combined a POS trigram and Bayesian methods for the same purpose", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "PAPER"], ["Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech. In this case, trigrams can distinguish between the words only by their prior probabilities\u00ad this follows from the way the method calculates sentence probabilities.", "##CITATION##  has applied a hybrid Bayesian method for real-word error correction and ##CITATION## and Schabes  have combined a POS trigram and Bayesian methods for the same purpose", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research. We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.", "##CITATION##  has applied a hybrid Bayesian method for real-word error correction and ##CITATION## and Schabes  have combined a POS trigram and Bayesian methods for the same purpose", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Conclusion."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "##CITATION##  has applied a hybrid Bayesian method for real-word error correction and ##CITATION## and Schabes  have combined a POS trigram and Bayesian methods for the same purpose", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \\Ve then apply each of the two component methods mentioned above\u00ad context words and collocations. \\Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.", "##CITATION##  has applied a hybrid Bayesian method for real-word error correction and ##CITATION## and Schabes  have combined a POS trigram and Bayesian methods for the same purpose", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["5 3 0 0.840 0.840 0.840 O . G 9. 0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0.. 538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction. 4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.", "Our module used for spelling correction was developed on the basis of works by Brill  Brill and Marcus [2 ##CITATION## [3 ##CITATION## and Schabes  and Powers [5", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence con\u00ad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence.", "Our module used for spelling correction was developed on the basis of works by Brill  Brill and Marcus [2 ##CITATION## [3 ##CITATION## and Schabes  and Powers [5", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "##other##"], ["In the balance, the reliability metric seemed to give higher performance. This metric is therefore the one that will be used from here on. It was also used for all experiments involving the method of collocations. Table 6 shows the performance of decision lists with each metric for the usual confusion sets. As with the practice confusion sets, we see sometimes dramatic performance differences between the two metrics, and no clear winner.", "Our module used for spelling correction was developed on the basis of works by Brill  Brill and Marcus [2 ##CITATION## [3 ##CITATION## and Schabes  and Powers [5", 0, "A Bayesian hybrid method for context-sensitive spelling correction", "Five methods for  spelling correction."], ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disam\u00ad biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert.", "Our module used for spelling correction was developed on the basis of works by Brill  Brill and Marcus [2 ##CITATION## [3 ##CITATION## and Schabes  and Powers [5", 1, "A Bayesian hybrid method for context-sensitive spelling correction", "Introduction"], ["Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve. We run our best Mincut LRMSL algorithm with two different settings on Wiebe. Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset. At the recall of50% we achieve a precision of 83.6% (in compari son to their precision of 55% at the same recall). Our F-score is 0.63 (vs. 0.52).", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy   and Andreevskaia and Bergler   show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.  . Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 0, "No title", "Related Work."], ["Binary classification of the vertices is equivalent to splitting the graph into two disconnected subsets of all vertices, S and T with s \u2208 S and t \u2208 T . This corresponds to removing a set of edges from the graph. As similar items should be in the same part of the split, the best split is one which removes edges with low weights. In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices. subjective or both objective.3 An example here is the antonym relation, where two antonyms such as good\u2014morally admirable and evil, wicked\u2014morally bad or wrong are both subjective. Second, Mincuts can be easily expanded into a semi-supervised framework  .", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 0, "No title", "Semi-Supervised Mincuts."], ["4 Experiments and Evaluation. 4.1 Datasets. We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/\u02dccjlin/libsvm/. Linear kernel and probability estimates are used in this work.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 1, "No title", "After graph construction  we then employ a."], ["4.1 Datasets. We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/\u02dccjlin/libsvm/. Linear kernel and probability estimates are used in this work. http://www.cs.pitt.edu/mpq a subjective 0.24 0.83 religio us similar-to 0.81 scrupulo us 0.76 0.17 objective baseline.8 Three different feature types are used.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 1, "No title", "After graph construction  we then employ a."], ["Relati on Feature s (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjectiv e or objective part of the training set, respectiv ely. This provides a non graph 0.16 0.84 flicker Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6. It includes 298 words with 703 objective and 358 subjective WordNet senses. The second one is the dataset created by Wiebe and Mihalcea  .7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses. As the MicroWNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 1, "No title", "Available at."], ["In our previous work  , we report 76.9% as the best accuracy on the same Micro Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Re lati on # unl ab ele d da ta Ac cu ra cy {\u2205 } 0 75 .3 % {si milar to } 41 8 79 .1 % {si milar to, ant on ym } 51 4 79 .5 % {si milarto, antonym, direct-hypernym, direct hy po ny m } 2, 72 1 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, also se e, ext en ded ant on ym } 3, 00 4 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, al so se e, ex te nd ed an to ny m, d eri ved fr o m , at tri bu te , d o m ai n, d o m ain m e m be r} 3, 22 0 84 .6 % 89 Option1 87 Option2. 85 83 81 79 77 75 0 500 1000 1500 2000 2500 3000 3500 Size of Unlabeled Data Figure 3: Learning curve with different sizes of unlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2). Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2). For comparison to Wiebe and Mihalcea  , we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description). Wiebe and Mihalcea   report their results in precision and recall curves for subjective senses, such as a precision of about 55% at a recall of 50% for subjective senses.", "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al  where on a set of 83 English words an accuracy of 88% was observed; and the method proposed in ##CITATION##  where an accuracy of 84% was obtained on another dataset of 298 words ", 1, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["This dataset was first used with a different annotation scheme in Esuli and Sebastiani   and we also used it in Su and Markert  . pubs/papers/goldstandard.total.acl06. classification vertices in the Mincut approach. 9 Available at http://www.d.umn.edu/\u02dctpederse/. similarity.html.", "Pang and Su both optimized their multi-classification results using the Mincut model", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%). To achieve the results of standard supervised approaches with our model, we need less than 20% of their training data. In addition, we compare our algorithm to previous state-of-the-art approaches, showing that our model performs better on the same datasets. Future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures.", "Pang and Su both optimized their multi-classification results using the Mincut model", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%). To achieve the results of standard supervised approaches with our model, we need less than 20% of their training data.", "Pang and Su both optimized their multi-classification results using the Mincut model", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "Pang and Su both optimized their multi-classification results using the Mincut model", 1, "No title", "ABSTRACT"], ["The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective. For example, the sense uncompromising, inflexible\u2014not making concessions is subjective, as \u201cuncompromising\u201d is a monosemous word and also in SL. We experiment with different combinations of features and the results are listed in Table 2, prefixed by \u201cSVM\u201d. All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Mark- ert  . However, improvements by adding more features remain small.", "Inspired by work of Pang and Su we also use Minimum cut Mincut model to optimize the Two-stage SVM result", 0, "No title", "Available   at   http://www.comp.leeds.ac.uk/."], ["Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work.", "Inspired by work of Pang and Su we also use Minimum cut Mincut model to optimize the Two-stage SVM result", 0, "No title", "Introduction"], ["All evaluations are carried out by 10-fold cross-validation. 4.3 Standard Supervised Learning. We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable summary of subjectivity-preserving links. Second, we manually collected a small set (denoted by SubjSet) of seven subjective verb and noun senses which are close to the root in WordNet\u2019s hypernym tree.", "Inspired by work of Pang and Su we also use Minimum cut Mincut model to optimize the Two-stage SVM result", 0, "No title", "Available at."], ["Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "Inspired by work of Pang and Su we also use Minimum cut Mincut model to optimize the Two-stage SVM result", 1, "No title", "ABSTRACT"], ["Wiebe&Mihalcea\u2019s dataset. 4.2 Baseline and Evaluation. We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalcea\u2019s dataset respectively. We use the McNemar test at the significance level of 5% for significance statements. All evaluations are carried out by 10-fold cross-validation.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner  and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target or the observation of ##CITATION##  that sentiment should be dependent on word senses instead of word forms which would capture a large number of examples within the expression strengthening category", 0, "No title", "Available at."], ["At the word level Takamura et al.   use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea   we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner  and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target or the observation of ##CITATION##  that sentiment should be dependent on word senses instead of word forms which would capture a large number of examples within the expression strengthening category", 0, "No title", "Related Work."], ["Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. There has been a large and diverse body of research in opinion mining, with most research at the text  , sentence   or word   level. An up-to-date overview is given in Pang and Lee  . Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level  , instead of aiming at dictionary annotation as we do.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner  and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target or the observation of ##CITATION##  that sentiment should be dependent on word senses instead of word forms which would capture a large number of examples within the expression strengthening category", 0, "No title", "Related Work."], ["We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.", "The MWE productions seem to overlap with well- known linguistic phenomena \u2013 consider Fahrni and Klenner  and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a \u2018prior\u2019 polarity that holds independently of the target or the observation of ##CITATION##  that sentiment should be dependent on word senses instead of word forms which would capture a large number of examples within the expression strengthening category", 1, "No title", "ABSTRACT"], ["However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).", "##CITATION##  propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "##CITATION##  propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information", 0, "No title", "ABSTRACT"], ["The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. There has been a large and diverse body of research in opinion mining, with most research at the text  , sentence   or word   level.", "##CITATION##  propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information", 0, "No title", "Introduction"], ["The SVM is also used as a baseline and its features are described in Section 4.3. As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie..", "##CITATION##  propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information", 1, "No title", "After graph construction  we then employ a."], ["As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective.", "##CITATION##  propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information", 1, "No title", "After graph construction  we then employ a."], ["Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective. We then count how often two senses related via a given relation have the same or a different subjectivity label.", "##CITATION##  propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information", 1, "No title", "After graph construction  we then employ a."], ["Wiebe and Mi- halcea   use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus  . One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification. Su and Markert   present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNet\u2019s relation structure. 3.1 Minimum Cuts: The Main Idea. Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items should be grouped in the same cut.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert ", 0, "No title", "Related Work."], ["3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations. Likewise, LMNoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list). 4.4 Semi-supervised Graph Mincuts. Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in MicroWNOp as unlabeled data. We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL).", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert ", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert ", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["3.2 Why might Semi-supervised Minimum. Cuts Work? We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. First, our problem satisfies two major conditions necessary for using minimum cuts. It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components.", "Semi-supervised techniques on text mining were applied by Fangzhong and Markert ", 1, "No title", "Semi-Supervised Mincuts."], ["As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill4 While sentiment Esuli and Sebastiani Wilson et al ##CITATION## and connotation lexicons Feng et al Kang et al are related sentiment connotation and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities for example", 0, "No title", "After graph construction  we then employ a."], [" . Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani   determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work   by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative).", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill4 While sentiment Esuli and Sebastiani Wilson et al ##CITATION## and connotation lexicons Feng et al Kang et al are related sentiment connotation and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities for example", 0, "No title", "Related Work."], ["However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective. We then count how often two senses related via a given relation have the same or a different subjectivity label. The weight is computed by #same/(#same+#different). Results are listed in Table 1.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill4 While sentiment Esuli and Sebastiani Wilson et al ##CITATION## and connotation lexicons Feng et al Kang et al are related sentiment connotation and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities for example", 0, "No title", "After graph construction  we then employ a."], ["in the sentence \u201cThis deal is a positive development for our company.\u201d gives a strong indication that 1 All examples in this paper are from WordNet 2.0.. 1 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1\u20139, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.", "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill4 While sentiment Esuli and Sebastiani Wilson et al ##CITATION## and connotation lexicons Feng et al Kang et al are related sentiment connotation and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities for example", 1, "No title", "Introduction"], ["However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).", "##CITATION##   adopt a semi-supervised mincut method to recognize the subjectivity of word senses", 0, "No title", "Available at http://www.cs.pitt.edu/\u02dcwiebe/."], ["We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.", "##CITATION##   adopt a semi-supervised mincut method to recognize the subjectivity of word senses", 0, "No title", "ABSTRACT"], ["However, such word-based indicators can be misleading for two reasons. First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications  . Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive\u2014having a positive electric charge;\u201cprotons are positive\u201d (objective) (2) plus, positive\u2014involving advantage or good; \u201ca plus (or positive) factor\u201d (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We   find that over 30% of words in our dataset are subjectivity-ambiguous.", "##CITATION##   adopt a semi-supervised mincut method to recognize the subjectivity of word senses", 0, "No title", "Introduction"], ["in the sentence \u201cThis deal is a positive development for our company.\u201d gives a strong indication that 1 All examples in this paper are from WordNet 2.0.. 1 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1\u20139, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.", "##CITATION##   adopt a semi-supervised mincut method to recognize the subjectivity of word senses", 1, "No title", "Introduction"], [" . Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani   determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work   by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative).", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary Valitutti et al An- dreevskaia and Bergler Wiebe and Mihalcea Esuli and Sebastiani ##CITATION##", 0, "No title", "Related Work."], ["However, such word-based indicators can be misleading for two reasons. First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications  . Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive\u2014having a positive electric charge;\u201cprotons are positive\u201d (objective) (2) plus, positive\u2014involving advantage or good; \u201ca plus (or positive) factor\u201d (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We   find that over 30% of words in our dataset are subjectivity-ambiguous.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary Valitutti et al An- dreevskaia and Bergler Wiebe and Mihalcea Esuli and Sebastiani ##CITATION##", 0, "No title", "Introduction"], ["At the word level Takamura et al.   use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea   we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary Valitutti et al An- dreevskaia and Bergler Wiebe and Mihalcea Esuli and Sebastiani ##CITATION##", 0, "No title", "Related Work."], ["We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.", "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary Valitutti et al An- dreevskaia and Bergler Wiebe and Mihalcea Esuli and Sebastiani ##CITATION##", 1, "No title", "ABSTRACT"], ["The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus   is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek,   is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm   can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models ##CITATION## ##CITATION## Rabiner though our zeroth order solution begins quite close to the desired result so it should converge very close to a global optimum", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The \"augmented network\" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}). Training a hidden Markov model having this topology corrected all nine instances of the error in the test data. An important point to note is that improving the model detail in this manner does not forcibly correct the error. The actual patterns of category usage must be distinct in the language.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models ##CITATION## ##CITATION## Rabiner though our zeroth order solution begins quite close to the desired result so it should converge very close to a global optimum", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models ##CITATION## ##CITATION## Rabiner though our zeroth order solution begins quite close to the desired result so it should converge very close to a global optimum", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This has the great advantage of eliminating the pre-tagged corpus. It minimizes the resources required, facilitates experimentation with different word categories, and is easily adapted for use with other languages. The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used  .", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models ##CITATION## ##CITATION## Rabiner though our zeroth order solution begins quite close to the desired result so it should converge very close to a global optimum", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled.", "In [##CITATION##] networks are used to selectively augment the context in a basic first- order model rather than using uniformly second-order dependencies", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The \"augmented network\" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}). Training a hidden Markov model having this topology corrected all nine instances of the error in the test data.", "In [##CITATION##] networks are used to selectively augment the context in a basic first- order model rather than using uniformly second-order dependencies", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Unless transition probabilities are highly constraining, the higher probability paths will tend to go through the to-infstate. This situation may be addressed in several ways, the simplest being to initially assign zero transition probabilities from the to-infstate to states other than verbs and the adverb state. ADJECTIVE DETERMINER To all states NOUN in Basic Network \"Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network. The lexical context available for modeling a word's category is solely the category of the preceding word (expressed via the transition probabilities P(Ci [ Ci1). Such limited context does not adequately model the constraint present in local word context.", "In [##CITATION##] networks are used to selectively augment the context in a basic first- order model rather than using uniformly second-order dependencies", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This problem could be resolved by tying corresponding transitions together. Alternatively, investigation of a trainable grammar   may be a fruitful way to further develop the model in terms of grammatical components. A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words). A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary. In the document, 142 words were tagged as unknown (their possible categories were not known).", "In [##CITATION##] networks are used to selectively augment the context in a basic first- order model rather than using uniformly second-order dependencies", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Alternatively, investigation of a trainable grammar   may be a fruitful way to further develop the model in terms of grammatical components. A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words). A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary. In the document, 142 words were tagged as unknown (their possible categories were not known). A total of 1,526 words had ambiguous categories (i.e. 40% of the document).", "In [##CITATION##] networks are used to selectively augment the context in a basic first- order model rather than using uniformly second-order dependencies", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus   is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek,   is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm   can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus.", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens [##CITATION##]", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge.", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens [##CITATION##]", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The basic network cannot model the dependency of the number of the verb on its subject, which precedes it by a prepositional phrase. To model such dependency across the phrase, the networks shown in Figure 2 can be used. It can be seen that only simple forms of prepositional phrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/or determiner. The final transitions in the networks serve to discriminate between the correct and incorrect category assignment given the selected preceding context. As in the previous section, the corrections are not programmed into the model.", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens [##CITATION##]", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens [##CITATION##]", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens [##CITATION##]", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail. The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network.", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens [##CITATION##]", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus   is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek,   is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm   can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text The effectiveness of such models is well known DeRose Church ##CITATION## Jelinek and they are currently in use in parsers eg de Mar\u00ad cken ", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled. The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed \"category\"). Application areas include speech recognition/synthesis and information retrieval. Several workers have addressed the problem of tagging text.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text The effectiveness of such models is well known DeRose Church ##CITATION## Jelinek and they are currently in use in parsers eg de Mar\u00ad cken ", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used  . There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text The effectiveness of such models is well known DeRose Church ##CITATION## Jelinek and they are currently in use in parsers eg de Mar\u00ad cken ", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.", "We report in Section 2 on our experiments on the assignment of part of speech to words in text The effectiveness of such models is well known DeRose Church ##CITATION## Jelinek and they are currently in use in parsers eg de Mar\u00ad cken ", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.", "##CITATION##  has experimented with the inclusion of networks to model mixed-order dependencies", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled.", "##CITATION##  has experimented with the inclusion of networks to model mixed-order dependencies", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["\"Temperatures in the upper mantle range apparently from....\". 2. \"The velocity of the seismic waves rises to...\". The basic model tagged these sentences correctly, except for- \"range\" and \"rises\" which were tagged as noun and plural-noun respectively 1. The basic network cannot model the dependency of the number of the verb on its subject, which precedes it by a prepositional phrase.", "##CITATION##  has experimented with the inclusion of networks to model mixed-order dependencies", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.", "##CITATION##  has experimented with the inclusion of networks to model mixed-order dependencies", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail. The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network. As an example, we may consider a systematic error made by the basic model.", "##CITATION##  has experimented with the inclusion of networks to model mixed-order dependencies", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network. As an example, we may consider a systematic error made by the basic model. It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1.", "The vocabulary entry may be a word or an equivalence class based on categories  ", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["State-dependent probabilities of the form P(Wi = Wa ] Ci = cz) represent the probability that word Wa is seen, given category c~. For instance, the word \"dog\" can be seen in the states noun and verb, and only has a nonzero probability in those states. A word sequence is considered as being generated from an underlying sequence of categories. Of all the possible category sequences from which a given word sequence can be generated, the one which maximizes the probability of the words is used. The Viterbi algorithm   will find this category sequence.", "The vocabulary entry may be a word or an equivalence class based on categories  ", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The basic network is then augmented with the extra state sequences which model certain category sequences in more detail. The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network. As an example, we may consider a systematic error made by the basic model. It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective.", "The vocabulary entry may be a word or an equivalence class based on categories  ", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used  . There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun.", "The vocabulary entry may be a word or an equivalence class based on categories  ", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used  . There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.", "The vocabulary entry may be a word or an equivalence class based on categories  ", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["A stochastic method for assigning part-of-speech categories to unrestricted English text has been described. It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text. It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters. I would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributions to this work.", "In a practical tagger ##CITATION## only the most frequent 100 words are lexicalized", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This indicates that modal auxiliaries can be recognized as a natural class via their pattern of usage. Extending the Basic Model The basic model was used as a benchmark for successive improvements. The first addition was the correct treatment of all non-words in a text. This includes hyphenation, punctuation, numbers and abbreviations. New categories were added for number, abbreviation, and comma.", "In a practical tagger ##CITATION## only the most frequent 100 words are lexicalized", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.", "In a practical tagger ##CITATION## only the most frequent 100 words are lexicalized", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["For example, many noun/verb ambiguities in front of past participles were incorrectly tagged as verbs. The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably. In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably. The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories. This leaves 50% of the corpus for training all the other equivalence classes.", "In a practical tagger ##CITATION## only the most frequent 100 words are lexicalized", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably. In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably. The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories. This leaves 50% of the corpus for training all the other equivalence classes. Editing the Transition Structure A common error in the basic model was the assignment of the word \"to\" to the to-infcategory (\"to\" acting as an infinitive marker) instead of preposition before noun phrases.", "In a practical tagger ##CITATION## only the most frequent 100 words are lexicalized", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious. In cases where the choice of tag was unclear (as often occurs in idioms), the tag was ruled as incorrect. For example, 9 errors are from 3 instances of \"... as well as ...\" that arise in the text. It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson  . Typical errors beyond the scope of the model described here are exemplified by incorrect adverbial and prepositional assignment.", "The parameters of the model can be estimated from tagged 1 3 4 6 12] or untagged [2 9 11] text", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The Brown Corpus   is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek,   is to view the training problem in terms of a \"hidden\" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm   can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus. It minimizes the resources required, facilitates experimentation with different word categories, and is easily adapted for use with other languages.", "The parameters of the model can be estimated from tagged 1 3 4 6 12] or untagged [2 9 11] text", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text. It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters. I would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributions to this work. I am also indebted to Sheldon Nicholl of the Univ. of Illinois, for his comments and valuable insight.", "The parameters of the model can be estimated from tagged 1 3 4 6 12] or untagged [2 9 11] text", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.", "The parameters of the model can be estimated from tagged 1 3 4 6 12] or untagged [2 9 11] text", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious. In cases where the choice of tag was unclear (as often occurs in idioms), the tag was ruled as incorrect. For example, 9 errors are from 3 instances of \"... as well as ...\" that arise in the text. It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson  . Typical errors beyond the scope of the model described here are exemplified by incorrect adverbial and prepositional assignment.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging as\u00ad signing each word in an input sentence its proper part of speech 1 2 3 4 6 9 11 12]", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["As an example, we may consider a systematic error made by the basic model. It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment \"The period of...\", where \"period\" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The \"augmented network\" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}).", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging as\u00ad signing each word in an input sentence its proper part of speech 1 2 3 4 6 9 11 12]", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["For example, \"dog\" is more likely to be a noun than a verb and \"see\" is more likely to be a verb than a noun. However they are both members of the equivalence class noun-or-verb, and so are considered to behave identically. It is then local word context (embodied in the transition probabilities) which must aid disambiguation of the word. In practice, word context provides significant constraint, so the trade-off appears to be a remarkably favorable one. The Basic Model The development of the model was guided by evaluation against a simple basic model (much of the development of the model was prompted by an analysis of the errors in its hehaviour).", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging as\u00ad signing each word in an input sentence its proper part of speech 1 2 3 4 6 9 11 12]", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The tagger could be extended by further category refinements (e.g. inclusion of a gerund category), and the single pronoun category currently causes erroneous tags for adjacent words. With respect to the problem of unknown words, alternative category assignments for them could be made by using the context embodied in transition probabilities. A stochastic method for assigning part-of-speech categories to unrestricted English text has been described. It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.", "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging as\u00ad signing each word in an input sentence its proper part of speech 1 2 3 4 6 9 11 12]", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.", "Instead only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes ##CITATION## such that all words that can function as a particular set of part-of-speech categori~ are given the same label", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "ABSTRACT"], ["The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi  . By exploiting the fact that the matrix of probabilities P(Eqvi I Ci) is sparse, a considerable improvement can be gained over the basic training algorithm in which iterations are made over all states. The initial values of the model parameters are calculated from word occurrence probabilities, such that words are initially assumed to function equally probably as any of their possible categories. Superlative and comparative adjectives were collapsed into a single adjective category, to economize on the overall number of categories. (If desired, after tagging the finer category can be replaced).", "Instead only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes ##CITATION## such that all words that can function as a particular set of part-of-speech categori~ are given the same label", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data. Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci). In the 21 category model reported in Kupiec   only 129 equivalence classes were required to cover a 30,000 word dictionary. In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model. Obviously, a trade-off is involved.", "Instead only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes ##CITATION## such that all words that can function as a particular set of part-of-speech categori~ are given the same label", 0, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used  . There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun.", "Instead only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes ##CITATION## such that all words that can function as a particular set of part-of-speech categori~ are given the same label", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used  . There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words \"play\" and \"touch\" are considered to behave identically, as members of the class noun-or-verb, and \"clay\" and \"zinc\"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.", "Instead only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes ##CITATION## such that all words that can function as a particular set of part-of-speech categori~ are given the same label", 1, "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging", "Introduction"], ["Of all the four systems, this one has access to the most information: contextual information (the words and tags in a window spanning three positions before and after the focus word) as well as lexical information (the existence of words formed by suffix/prefix addition/deletion). However, the actual use of this information is severely limited in that the individual information items can only be combined according to the patterns laid down in the rule templates. The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory). During the training phase, cases containing information about the word, the context and the correct tag are stored in memory. During tagging, the case most similar to that of the focus word is retrieved from the memory, which is indexed on the basis of the Information Gain of each feature, and the accompanying tag is selected.", "And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  . Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Now there are more varied systems available, a variety which we hope will lead to better combination effects. For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies. The Viterbi algorithm is used to determine the most probable tag sequence.", "And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "And nally TAGPAIR uses classication pair weights based on the probability of a classication for some predicted classication pair van Halteren et al", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  . Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.", "Like ##CITATION##   we evaluated two features combinations ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types. Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate. Here we use a slight adaptation of the tagset. The changes are mainly cosmetic, e.g. non-alphabetic characters such as \"$\" in tag names have been replaced. However, there has also been some retokenization: genitive markers have been split off and the negative marker \"n't\" has been reattached.", "Like ##CITATION##   we evaluated two features combinations ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["First, the combined votes will make the system more robust to the quirks of each learner's particular bias. Also, the use of information about each individual method's behaviour in principle even admits the possibility to fix collective errors. We will execute our investigation by means of an experiment. The NLP task used in the experiment is morpho-syntactic wordclass tagging. The reasons for this choice are several.", "Like ##CITATION##   we evaluated two features combinations ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf.", "Like ##CITATION##   we evaluated two features combinations ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["It shows that for 99.22% of Tune, at least one tagger selects the correct tag. However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging. We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation. All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune. The patterns between the brackets give the distribution of correct/incorrect tags over the systems.", "Like ##CITATION##   we evaluated two features combinations ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "##CITATION##  introduce a modied version of voting called TagPair Under this model the conditional probability that the word sense is s given that classier ioutputs sand classier joutputs s2 Psls ixd=ss jxd=s2 is computed on development data and the posterior probability is estimated as N PslxdessAkxd+ssA jxd 7 k j where sc;jxfd=argmaxtPtlsc;xfdfscjxfd Each classier votes for its classication and every pair of classiers votes for the sense that is most likely given the joint classication In the experiments presented in van Halteren et al  this method was the best performer among the presented methods      ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic. Recent work   has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven). Data driven methods appear to be the more popular. This can be explained by the fact that, in general, hand crafting an explicit model is rather difficult, especially since what is being modelled, natural language, is not (yet) well- understood.", "##CITATION##  introduce a modied version of voting called TagPair Under this model the conditional probability that the word sense is s given that classier ioutputs sand classier joutputs s2 Psls ixd=ss jxd=s2 is computed on development data and the posterior probability is estimated as N PslxdessAkxd+ssA jxd 7 k j where sc;jxfd=argmaxtPtlsc;xfdfscjxfd Each classier votes for its classication and every pair of classiers votes for the sense that is most likely given the joint classication In the experiments presented in van Halteren et al  this method was the best performer among the presented methods      ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited. A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors. In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.", "##CITATION##  introduce a modied version of voting called TagPair Under this model the conditional probability that the word sense is s given that classier ioutputs sand classier joutputs s2 Psls ixd=ss jxd=s2 is computed on development data and the posterior probability is estimated as N PslxdessAkxd+ssA jxd 7 k j where sc;jxfd=argmaxtPtlsc;xfdfscjxfd Each classier votes for its classication and every pair of classiers votes for the sense that is most likely given the joint classication In the experiments presented in van Halteren et al  this method was the best performer among the presented methods      ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "##CITATION##  introduce a modied version of voting called TagPair Under this model the conditional probability that the word sense is s given that classier ioutputs sand classier joutputs s2 Psls ixd=ss jxd=s2 is computed on development data and the posterior probability is estimated as N PslxdessAkxd+ssA jxd 7 k j where sc;jxfd=argmaxtPtlsc;xfdfscjxfd Each classier votes for its classication and every pair of classiers votes for the sense that is most likely given the joint classication In the experiments presented in van Halteren et al  this method was the best performer among the presented methods      ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "##CITATION##  introduce a modied version of voting called TagPair Under this model the conditional probability that the word sense is s given that classier ioutputs sand classier joutputs s2 Psls ixd=ss jxd=s2 is computed on development data and the posterior probability is estimated as N PslxdessAkxd+ssA jxd 7 k j where sc;jxfd=argmaxtPtlsc;xfdfscjxfd Each classier votes for its classication and every pair of classiers votes for the sense that is most likely given the joint classication In the experiments presented in van Halteren et al  this method was the best performer among the presented methods      ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely.", "##CITATION##  introduce a modied version of voting called TagPair Under this model the conditional probability that the word sense is s given that classier ioutputs sand classier joutputs s2 Psls ixd=ss jxd=s2 is computed on development data and the posterior probability is estimated as N PslxdessAkxd+ssA jxd 7 k j where sc;jxfd=argmaxtPtlsc;xfdfscjxfd Each classier votes for its classication and every pair of classiers votes for the sense that is most likely given the joint classication In the experiments presented in van Halteren et al  this method was the best performer among the presented methods      ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality.", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  .", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti.", "We consider three voting strategies suggested by van Halteren et al : equal vote where each classifier's vote is weighted equally overall accuracy where the weight depends on the overall accuracy of a classifier and pair'wise voting ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "Halteren et al  compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["1998). Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996).", "Halteren et al  compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority).", "Halteren et al  compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).", "Halteren et al  compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.", "Thirdly this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance cf van Halteren et al who found that combining the results of several part of speech taggers increased performance ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem. 9Tags (Memory-Based) scores significantly worse than TagPair (p=0.0274) and not significantly better than Precision-Recall (p=0.2766). 1Tags+Word could not be handled by C5.0 due to the huge number of feature values.", "Thirdly this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance cf van Halteren et al who found that combining the results of several part of speech taggers increased performance ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "Thirdly this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance cf van Halteren et al who found that combining the results of several part of speech taggers increased performance ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction). Conclusion. Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems. Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf. Ratnaparkhi 1996 on such effects in the Penn Treebank corpus).", "Thirdly this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance cf van Halteren et al who found that combining the results of several part of speech taggers increased performance ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf.", "Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11). After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone. A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608.", "Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.", "Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune.", "Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.", "Combination techniques have been successfully applied to part of speech tagging van Halteren et al Brill and Wu van Halteren et al In both cases the investigators were able to achieve significant improvements over the previous best tagging results ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority).", "##CITATION##   have generalized this approach for higher number of classifiers in their TotPrecision voting method The vote of each classifier parser is weighted by their respective accuracy  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors. In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers. It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems  . The underlying assumption is twofold.", "##CITATION##   have generalized this approach for higher number of classifiers in their TotPrecision voting method The vote of each classifier parser is weighted by their respective accuracy  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune). Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote.", "##CITATION##   have generalized this approach for higher number of classifiers in their TotPrecision voting method The vote of each classifier parser is weighted by their respective accuracy  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "##CITATION##   have generalized this approach for higher number of classifiers in their TotPrecision voting method The vote of each classifier parser is weighted by their respective accuracy  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%).", "##CITATION##   have generalized this approach for higher number of classifiers in their TotPrecision voting method The vote of each classifier parser is weighted by their respective accuracy  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "##CITATION##   have generalized this approach for higher number of classifiers in their TotPrecision voting method The vote of each classifier parser is weighted by their respective accuracy  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5   for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem.", "Parallel to   we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["is usually called stacking  . The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).", "Parallel to   we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "Parallel to   we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune.", "Parallel to   we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.", "Parallel to   we ran experiments with two stacked classifiers Memory-Based and Decision-Tree-Based", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.", "In all experiments the TotPrecision voting scheme of   has been used", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision). 7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "In all experiments the TotPrecision voting scheme of   has been used", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Also, the use of information about each individual method's behaviour in principle even admits the possibility to fix collective errors. We will execute our investigation by means of an experiment. The NLP task used in the experiment is morpho-syntactic wordclass tagging. The reasons for this choice are several. First of all, tagging is a widely researched and well-understood task (cf.", "In all experiments the TotPrecision voting scheme of   has been used", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["is usually called stacking  . The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).", "In all experiments the TotPrecision voting scheme of   has been used", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5   for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare.", "In all experiments the TotPrecision voting scheme of   has been used", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data Tag\u00ad Pair ##CITATION## ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data Tag\u00ad Pair ##CITATION## ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  .", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data Tag\u00ad Pair ##CITATION## ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data Tag\u00ad Pair ##CITATION## ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data Tag\u00ad Pair ##CITATION## ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies. The Viterbi algorithm is used to determine the most probable tag sequence. Since this model has no facilities for handling unknown words, a Memory-Based system (see below) is used to propose distributions of potential tags for words not in the lexicon. The second system is the Transformation Based Learning system as described by Brill (19941; henceforth tagger R, for Rules).", "We will evaluate nine different methods for combining the output of our five chunkers   Five are so-called voting methods They assign weights to the output of the individual systems and use these weights to determine the most probable output tag  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Both this system and Brill's system are used with the default settings that are suggested in their documentation. 2Ratnaparkhi's Java implementation of this system is available at ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/ The data we use for our experiment consists of the tagged LOB corpus  . The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types. Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate.", "We will evaluate nine different methods for combining the output of our five chunkers   Five are so-called voting methods They assign weights to the output of the individual systems and use these weights to determine the most probable output tag  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision). 7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "We will evaluate nine different methods for combining the output of our five chunkers   Five are so-called voting methods They assign weights to the output of the individual systems and use these weights to determine the most probable output tag  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["tag in each case. We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune). Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2.", "We will evaluate nine different methods for combining the output of our five chunkers   Five are so-called voting methods They assign weights to the output of the individual systems and use these weights to determine the most probable output tag  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune.", "We will evaluate nine different methods for combining the output of our five chunkers   Five are so-called voting methods They assign weights to the output of the individual systems and use these weights to determine the most probable output tag  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited. Now there are more varied systems available, a variety which we hope will lead to better combination effects. For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if \"as well as\" is taken to be a coordination conjunction, it is tagged \"as_CC1 well_CC2 as_CC3\", using three related but different ditto tags. by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision). 7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking  . The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.", "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic.", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf. Table 2 below) certainly still leaves room for improvement, although tagger E surprises us with an accuracy well above any results reported so far and makes us less confident about the gain to be accomplished with combination.", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5   for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%).", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf. Ratnaparkhi 1996 on such effects in the Penn Treebank corpus). Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages. But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements. Our thanks go to the creators of the tagger generators used here for making their systems available.", "First experiments van Halteren Zavrel and Daelemans Brill and Wu demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191% lower than that of the best individual tagger van Halteren Zavrel and Daelemans However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans we were unable to confirm the latter half of the hypothesis unequivocally  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608. 12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination.", "Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans   This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners   ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags. The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.", "Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans   This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners   ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).", "Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans   This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners   ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.", "Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans   This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners   ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging. We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation. All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune. The patterns between the brackets give the distribution of correct/incorrect tags over the systems. tag in each case.", "Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans   This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners   ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5   for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.", "Furthermore the Wotan tagset is a very detailed one so that the error rate of the individual taggers 9 Compare this to the \"tune\" set in van Halteren Zavrel and Daelemans   This consisted of 114K tokens but because of a 925% agreement over all four taggers it yielded less than 9K tokens of useful training material to resolve disagreements This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners   ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The difficulty of the tagging task can be judged by the two baseline measurements in Table 2 below, representing a completely random choice from the potential tags for each token (Random) and selection of the lexically most likely tag (LexProb). For our experiment, we divide the corpus into three parts. The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if \"as well as\" is taken to be a coordination conjunction, it is tagged \"as_CC1 well_CC2 as_CC3\", using three related but different ditto tags. by taking the first eight utterances of every ten. This part is used to train the individual tag- gers.", "For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans   and Brill and Wu   In both approaches different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans   to WSJ as well it is easier to make a comparison  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf.", "For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans   and Brill and Wu   In both approaches different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans   to WSJ as well it is easier to make a comparison  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf. Table 2 below) certainly still leaves room for improvement, although tagger E surprises us with an accuracy well above any results reported so far and makes us less confident about the gain to be accomplished with combination.", "For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans   and Brill and Wu   In both approaches different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans   to WSJ as well it is easier to make a comparison  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.", "For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans   and Brill and Wu   In both approaches different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans   to WSJ as well it is easier to make a comparison  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations.", "For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans   and Brill and Wu   In both approaches different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans   to WSJ as well it is easier to make a comparison  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.", "For part-of-speech tagging a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren Zavrel and Daelemans   and Brill and Wu   In both approaches different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods including stacking As we now apply the methods of van Halteren Zavrel and Daelemans   to WSJ as well it is easier to make a comparison  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality. Obviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking  . The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking  . The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  . Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "One of the best methods for tagger combination in   is the TagPair method It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx Although it is presented as a variant of voting in that paper it is in fact also a stacked classifier because it does not necessarily select one of the tags suggested by the component taggers  ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11). After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone. A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608. 12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant.", "The most important result that has undergone a change between van Halteren Zavrel and Daelemans   and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["1Tags+Word could not be handled by C5.0 due to the huge number of feature values. Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination.", "The most important result that has undergone a change between van Halteren Zavrel and Daelemans   and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers. It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems  . The underlying assumption is twofold. First, the combined votes will make the system more robust to the quirks of each learner's particular bias.", "The most important result that has undergone a change between van Halteren Zavrel and Daelemans   and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed ", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely.", "The most important result that has undergone a change between van Halteren Zavrel and Daelemans   and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  . Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "The most important result that has undergone a change between van Halteren Zavrel and Daelemans   and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL Where TagPair used to be significantly better than MBL the roles are now well reversed ", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags. The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement. In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.", "The first is the LOB corpus   which we used in the earlier experiments as well   and which has proved to be a good testing ground", 0, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).", "The first is the LOB corpus   which we used in the earlier experiments as well   and which has proved to be a good testing ground", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  . Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.", "The first is the LOB corpus   which we used in the earlier experiments as well   and which has proved to be a good testing ground", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Both this system and Brill's system are used with the default settings that are suggested in their documentation. 2Ratnaparkhi's Java implementation of this system is available at ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/ The data we use for our experiment consists of the tagged LOB corpus  . The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types. Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate.", "The first is the LOB corpus   which we used in the earlier experiments as well   and which has proved to be a good testing ground", 1, "Improving Data Driven Wordclass Tagging by System Combination", "The data."], ["We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune). Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote.", "In van Halteren Zavrel and Daelemans   we used a straightforward im\u00ad plementation of HMM's which turned out to have the worst accuracy of the four competing methods", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.", "In van Halteren Zavrel and Daelemans   we used a straightforward im\u00ad plementation of HMM's which turned out to have the worst accuracy of the four competing methods", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited. Now there are more varied systems available, a variety which we hope will lead to better combination effects. For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.", "In van Halteren Zavrel and Daelemans   we used a straightforward im\u00ad plementation of HMM's which turned out to have the worst accuracy of the four competing methods", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.", "In van Halteren Zavrel and Daelemans   we used a straightforward im\u00ad plementation of HMM's which turned out to have the worst accuracy of the four competing methods", 1, "Improving Data Driven Wordclass Tagging by System Combination", "ABSTRACT"], ["When abstracting away from individual tags, precision and recall are equal and measure how many tokens are tagged correctly; in this case we also use the more generic term accuracy. 6In our experiment, a random selection from among the winning tags is made whenever there is a tie. Table 2: Accuracy of individual taggers and combination methods. But we have even more information on how well the taggers perform. We not only know whether we should believe what they propose (precision) but also know how often they fail to recognize the correct tag (recall).", "In van Halteren Zavrel and Daelemans   we used a straightforward im\u00ad plementation of HMM's which turned out to have the worst accuracy of the four competing methods", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["Obviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited.", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Introduction"], ["In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11). After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone. A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608.", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 0, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["is usually called stacking  . The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting  . Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5   for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare.", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5   for the induction of decision trees, on the same training material. 1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem.", "With LOB and a single 114K tune set   both MBL and Decision Trees degraded significantly when adding context and MBL degraded when adding the word24 With the increased amount of training material addition of the context gener\u00ad ally leads to better results", 1, "Improving Data Driven Wordclass Tagging by System Combination", "Potential for improvement."], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  . In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew  , we demonstrated that accurate subcategorization statistics are unnecessary  . By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages.", "Merlo and Stevensonand ##CITATION##for English semantic verb classes or Schulte im Waldefor German semantic verb classes", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew   and Schulte im Walde  , on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde   range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "Merlo and Stevensonand ##CITATION##for English semantic verb classes or Schulte im Waldefor German semantic verb classes", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes. Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks. These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions. For example, some classes differ in both their semantic roles and frames, while others have the same roles in different frames, or different roles in the same frames.1 Here we summarize the argument structure distinctions between the classes; Table 1 below lists the classes with their Levin class numbers.", "Merlo and Stevensonand ##CITATION##for English semantic verb classes or Schulte im Waldefor German semantic verb classes", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles  , selectional preferences  , and lexical semantic classification  . Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results  , on the hand- selection of features  , or on the use of an extensive grammar  . We focus here on extending the applicability of unsupervised methods, as in  , to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics  . As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon  .", "Merlo and Stevensonand ##CITATION##for English semantic verb classes or Schulte im Waldefor German semantic verb classes", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be \u201ceasier\u201d than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together. It is a question for future research to explore the effect of these variables in clustering performance. We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried   did not prove useful, because of the problem of having no consistent threshold for feature inclusion.", "Supervised methods for automatic verb classification have been extensively investigated Stevenson et al Stevenson and Merlo Merlo and Stevenson ##CITATION## Joanis and Stevenson Joanis et al", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others. This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach. Using the same measure as ours, Stevenson and Merlo   achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.", "Supervised methods for automatic verb classification have been extensively investigated Stevenson et al Stevenson and Merlo Merlo and Stevenson ##CITATION## Joanis and Stevenson Joanis et al", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson  . We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases  . We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson   to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "Supervised methods for automatic verb classification have been extensively investigated Stevenson et al Stevenson and Merlo Merlo and Stevenson ##CITATION## Joanis and Stevenson Joanis et al", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon  . However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise. Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  .", "Supervised methods for automatic verb classification have been extensively investigated Stevenson et al Stevenson and Merlo Merlo and Stevenson ##CITATION## Joanis and Stevenson Joanis et al", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["This performance comparison tentatively suggests that good feature selection can be helpful in our task. However, it is important to find a method that does not depend on having an existing classification, since we are interested in applying the approach when such a classification does not exist. In the next two sections, we present unsupervised and minimally supervised approaches to this problem. 5.3 Unsupervised Feature Selection. In order to deal with excessive dimensionality, Dash et al.", "The present work inherits the spirit of the supervised approaches to verb classification  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task. We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.", "The present work inherits the spirit of the supervised approaches to verb classification  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "ABSTRACT"], ["In addition to approximating the syntactic frames themselves, we also want to capture regularities in the mapping of arguments to particular slots. For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck. These allowable alternations in the expressions of arguments vary according to the class of a verb. We measure this behaviour using features that encode the degree to which two slots contain the same entities\u2014that is, we calculate the overlap in noun (lemma) usage between pairs of syntactic slots. Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect  .", "The present work inherits the spirit of the supervised approaches to verb classification  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  . In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "The present work inherits the spirit of the supervised approaches to verb classification  ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  . To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering AGG \u2013 which has been previously used to acquire flat Levin-style classifications ##CITATION## as well as hierarchical verb classifications not based on Levin Fer- rer Schulte im Walde", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains. Since it is a general corpus, we do not expect any strong overall domain bias in verb usage. We used the chunker   to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it. Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame. From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering AGG \u2013 which has been previously used to acquire flat Levin-style classifications ##CITATION## as well as hierarchical verb classifications not based on Levin Fer- rer Schulte im Walde", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["These figures are reported with our results in Table 2 below. 4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good. Our second measure, the adjusted Rand measure used by Schulte im Walde  , instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification. The formula is as follows  : where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between them\u2014 there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not. It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering AGG \u2013 which has been previously used to acquire flat Levin-style classifications ##CITATION## as well as hierarchical verb classifications not based on Levin Fer- rer Schulte im Walde", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments. 4.1 Clustering Parameters. We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. In performing hierarchical clustering, both a vector distance measure and a cluster distance (\u201clinkage\u201d) measure are specified. We used the simple Euclidean distance for the former, and Ward linkage for the latter.", "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering AGG \u2013 which has been previously used to acquire flat Levin-style classifications ##CITATION## as well as hierarchical verb classifications not based on Levin Fer- rer Schulte im Walde", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["We used the chunker   to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it. Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame. From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments. 4.1 Clustering Parameters. We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.", "We used three gold standards and corresponding test sets extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  . To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data.", "We used three gold standards and corresponding test sets extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data. In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs. We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions.", "We used three gold standards and corresponding test sets extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin    . Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "We used three gold standards and corresponding test sets extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy  ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process.", "Following ##CITATION##   we selected 20 verbs from each class which occur at least 100 times in our corpus", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson  . We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "Following ##CITATION##   we selected 20 verbs from each class which occur at least 100 times in our corpus", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["In our domain in particular, verb class discovery \u201cin a vacuum\u201d is not necessary. A plausible scenario is that researchers would have examples of verbs which they believe fall into different classes of interest, and they want to separate other verbs along the same lines. To model this kind of approach, we selected a sample of five seed verbs from each class. Each set of verbs was judged (by the authors\u2019 intuition alone) to be \u201crepresentative\u201d of the class. We purposely did not carry out any linguistic analysis, although we did check that each verb was reasonably frequent (with log frequencies ranging from 2.6 to 5.1).", "Following ##CITATION##   we selected 20 verbs from each class which occur at least 100 times in our corpus", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs). All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments). 3.3 Feature Extraction. All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.", "Following ##CITATION##   we selected 20 verbs from each class which occur at least 100 times in our corpus", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases). Interestingly, was generally very high, indicating that there is structure in the data, but not what matches our classification. This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery. We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling). We might also ask, would any subset of verbs do as well?", "Previous works on Levin style verb classification have investigated optimal features for this task  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  . To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data.", "Previous works on Levin style verb classification have investigated optimal features for this task  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  . In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Previous works on Levin style verb classification have investigated optimal features for this task  ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin    . Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.", "Previous works on Levin style verb classification have investigated optimal features for this task  ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process.", "Previous works on Levin style verb classification have investigated optimal features for this task  ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods. We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes. To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy. In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff. However, we did experiment with  , and found that performance was generally better (even on our measure, described below, that discounts oversplitting).", "Although they can be removed using a cut-based method this requires a predefined cutoff value which is difficult to set   In addition a significant amount of information is lost in pairwise clustering ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["5.3 Unsupervised Feature Selection. In order to deal with excessive dimensionality, Dash et al.   propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).", "Although they can be removed using a cut-based method this requires a predefined cutoff value which is difficult to set   In addition a significant amount of information is lost in pairwise clustering ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried   did not prove useful, because of the problem of having no consistent threshold for feature inclusion. We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering. We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics. Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set.", "Although they can be removed using a cut-based method this requires a predefined cutoff value which is difficult to set   In addition a significant amount of information is lost in pairwise clustering ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes. To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy. In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff. However, we did experiment with  , and found that performance was generally better (even on our measure, described below, that discounts oversplitting). This supports our intuition that the approach may enable us to find more consistent clusters at a finer grain, without too much fragmentation.", "Although they can be removed using a cut-based method this requires a predefined cutoff value which is difficult to set   In addition a significant amount of information is lost in pairwise clustering ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.", "Table 1: Comparison against ##CITATION##  \u2019s result on T1 using similar features", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "PAPER"], ["We use this baseline in calculating reductions in error rate of . The remaining columns of the table give the , , and measures as described in Section 4.2, for each of the feature sets we explored in clustering, which we discuss in turn below. 5.1 Full Feature Set. The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection). Although generally higher than the baseline, is well below that of the supervised learner, and and are generally low. 5.2 Manual Feature Selection.", "Table 1: Comparison against ##CITATION##  \u2019s result on T1 using similar features", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["5.5 Further Discussion. In our clustering experiments, we find that smaller subsets of features generally perform better than the full set of features. (See Table 3 for the number of features in the Ling and Seed sets.) However, not just any small set of features is adequate. We ran 50 experiments using randomly selected sets of features of cardinality , where 5We also tried directly applying the mutual information  .", "Table 1: Comparison against ##CITATION##  \u2019s result on T1 using similar features", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner. Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  . To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.", "Table 1: Comparison against ##CITATION##  \u2019s result on T1 using similar features", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson  . We began with this same set of 20 verbs per class for our current work.", "Table 1 shows our results and the results of ##CITATION##   on T1 when employing AGG using Ward as the linkage criterion", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["We use this baseline in calculating reductions in error rate of . The remaining columns of the table give the , , and measures as described in Section 4.2, for each of the feature sets we explored in clustering, which we discuss in turn below. 5.1 Full Feature Set. The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection). Although generally higher than the baseline, is well below that of the supervised learner, and and are generally low. 5.2 Manual Feature Selection.", "Table 1 shows our results and the results of ##CITATION##   on T1 when employing AGG using Ward as the linkage criterion", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Essentially, the measure numerically captures what we can intuitively grasp in the visual differences between the dendrograms of \u201cbetter\u201d and \u201cworse\u201d clusterings. (A dendrogram is a tree diagram whose leaves are the data points, and whose branch lengths indicate similarity of subclusters; roughly, shorter vertical lines indicate closer clusters.) For example, Figure 1 shows two dendrograms using different feature sets (Ling and Seed, described in Section 5) for the same set of verbs from two classes. The Seed set has slightly lower values for and , but a much higher value (.89) for , indicating a better separation of the data. This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters.", "Table 1 shows our results and the results of ##CITATION##   on T1 when employing AGG using Ward as the linkage criterion", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  . To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data.", "Table 1 shows our results and the results of ##CITATION##   on T1 when employing AGG using Ward as the linkage criterion", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1. To calculate a random baseline, we evaluated 10,000 random clusterings with the same number of verbs and classes as in each of our experimental tasks. Because the achieved depends on the precise size of clusters, we calculated mean over the best scenario (with equal-sized clusters), yielding a conservative estimate (i.e., an upper bound) of the baseline. These figures are reported with our results in Table 2 below.", "In this experiment we used the same feature set as ##CITATION##   set B see section 31 and were therefore able to reproduce their AGG result with a difference smaller than 2%", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson  . We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases  . We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson   to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "In this experiment we used the same feature set as ##CITATION##   set B see section 31 and were therefore able to reproduce their AGG result with a difference smaller than 2%", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner. Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  . To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.", "In this experiment we used the same feature set as ##CITATION##   set B see section 31 and were therefore able to reproduce their AGG result with a difference smaller than 2%", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["1, 26. 3, 26. 7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2). 3.2 Verb Selection. Our experimental verbs were selected as follows.", "In this experiment we used the same feature set as ##CITATION##   set B see section 31 and were therefore able to reproduce their AGG result with a difference smaller than 2%", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["However, it is useful as a relative measure of good-. ness, in comparing clusterings arising from different feature sets. 4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative.", "For example the accuracy/purity measure ##CITATION## Korhonen Krymolowski and Marx evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["These figures are reported with our results in Table 2 below. 4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good. Our second measure, the adjusted Rand measure used by Schulte im Walde  , instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification. The formula is as follows  : where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between them\u2014 there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not. It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0.", "For example the accuracy/purity measure ##CITATION## Korhonen Krymolowski and Marx evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["We use three separate evaluation measures, that tap into very different properties of the clusterings. 4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be.", "For example the accuracy/purity measure ##CITATION## Korhonen Krymolowski and Marx evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1.", "For example the accuracy/purity measure ##CITATION## Korhonen Krymolowski and Marx evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["5.3 Unsupervised Feature Selection. In order to deal with excessive dimensionality, Dash et al.   propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).", "In recent work ##CITATION##   compared their supervised method for verb classification with semisupervised and unsupervised techniques In these experiments they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes with a total of 841 verbs Low- frequency and ambiguous verbs were excluded from the classes They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash Liu and Yao   which used an entropy measure to organize data into a multidimensional space   ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["  propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure). Many feature sets performed very well, and some far outperformed our best results using other feature selection methods. However, across our 10 experimental tasks, there was no consistent range of feature ranks or feature set sizes that was correlated with good performance.", "In recent work ##CITATION##   compared their supervised method for verb classification with semisupervised and unsupervised techniques In these experiments they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes with a total of 841 verbs Low- frequency and ambiguous verbs were excluded from the classes They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash Liu and Yao   which used an entropy measure to organize data into a multidimensional space   ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson  . We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "In recent work ##CITATION##   compared their supervised method for verb classification with semisupervised and unsupervised techniques In these experiments they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes with a total of 841 verbs Low- frequency and ambiguous verbs were excluded from the classes They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash Liu and Yao   which used an entropy measure to organize data into a multidimensional space   ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process.", "In recent work ##CITATION##   compared their supervised method for verb classification with semisupervised and unsupervised techniques In these experiments they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes with a total of 841 verbs Low- frequency and ambiguous verbs were excluded from the classes They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash Liu and Yao   which used an entropy measure to organize data into a multidimensional space   ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task. Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2. This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder. Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks. More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set.", "In recent work ##CITATION##   compared their supervised method for verb classification with semisupervised and unsupervised techniques In these experiments they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes with a total of 841 verbs Low- frequency and ambiguous verbs were excluded from the classes They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash Liu and Yao   which used an entropy measure to organize data into a multidimensional space   ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2. This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder. Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks. More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set. The increased reduction in error rate is particularly striking for the 2-way tasks, of 37% for the Seed set compared to 20% for the Ling set.", "In recent work ##CITATION##   compared their supervised method for verb classification with semisupervised and unsupervised techniques In these experiments they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes with a total of 841 verbs Low- frequency and ambiguous verbs were excluded from the classes They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash Liu and Yao   which used an entropy measure to organize data into a multidimensional space   ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["The higher (.89 vs. .33) reflects the better separation of the data. regard to the target classes. We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters. Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster). A value of 0 suggests that a point is not clearly in a particular cluster.", "Our second measure is derived from purity a global measure which evaluates the mean precision of the clusters weighted according to the cluster size ##CITATION##", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative. 3 In our experiments for estimating the baseline, we in-. deed found a mean value of 0.00 for all random clusterings.", "Our second measure is derived from purity a global measure which evaluates the mean precision of the clusters weighted according to the cluster size ##CITATION##", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["  propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure). Many feature sets performed very well, and some far outperformed our best results using other feature selection methods. However, across our 10 experimental tasks, there was no consistent range of feature ranks or feature set sizes that was correlated with good performance.", "Our second measure is derived from purity a global measure which evaluates the mean precision of the clusters weighted according to the cluster size ##CITATION##", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1.", "Our second measure is derived from purity a global measure which evaluates the mean precision of the clusters weighted according to the cluster size ##CITATION##", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["However, it is useful as a relative measure of good-. ness, in comparing clusterings arising from different feature sets. 4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative.", "Our second measure is derived from purity a global measure which evaluates the mean precision of the clusters weighted according to the cluster size ##CITATION##", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["5.3 Unsupervised Feature Selection. In order to deal with excessive dimensionality, Dash et al.   propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).", "For example ##CITATION##   report an accuracy of 29% which implies mP U R \u2264 29% but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well. Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies. Learning the argument structure properties of verbs\u2014the semantic roles they assign and their mapping to syntactic positions\u2014is both particularly important and difficult. A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles  , selectional preferences  , and lexical semantic classification  . Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results  , on the hand- selection of features  , or on the use of an extensive grammar  .", "For example ##CITATION##   report an accuracy of 29% which implies mP U R \u2264 29% but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["ness, in comparing clusterings arising from different feature sets. 4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative. 3 In our experiments for estimating the baseline, we in-.", "For example ##CITATION##   report an accuracy of 29% which implies mP U R \u2264 29% but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal\u2013Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal\u2013Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23. All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results. C5.0 is supervised accuracy; Base is on random clusters. Full is full feature set; Ling is manually selected subset; Seed is seed-verb-selected set.", "For example ##CITATION##   report an accuracy of 29% which implies mP U R \u2264 29% but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  . In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew  , we demonstrated that accurate subcategorization statistics are unnecessary  . By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages.", "For this reason various methods for automatically classifying verbs using machine learning techniques have been attempted Merlo and Stevenson ##CITATION## Schulte im Walde", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew   and Schulte im Walde  , on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde   range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "For this reason various methods for automatically classifying verbs using machine learning techniques have been attempted Merlo and Stevenson ##CITATION## Schulte im Walde", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be \u201ceasier\u201d than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together. It is a question for future research to explore the effect of these variables in clustering performance. We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried   did not prove useful, because of the problem of having no consistent threshold for feature inclusion.", "For this reason various methods for automatically classifying verbs using machine learning techniques have been attempted Merlo and Stevenson ##CITATION## Schulte im Walde", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles  , selectional preferences  , and lexical semantic classification  . Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results  , on the hand- selection of features  , or on the use of an extensive grammar  . We focus here on extending the applicability of unsupervised methods, as in  , to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics  . As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon  .", "For this reason various methods for automatically classifying verbs using machine learning techniques have been attempted Merlo and Stevenson ##CITATION## Schulte im Walde", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set. As successful as our seed set of features is, it still does not achieve the accuracy of a supervised learner. More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering. Furthermore, we might question the clustering approach itself, in the context of verb class discovery. Rather than trying to separate a set of new verbs into coherent clusters, we suggest that it may be useful to perform a nearest-neighbour type of classification using a seed set, asking for each new verb \u201cis it like these or not?\u201d In some ways our current clustering task is too easy, because all of the verbs are from one of the target classes.", "##CITATION## investigate the applicability of this general feature space to unsupervised verb clustering tasks", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles  , selectional preferences  , and lexical semantic classification  . Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results  , on the hand- selection of features  , or on the use of an extensive grammar  . We focus here on extending the applicability of unsupervised methods, as in  , to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics  . As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon  .", "##CITATION## investigate the applicability of this general feature space to unsupervised verb clustering tasks", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["On the other hand, in contrast to Schulte im Walde and Brew  , we demonstrated that accurate subcategorization statistics are unnecessary  . By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages. However, a general feature space means that most features will be irrelevant to any given verb discrimination task. In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d? In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand.", "##CITATION## investigate the applicability of this general feature space to unsupervised verb clustering tasks", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages. However, a general feature space means that most features will be irrelevant to any given verb discrimination task. In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d? In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand. In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner.", "##CITATION## investigate the applicability of this general feature space to unsupervised verb clustering tasks", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["While we could have selected a threshold that might work reasonably well with our data, we would have little confidence that it would work well in general, considering the inconsistent pattern of results. 5.4 Semi-Supervised Feature Selection. Unsupervised methods such as Dash et al.\u2019s   are appealing because they require no knowledge external to the data. However, in many aspects of computational linguistics, it has been found that a small amount of labelled data contains sufficient information to allow us to go beyond the limits of completely unsupervised approaches. In our domain in particular, verb class discovery \u201cin a vacuum\u201d is not necessary.", "As an alternative to the resource-intensive manual classifications automatic methods such as classification and clustering are applied to induce verb classes from corpus data eg Merlo and Stevenson Joanis and Stevenson Korhonen et al ##CITATION## Schulte im Walde Fer- rer ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others. This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach. Using the same measure as ours, Stevenson and Merlo   achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.", "As an alternative to the resource-intensive manual classifications automatic methods such as classification and clustering are applied to induce verb classes from corpus data eg Merlo and Stevenson Joanis and Stevenson Korhonen et al ##CITATION## Schulte im Walde Fer- rer ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson  . We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases  . We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson   to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes.", "As an alternative to the resource-intensive manual classifications automatic methods such as classification and clustering are applied to induce verb classes from corpus data eg Merlo and Stevenson Joanis and Stevenson Korhonen et al ##CITATION## Schulte im Walde Fer- rer ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Classes and Verbs."], ["While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be \u201ceasier\u201d than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together. It is a question for future research to explore the effect of these variables in clustering performance. We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried   did not prove useful, because of the problem of having no consistent threshold for feature inclusion.", "As an alternative to the resource-intensive manual classifications automatic methods such as classification and clustering are applied to induce verb classes from corpus data eg Merlo and Stevenson Joanis and Stevenson Korhonen et al ##CITATION## Schulte im Walde Fer- rer ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics. Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set. As successful as our seed set of features is, it still does not achieve the accuracy of a supervised learner. More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering. Furthermore, we might question the clustering approach itself, in the context of verb class discovery.", "In larger-scale classifications such as   which model verb classes with similarity at the syntax-semantics interface it is not clear which features are the most salient", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["However, a general feature space means that most features will be irrelevant to any given verb discrimination task. In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d? In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand. In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner. Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.", "In larger-scale classifications such as   which model verb classes with similarity at the syntax-semantics interface it is not clear which features are the most salient", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Moreover, the value for the manually selected features is almost always very much higher than that of the full feature set, indicating that the subset of features is more focused on the properties that lead to a better separation of the data. This performance comparison tentatively suggests that good feature selection can be helpful in our task. However, it is important to find a method that does not depend on having an existing classification, since we are interested in applying the approach when such a classification does not exist. In the next two sections, we present unsupervised and minimally supervised approaches to this problem. 5.3 Unsupervised Feature Selection.", "In larger-scale classifications such as   which model verb classes with similarity at the syntax-semantics interface it is not clear which features are the most salient", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["We conclude with a discussion of related work, our contributions, and future directions. Like others, we have assumed lexical semantic classes of verbs as defined in Levin    . Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions. It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind.", "In larger-scale classifications such as   which model verb classes with similarity at the syntax-semantics interface it is not clear which features are the most salient", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "The Feature Space."], ["Using the same measure as ours, Stevenson and Merlo   achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew   and Schulte im Walde  , on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde   range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks.", "In larger-scale classifications such as   which model verb classes with similarity at the syntax-semantics interface it is not clear which features are the most salient", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried   did not prove useful, because of the problem of having no consistent threshold for feature inclusion. We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering. We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics. Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set.", "For the evaluation of the clustering results we calculated the accuracy of the clusters a cluster similarity measure that has been applied before cf  4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Conclusions and Future Work."], ["The higher (.89 vs. .33) reflects the better separation of the data. regard to the target classes. We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters. Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster). A value of 0 suggests that a point is not clearly in a particular cluster.", "For the evaluation of the clustering results we calculated the accuracy of the clusters a cluster similarity measure that has been applied before cf  4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Experimental Results."], ["Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods. We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes. To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy. In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff. However, we did experiment with  , and found that performance was generally better (even on our measure, described below, that discounts oversplitting).", "For the evaluation of the clustering results we calculated the accuracy of the clusters a cluster similarity measure that has been applied before cf  4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because ", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering\u2014that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1.", "For the evaluation of the clustering results we calculated the accuracy of the clusters a cluster similarity measure that has been applied before cf  4 Accuracy is determined in two steps: \u2022 GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because ", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Clustering and Evaluation Methods."], ["However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew   and Schulte im Walde  , on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde   range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Walde\u2019s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).", "Following a strategy in line with work on verb classification Merlo and Stevenson ##CITATION## we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others. This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach. Using the same measure as ours, Stevenson and Merlo   achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.", "Following a strategy in line with work on verb classification Merlo and Stevenson ##CITATION## we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Other Verb Clustering Work."], ["It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  . In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew  , we demonstrated that accurate subcategorization statistics are unnecessary  .", "Following a strategy in line with work on verb classification Merlo and Stevenson ##CITATION## we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus", 0, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  . In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).", "Following a strategy in line with work on verb classification Merlo and Stevenson ##CITATION## we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus", 1, "Semi-supervised Verb Class Discovery Using Noisy Features", "Introduction"], ["T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails?", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German Hinrichs et al in the Troll system ##CITATION##", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7]. 1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities. The a.lnount of dis- junction is a.lso kept small by the technique of unlilli,g described in [9]. This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in 4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies. As discussed iu Gerdemann ,~ King [8], one ca.n view a.pl}rol)ria.teness CO[lditions as (lelining GPSG style fea,1;tl re cooccurence restrict:ions (FCRs).", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German Hinrichs et al in the Troll system ##CITATION##", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "INTRODUCTION"], ["Type resolution, on the other hand, always considers species. Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F').", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German Hinrichs et al in the Troll system ##CITATION##", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German Hinrichs et al in the Troll system ##CITATION##", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,. Sag (rorthcoming) [14].. well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed. Unfortunately, well-typability is not sufficient to ensure that disjunctive FCRs are satisfied. Consider, For exam- pie, our encoding of the disjunctive FCR p and suppose that 99 is the fe, ature structure t[f : +,9 : -].", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where.", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch. !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "PAPER"], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7).", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Type resolution, on the other hand, always considers species. Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F').", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F'). Uni/ication of sets of fca.ture structures is defined here ill the standard way: S t2 ,S\" = {1\"[ I\"' 6 S and l\"\" G S\" and 1\" = 1\"' H 1\"\"}.", "Gerdemann and G6tz's Troll system see [G6Tz [GFRDEIVIANN AND KING and [GERDEMANN FC] employs an efficient refinement of RES to test the satisfiability of feature structures In fact Troll represents each feature structure as a disjunction of the resolvants of the feature structure Loosely speaking the resolvants of a feature structure have the same underlying finite state automaton as the feature structure and differ only in their output fllnction Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions The '1oll unifier is closed on these representations Thus though RES is computationally expensive Troll uses RES only during compilation never during run time     ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails?", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions ##CITATION## required by standard HPSG theories", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions ##CITATION## required by standard HPSG theories", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7]. 1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities. The a.lnount of dis- junction is a.lso kept small by the technique of unlilli,g described in [9]. This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in 4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies. As discussed iu Gerdemann ,~ King [8], one ca.n view a.pl}rol)ria.teness CO[lditions as (lelining GPSG style fea,1;tl re cooccurence restrict:ions (FCRs).", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions ##CITATION## required by standard HPSG theories", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "INTRODUCTION"], ["a.n N)propria.teness specification. Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7]. 1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities. The a.lnount of dis- junction is a.lso kept small by the technique of unlilli,g described in [9]. This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in 4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies.", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions ##CITATION## required by standard HPSG theories", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "INTRODUCTION"], ["90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.", "449 PIION ~ lleben liebt / [dl VFORM~ bse fill} I el SUBJ [] f rVFORM bse] ]  [lieben -] [AaG~ 2tl]] { rv'OaMbsol } ss d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in ##CITATION## Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types Many of the groups of disjunctions in their feature structures can be made more efficient via modularization   ", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Type resolution, on the other hand, always considers species. Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'. APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t\" a.nd 1'\" 6 fS then \"R ( F) tJ 1\"(1\"') = \"R ( F tO F').", "449 PIION ~ lleben liebt / [dl VFORM~ bse fill} I el SUBJ [] f rVFORM bse] ]  [lieben -] [AaG~ 2tl]] { rv'OaMbsol } ss d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in ##CITATION## Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types Many of the groups of disjunctions in their feature structures can be made more efficient via modularization   ", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "MAINTAINING."], ["Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,. Sag (rorthcoming) [14].. well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed. Unfortunately, well-typability is not sufficient to ensure that disjunctive FCRs are satisfied. Consider, For exam- pie, our encoding of the disjunctive FCR p and suppose that 99 is the fe, ature structure t[f : +,9 : -].", "449 PIION ~ lleben liebt / [dl VFORM~ bse fill} I el SUBJ [] f rVFORM bse] ]  [lieben -] [AaG~ 2tl]] { rv'OaMbsol } ss d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in ##CITATION## Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types Many of the groups of disjunctions in their feature structures can be made more efficient via modularization   ", 0, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.", "449 PIION ~ lleben liebt / [dl VFORM~ bse fill} I el SUBJ [] f rVFORM bse] ]  [lieben -] [AaG~ 2tl]] { rv'OaMbsol } ss d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in ##CITATION## Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types Many of the groups of disjunctions in their feature structures can be made more efficient via modularization   ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where.", "449 PIION ~ lleben liebt / [dl VFORM~ bse fill} I el SUBJ [] f rVFORM bse] ]  [lieben -] [AaG~ 2tl]] { rv'OaMbsol } ss d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in ##CITATION## Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types Many of the groups of disjunctions in their feature structures can be made more efficient via modularization   ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails?", "449 PIION ~ lleben liebt / [dl VFORM~ bse fill} I el SUBJ [] f rVFORM bse] ]  [lieben -] [AaG~ 2tl]] { rv'OaMbsol } ss d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in ##CITATION## Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types Many of the groups of disjunctions in their feature structures can be made more efficient via modularization   ", 1, "THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES", "APPROPRIATENESS FOR, MALISMS."], ["However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines.", "Recently open-source tools have been released: in this paper we used Foma ##CITATION## to develop the Russian guesser", 0, "Foma: a finite-state compiler and library", "Efficiency."], ["For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged.", "Recently open-source tools have been released: in this paper we used Foma ##CITATION## to develop the Russian guesser", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (\u2203x)(x \u2208 L \u2227 (\u2203y)(y \u2208 L \u2227 (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to \u2208 and \u2227, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes. This practice stems back from the earliest two-level compilers  .", "Recently open-source tools have been released: in this paper we used Foma ##CITATION## to develop the Russian guesser", 0, "Foma: a finite-state compiler and library", "Basic Regular Expressions."], ["Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm   and Lextools  , the Xerox/PARC finite- state toolkit   and the SFST toolkit  . One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit.", "Recently open-source tools have been released: in this paper we used Foma ##CITATION## to develop the Russian guesser", 1, "Foma: a finite-state compiler and library", "Introduction"], ["Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes. This practice stems back from the earliest two-level compilers  . Below is a simple example of the format: Multichar_Symbols +Pl +Sing LEXICON Root Nouns; LEXICON Nouns cat Plural; church Plural; LEXICON Plural +Pl:%\u02c6s #; +Sing #;", "The rules that are learned are in the format of so-called phonological replacement rules Beesley and Karttunen which we have later converted into equivalent finite-state transducers using the freely available foma toolkit ##CITATION##", 0, "Foma: a finite-state compiler and library", "Building morphological analyzers."], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "The rules that are learned are in the format of so-called phonological replacement rules Beesley and Karttunen which we have later converted into equivalent finite-state transducers using the freely available foma toolkit ##CITATION##", 0, "Foma: a finite-state compiler and library", "ABSTRACT"], ["Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions. However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.", "The rules that are learned are in the format of so-called phonological replacement rules Beesley and Karttunen which we have later converted into equivalent finite-state transducers using the freely available foma toolkit ##CITATION##", 0, "Foma: a finite-state compiler and library", "Introduction"], ["It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm   and Lextools  , the Xerox/PARC finite- state toolkit   and the SFST toolkit  . One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.", "The rules that are learned are in the format of so-called phonological replacement rules Beesley and Karttunen which we have later converted into equivalent finite-state transducers using the freely available foma toolkit ##CITATION##", 1, "Foma: a finite-state compiler and library", "Introduction"], ["The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries. North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no.", "The syllable counter is implemented using the foma software ##CITATION## and the implementation ##CITATION## can be found on the homepage of Figure 1: A verse written in the BAD web application", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions. However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.", "The syllable counter is implemented using the foma software ##CITATION## and the implementation ##CITATION## can be found on the homepage of Figure 1: A verse written in the BAD web application", 0, "Foma: a finite-state compiler and library", "Introduction"], ["The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged. GNU AT&T Foma xfst flex fsm 4 \u03a3\u2217a\u03a315 0.216s 16.23s 17.17s 1.884s \u03a3\u2217a\u03a320 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.", "The syllable counter is implemented using the foma software ##CITATION## and the implementation ##CITATION## can be found on the homepage of Figure 1: A verse written in the BAD web application", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions.", "The syllable counter is implemented using the foma software ##CITATION## and the implementation ##CITATION## can be found on the homepage of Figure 1: A verse written in the BAD web application", 1, "Foma: a finite-state compiler and library", "Introduction"], ["One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions  . For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (\u2203x)(x \u2208 L \u2227 (\u2203y)(y \u2208 L \u2227 (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to \u2208 and \u2227, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.", "Since the question of transducer functionality is known to be decidable Blattner and Head and an efficient algorithm is given in ##CITATION##  which is included in foma with the command test functional we can address this question by calculating the above for each constraint if necessary and then permute the violation markers until the above transducer is functional", 0, "Foma: a finite-state compiler and library", "Basic Regular Expressions."], ["Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines. For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.", "Since the question of transducer functionality is known to be decidable Blattner and Head and an efficient algorithm is given in ##CITATION##  which is included in foma with the command test functional we can address this question by calculating the above for each constraint if necessary and then permute the violation markers until the above transducer is functional", 0, "Foma: a finite-state compiler and library", "Efficiency."], ["The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton. This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately. Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma. It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort. Retaining backwards compatibility with Xerox/PARC and at the same time extending the formalism means that one is often able to construct finite-state networks in equivalent various ways, either through ASCII-based operators or through the Unicode-based extensions.", "Since the question of transducer functionality is known to be decidable Blattner and Head and an efficient algorithm is given in ##CITATION##  which is included in foma with the command test functional we can address this question by calculating the above for each constraint if necessary and then permute the violation markers until the above transducer is functional", 0, "Foma: a finite-state compiler and library", "Introduction"], ["Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata. However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.", "Since the question of transducer functionality is known to be decidable Blattner and Head and an efficient algorithm is given in ##CITATION##  which is included in foma with the command test functional we can address this question by calculating the above for each constraint if necessary and then permute the violation markers until the above transducer is functional", 1, "Foma: a finite-state compiler and library", "Efficiency."], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "Foma ##CITATION## is a freely available2 toolkit that allows to both build and parse FS automata and transducers", 0, "Foma: a finite-state compiler and library", "ABSTRACT"], ["It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm   and Lextools  , the Xerox/PARC finite- state toolkit   and the SFST toolkit  . One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.", "Foma ##CITATION## is a freely available2 toolkit that allows to both build and parse FS automata and transducers", 0, "Foma: a finite-state compiler and library", "Introduction"], ["Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines. For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).", "Foma ##CITATION## is a freely available2 toolkit that allows to both build and parse FS automata and transducers", 0, "Foma: a finite-state compiler and library", "Efficiency."], ["One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions.", "Foma ##CITATION## is a freely available2 toolkit that allows to both build and parse FS automata and transducers", 1, "Foma: a finite-state compiler and library", "Introduction"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.", "This verb chain transfer module is implemented as a series of ordered replacement rules Beesley and Karttunen using the foma finite-state toolkit ##CITATION##", 0, "Foma: a finite-state compiler and library", "PAPER"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit.", "This verb chain transfer module is implemented as a series of ordered replacement rules Beesley and Karttunen using the foma finite-state toolkit ##CITATION##", 0, "Foma: a finite-state compiler and library", "ABSTRACT"], ["One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines. For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License.", "This verb chain transfer module is implemented as a series of ordered replacement rules Beesley and Karttunen using the foma finite-state toolkit ##CITATION##", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "This verb chain transfer module is implemented as a series of ordered replacement rules Beesley and Karttunen using the foma finite-state toolkit ##CITATION##", 1, "Foma: a finite-state compiler and library", "ABSTRACT"], ["For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words). The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged.", "In the work presented here we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit  ", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["GNU AT&T Foma xfst flex fsm 4 \u03a3\u2217a\u03a315 0.216s 16.23s 17.17s 1.884s \u03a3\u2217a\u03a320 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits. The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries.", "In the work presented here we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit  ", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u2019s fsm   and Lextools  , the Xerox/PARC finite- state toolkit   and the SFST toolkit  . One of Foma\u2019s design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available.", "In the work presented here we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit  ", 0, "Foma: a finite-state compiler and library", "Introduction"], ["Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the \u2018Mathematical Operators\u2019 Unicode block. Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.", "In the work presented here we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit  ", 1, "Foma: a finite-state compiler and library", "ABSTRACT"], ["One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions  . For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (\u2203x)(x \u2208 L \u2227 (\u2203y)(y \u2208 L \u2227 (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to \u2208 and \u2227, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions. As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.", "This can be then be used in spell checking applications for example by integrating the lexicon with weighted transduc ers reflecting frequency information and error models ##CITATION##; Pirinen et al", 0, "Foma: a finite-state compiler and library", "Basic Regular Expressions."], ["GNU AT&T Foma xfst flex fsm 4 \u03a3\u2217a\u03a315 0.216s 16.23s 17.17s 1.884s \u03a3\u2217a\u03a320 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits. The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries.", "This can be then be used in spell checking applications for example by integrating the lexicon with weighted transduc ers reflecting frequency information and error models ##CITATION##; Pirinen et al", 0, "Foma: a finite-state compiler and library", "Conclusion."], ["For instance, by default, for efficiency reasons, Foma determinizes and minimizes automata between nearly every incremental operation. Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata. However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible\u2014non-deterministic automata naturally being easier to inspect and analyze. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms.", "This can be then be used in spell checking applications for example by integrating the lexicon with weighted transduc ers reflecting frequency information and error models ##CITATION##; Pirinen et al", 0, "Foma: a finite-state compiler and library", "Automata visualization and."], ["These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions. The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton. This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately. Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma. It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort.", "This can be then be used in spell checking applications for example by integrating the lexicon with weighted transduc ers reflecting frequency information and error models ##CITATION##; Pirinen et al", 1, "Foma: a finite-state compiler and library", "Introduction"], ["(t\u2217, \u03c4 \u2217 \u2217 T s t t,\u03c4t ,aa Formally, for a parent-child pair (t\u03c4t (j), tj ) in \u03c4t, we consider the relationship between a(\u03c4t(j)) and a(j), the source-side words to which t\u03c4t (j) and tj align. If, for example, we require that, for all j, a(\u03c4t(j)) = \u03c4s(a(j)) or a(j) = 0, and that the root of \u03c4t must align to the root of \u03c4s or to NULL, then strict isomorphism must hold between \u03c4s and \u03c4t, and we have implemented a synchronous CF dependency grammar  . Smith and Eisner   grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (\u201ca(\u03c4t(j)) = \u03c4s(a(j))\u201d corresponds to their \u201cparent-child\u201d configuration; see Fig. 3 in Smith and Eisner   for illustrations of the rest.)", "In that paper QG was applied to word alignment and has since found applications in question answering Wang et al paraphrase detection Das and Smith and machine translation ##CITATION##", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["3). The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown). The numerators are identical to each other and to that in Eq. 9. The denominators are much more manageable than in Eq. 9, never requiring summation over more than two structures at a time. We must sum over target word sequences and word alignments (with fixed \u03c4t), and separately over target trees and word alignments (with fixed t).", "In that paper QG was applied to word alignment and has since found applications in question answering Wang et al paraphrase detection Das and Smith and machine translation ##CITATION##", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["of s should be aligned to some part of t (alignment to NULL incurs an explicit cost). Phrase-based systems such as Moses   explicitly search for the highest-scoring string in which all source words are translated. Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in \u03c4t (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in \u03c4s apart from a single recently aligned word.", "In that paper QG was applied to word alignment and has since found applications in question answering Wang et al paraphrase detection Das and Smith and machine translation ##CITATION##", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment  , adaptation and projection in parsing  , and various monolingual recognition and scoring tasks  ; this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "In that paper QG was applied to word alignment and has since found applications in question answering Wang et al paraphrase detection Das and Smith and machine translation ##CITATION##", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["Smith and Eisner   grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (\u201ca(\u03c4t(j)) = \u03c4s(a(j))\u201d corresponds to their \u201cparent-child\u201d configuration; see Fig. 3 in Smith and Eisner   for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2).", "We present a quasi-synchronous dependency grammar Smith and Eisner for machine translation in which the leaves of the tree are phrases rather than words as in previous work ##CITATION##", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features.", "We present a quasi-synchronous dependency grammar Smith and Eisner for machine translation in which the leaves of the tree are phrases rather than words as in previous work ##CITATION##", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["There is also substantial work in the use of target-side syntax  . In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models  . In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2).", "We present a quasi-synchronous dependency grammar Smith and Eisner for machine translation in which the leaves of the tree are phrases rather than words as in previous work ##CITATION##", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment  , adaptation and projection in parsing  , and various monolingual recognition and scoring tasks  ; this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "We present a quasi-synchronous dependency grammar Smith and Eisner for machine translation in which the leaves of the tree are phrases rather than words as in previous work ##CITATION##", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features.", "Quasi-synchronous grammar  ; we describe a coarse-to-fine approach for decoding within this framework advancing substantially over earlier QG machine translation systems  ", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["Given a sentence s and its parse \u03c4s, at decoding time we seek the target sentence t\u2217, the target tree For a QDG model, the decoding problem has not been addressed before. It equates to finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known. The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist. A major advantage of DP is that, with small modifications, summing over structures is also possible with \u201cinside\u201d DP algorithms. We will exploit this in training(\u00a75).", "Quasi-synchronous grammar  ; we describe a coarse-to-fine approach for decoding within this framework advancing substantially over earlier QG machine translation systems  ", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "Quasi-synchronous grammar  ; we describe a coarse-to-fine approach for decoding within this framework advancing substantially over earlier QG machine translation systems  ", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment  , adaptation and projection in parsing  , and various monolingual recognition and scoring tasks  ; this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "Quasi-synchronous grammar  ; we describe a coarse-to-fine approach for decoding within this framework advancing substantially over earlier QG machine translation systems  ", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "We previously applied quasi-synchronous grammar to machine translation   but that system performed translation fundamentally at the word level", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["As usual, the normalization constant is not required for decoding; it suffices to solve: t , a ) = argmax \u03b8 g(s, \u03c4 , a, t, \u03c4 ) (8)Which translations are possible depends heav ily on the configurations that the QDG permits. (t\u2217, \u03c4 \u2217 \u2217 T s t t,\u03c4t ,aa Formally, for a parent-child pair (t\u03c4t (j), tj ) in \u03c4t, we consider the relationship between a(\u03c4t(j)) and a(j), the source-side words to which t\u03c4t (j) and tj align. If, for example, we require that, for all j, a(\u03c4t(j)) = \u03c4s(a(j)) or a(j) = 0, and that the root of \u03c4t must align to the root of \u03c4s or to NULL, then strict isomorphism must hold between \u03c4s and \u03c4t, and we have implemented a synchronous CF dependency grammar  . Smith and Eisner   grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (\u201ca(\u03c4t(j)) = \u03c4s(a(j))\u201d corresponds to their \u201cparent-child\u201d configuration; see Fig.", "We previously applied quasi-synchronous grammar to machine translation   but that system performed translation fundamentally at the word level", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic.", "We previously applied quasi-synchronous grammar to machine translation   but that system performed translation fundamentally at the word level", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "PAPER"], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment  , adaptation and projection in parsing  , and various monolingual recognition and scoring tasks  ; this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "We previously applied quasi-synchronous grammar to machine translation   but that system performed translation fundamentally at the word level", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist. A major advantage of DP is that, with small modifications, summing over structures is also possible with \u201cinside\u201d DP algorithms. We will exploit this in training(\u00a75). Efficient summing opens up many possibilities for training \u03b8, such as likelihood and pseudo likelihood, and provides principled ways to handle hidden variables during learning. 4.1 Translation as Monolingual Parsing.", "We denote this grammar by Gs\u03c4s ; its   including previous work in MT  ", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["6.7 Discussion. We note that these results are not state-of-the- art on this dataset (on this task, Moses/MERT achieves 0.6838 BLEU and 0.8523 METEOR with maximum phrase length 3).14 Our aim has been to 13 In fact, the strictest \u201csynchronous\u201d model used the almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized. 14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation. illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.", "We denote this grammar by Gs\u03c4s ; its   including previous work in MT  ", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["5 Segmentation might be modeled as a hidden variable in future work. 2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs   and lexicalized reordering models  . In syntax-based systems, reordering is typically parameterized by grammar rules. For generality we permit these features to \u201csee\u201d all structures and denote them greor (s, \u03c4s, a, t, \u03c4t). Eq. 6 (Tab.", "We denote this grammar by Gs\u03c4s ; its   including previous work in MT  ", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment  , adaptation and projection in parsing  , and various monolingual recognition and scoring tasks  ; this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219\u2013228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP \u03a3, T Trans : \u03a3 \u222a {NULL} \u2192 2T s = (s0 , . . . , sn ) \u2208 \u03a3n t = (t1 , . . .", "We denote this grammar by Gs\u03c4s ; its   including previous work in MT  ", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the \u201cbackbone\u201d model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Grammars A quasi-synchronous dependency grammar   specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree.", "For a QPDG model decoding consists of finding the highest-scoring tuple   in constructing a lattice to represent Gs\u03c4s and using lattice parsing to search for the best derivation but we construct the lattice differently and employ a coarse-to- fine strategy   to speed up decoding", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["6.7 Discussion. We note that these results are not state-of-the- art on this dataset (on this task, Moses/MERT achieves 0.6838 BLEU and 0.8523 METEOR with maximum phrase length 3).14 Our aim has been to 13 In fact, the strictest \u201csynchronous\u201d model used the almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized. 14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation. illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.", "For a QPDG model decoding consists of finding the highest-scoring tuple   in constructing a lattice to represent Gs\u03c4s and using lattice parsing to search for the best derivation but we construct the lattice differently and employ a coarse-to- fine strategy   to speed up decoding", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["The corpus has approximately 100K sentence pairs. We filter sentences of length more than 15 words, which only removes 6% of the data. We end up with a training set of 82,299 sentences, a develop we use features similar to lexicalized CFG events  , specifically following the dependency model of Klein and Manning  . These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ). These probabilities are estimated on the training corpus parsed using the Stanford factored parser  .", "For a QPDG model decoding consists of finding the highest-scoring tuple   in constructing a lattice to represent Gs\u03c4s and using lattice parsing to search for the best derivation but we construct the lattice differently and employ a coarse-to- fine strategy   to speed up decoding", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["cmu.edu/Quipu. We presented feature-rich MT using a principled probabilistic framework that separates features from inference. Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle \u201cnon-local\u201d features using generic techniques that also support efficient parameter estimation. Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints. We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper.", "For a QPDG model decoding consists of finding the highest-scoring tuple   in constructing a lattice to represent Gs\u03c4s and using lattice parsing to search for the best derivation but we construct the lattice differently and employ a coarse-to- fine strategy   to speed up decoding", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Conclusion."], ["We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality. 6.5 Varying k During Decoding. For models without syntactic features, we constrained the decoder to produce dependency trees in which every word\u2019s parent is immediately to its right and ignored syntactic features while scoring structures. This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate. Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.", " Ar\u2192En[UN NIST 06][L] SL:lexical morphological and syntactic features ##CITATION##  De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English Fr French De German Zh Chinese Ar Arabic CPH Canadian Parliament Hansards UN United Nations BTEC basic travel expression corpus FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al.   use features involving phrases and source- side dependency trees and Mi et al.   use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax  .", " Ar\u2192En[UN NIST 06][L] SL:lexical morphological and syntactic features ##CITATION##  De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English Fr French De German Zh Chinese Ar Arabic CPH Canadian Parliament Hansards UN United Nations BTEC basic travel expression corpus FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], [", m} \u2192 {0, . . . , m} a : {1, . . . , m} \u2192 2{1,...,n} \u03b8 source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where \u03c4s (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where \u03c4t (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s; \u2205 denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (\u00a72.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tj\u2212N +1 ) language model features (\u00a72.2): N -gram probabilities gsyn (t, \u03c4t ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (\u00a72.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I \u2286 {1, . . . , m} greor (s, \u03c4s , a, t, \u03c4t ) f dist (i, j) reordering features (\u00a72.4): distortion features for a source word at position i aligned to a target word at position j gtree 2 (\u03c4s , a, \u03c4t ) f qg (i, il, j, k) tree-to-tree syntactic features (\u00a73): configuration features for source pair si /sil being aligned to target pair tj /tk gcov (a) f scov (a), f zth (a), f sunc (a) coverage features (\u00a74.2) counters for \u201ccovering\u201d each s word each time, the zth time, and leaving it \u201cuncovered\u201d Table 1: Key notation. Feature factorings are elaborated in Tab.", " Ar\u2192En[UN NIST 06][L] SL:lexical morphological and syntactic features ##CITATION##  De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English Fr French De German Zh Chinese Ar Arabic CPH Canadian Parliament Hansards UN United Nations BTEC basic travel expression corpus FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.) The tree-to-tree syntactic features gtree 2 in our model are binary features f qg that fire for particular QG configurations. We use one feature for each of the configurations in  , adding 7 additional features that score configura Phrase Syntactic Features: features: +f att \u222a f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU). tions involving root words and NULL-alignments more finely. There are 14 features in this category.", " Ar\u2192En[UN NIST 06][L] SL:lexical morphological and syntactic features ##CITATION##  De\u2192En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English Fr French De German Zh Chinese Ar Arabic CPH Canadian Parliament Hansards UN United Nations BTEC basic travel expression corpus FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL\u2192TL[DS][S/L] Contextual features Integrated into Berger et al", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features. Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "##CITATION##   present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences Table 5", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar  .2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm  .Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation   with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (\u00a76): The flexibility of our model/decoder permits carefully controlled experiments.", "##CITATION##   present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences Table 5", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["of s should be aligned to some part of t (alignment to NULL incurs an explicit cost). Phrase-based systems such as Moses   explicitly search for the highest-scoring string in which all source words are translated. Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in \u03c4t (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in \u03c4s apart from a single recently aligned word.", "##CITATION##   present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences Table 5", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle \u201cnon-local\u201d features.", "##CITATION##   present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences Table 5", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "ABSTRACT"], ["We will exploit this in training(\u00a75). Efficient summing opens up many possibilities for training \u03b8, such as likelihood and pseudo likelihood, and provides principled ways to handle hidden variables during learning. 4.1 Translation as Monolingual Parsing. We decode by performing lattice parsing on a lattice encoding the set of possible translations. The lattice is a weighted \u201csausage\u201d lattice that permits sentences up to some maximum length \u00a3; \u00a3 is derived from the source sentence length.", "##CITATION##  treat translation as a monolingual dependency parsing problem creating a dependency structure over the translation during decoding", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["We turn next to the \u201cbackbone\u201d model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Grammars A quasi-synchronous dependency grammar   specifies a conditional model p(t, \u03c4t, a | s, \u03c4s). Given a source sentence s and its parse \u03c4s, a QDG induces a probabilistic monolingual dependency grammar over sentences \u201cinspired\u201d by the source sentence and tree. We denote this grammar by Gs,\u03c4s ; its (weighted) language is the set of translations of s. Each word generated by Gs,\u03c4s is annotated with a \u201csense,\u201d which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in \u03c4t and nodes in \u03c4s. In principle, any portion of \u03c4t may align to any portion of \u03c4s, but in practice we often make restrictions on the alignments to simplify computation. Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.)", "##CITATION##  treat translation as a monolingual dependency parsing problem creating a dependency structure over the translation during decoding", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Quasi-Synchronous."], ["In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in \u03c4s apart from a single recently aligned word. This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model  . Thissacrifice is the result of our choice to use a condi Of these, only f scov is local. 4.3 Non-Local Features.", "##CITATION##  treat translation as a monolingual dependency parsing problem creating a dependency structure over the translation during decoding", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["\u00fcbersetzen: sie:you sie:you konnten:could translate \u00fcbersetzen: translate \u00fcbersetzen: konnten:couldn es:it sie :you translated translated konnten:might es:it sie:let sie:them ?:? \u00fcbersetzen: translate es:it konnten:could es:it NULL:to ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them. selected at each position and a dependency tree over them. 4.2 Source-Side Coverage Features.", "##CITATION##  treat translation as a monolingual dependency parsing problem creating a dependency structure over the translation during decoding", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model   that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar  .2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm  .Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation   with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible.", "Log-linear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos and Gimpel andSmith; the idea can be traced back to Pap ineni et al", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model   that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (\u00a73\u20134): We present anovel decoder based on lattice parsing with quasi synchronous grammar  .2 Further, we exploit generic approximate inference techniques to incorporate arbitrary \u201cnon- local\u201d features in the dynamic programming algorithm  .Parameter estimation (\u00a75): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation   with hidden variables to discriminatively and efficiently train our model.", "Log-linear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos and Gimpel andSmith; the idea can be traced back to Pap ineni et al", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation. illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism. We have validated cube summing and decoding as practical methods for approximate inference. Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses\u2019 phrase segmentation variable), and, of course, additional feature representations.", "Log-linear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos and Gimpel andSmith; the idea can be traced back to Pap ineni et al", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["Features are often implied by a choice of formalism. dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a \u201cuniversal\u201d decoder, making the following contributions:Arbitrary feature model (\u00a72): We define a sin gle, direct log-linear translation model   that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of \u201cstring-to-tree,\u201d \u201ctree-to-string,\u201d \u201ctree- to-tree,\u201d and \u201cphrase-based\u201d models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.", "Log-linear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos and Gimpel andSmith; the idea can be traced back to Pap ineni et al", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Introduction"], ["There is also substantial work in the use of target-side syntax  . In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models  . In this work, we focus on syntactic features of target-side dependency trees, \u03c4t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2).", "On the other hand it has been shown that incorporating syntactic information in the form of features can lead to improved performance Chiang ##CITATION## Marton and Resnik", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["3). The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown). The numerators are identical to each other and to that in Eq. 9. The denominators are much more manageable than in Eq. 9, never requiring summation over more than two structures at a time. We must sum over target word sequences and word alignments (with fixed \u03c4t), and separately over target trees and word alignments (with fixed t).", "On the other hand it has been shown that incorporating syntactic information in the form of features can lead to improved performance Chiang ##CITATION## Marton and Resnik", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Training."], ["6.6 QG Configuration Comparison. We next compare different constraints on isomorphism between the source and target dependency 0.55 0.50 0.45 0.40 0.35 Phrase + Syntactic lation. In particular, we compare the effects of combining phrase features and syntactic features. The base model contains f lex , glm , greor , and 12 We made this choice both for similarity to standard MT. 0.30 0.25 0.20 Phrase Syntactic Neither 0 5 10 15 20 Value of k for Decoding systems and a more rapid experiment cycle. Figure 2: Comparison of size of k-best list for cube decoding with various feature sets.", "On the other hand it has been shown that incorporating syntactic information in the form of features can lead to improved performance Chiang ##CITATION## Marton and Resnik", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation. illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism. We have validated cube summing and decoding as practical methods for approximate inference. Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses\u2019 phrase segmentation variable), and, of course, additional feature representations.", "On the other hand it has been shown that incorporating syntactic information in the form of features can lead to improved performance Chiang ##CITATION## Marton and Resnik", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["For models without syntactic features, we constrained the decoder to produce dependency trees in which every word\u2019s parent is immediately to its right and ignored syntactic features while scoring structures. This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate. Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice. Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig. 2.", "Many translation models use such knowledge before decoding Xia and McCord and during decoding Birch et al ##CITATION## Koehn and Hoang Chiang et al but they are limited to simpler features for practical reasons often restricted to conditioning left-to- right on the target sentence", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["2). 2.3 Target Syntax. There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al.", "Many translation models use such knowledge before decoding Xia and McCord and during decoding Birch et al ##CITATION## Koehn and Hoang Chiang et al but they are limited to simpler features for practical reasons often restricted to conditioning left-to- right on the target sentence", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Model."], ["Note that \u201cnon locality\u201d is relative to a choice of formalism; in \u00a72 we did not commit to any formalism, so it is only now that we can describe phrase and N -gram features as non-local. Non-local features will present a challenge for decoding and training (\u00a74.3). Given a sentence s and its parse \u03c4s, at decoding time we seek the target sentence t\u2217, the target tree For a QDG model, the decoding problem has not been addressed before. It equates to finding the most probable derivation under the s/\u03c4s-specific grammar Gs,\u03c4s . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known. The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist.", "Many translation models use such knowledge before decoding Xia and McCord and during decoding Birch et al ##CITATION## Koehn and Hoang Chiang et al but they are limited to simpler features for practical reasons often restricted to conditioning left-to- right on the target sentence", 0, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Decoding."], ["Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit   with modified KneserNey smoothing  .Our approach permits an alternative to mini mum error-rate training  ; it is For our target-language syntactic features g syn , discriminative but handles latent structure and regularization in more principled ways. The pseudo- likelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA\u2019s inner loop faster than MERT\u2019s inner loop. Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output. 6.1 Data and Evaluation. We use the GermanEnglish portion of the Basic Travel Expression Corpus (BTEC).", "Many translation models use such knowledge before decoding Xia and McCord and during decoding Birch et al ##CITATION## Koehn and Hoang Chiang et al but they are limited to simpler features for practical reasons often restricted to conditioning left-to- right on the target sentence", 1, "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "Experiments."], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].", "This sparked intensive research on unsupervised acquisition of entailment rules and similarly paraphrases eg Lin and Pantel Szpektor et al ##CITATION## ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "This sparked intensive research on unsupervised acquisition of entailment rules and similarly paraphrases eg Lin and Pantel Szpektor et al ##CITATION## ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["The frequency of the Company \u2013 Company domain ranks 11th with 35,567 examples. As lower frequency examples include noise, we set a threshold that an NE category pair should appear at least 5 times to be considered and an NE instance pair should appear at least twice to be considered. This limits the number of NE category pairs to 2,000 and the number of NE pair instances to 0.63 million. Step 2. Find keywords for each NE pair The keywords are found for each NE category pair.", "This sparked intensive research on unsupervised acquisition of entailment rules and similarly paraphrases eg Lin and Pantel Szpektor et al ##CITATION## ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.", "This sparked intensive research on unsupervised acquisition of entailment rules and similarly paraphrases eg Lin and Pantel Szpektor et al ##CITATION## ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).", "In the same way most NLP systems like information retrieval ##CITATION## or question answering Duclaye et al based on pattern recognition can be improved by a paraphrase generator", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "In the same way most NLP systems like information retrieval ##CITATION## or question answering Duclaye et al based on pattern recognition can be improved by a paraphrase generator", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["They first collect the NE instance pairs and contexts, just like our method. However, the next step is clearly different. They cluster NE instance pairs based on the words in the contexts using a bag- of-words method. In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30. Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited.", "In the same way most NLP systems like information retrieval ##CITATION## or question answering Duclaye et al based on pattern recognition can be improved by a paraphrase generator", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.", "In the same way most NLP systems like information retrieval ##CITATION## or question answering Duclaye et al based on pattern recognition can be improved by a paraphrase generator", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events.", "In the same way most NLP systems like information retrieval ##CITATION## or question answering Duclaye et al based on pattern recognition can be improved by a paraphrase generator", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "In the same way most NLP systems like information retrieval ##CITATION## or question answering Duclaye et al based on pattern recognition can be improved by a paraphrase generator", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue. In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets. The accuracies for link were 73% and 86% on two evaluated domains. These results are promising and there are several avenues for improving on these results.", "Data-driven paraphrase discovery methods Lin and Pantel Pasca and Dienes Wu and Zhou ##CITATION## extends the idea of distributional similarity to phrases", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].", "Data-driven paraphrase discovery methods Lin and Pantel Pasca and Dienes Wu and Zhou ##CITATION## extends the idea of distributional similarity to phrases", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30. Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited. Instead, we focused on phrases and set the frequency threshold to 2, and so were able to utilize a lot of phrases while minimizing noise. [Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results. The number of NE instance pairs used in their experiment is less than half of our method.", "Data-driven paraphrase discovery methods Lin and Pantel Pasca and Dienes Wu and Zhou ##CITATION## extends the idea of distributional similarity to phrases", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "Data-driven paraphrase discovery methods Lin and Pantel Pasca and Dienes Wu and Zhou ##CITATION## extends the idea of distributional similarity to phrases", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. If the expression is a word or a short phrase (like \u201ccorporation\u201d and \u201ccompany\u201d), it is called a \u201csynonym\u201d. There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "A number of automatically acquired inference rule/paraphrase collections are available such as Szpektor et al ##CITATION##", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["One possibility is to use n-grams based on mutual information. If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate. Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain. As was explained in the results section, \u201cstrength\u201d or \u201cadd\u201d are not desirable keywords in the CC-domain. In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords.", "A number of automatically acquired inference rule/paraphrase collections are available such as Szpektor et al ##CITATION##", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link \u201cIBM\u201d and \u201cLotus\u201d) (Step 4). As we shall see, most of the linked sets are paraphrases. This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1.", "A number of automatically acquired inference rule/paraphrase collections are available such as Szpektor et al ##CITATION##", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "A number of automatically acquired inference rule/paraphrase collections are available such as Szpektor et al ##CITATION##", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "A number of automatically acquired inference rule/paraphrase collections are available such as Szpektor et al ##CITATION##", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995. They contain about 200M words (25M, 110M, 40M and 19M words, respectively). All the sentences have been analyzed by our chunker and NE tag- ger. The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory. 3.2 Results.", "To avoid the drawback several linguistic clues such as fine-grained classification of named entities and coordinated sentences have been utilized ##CITATION## Torisawa", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. If the expression is a word or a short phrase (like \u201ccorporation\u201d and \u201ccompany\u201d), it is called a \u201csynonym\u201d. There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "To avoid the drawback several linguistic clues such as fine-grained classification of named entities and coordinated sentences have been utilized ##CITATION## Torisawa", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview.", "To avoid the drawback several linguistic clues such as fine-grained classification of named entities and coordinated sentences have been utilized ##CITATION## Torisawa", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019).", "To avoid the drawback several linguistic clues such as fine-grained classification of named entities and coordinated sentences have been utilized ##CITATION## Torisawa", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019). Figure 2 shows examples of extracted NE pair instances and their contexts.", "To avoid the drawback several linguistic clues such as fine-grained classification of named entities and coordinated sentences have been utilized ##CITATION## Torisawa", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "For example the algorithm may generate the following two patterns as paraphrases: PERSON is promoted to POST the promotion of PERSON to POST is decided As a later re\ufb01nement ##CITATION##  makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["For example, the phrase \u201c's New York-based trust unit,\u201d is not a paraphrase of the other phrases in the \u201cunit\u201d set. As you can see in the figure, the accuracy for the domain is quite high except for the \u201cagree\u201d set, which contains various expressions representing different relationships for an IE application. The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set. The results, along with the total number of phrases, are shown in Table 1. D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1.", "For example the algorithm may generate the following two patterns as paraphrases: PERSON is promoted to POST the promotion of PERSON to POST is decided As a later re\ufb01nement ##CITATION##  makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02]. The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs. This can be repeated several times to collect a list of author / book title pairs and expressions. However, those methods need initial seeds, so the relation between entities has to be known in advance.", "For example the algorithm may generate the following two patterns as paraphrases: PERSON is promoted to POST the promotion of PERSON to POST is decided As a later re\ufb01nement ##CITATION##  makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["We concentrate on those sets. Among these 32 sets, we found the following pairs of sets which have two or more links. Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances. buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct. We will describe the evaluation of such clusters in the next subsection.", "For example the algorithm may generate the following two patterns as paraphrases: PERSON is promoted to POST the promotion of PERSON to POST is decided As a later re\ufb01nement ##CITATION##  makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus ##CITATION## 05", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus ##CITATION## 05", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview. Before explaining our method in detail, we present a brief overview in this subsection.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus ##CITATION## 05", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["First, from a large corpus, we extract all the NE instance pairs. Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, \u201cIBM plans to acquire Lotus\u201d. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words.", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus ##CITATION## 05", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, \u201cIBM plans to acquire Lotus\u201d. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2).", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus ##CITATION## 05", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["All the links in the \u201cCC-domain are shown in Step 4 in subsection 3.2. Out of those 15 links, 4 are errors, namely \u201cbuy - pay\u201d, \u201cacquire - pay\u201d, \u201cpurchase - stake\u201d \u201cacquisition - stake\u201d. When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event. The similar explanation applies to the link to the \u201cstake\u201d set. We checked whether the discovered links are listed in WordNet.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules ##CITATION## but most don\u2019t However NLP applications usually implicitly incorporate some contextual constraints when applying a rule", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords. Also, \u201cagree\u201d in the CC-domain is not a desirable keyword. It is a relatively frequent word in the domain, but it can be used in different extraction scenarios. In this domain the major scenarios involve the things they agreed on, rather than the mere fact that they agreed. \u201cAgree\u201d is a subject control verb, which dominates another verb whose subject is the same as that of \u201cagree\u201d; the latter verb is generally the one of interest for extraction.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules ##CITATION## but most don\u2019t However NLP applications usually implicitly incorporate some contextual constraints when applying a rule", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["Limitations There are several limitations in the methods. The phrases have to be the expressions of length less than 5 chunks, appear between two NEs. Also, the method of using keywords rules out phrases which don\u2019t contain popular words in the domain. We are not claiming that this method is almighty. Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules ##CITATION## but most don\u2019t However NLP applications usually implicitly incorporate some contextual constraints when applying a rule", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules ##CITATION## but most don\u2019t However NLP applications usually implicitly incorporate some contextual constraints when applying a rule", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results. The number of NE instance pairs used in their experiment is less than half of our method. There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach.", "Many methods for automatic acquisition of rules have been suggested in recent years ranging from distributional similarity to finding shared contexts  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["If the expression is longer or complicated (like \u201cA buys B\u201d and \u201cA\u2019s purchase of B\u201d), it is called \u201cparaphrase\u201d, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.", "Many methods for automatic acquisition of rules have been suggested in recent years ranging from distributional similarity to finding shared contexts  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["The second stage links sets which involve the same pairs of individual NEs. A total of 13,976 phrases were grouped. The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%. One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. If the expression is a word or a short phrase (like \u201ccorporation\u201d and \u201ccompany\u201d), it is called a \u201csynonym\u201d.", "Many methods for automatic acquisition of rules have been suggested in recent years ranging from distributional similarity to finding shared contexts  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "ABSTRACT"], ["This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].", "Many methods for automatic acquisition of rules have been suggested in recent years ranging from distributional similarity to finding shared contexts  ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited.", "Indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["Here, the term frequency (TF) is the frequency of a word in the bag and the inverse term frequency (ITF) is the inverse of the log of the frequency in the entire corpus. Figure 3 Figure 1. Overview of the method 2.2 Step by Step Algorithm. In this section, we will explain the algorithm step by step with examples. Because of their size, the examples (Figures 2 to 4) appear at the end of the paper.", "Indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "Indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules  ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "Indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules  ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.", "Similarly ##CITATION## improved information retrieval based on pattern recognition by introducing paraphrase generation", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "PAPER"], ["This problem arises because our keywords consist of only one word. Sometime, multiple words are needed, like \u201cvice chairman\u201d, \u201cprime minister\u201d or \u201cpay for\u201d (\u201cpay\u201d and \u201cpay for\u201d are different senses in the CC-domain). One possibility is to use n-grams based on mutual information. If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate. Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain.", "Similarly ##CITATION## improved information retrieval based on pattern recognition by introducing paraphrase generation", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["We will describe the evaluation of such clusters in the next subsection. 3.3 Evaluation Results. We evaluated the results based on two metrics. One is the accuracy within a set of phrases which share the same keyword; the other is the accuracy of links. We picked two domains, the CC-domain and the \u201cPerson \u2013 Company\u201d domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate.", "Similarly ##CITATION## improved information retrieval based on pattern recognition by introducing paraphrase generation", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "Similarly ##CITATION## improved information retrieval based on pattern recognition by introducing paraphrase generation", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview.", "Similarly ##CITATION## improved information retrieval based on pattern recognition by introducing paraphrase generation", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["and \u201cH\u201d represents \u201cHanson Plc\u201d. x EG, has agreed to be bought by H x EG, now owned by H x H to acquire EG x H\u2019s agreement to buy EG Three of those phrases are actually paraphrases, but sometime there could be some noise; such as the second phrase above. So, we set a threshold that at least two examples are required to build a link. More examples are shown in Figure 5. Notice that the CC-domain is a special case.", "We agree with ##CITATION##  who claims that several different methods are required to discover a wider variety of paraphrases", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["At this step, we will try to link those sets, and put them into a single cluster. Our clue is the NE instance pairs. If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases. For example, the two NEs \u201cEastern Group Plc\u201d and \u201cHanson Plc\u201d have the following contexts. Here, \u201cEG\u201d represents \u201cEastern Group Plc\u201d.", "We agree with ##CITATION##  who claims that several different methods are required to discover a wider variety of paraphrases", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.", "We agree with ##CITATION##  who claims that several different methods are required to discover a wider variety of paraphrases", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["Also, the method of using keywords rules out phrases which don\u2019t contain popular words in the domain. We are not claiming that this method is almighty. Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases. Applications The discovered paraphrases have multiple applications. One obvious application is information extraction.", "We agree with ##CITATION##  who claims that several different methods are required to discover a wider variety of paraphrases", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules Lin and Pantel ##CITATION## Schoenmackers et al and generate knowledge resources for inference systems", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules Lin and Pantel ##CITATION## Schoenmackers et al and generate knowledge resources for inference systems", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, \u201cIBM plans to acquire Lotus\u201d. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2).", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules Lin and Pantel ##CITATION## Schoenmackers et al and generate knowledge resources for inference systems", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results. The number of NE instance pairs used in their experiment is less than half of our method. There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach.", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules Lin and Pantel ##CITATION## Schoenmackers et al and generate knowledge resources for inference systems", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Related Work."], ["Using structural information As was explained in the results section, we extracted examples like \u201cSmith estimates Lotus\u201d, from a sentence like \u201cMr. Smith estimates Lotus will make profit this quarter\u2026\u201d. In order to solve this problem, a parse tree is needed to understand that \u201cLotus\u201d is not the object of \u201cestimates\u201d. Chunking is not enough to find such relationships. This remains as future work.", "Some attempts were made to let annotators judge rule correctness directly that is by asking them to judge the correctness of a given rule  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Discussion."], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Some attempts were made to let annotators judge rule correctness directly that is by asking them to judge the correctness of a given rule  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Step 3. Gather phrases using keywords Next, we select a keyword for each phrase \u2013 the top-ranked word based on the TF/IDF metric. (If the TF/IDF score of that word is below a threshold, the phrase is discarded.) We then gather all phrases with the same keyword. Figure 4 shows some such phrase sets based on keywords in the CC-domain.", "Some attempts were made to let annotators judge rule correctness directly that is by asking them to judge the correctness of a given rule  ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "Some attempts were made to let annotators judge rule correctness directly that is by asking them to judge the correctness of a given rule  ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "Yet many of these templates share a similar meaning eg \u2018X accommodate up to Y \u2019 \u2018X can accommodate up to Y \u2019 \u2018X will accommodate up to Y \u2019 etc Following ##CITATION##  we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["As the two NE categories are the same, we can\u2019t differentiate phrases with different orders of par ticipants \u2013 whether the buying company or the to-be-bought company comes first. The links can solve the problem. As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation. In figure 4, reverse relations are indicated by `*\u2019 next to the frequency. Now we have sets of phrases which share a keyword and we have links between those sets.", "Yet many of these templates share a similar meaning eg \u2018X accommodate up to Y \u2019 \u2018X can accommodate up to Y \u2019 \u2018X will accommodate up to Y \u2019 etc Following ##CITATION##  we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the \u2018context\u2019). Figure 2 shows examples of extracted NE pair instances and their contexts. The data is sorted based on the frequency of the context (\u201ca unit of\u201d appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. \u201cNBC\u201d and \u201cGeneral Electric Co.\u201d appeared 10 times with the context \u201ca unit of\u201d). Step 2. Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word \u201cunit\u201d for the phrase \u201ca unit of\u201d).", "Yet many of these templates share a similar meaning eg \u2018X accommodate up to Y \u2019 \u2018X can accommodate up to Y \u2019 \u2018X will accommodate up to Y \u2019 etc Following ##CITATION##  we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Figure 4 shows some such phrase sets based on keywords in the CC-domain. Step 4. Cluster phrases based on Links We now have a set of phrases which share a keyword. However, there are phrases which express the same meanings even though they do not share the same keyword. For example, in Figure 3, we can see that the phrases in the \u201cbuy\u201d, \u201cacquire\u201d and \u201cpurchase\u201d sets are mostly paraphrases.", "Yet many of these templates share a similar meaning eg \u2018X accommodate up to Y \u2019 \u2018X can accommodate up to Y \u2019 \u2018X will accommodate up to Y \u2019 etc Following ##CITATION##  we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["However, there are phrases which express the same meanings even though they do not share the same keyword. For example, in Figure 3, we can see that the phrases in the \u201cbuy\u201d, \u201cacquire\u201d and \u201cpurchase\u201d sets are mostly paraphrases. At this step, we will try to link those sets, and put them into a single cluster. Our clue is the NE instance pairs. If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.", "Yet many of these templates share a similar meaning eg \u2018X accommodate up to Y \u2019 \u2018X can accommodate up to Y \u2019 \u2018X will accommodate up to Y \u2019 etc Following ##CITATION##  we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["We picked two domains, the CC-domain and the \u201cPerson \u2013 Company\u201d domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate. It is not easy to make a clear definition of \u201cparaphrase\u201d. Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria. If two phrases can be used to express the same relationship within an information extraction application (\u201cscenario\u201d), these two phrases are paraphrases. Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules eg ##CITATION## CallisonBurch but most do not ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["All the links in the \u201cCC-domain are shown in Step 4 in subsection 3.2. Out of those 15 links, 4 are errors, namely \u201cbuy - pay\u201d, \u201cacquire - pay\u201d, \u201cpurchase - stake\u201d \u201cacquisition - stake\u201d. When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event. The similar explanation applies to the link to the \u201cstake\u201d set. We checked whether the discovered links are listed in WordNet.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules eg ##CITATION## CallisonBurch but most do not ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["For example, we can easily imagine that the number of paraphrases for \u201cA buys B\u201d is enormous and it is not possible to create a comprehensive inventory by hand. Also, we don\u2019t know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like \u201ccorporate mergers\u201d or \u201cmanagement succession\u201d. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules eg ##CITATION## CallisonBurch but most do not ", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2). For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link \u201cIBM\u201d and \u201cLotus\u201d) (Step 4).", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules eg ##CITATION## CallisonBurch but most do not ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2). For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link \u201cIBM\u201d and \u201cLotus\u201d) (Step 4). As we shall see, most of the linked sets are paraphrases.", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules eg ##CITATION## CallisonBurch but most do not ", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Algorithm."], ["Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue. We focus on phrases which connect two Named Entities (NEs), and proceed in two stages. The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets. The second stage links sets which involve the same pairs of individual NEs.", "As for paraphrase ##CITATION##\u2019s Paraphrase Database   is collected using an unsupervised method and focuses on phrases connecting two Named Entities", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "ABSTRACT"], ["While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue. In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets.", "As for paraphrase ##CITATION##\u2019s Paraphrase Database   is collected using an unsupervised method and focuses on phrases connecting two Named Entities", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Conclusion."], ["Evaluation within a set The evaluation of paraphrases within a set of phrases which share a keyword is illustrated in Figure 4. For each set, the phrases with bracketed frequencies are considered not paraphrases in the set. For example, the phrase \u201c's New York-based trust unit,\u201d is not a paraphrase of the other phrases in the \u201cunit\u201d set. As you can see in the figure, the accuracy for the domain is quite high except for the \u201cagree\u201d set, which contains various expressions representing different relationships for an IE application. The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.", "As for paraphrase ##CITATION##\u2019s Paraphrase Database   is collected using an unsupervised method and focuses on phrases connecting two Named Entities", 0, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Experiments."], ["In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.", "As for paraphrase ##CITATION##\u2019s Paraphrase Database   is collected using an unsupervised method and focuses on phrases connecting two Named Entities", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["So, it is too costly to make IE technology \u201copen- domain\u201d or \u201con-demand\u201d like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue. 2.1 Overview.", "As for paraphrase ##CITATION##\u2019s Paraphrase Database   is collected using an unsupervised method and focuses on phrases connecting two Named Entities", 1, "Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs", "Introduction"], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "Nevertheless recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy cf ##CITATION## Kennedy and Boguraev Kameyama ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word.", "Nevertheless recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy cf ##CITATION## Kennedy and Boguraev Kameyama ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "Nevertheless recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy cf ##CITATION## Kennedy and Boguraev Kameyama ", 0, "Robust pronoun resolution with limited knowledge", "PAPER"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy cf ##CITATION## Kennedy and Boguraev Kameyama ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy cf ##CITATION## Kennedy and Boguraev Kameyama ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Nevertheless recent results show that knowledge-poor methods perform with amazing ac\u00ad curacy cf ##CITATION## Kennedy and Boguraev Kameyama ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "The anaphora resolver is an adaptation for Bulgarian of ##CITATION##s knowledge-poor pronoun resolution approach ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["\"Non-prepositional\" noun phrases A \"pure\", \"non-prepositional\" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ). Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording. Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\". This preference can be explained in terms of sali\u00ad ence from the point of view of the centering theory. The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\"   and noun phrases which are parts of prepositional phrases are usually indirect objects.", "The anaphora resolver is an adaptation for Bulgarian of ##CITATION##s knowledge-poor pronoun resolution approach ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1. Examine the current sentence and the two pre\u00ad. ceding sentences (if available).", "The anaphora resolver is an adaptation for Bulgarian of ##CITATION##s knowledge-poor pronoun resolution approach ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The anaphora resolver is an adaptation for Bulgarian of ##CITATION##s knowledge-poor pronoun resolution approach ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "This module resolves third-person personal pronouns and is an adaptation of ##CITATION##\u2019s robust knowledge-poor multilingual approach   whose latest implementation by R Evans is referred to as MARS 2  ", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  .", "This module resolves third-person personal pronouns and is an adaptation of ##CITATION##\u2019s robust knowledge-poor multilingual approach   whose latest implementation by R Evans is referred to as MARS 2  ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "This module resolves third-person personal pronouns and is an adaptation of ##CITATION##\u2019s robust knowledge-poor multilingual approach   whose latest implementation by R Evans is referred to as MARS 2  ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "This module resolves third-person personal pronouns and is an adaptation of ##CITATION##\u2019s robust knowledge-poor multilingual approach   whose latest implementation by R Evans is referred to as MARS 2  ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "LINGUA performs the pre-processing needed as an input to the anaphora resolution algorithm: sentence paragraph and clause splitters NP grammar part-of-speech tagger 2 MARS stands for ##CITATION##\u2019s Anaphora Resolution System 3 For a detailed procedure how candidates are handled in the event of a tie see    ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "LINGUA performs the pre-processing needed as an input to the anaphora resolution algorithm: sentence paragraph and clause splitters NP grammar part-of-speech tagger 2 MARS stands for ##CITATION##\u2019s Anaphora Resolution System 3 For a detailed procedure how candidates are handled in the event of a tie see    ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "LINGUA performs the pre-processing needed as an input to the anaphora resolution algorithm: sentence paragraph and clause splitters NP grammar part-of-speech tagger 2 MARS stands for ##CITATION##\u2019s Anaphora Resolution System 3 For a detailed procedure how candidates are handled in the event of a tie see    ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "LINGUA performs the pre-processing needed as an input to the anaphora resolution algorithm: sentence paragraph and clause splitters NP grammar part-of-speech tagger 2 MARS stands for ##CITATION##\u2019s Anaphora Resolution System 3 For a detailed procedure how candidates are handled in the event of a tie see    ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "LINGUA performs the pre-processing needed as an input to the anaphora resolution algorithm: sentence paragraph and clause splitters NP grammar part-of-speech tagger 2 MARS stands for ##CITATION##\u2019s Anaphora Resolution System 3 For a detailed procedure how candidates are handled in the event of a tie see    ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Most of the indicators have been adopted in LINGUA without modification from the original English version   for more details", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Most of the indicators have been adopted in LINGUA without modification from the original English version   for more details", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\u00ad perative zero-subject sentences). In the second experiment we evaluated the ap\u00ad proach from the point of view also of its \"critical success rate\". This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence.", "Most of the indicators have been adopted in LINGUA without modification from the original English version   for more details", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Most of the indicators have been adopted in LINGUA without modification from the original English version   for more details", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "Most of the indicators have been adopted in LINGUA without modification from the original English version   for more details", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.", "Binding constraints have been in the focus of linguistic research for more than thirty years They provide restrictions on coindexation of pronouns with clause siblings and therefore can only be applied with systems that determine clause boundaries ie parsers ##CITATION## ", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").", "Binding constraints have been in the focus of linguistic research for more than thirty years They provide restrictions on coindexation of pronouns with clause siblings and therefore can only be applied with systems that determine clause boundaries ie parsers ##CITATION## ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Binding constraints have been in the focus of linguistic research for more than thirty years They provide restrictions on coindexation of pronouns with clause siblings and therefore can only be applied with systems that determine clause boundaries ie parsers ##CITATION## ", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred. If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent.", "Binding constraints have been in the focus of linguistic research for more than thirty years They provide restrictions on coindexation of pronouns with clause siblings and therefore can only be applied with systems that determine clause boundaries ie parsers ##CITATION## ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\"   and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\". Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in  . Example: Press the keyi down and turn the volume up...", "Binding constraints have been in the focus of linguistic research for more than thirty years They provide restrictions on coindexation of pronouns with clause siblings and therefore can only be applied with systems that determine clause boundaries ie parsers ##CITATION## ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "However the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies Dagan and Itai Lappin and Leass ##CITATION## Soon Ng and Lim Ng and Cardie which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources eg ontology", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "However the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies Dagan and Itai Lappin and Leass ##CITATION## Soon Ng and Lim Ng and Cardie which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources eg ontology", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "However the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies Dagan and Itai Lappin and Leass ##CITATION## Soon Ng and Lim Ng and Cardie which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources eg ontology", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "However the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies Dagan and Itai Lappin and Leass ##CITATION## Soon Ng and Lim Ng and Cardie which was further motivated by the emergence of cheaper and more reliable corpus- based NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources eg ontology", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in ##CITATION## as empirical studies show that more than 85% of all cases are handled correctly with this window size ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\". Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in  . Example: Press the keyi down and turn the volume up... Press iti again.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in ##CITATION## as empirical studies show that more than 85% of all cases are handled correctly with this window size ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in ##CITATION## as empirical studies show that more than 85% of all cases are handled correctly with this window size ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in ##CITATION## as empirical studies show that more than 85% of all cases are handled correctly with this window size ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Look for noun phrases3 only to the left of the anaphor4 2. Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\".", "Like many heuristic-based pronoun resolvers eg ##CITATION##  they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "Like many heuristic-based pronoun resolvers eg ##CITATION##  they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Like many heuristic-based pronoun resolvers eg ##CITATION##  they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Like many heuristic-based pronoun resolvers eg ##CITATION##  they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent.", "Like many heuristic-based pronoun resolvers eg ##CITATION##  they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  .", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score similar to Carbonell and Brown ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent. For in\u00ad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score similar to Carbonell and Brown ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples. Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1). We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses\u00ad sive pronouns.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score similar to Carbonell and Brown ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score similar to Carbonell and Brown ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "We implemented meta-modules to in\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00ad all score similar to Carbonell and Brown ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1. Examine the current sentence and the two pre\u00ad. ceding sentences (if available).", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm ##CITATION## and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm ##CITATION## and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm ##CITATION## and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio ", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The current version of the system includes an implementation of the MARS pronoun resolution algorithm ##CITATION## and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "321 Pronoun Resolution ##CITATION##   developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified ##CITATION##\u2019s algorithm operates on the basis of antecedent-tracking preferences referred to hereafter as \u201dantecedent indicators\u201d ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "321 Pronoun Resolution ##CITATION##   developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified ##CITATION##\u2019s algorithm operates on the basis of antecedent-tracking preferences referred to hereafter as \u201dantecedent indicators\u201d ", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "321 Pronoun Resolution ##CITATION##   developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified ##CITATION##\u2019s algorithm operates on the basis of antecedent-tracking preferences referred to hereafter as \u201dantecedent indicators\u201d ", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "321 Pronoun Resolution ##CITATION##   developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified ##CITATION##\u2019s algorithm operates on the basis of antecedent-tracking preferences referred to hereafter as \u201dantecedent indicators\u201d ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "321 Pronoun Resolution ##CITATION##   developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified ##CITATION##\u2019s algorithm operates on the basis of antecedent-tracking preferences referred to hereafter as \u201dantecedent indicators\u201d ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The sample texts con\u00ad tained 180 pronouns among which were 120 in\u00ad stances of exophoric reference (most being zero pro\u00ad nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences checks them for gender and number agreement with the anaphor and then applies genre- specific antecedent indicators to the remaining candidates  ", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Look for noun phrases3 only to the left of the anaphor4 2. Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\".", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences checks them for gender and number agreement with the anaphor and then applies genre- specific antecedent indicators to the remaining candidates  ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences checks them for gender and number agreement with the anaphor and then applies genre- specific antecedent indicators to the remaining candidates  ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.", "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences checks them for gender and number agreement with the anaphor and then applies genre- specific antecedent indicators to the remaining candidates  ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Early work on pronoun anaphora resolution usually uses rule-based methods eg Hobbs Ge et al ##CITATION## which try to mine the cues of the relation between the pronouns and its antecedents", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared.", "Early work on pronoun anaphora resolution usually uses rule-based methods eg Hobbs Ge et al ##CITATION## which try to mine the cues of the relation between the pronouns and its antecedents", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Early work on pronoun anaphora resolution usually uses rule-based methods eg Hobbs Ge et al ##CITATION## which try to mine the cues of the relation between the pronouns and its antecedents", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Early work on pronoun anaphora resolution usually uses rule-based methods eg Hobbs Ge et al ##CITATION## which try to mine the cues of the relation between the pronouns and its antecedents", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages pages 81\u201390 Hyderabad India January QcAsian Federation of Natural Language Processing algorithms have been devised for this purpose Aone and Bennette Brenan  Friedman and Pollard Ge Hale and Charniak Grosz Aravind and Weinstein McCarthy and Lehnert Lappins and Leass ##CITATION## Soon Ng and Lim ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent. If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages pages 81\u201390 Hyderabad India January QcAsian Federation of Natural Language Processing algorithms have been devised for this purpose Aone and Bennette Brenan  Friedman and Pollard Ge Hale and Charniak Grosz Aravind and Weinstein McCarthy and Lehnert Lappins and Leass ##CITATION## Soon Ng and Lim ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages pages 81\u201390 Hyderabad India January QcAsian Federation of Natural Language Processing algorithms have been devised for this purpose Aone and Bennette Brenan  Friedman and Pollard Ge Hale and Charniak Grosz Aravind and Weinstein McCarthy and Lehnert Lappins and Leass ##CITATION## Soon Ng and Lim ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "A lot of work has been done in English for the purpose of anaphora resolution and various Proceedings of the IJCNLP08 Workshop on NLP for Less Privileged Languages pages 81\u201390 Hyderabad India January QcAsian Federation of Natural Language Processing algorithms have been devised for this purpose Aone and Bennette Brenan  Friedman and Pollard Ge Hale and Charniak Grosz Aravind and Weinstein McCarthy and Lehnert Lappins and Leass ##CITATION## Soon Ng and Lim ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "How these factors are helpful in anaphora resolution in English language was worked out by ##CITATION##   but their role in Urdu discourse for the resolution of personal pronouns is more cherished", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["3.1 Evaluation A. Our first evaluation exercise   was based on a random sample text from a technical manual in English  . There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation.", "How these factors are helpful in anaphora resolution in English language was worked out by ##CITATION##   but their role in Urdu discourse for the resolution of personal pronouns is more cherished", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  . For the training data from the genre of technical manuals, it was rule 5   which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).", "How these factors are helpful in anaphora resolution in English language was worked out by ##CITATION##   but their role in Urdu discourse for the resolution of personal pronouns is more cherished", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "How these factors are helpful in anaphora resolution in English language was worked out by ##CITATION##   but their role in Urdu discourse for the resolution of personal pronouns is more cherished", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.", "Many hand-tested corpus evaluations have been done in the past eg Walker Strube ##CITATION## Strube and Hahn but these have the drawback of being carried out on small corpora", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6. 3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide  . Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "Many hand-tested corpus evaluations have been done in the past eg Walker Strube ##CITATION## Strube and Hahn but these have the drawback of being carried out on small corpora", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "Many hand-tested corpus evaluations have been done in the past eg Walker Strube ##CITATION## Strube and Hahn but these have the drawback of being carried out on small corpora", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise   was based on a random sample text from a technical manual in English  . There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%.", "Many hand-tested corpus evaluations have been done in the past eg Walker Strube ##CITATION## Strube and Hahn but these have the drawback of being carried out on small corpora", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide  . Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3). The evaluation indicated 83.6% success rate.", "Many hand-tested corpus evaluations have been done in the past eg Walker Strube ##CITATION## Strube and Hahn but these have the drawback of being carried out on small corpora", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3). The evaluation indicated 83.6% success rate. The \"Baseline subject\" model tested on the same data scored 33.9% recall and 67.9% precision, whereas \"Baseline most recent\" scored 66.7%.", "Many hand-tested corpus evaluations have been done in the past eg Walker Strube ##CITATION## Strube and Hahn but these have the drawback of being carried out on small corpora", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Consequently current anaphora resolution methods rely mainly on constraint and preference heuristics which employ morpho-syntactic information or shallow semantic analysis see for example ##CITATION## ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Consequently current anaphora resolution methods rely mainly on constraint and preference heuristics which employ morpho-syntactic information or shallow semantic analysis see for example ##CITATION## ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Consequently current anaphora resolution methods rely mainly on constraint and preference heuristics which employ morpho-syntactic information or shallow semantic analysis see for example ##CITATION## ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Consequently current anaphora resolution methods rely mainly on constraint and preference heuristics which employ morpho-syntactic information or shallow semantic analysis see for example ##CITATION## ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6. 3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide  . Out of 223 pro\u00ad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\"). The evaluation carried out was manual to ensure that no added error was gen\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "Exa mple s: we athe r It is raini ng tim e It is 5 o'clo ck and am bien t envi ron men t It is hot in here  reports provide no exclusion details at all and even when authors do provide them the descriptions they use are often incomplete or confusing as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" ##CITATION## page 872", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  .", "Exa mple s: we athe r It is raini ng tim e It is 5 o'clo ck and am bien t envi ron men t It is hot in here  reports provide no exclusion details at all and even when authors do provide them the descriptions they use are often incomplete or confusing as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" ##CITATION## page 872", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent.", "Exa mple s: we athe r It is raini ng tim e It is 5 o'clo ck and am bien t envi ron men t It is hot in here  reports provide no exclusion details at all and even when authors do provide them the descriptions they use are often incomplete or confusing as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" ##CITATION## page 872", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below). 3.1 Evaluation A. Our first evaluation exercise   was based on a random sample text from a technical manual in English  . There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology.", "Exa mple s: we athe r It is raini ng tim e It is 5 o'clo ck and am bien t envi ron men t It is hot in here  reports provide no exclusion details at all and even when authors do provide them the descriptions they use are often incomplete or confusing as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" ##CITATION## page 872", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Since the so-called integrative approach to anaphor resolution was developed in the late Carbonell and Brown Rich and LuperFoy Asher and Wada and its practical viability extensively tested eg Lappin and Leass ##CITATION## it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "Since the so-called integrative approach to anaphor resolution was developed in the late Carbonell and Brown Rich and LuperFoy Asher and Wada and its practical viability extensively tested eg Lappin and Leass ##CITATION## it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Since the so-called integrative approach to anaphor resolution was developed in the late Carbonell and Brown Rich and LuperFoy Asher and Wada and its practical viability extensively tested eg Lappin and Leass ##CITATION## it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Since the so-called integrative approach to anaphor resolution was developed in the late Carbonell and Brown Rich and LuperFoy Asher and Wada and its practical viability extensively tested eg Lappin and Leass ##CITATION## it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Most work on anaphora resolution has focused on pronominal anaphora often achieving good accuracy Kennedy and Boguraev  ##CITATION##  and Strube Rapp and Mueller  for example report accuracies of 750% 897% and an F-measure of 828% for personal pronouns respectively ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Most work on anaphora resolution has focused on pronominal anaphora often achieving good accuracy Kennedy and Boguraev  ##CITATION##  and Strube Rapp and Mueller  for example report accuracies of 750% 897% and an F-measure of 828% for personal pronouns respectively ", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "Most work on anaphora resolution has focused on pronominal anaphora often achieving good accuracy Kennedy and Boguraev  ##CITATION##  and Strube Rapp and Mueller  for example report accuracies of 750% 897% and an F-measure of 828% for personal pronouns respectively ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Most work on anaphora resolution has focused on pronominal anaphora often achieving good accuracy Kennedy and Boguraev  ##CITATION##  and Strube Rapp and Mueller  for example report accuracies of 750% 897% and an F-measure of 828% for personal pronouns respectively ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Most work on anaphora resolution has focused on pronominal anaphora often achieving good accuracy Kennedy and Boguraev  ##CITATION##  and Strube Rapp and Mueller  for example report accuracies of 750% 897% and an F-measure of 828% for personal pronouns respectively ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  .", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming diffi cult and error-prone Neverthless recent resultsshow that knowledge-poor empirical methods per form with amazing accuracy on certain forms ofcoreference cf ##CITATION## Kennedy and Boguraev Kameyama  ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Unlike other knowledge-poor methods for coreference resolution     COCK TAIL filters its most performant rules through massivetraining data generated by its AUTOTAGCOFtEF com ponent", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Unlike other knowledge-poor methods for coreference resolution     COCK TAIL filters its most performant rules through massivetraining data generated by its AUTOTAGCOFtEF com ponent", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  . For the training data from the genre of technical manuals, it was rule 5   which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\".", "Unlike other knowledge-poor methods for coreference resolution     COCK TAIL filters its most performant rules through massivetraining data generated by its AUTOTAGCOFtEF com ponent", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Unlike other knowledge-poor methods for coreference resolution     COCK TAIL filters its most performant rules through massivetraining data generated by its AUTOTAGCOFtEF com ponent", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word.", "##CITATION## showed that a salience-based approach can be applied across genres and without complex syntactic semantic and discourse analysis ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj. where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6.", "##CITATION## showed that a salience-based approach can be applied across genres and without complex syntactic semantic and discourse analysis ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj. where \"blank sheet of paper\" scores only 2 as op\u00ad posed to the \"the paper through key\" which scores 6. 3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide  .", "##CITATION## showed that a salience-based approach can be applied across genres and without complex syntactic semantic and discourse analysis ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "##CITATION## showed that a salience-based approach can be applied across genres and without complex syntactic semantic and discourse analysis ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "##CITATION## showed that a salience-based approach can be applied across genres and without complex syntactic semantic and discourse analysis ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "##CITATION## showed that a salience-based approach can be applied across genres and without complex syntactic semantic and discourse analysis ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "Consequently current anaphora resolution methods rely mainly on restrictions and preference heuristics which employ information originating from morpho-syntactic or shallow semantic analysis see ##CITATION##  for example", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Consequently current anaphora resolution methods rely mainly on restrictions and preference heuristics which employ information originating from morpho-syntactic or shallow semantic analysis see ##CITATION##  for example", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Consequently current anaphora resolution methods rely mainly on restrictions and preference heuristics which employ information originating from morpho-syntactic or shallow semantic analysis see ##CITATION##  for example", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Consequently current anaphora resolution methods rely mainly on restrictions and preference heuristics which employ information originating from morpho-syntactic or shallow semantic analysis see ##CITATION##  for example", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The evaluation for Polish was based technical manuals available on the Internet  . The sample texts con\u00ad tained 180 pronouns among which were 120 in\u00ad stances of exophoric reference (most being zero pro\u00ad nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models.", "##CITATION##   obtains a success rate of 897% for pronominal references working with English technical manuals", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["3.1 Evaluation A. Our first evaluation exercise   was based on a random sample text from a technical manual in English  . There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a suc\u00ad cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation.", "##CITATION##   obtains a success rate of 897% for pronominal references working with English technical manuals", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "##CITATION##   obtains a success rate of 897% for pronominal references working with English technical manuals", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "##CITATION##   obtains a success rate of 897% for pronominal references working with English technical manuals", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  .", "##CITATION##   obtains a success rate of 897% for pronominal references working with English technical manuals", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "Ruslan ##CITATION##   Robust pronoun resolution th evaluation several baselines on pronominal anaphora resolution have been implemented and with limited knowledge", 0, "Robust pronoun resolution with limited knowledge", "PAPER"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Ruslan ##CITATION##   Robust pronoun resolution th evaluation several baselines on pronominal anaphora resolution have been implemented and with limited knowledge", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Ruslan ##CITATION##   Robust pronoun resolution th evaluation several baselines on pronominal anaphora resolution have been implemented and with limited knowledge", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Ruslan ##CITATION##   Robust pronoun resolution th evaluation several baselines on pronominal anaphora resolution have been implemented and with limited knowledge", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP Kennedy and Boguraev Baldwin\u2019s pronoun resolution method Baldwin and ##CITATION##\u2019s knowledge-poor pronoun resolution approach ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP Kennedy and Boguraev Baldwin\u2019s pronoun resolution method Baldwin and ##CITATION##\u2019s knowledge-poor pronoun resolution approach ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  .", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP Kennedy and Boguraev Baldwin\u2019s pronoun resolution method Baldwin and ##CITATION##\u2019s knowledge-poor pronoun resolution approach ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u2019s parser- free version of Lappin and Leass\u2019 RAP Kennedy and Boguraev Baldwin\u2019s pronoun resolution method Baldwin and ##CITATION##\u2019s knowledge-poor pronoun resolution approach ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "##CITATION##\u2019s approach ##CITATION##\u2019s approach ##CITATION## is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "##CITATION##\u2019s approach ##CITATION##\u2019s approach ##CITATION## is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "##CITATION##\u2019s approach ##CITATION##\u2019s approach ##CITATION## is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "##CITATION##\u2019s approach ##CITATION##\u2019s approach ##CITATION## is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "##CITATION##\u2019s approach ##CITATION##\u2019s approach ##CITATION## is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s   parser-free algorithm Baldwin\u2019s   CogNiac and ##CITATION##\u2019s  knowledge-poor approach", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["As expected, the most frequent indica\u00ad tors were not the most discriminative ones. 3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s   parser-free algorithm Baldwin\u2019s   CogNiac and ##CITATION##\u2019s  knowledge-poor approach", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s   parser-free algorithm Baldwin\u2019s   CogNiac and ##CITATION##\u2019s  knowledge-poor approach", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u201dknowledge-poor philosophy\u201d: Kennedy and Boguraev\u2019s   parser-free algorithm Baldwin\u2019s   CogNiac and ##CITATION##\u2019s  knowledge-poor approach", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "##CITATION##\u2019s knowledge-poor pronoun resolution method ##CITATION## for example uses the scores from a set of antecedent indicators to rank the candidates", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "##CITATION##\u2019s knowledge-poor pronoun resolution method ##CITATION## for example uses the scores from a set of antecedent indicators to rank the candidates", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "##CITATION##\u2019s knowledge-poor pronoun resolution method ##CITATION## for example uses the scores from a set of antecedent indicators to rank the candidates", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "##CITATION##\u2019s knowledge-poor pronoun resolution method ##CITATION## for example uses the scores from a set of antecedent indicators to rank the candidates", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "##CITATION##\u2019s knowledge-poor pronoun resolution method ##CITATION## for example uses the scores from a set of antecedent indicators to rank the candidates", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish  . For the time being, we are using the same scores for Polish.", "The coreferential chain length of a candidate or its variants such as occurrence frequency and TFIDF has been used as a salience factor in some learning-based reference resolution systems Iida et al ##CITATION## Paul et al Strube and Muller", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "The coreferential chain length of a candidate or its variants such as occurrence frequency and TFIDF has been used as a salience factor in some learning-based reference resolution systems Iida et al ##CITATION## Paul et al Strube and Muller", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Look for noun phrases3 only to the left of the anaphor4 2. Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\".", "The coreferential chain length of a candidate or its variants such as occurrence frequency and TFIDF has been used as a salience factor in some learning-based reference resolution systems Iida et al ##CITATION## Paul et al Strube and Muller", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "The coreferential chain length of a candidate or its variants such as occurrence frequency and TFIDF has been used as a salience factor in some learning-based reference resolution systems Iida et al ##CITATION## Paul et al Strube and Muller", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%. Given that our knowledge\u00ad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\u00ad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.", "For example in the following sentence mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect rejecting a strong upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership\u201d \u201cAmerican Medical Association\u201d \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same objectEarly work of anaphora resolution focuses on find ing antecedents of pronouns Hobbs Ge et al ##CITATION## while recent advances Soon et al Yang et al Ng and Cardie Ittycheriah et al employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases NP be it a name nominal or pronominal phrase \u2013 which is the scope of this paper as well", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  .", "For example in the following sentence mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect rejecting a strong upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership\u201d \u201cAmerican Medical Association\u201d \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same objectEarly work of anaphora resolution focuses on find ing antecedents of pronouns Hobbs Ge et al ##CITATION## while recent advances Soon et al Yang et al Ng and Cardie Ittycheriah et al employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases NP be it a name nominal or pronominal phrase \u2013 which is the scope of this paper as well", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent.", "For example in the following sentence mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect rejecting a strong upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership\u201d \u201cAmerican Medical Association\u201d \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same objectEarly work of anaphora resolution focuses on find ing antecedents of pronouns Hobbs Ge et al ##CITATION## while recent advances Soon et al Yang et al Ng and Cardie Ittycheriah et al employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases NP be it a name nominal or pronominal phrase \u2013 which is the scope of this paper as well", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "For example in the following sentence mentions are underlined: \u201cThe American Medical Association voted yesterday to install the heir apparent as its president-elect rejecting a strong upstart challenge by a District doctor who argued that the nation\u2019s largest physicians\u2019 group needs stronger ethics and new leadership\u201d \u201cAmerican Medical Association\u201d \u201cits\u201d and \u201cgroup\u201d belong to the same entity as they refer to the same objectEarly work of anaphora resolution focuses on find ing antecedents of pronouns Hobbs Ge et al ##CITATION## while recent advances Soon et al Yang et al Ng and Cardie Ittycheriah et al employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases NP be it a name nominal or pronominal phrase \u2013 which is the scope of this paper as well", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Indeed existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge eg ##CITATION##  Soon et al", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Indeed existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge eg ##CITATION##  Soon et al", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task.", "Indeed existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge eg ##CITATION##  Soon et al", 0, "Robust pronoun resolution with limited knowledge", "PAPER"], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Indeed existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge eg ##CITATION##  Soon et al", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estab\u00ad lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same sen\u00ad tence and where preference was given to a candidate in the preceding sentence. This case and other cases suggest that it might be worthwhile reconsider\u00ad ing/refining the weights for the indicator \"referential distance\". Similarly to the first evaluation, we found that the robust approach was not very successful on sen\u00ad tences with too complicated syntax - a price we have to pay for the \"convenience\" of developing a knowl\u00ad edge-poor system.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN  ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN  ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["This preference can be explained in terms of sali\u00ad ence from the point of view of the centering theory. The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\"   and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\". Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in  .", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN  ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN  ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism.", "These features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules Lappin and Leass Kennedy and Boguraev ##CITATION## or using machine-learning methods Aone and Bennett Yang et al Luo and Zitouni", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "These features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules Lappin and Leass Kennedy and Boguraev ##CITATION## or using machine-learning methods Aone and Bennett Yang et al Luo and Zitouni", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["This result is comparable with the results described in  . For the training data from the genre of technical manuals, it was rule 5   which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion.", "These features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules Lappin and Leass Kennedy and Boguraev ##CITATION## or using machine-learning methods Aone and Bennett Yang et al Luo and Zitouni", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "These features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules Lappin and Leass Kennedy and Boguraev ##CITATION## or using machine-learning methods Aone and Bennett Yang et al Luo and Zitouni", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish  . For the time being, we are using the same scores for Polish.", "In knowledge-lean approaches coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process eg ##CITATION##  Tetreault ", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure.", "In knowledge-lean approaches coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process eg ##CITATION##  Tetreault ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word.", "In knowledge-lean approaches coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process eg ##CITATION##  Tetreault ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "In knowledge-lean approaches coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process eg ##CITATION##  Tetreault ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "In knowledge-lean approaches coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process eg ##CITATION##  Tetreault ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be de\u00ad scribed informally as follows: 1. Examine the current sentence and the two pre\u00ad. ceding sentences (if available).", "While not developed within a graph-based framework factor-based approaches for pronoun resolution ##CITATION## can be regarded as greedy clustering in a multigraph where edges representing factors for pronoun resolution have negative or positive weight", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure.", "While not developed within a graph-based framework factor-based approaches for pronoun resolution ##CITATION## can be regarded as greedy clustering in a multigraph where edges representing factors for pronoun resolution have negative or positive weight", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["55 % I 48 .5 5 % 6 5 . 9 5 % The lower figure in \"Baseline subject\" corresponds to \"recall\" and the higher figure- to \"precision\". If we regard as \"discriminative power\" of each antecedent indicator the ratio \"number of successful antecedent identifications when this indicator was applied\"/\"number of applications of this indicator\" (for the non-prepositional noun phrase and definite\u00ad ness being penalising indicators, this figure is calcu\u00ad lated as the ratio \"number of unsuccessful antece\u00ad dent identifications\"/\"number of applications\"), the immediate reference emerges as the most discrimi\u00ad native indicator (100%), followed by non\u00ad prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%). The rela\u00ad tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the ba\u00ad sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent. In terms of frequency of use (\"number of nonzero applications\"/\"number of anaphors\"), the most fre\u00ad quently used indicator proved to be referential dis\u00ad tance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reit\u00ad eration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and colloca\u00ad tion (11.1%). As expected, the most frequent indica\u00ad tors were not the most discriminative ones.", "While not developed within a graph-based framework factor-based approaches for pronoun resolution ##CITATION## can be regarded as greedy clustering in a multigraph where edges representing factors for pronoun resolution have negative or positive weight", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "While not developed within a graph-based framework factor-based approaches for pronoun resolution ##CITATION## can be regarded as greedy clustering in a multigraph where edges representing factors for pronoun resolution have negative or positive weight", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent.", "While not developed within a graph-based framework factor-based approaches for pronoun resolution ##CITATION## can be regarded as greedy clustering in a multigraph where edges representing factors for pronoun resolution have negative or positive weight", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent. For in\u00ad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.", "While not developed within a graph-based framework factor-based approaches for pronoun resolution ##CITATION## can be regarded as greedy clustering in a multigraph where edges representing factors for pronoun resolution have negative or positive weight", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous sen\u00ad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not). 21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}.", "Coreference resolution is a field in which major progress has been made in the last decade After a concentration on rule-based systems cf eg ##CITATION## Poesio et al Markert and Nissim machine learning methods were embraced cf   ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "Coreference resolution is a field in which major progress has been made in the last decade After a concentration on rule-based systems cf eg ##CITATION## Poesio et al Markert and Nissim machine learning methods were embraced cf   ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources.", "Coreference resolution is a field in which major progress has been made in the last decade After a concentration on rule-based systems cf eg ##CITATION## Poesio et al Markert and Nissim machine learning methods were embraced cf   ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Coreference resolution is a field in which major progress has been made in the last decade After a concentration on rule-based systems cf eg ##CITATION## Poesio et al Markert and Nissim machine learning methods were embraced cf   ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Coreference resolution is a field in which major progress has been made in the last decade After a concentration on rule-based systems cf eg ##CITATION## Poesio et al Markert and Nissim machine learning methods were embraced cf   ", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "They use limited knowledge lexical morphological and syntacticinformation sources for the detection of the cor rect antecedent These proposals have report high success rates for English 897% ##CITATION## and for Spanish 83% Ferr\u00abandez et alTaking this basis it is possible to improve the re sults of a resolution method adding other sources such us semantic pragmatic world-knowledge or indeed statistical informationWe have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "They use limited knowledge lexical morphological and syntacticinformation sources for the detection of the cor rect antecedent These proposals have report high success rates for English 897% ##CITATION## and for Spanish 83% Ferr\u00abandez et alTaking this basis it is possible to improve the re sults of a resolution method adding other sources such us semantic pragmatic world-knowledge or indeed statistical informationWe have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess ", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\u00ad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\u00ad ment test. For this purpose we have drawn up a compre\u00ad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\u00ad tion has addressed the problem of \"agreement excep\u00ad tions\". antecedent. If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.", "They use limited knowledge lexical morphological and syntacticinformation sources for the detection of the cor rect antecedent These proposals have report high success rates for English 897% ##CITATION## and for Spanish 83% Ferr\u00abandez et alTaking this basis it is possible to improve the re sults of a resolution method adding other sources such us semantic pragmatic world-knowledge or indeed statistical informationWe have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess ", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "They use limited knowledge lexical morphological and syntacticinformation sources for the detection of the cor rect antecedent These proposals have report high success rates for English 897% ##CITATION## and for Spanish 83% Ferr\u00abandez et alTaking this basis it is possible to improve the re sults of a resolution method adding other sources such us semantic pragmatic world-knowledge or indeed statistical informationWe have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "They use limited knowledge lexical morphological and syntacticinformation sources for the detection of the cor rect antecedent These proposals have report high success rates for English 897% ##CITATION## and for Spanish 83% Ferr\u00abandez et alTaking this basis it is possible to improve the re sults of a resolution method adding other sources such us semantic pragmatic world-knowledge or indeed statistical informationWe have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess ", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "G U I TA R Poesio and AlexandrovKabadjov is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of ##CITATION##\u2019s algorithm for pronoun resolution ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators.", "G U I TA R Poesio and AlexandrovKabadjov is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of ##CITATION##\u2019s algorithm for pronoun resolution ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.", "G U I TA R Poesio and AlexandrovKabadjov is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of ##CITATION##\u2019s algorithm for pronoun resolution ##CITATION##", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "G U I TA R Poesio and AlexandrovKabadjov is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of ##CITATION##\u2019s algorithm for pronoun resolution ##CITATION##", 1, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "In most systems ##CITATION##Lappin and Leass the weights that are assigned for different anaphor-antecedent relationships are programmer dependent", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\u00ad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\u00ad tion 0 + section heading 0 + collocation 0 + referen\u00ad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the\u00b7 antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word.", "In most systems ##CITATION##Lappin and Leass the weights that are assigned for different anaphor-antecedent relationships are programmer dependent", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "In most systems ##CITATION##Lappin and Leass the weights that are assigned for different anaphor-antecedent relationships are programmer dependent", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.", "In most systems ##CITATION##Lappin and Leass the weights that are assigned for different anaphor-antecedent relationships are programmer dependent", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["As already mentioned, each of the antecedent in\u00ad dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent.", "In most systems ##CITATION##Lappin and Leass the weights that are assigned for different anaphor-antecedent relationships are programmer dependent", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated. Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent. For in\u00ad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.", "In most systems ##CITATION##Lappin and Leass the weights that are assigned for different anaphor-antecedent relationships are programmer dependent", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish  . For the time being, we are using the same scores for Polish.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus such as in  ", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus such as in  ", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["It is also quite fre\u00ad quent with imperative constructions. Example: To print the paper, you can stand the printeri up or lay iti flat. To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperi\u2022 form iti and align iti\u2022 then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus such as in  ", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus such as in  ", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["This result is comparable with the results described in  . For the training data from the genre of technical manuals, it was rule 5   which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion.", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus such as in  ", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimi\u00ad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "The approach is presented as a knowledge poor anaphora resolution algorithm ##CITATION## R 1998] which makes use of POS and NP chunking it tries to individuate pleonastic \u201cit\u201d occurrences and assigns animacy", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["3.3 Comparison to similar approaches: compara\u00ad. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog\u00ad NIAC   approach which features \"high precision coreference with limited knowledge and linguistics resources\". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns an\u00ad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases.", "The approach is presented as a knowledge poor anaphora resolution algorithm ##CITATION## R 1998] which makes use of POS and NP chunking it tries to individuate pleonastic \u201cit\u201d occurrences and assigns animacy", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Usually knowledge-based ap\u00ad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\u00ad tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\u00ad plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.", "The approach is presented as a knowledge poor anaphora resolution algorithm ##CITATION## R 1998] which makes use of POS and NP chunking it tries to individuate pleonastic \u201cit\u201d occurrences and assigns animacy", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "The approach is presented as a knowledge poor anaphora resolution algorithm ##CITATION## R 1998] which makes use of POS and NP chunking it tries to individuate pleonastic \u201cit\u201d occurrences and assigns animacy", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "The approach is presented as a knowledge poor anaphora resolution algorithm ##CITATION## R 1998] which makes use of POS and NP chunking it tries to individuate pleonastic \u201cit\u201d occurrences and assigns animacy", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in  . For the training data from the genre of technical manuals, it was rule 5   which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. languages An attractive feature of any NLP approach would be its language \"universality\".", "Some of the limitations of the traditional rule based approaches ##CITATION## could be overcome by machine learning techniques which allow automating the acquisition of knowledge from annotated corpora", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording. Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\". This preference can be explained in terms of sali\u00ad ence from the point of view of the centering theory. The latter proposes the ranking \"subject, direct ob\u00ad ject, indirect object\"   and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0).", "Some of the limitations of the traditional rule based approaches ##CITATION## could be overcome by machine learning techniques which allow automating the acquisition of knowledge from annotated corpora", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\u00ad tion. We used the robust approach as a basis for devel\u00ad oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish  . For the time being, we are using the same scores for Polish. The evaluation for Polish was based technical manuals available on the Internet  .", "Some of the limitations of the traditional rule based approaches ##CITATION## could be overcome by machine learning techniques which allow automating the acquisition of knowledge from annotated corpora", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section).", "Some of the limitations of the traditional rule based approaches ##CITATION## could be overcome by machine learning techniques which allow automating the acquisition of knowledge from annotated corpora", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.", "Some of the limitations of the traditional rule based approaches ##CITATION## could be overcome by machine learning techniques which allow automating the acquisition of knowledge from annotated corpora", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "Other pronominal resolution approaches promote knowledge-poor methods ##CITATION## eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Other pronominal resolution approaches promote knowledge-poor methods ##CITATION## eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.", "Other pronominal resolution approaches promote knowledge-poor methods ##CITATION## eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Other pronominal resolution approaches promote knowledge-poor methods ##CITATION## eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications.", "Other pronominal resolution approaches promote knowledge-poor methods ##CITATION## eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents", 1, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["If still no choice is possible, the most recent from the remaining candi\u00ad dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "The CogNIAC algorithm {Baldwin uses six heuristic rules to resalve coreference whereas the algorithm presented in   is based on a limited set of preferences eg definitiveness l cal reiteration or immediate reference", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well  . Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %).", "The CogNIAC algorithm {Baldwin uses six heuristic rules to resalve coreference whereas the algorithm presented in   is based on a limited set of preferences eg definitiveness l cal reiteration or immediate reference", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00ad maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred.", "The CogNIAC algorithm {Baldwin uses six heuristic rules to resalve coreference whereas the algorithm presented in   is based on a limited set of preferences eg definitiveness l cal reiteration or immediate reference", 0, "Robust pronoun resolution with limited knowledge", "The approach."], ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent. The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term prefer\u00ad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\u00ad independent. In the following we shall outline some the indicators used and shall illustrate them by ex\u00ad amples.", "The CogNIAC algorithm {Baldwin uses six heuristic rules to resalve coreference whereas the algorithm presented in   is based on a limited set of preferences eg definitiveness l cal reiteration or immediate reference", 1, "Robust pronoun resolution with limited knowledge", "The approach."], ["Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. There\u00ad fore, the 93.3% success rate (see above) demon\u00ad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for Ara\u00ad bic as well  .", "However the difficulty of our task can be verified according to the baseline experiment results reported in ##CITATION## Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 625% whereas in our experiments only 439% of the most recent candidates are resolved correctly as the an\u00ad tecedent cf ", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["The sample texts con\u00ad tained 180 pronouns among which were 120 in\u00ad stances of exophoric reference (most being zero pro\u00ad nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\u00ad ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we com\u00ad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear su\u00ad periority over both baseline models. The first Base\u00ad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "However the difficulty of our task can be verified according to the baseline experiment results reported in ##CITATION## Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 625% whereas in our experiments only 439% of the most recent candidates are resolved correctly as the an\u00ad tecedent cf ", 0, "Robust pronoun resolution with limited knowledge", "Adapting the robust  approach for other."], ["Our evalua\u00ad tion, based on 63 examples  , indicates a success rate of 95.2% (and critical success rate 89.3 %). We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).", "However the difficulty of our task can be verified according to the baseline experiment results reported in ##CITATION## Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 625% whereas in our experiments only 439% of the most recent candidates are resolved correctly as the an\u00ad tecedent cf ", 0, "Robust pronoun resolution with limited knowledge", "Conclusion."], ["The approach being robust (an attempt is made to resolve each anaphor and a pro\u00ad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%. Given that our knowledge\u00ad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\u00ad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.", "However the difficulty of our task can be verified according to the baseline experiment results reported in ##CITATION## Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 625% whereas in our experiments only 439% of the most recent candidates are resolved correctly as the an\u00ad tecedent cf ", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation. In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%. Given that our knowledge\u00ad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\u00ad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon. Typically, our preference-based model proved superior to both baseline models when the antece\u00ad dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.", "However the difficulty of our task can be verified according to the baseline experiment results reported in ##CITATION## Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 625% whereas in our experiments only 439% of the most recent candidates are resolved correctly as the an\u00ad tecedent cf ", 1, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["In addition, preliminary experiments show that the approach can be success\u00ad fully adapted for other languages with minimum modifications. For the most part, anaphora resolution has focused on traditional linguistic methods  . However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  .", "Whereas knowledge-based systems like   and   combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains more re\u00ad cent knowledge-poor approaches like   and   address the problem without sophisticated linguistic knowledge", 0, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge\u00ad based system, however, is that it is a very labour\u00ad intensive and time-consuming task. This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Whereas knowledge-based systems like   and   combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains more re\u00ad cent knowledge-poor approaches like   and   address the problem without sophisticated linguistic knowledge", 0, "Robust pronoun resolution with limited knowledge", "ABSTRACT"], ["Lack of semantic knowledge rules out the use of verb se\u00ad mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the re\u00ad sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of ante\u00ad cedent indicators are taken into account and no fac\u00ad tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\u00ad ences (see below).", "Whereas knowledge-based systems like   and   combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains more re\u00ad cent knowledge-poor approaches like   and   address the problem without sophisticated linguistic knowledge", 0, "Robust pronoun resolution with limited knowledge", "Apply  the antecedent  indicators  to each  poten\u00ad."], ["While various alternatives have been proposed, making use of e.g. neural networks, a situation se\u00ad mantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\u00ad cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "Whereas knowledge-based systems like   and   combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains more re\u00ad cent knowledge-poor approaches like   and   address the problem without sophisticated linguistic knowledge", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\u00ad guistic knowledge  . Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic). With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Whereas knowledge-based systems like   and   combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains more re\u00ad cent knowledge-poor approaches like   and   address the problem without sophisticated linguistic knowledge", 1, "Robust pronoun resolution with limited knowledge", "Introduction"], ["Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT  . A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: ##CITATION## combined word recurrence collocations and a thesaurus; Beeferman et al relied on both collocations and linguistic cues", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["The subject change point occurred at sentence 13, just two sentences after a predicted subject change at sentence 11. In this investigation, word repetition alone achieved better results than using either collocation or relation weights individually. The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text. The objective of the current investigation was to determine whether all troughs coincide with a subject change. The troughs placed by the algorithm were compared to the segmentations identified by test subjects for the same texts.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: ##CITATION## combined word recurrence collocations and a thesaurus; Beeferman et al relied on both collocations and linguistic cues", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 1: Locating Subject Change."], ["Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by look\u00ad up in a lexicon   comprising inflection and stem word pair records (e.g. \"orange oranges\"). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio   and outputted to a lexicon. Collocations were automatically located in a text by looking up pairwise words in this lexicon. Figure 1 shows the record for the headword orange followed by its collocates.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: ##CITATION## combined word recurrence collocations and a thesaurus; Beeferman et al relied on both collocations and linguistic cues", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: ##CITATION## combined word recurrence collocations and a thesaurus; Beeferman et al relied on both collocations and linguistic cues", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Reynar   compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information  . Another approach to text segmentation is the detection of semantically related words. Hearst   incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results  . Related words have been located using spreading activation on a semantic network  , although only one text was segmented.", "Thus Table 1 confirms the fact reported in   that using collocations together with word recurrence is an interesting approach for text segmentation", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Ponte and Croft   used word co-occurrences to expand the number of terms for matching. Reynar   compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information  . Another approach to text segmentation is the detection of semantically related words. Hearst   incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results  .", "Thus Table 1 confirms the fact reported in   that using collocations together with word recurrence is an interesting approach for text segmentation", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["The transition from the first article to the second represented a known subject change point. Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms  . For each text, the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text. An error margin of one sentence either side of this point, determined by empirical analysis, was allowed. Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points.", "Thus Table 1 confirms the fact reported in   that using collocations together with word recurrence is an interesting approach for text segmentation", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 1: Locating Subject Change."], ["Comparison of troughs to segmentation points placed by the test subjects. changes. Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62. These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation. As an example, a text segmentation algorithm developed by Hearst   based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61.", "Thus Table 1 confirms the fact reported in   that using collocations together with word recurrence is an interesting approach for text segmentation", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights  .", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: Job- bins and Evett combined word recurrence co-occurrences and a thesaurus; Beeferman et al relied on both lexical modeling and discourse cues; Galley et al made use of word reiteration through lexical chains and discourse cues", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT  . A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: Job- bins and Evett combined word recurrence co-occurrences and a thesaurus; Beeferman et al relied on both lexical modeling and discourse cues; Galley et al made use of word reiteration through lexical chains and discourse cues", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Comparison of troughs to segmentation points placed by the test subjects. changes. Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62. These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation. As an example, a text segmentation algorithm developed by Hearst   based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: Job- bins and Evett combined word recurrence co-occurrences and a thesaurus; Beeferman et al relied on both lexical modeling and discourse cues; Galley et al made use of word reiteration through lexical chains and discourse cues", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: Job- bins and Evett combined word recurrence co-occurrences and a thesaurus; Beeferman et al relied on both lexical modeling and discourse cues; Galley et al made use of word reiteration through lexical chains and discourse cues", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation.", "When no external knowledge is used this similarity is only based on the strict reiteration of words But it can be enhanced by taking into account semantic relations between words This was done for instance in   by taking semantic relations from Roget\u2019s Thesaurus  ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine   and between individual sentences is unreliable  . Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. troughs placed subject change linguistic feature points located average std.", "When no external knowledge is used this similarity is only based on the strict reiteration of words But it can be enhanced by taking into account semantic relations between words This was done for instance in   by taking semantic relations from Roget\u2019s Thesaurus  ", 0, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT  . A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex).", "When no external knowledge is used this similarity is only based on the strict reiteration of words But it can be enhanced by taking into account semantic relations between words This was done for instance in   by taking semantic relations from Roget\u2019s Thesaurus  ", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Hearst   incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results  . Related words have been located using spreading activation on a semantic network  , although only one text was segmented. Another approach extracted semantic information from Roget's Thesaurus (RT). Lexical cohesion relations   between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ). It was reported that the lexical chains closely correlated to the intentional structure   of the texts, where the start and end of chains coincided with the intention ranges.", "When no external knowledge is used this similarity is only based on the strict reiteration of words But it can be enhanced by taking into account semantic relations between words This was done for instance in   by taking semantic relations from Roget\u2019s Thesaurus  ", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "When no external knowledge is used this similarity is only based on the strict reiteration of words But it can be enhanced by taking into account semantic relations between words This was done for instance in   by taking semantic relations from Roget\u2019s Thesaurus  ", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides This is done in our case by applying the Dice coefficient between the two sides of the focus window following   ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) 41 Collocations are located by looking up word pairs in the collocation lexicon. Relation weights are word repetition 8.5 3.62 (97.6%) calculated between pairwise words according to their location in RT. The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows. Scores plotted on a graph show a series of peaks (high scores) and troughs (low scores). Low scores indicate a weak collocation 5.8 3.70 40 relation weights (95.2%) word repetition 40 collocation 6.4 4.72 (95.2%) relation weights 39 level of cohesion.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides This is done in our case by applying the Dice coefficient between the two sides of the focus window following   ", 0, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights. This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects. Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides This is done in our case by applying the Dice coefficient between the two sides of the focus window following   ", 0, "Text Segmentation Using Reiteration and Collocation", "ABSTRACT"], ["General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role. Consequently, it was not further considered. Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing). The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers. A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel).", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides This is done in our case by applying the Dice coefficient between the two sides of the focus window following   ", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights  .", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides This is done in our case by applying the Dice coefficient between the two sides of the focus window following   ", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights  . Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by look\u00ad up in a lexicon   comprising inflection and stem word pair records (e.g. \"orange oranges\"). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio   and outputted to a lexicon.", "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides This is done in our case by applying the Dice coefficient between the two sides of the focus window following   ", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine   and between individual sentences is unreliable  . Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. troughs placed subject change linguistic feature points located average std.", "This evaluation is also a weak point as cardWl \u2229 Wr  only relies on word reiteration As a consequence two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations as in   ", 0, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated. This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text. 1 Background Theory: Lexical Cohesion. Cohesion concerns how words in a text are related.", "This evaluation is also a weak point as cardWl \u2229 Wr  only relies on word reiteration As a consequence two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations as in   ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights  .", "This evaluation is also a weak point as cardWl \u2229 Wr  only relies on word reiteration As a consequence two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations as in   ", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect. Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows. The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered.", "This evaluation is also a weak point as cardWl \u2229 Wr  only relies on word reiteration As a consequence two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations as in   ", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows. The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered. Areas for improving the segmentation algorithm include incorporation of a threshold for troughs.", "This evaluation is also a weak point as cardWl \u2229 Wr  only relies on word reiteration As a consequence two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations as in   ", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered. Areas for improving the segmentation algorithm include incorporation of a threshold for troughs. Currently, all troughs indicate a subject change, however, minor fluctuations in scores may be discounted.", "This evaluation is also a weak point as cardWl \u2229 Wr  only relies on word reiteration As a consequence two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations as in   ", 1, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["A method is presented for segmenting text into subtopic areas. The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity. The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.", "In fact the way we use relations between words is closer to   even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units ", 0, "Text Segmentation Using Reiteration and Collocation", "ABSTRACT"], ["Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated. This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text. 1 Background Theory: Lexical Cohesion. Cohesion concerns how words in a text are related.", "In fact the way we use relations between words is closer to   even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT  . A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.", "In fact the way we use relations between words is closer to   even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units ", 0, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation. To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.", "In fact the way we use relations between words is closer to   even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units ", 1, "Text Segmentation Using Reiteration and Collocation", "Identifying Lexical Cohesion."], ["The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text.", "This network could also be used more directly for topic segmentation as in  ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Future work with this algorithm should include application to longer documents. With trough thresholding the segments identified in longer documents could detect significant subject changes. Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.", "This network could also be used more directly for topic segmentation as in  ", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley   and later, Hearst   extracted related text pmtions by matching high frequency terms.", "This network could also be used more directly for topic segmentation as in  ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex. The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine   and between individual sentences is unreliable  .", "This network could also be used more directly for topic segmentation as in  ", 1, "Text Segmentation Using Reiteration and Collocation", "Proposed Segmentation Algorithm."], ["Future work with this algorithm should include application to longer documents. With trough thresholding the segments identified in longer documents could detect significant subject changes. Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.", "In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved ", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["Ponte and Croft   used word co-occurrences to expand the number of terms for matching. Reynar   compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information  . Another approach to text segmentation is the detection of semantically related words. Hearst   incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results  .", "In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness  . The degree to which a segmentation point was 'missed' by a trough, for instance, is not considered.", "In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved ", 0, "Text Segmentation Using Reiteration and Collocation", "Experiment 2: Test Subject Evaluation."], ["This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects. Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation.", "In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved ", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document.", "In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the user\u2019s needs are retrieved ", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Related words have been located using spreading activation on a semantic network  , although only one text was segmented. Another approach extracted semantic information from Roget's Thesaurus (RT). Lexical cohesion relations   between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ). It was reported that the lexical chains closely correlated to the intentional structure   of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations.", "Indeed the primary goal of semantic relations is obviously to ensure that two semantically related words eg \u201ccar\u201d and \u201cdrive\u201d contribute to the lexical cohesion thus avoiding erroneous topic boundaries between two such words These different methods can use semantic relations that are manually defined by experts as in Morris and Hirst  or extracted automatically from corpora Ferret ##CITATION## ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["A method is presented for segmenting text into subtopic areas. The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity. The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights. This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.", "Indeed the primary goal of semantic relations is obviously to ensure that two semantically related words eg \u201ccar\u201d and \u201cdrive\u201d contribute to the lexical cohesion thus avoiding erroneous topic boundaries between two such words These different methods can use semantic relations that are manually defined by experts as in Morris and Hirst  or extracted automatically from corpora Ferret ##CITATION## ", 0, "Text Segmentation Using Reiteration and Collocation", "ABSTRACT"], ["Ties can be anaphoric or cataphoric, and located at both the sentential and suprasentential level. Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure.", "Indeed the primary goal of semantic relations is obviously to ensure that two semantically related words eg \u201ccar\u201d and \u201cdrive\u201d contribute to the lexical cohesion thus avoiding erroneous topic boundaries between two such words These different methods can use semantic relations that are manually defined by experts as in Morris and Hirst  or extracted automatically from corpora Ferret ##CITATION## ", 0, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation.", "Indeed the primary goal of semantic relations is obviously to ensure that two semantically related words eg \u201ccar\u201d and \u201cdrive\u201d contribute to the lexical cohesion thus avoiding erroneous topic boundaries between two such words These different methods can use semantic relations that are manually defined by experts as in Morris and Hirst  or extracted automatically from corpora Ferret ##CITATION## ", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation. General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role.", "Indeed the primary goal of semantic relations is obviously to ensure that two semantically related words eg \u201ccar\u201d and \u201cdrive\u201d contribute to the lexical cohesion thus avoiding erroneous topic boundaries between two such words These different methods can use semantic relations that are manually defined by experts as in Morris and Hirst  or extracted automatically from corpora Ferret ##CITATION## ", 1, "Text Segmentation Using Reiteration and Collocation", "Introduction"], ["Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here. Al ig n m en t M o d e l P R A E R e n \u2192 c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.", "##CITATION##  explored a model with a word order and fertility model as described above but based their work on the EM algorithm using Gibbs sampling only for approximating the expectations", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score   using the Moses system  . The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations.", "##CITATION##  explored a model with a word order and fertility model as described above but based their work on the EM algorithm using Gibbs sampling only for approximating the expectations", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128).", "##CITATION##  explored a model with a word order and fertility model as described above but based their work on the EM algorithm using Gibbs sampling only for approximating the expectations", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "##CITATION##  explored a model with a word order and fertility model as described above but based their work on the EM algorithm using Gibbs sampling only for approximating the expectations", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "##CITATION##  explored a model with a word order and fertility model as described above but based their work on the EM algorithm using Gibbs sampling only for approximating the expectations", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model  . There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "##CITATION##  explored a model with a word order and fertility model as described above but based their work on the EM algorithm using Gibbs sampling only for approximating the expectations", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "PAPER"], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["There have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments   or IBM Model 4 alignments   as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model  , but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion  . Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al.", "Recent work ##CITATION## described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.   applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .", " fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.", " fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128).", " fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model  . There are three kinds of important information for word alignment models: lexicality, locality and fertility.", " fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.   applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", " fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Following prior work   we augment the standard HMM with a fertility distribution", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Following prior work   we augment the standard HMM with a fertility distribution", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Following prior work   we augment the standard HMM with a fertility distribution", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency   from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably.", "Following prior work   we augment the standard HMM with a fertility distribution", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e). Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter.", "Prior work addressed this by using the single parameter Poisson distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words. Table 1, Figure 1, and Figure 2 shows the AER results for different models.", "Prior work addressed this by using the single parameter Poisson distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words.", "Prior work addressed this by using the single parameter Poisson distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)).", "Prior work addressed this by using the single parameter Poisson distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence.", "Prior work addressed this by using the single parameter Poisson distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "The prior work compared Viterbi with a form of local search  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence. Therefore, we assume that \u03c6\u01eb follows a Poisson distribution with parameter I \u03bb(\u01eb). Now P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (\u03c6I |e2I +1)P (\u03c6\u01eb|\u03c6I , e2I +1) \u00d7 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f j\u22121, aj\u22121, e2I +1, \u03c6I , \u03c6\u01eb) j=1 1 1 1 1 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) = n \u03bb(ei) e\u2212\u03bb(ei ) 1 1 1 I \u03c6 1 \u03bb(e ) \u03c6i! \u00d7 = n \u03bb(ei) i e\u2212 i i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212I \u03bb(\u01eb) \u03c6\u01eb!", "The prior work compared Viterbi with a form of local search  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "The prior work compared Viterbi with a form of local search  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.", "The prior work compared Viterbi with a form of local search  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "##CITATION##  use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing. More samples give no further improvement. Initially, the fertility IBM Model 1 and fertility HMM did not perform well. If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e). Hence, smoothing is needed.", "##CITATION##  use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["(2I +1)J \u03b4(fj , f )\u03b4(ei, e) J ! , we have: c(a|a\u2032; f J , e2I +1) = j P\u02dc(aJ |f J , e2I +1) \u00d7 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n \u03bb(ei)\u03c6i e\u2212\u03bb(ei ) \u00d7 i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u00d7 c(\u03c6|e; f1 , e1 ) = \u03b4(aj , a)\u03b4(aj\u22121, a\u2032) j P\u02dc(a1 |f1 , e1 ) \u00d7 J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 \u03c6 \u03b4(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)\u03b4(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency. We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability. Although we can estimate the parameters by using 1 1  . The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.", "##CITATION##  use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Expectation Maximization Algorithm."], ["During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6. This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).", "##CITATION##  use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6. This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step.", "##CITATION##  use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["This Gibbs sampling method updates parameters constantly, so it is an \u201conline learning\u201d algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step. In this case, the fertility hidden Markov model is not faster than the HMM.", "##CITATION##  use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We need more the HMM do in the EM algorithms. The algorithm aJ J J 1 for the E-step on one machine (all machines are independent) is in Algorithm 1. For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . . , J ) is O(tI J ). This is the same complexity as the HMM if t is O(I ), and it has lower complexity if t is a constant.", "Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.   applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. where \u03b4 is the Kronecker delta function: ( 1 if x = y \u03b4(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is \"\u00a32I +1 \"\u00a32I +1 \u03c6i. We define \u03c6\u01eb \u2261 2I +1 i=I +1 \u03c6i. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .", "Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Words from position I + 1 to 2I + 1 in the target sentence are all empty words. We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.", "Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["However, IBM Model 4 is so complex that most researches use the GIZA++ software package  , and IBM Model 4 itself is treated as a black box. The complexity in IBM Model 4 makes it hard to understand and to improve. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. We also want it to be accurate and computationally efficient. There have been many years of research on word alignment.", "Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model  , but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4.", "Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["There have been works that try to simulate fertility using the hidden Markov model  , but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596\u2013605, MIT, Massachusetts, USA, 911 October 2010.", "Zhao proposes a brief fertility based HMM model8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["Instead, we do \u201cbatch learning\u201d: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step. In this case, the fertility hidden Markov model is not faster than the HMM. Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm. In the testing stage, the sampling algorithm is the same as above except that we keep the alignments 1 that maximize P (a1 , f1 |e2I +1).", "Another interesting extension of the HMM alignment is presented in ##CITATION##  who added a fertility distribution to the HMM", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["Words from position I + 1 to 2I + 1 in the target sentence are all empty words. We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.", "Another interesting extension of the HMM alignment is presented in ##CITATION##  who added a fertility distribution to the HMM", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["2 For fertility IBM Model 1, we only need to compute I + 1. values because e2I +1 are identical empty words. samples in the testing stage because it is unlikely to get to the optimal alignments by sampling a few times for each alignment. On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level. Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.", "Another interesting extension of the HMM alignment is presented in ##CITATION##  who added a fertility distribution to the HMM", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Another interesting extension of the HMM alignment is presented in ##CITATION##  who added a fertility distribution to the HMM", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.", "Recent work ##CITATION## de\u00ad scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es\u00ad timation", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "PAPER"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Recent work ##CITATION## de\u00ad scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es\u00ad timation", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["(2I +1)J \u03b4(fj , f )\u03b4(ei, e) J ! , we have: c(a|a\u2032; f J , e2I +1) = j P\u02dc(aJ |f J , e2I +1) \u00d7 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n \u03bb(ei)\u03c6i e\u2212\u03bb(ei ) \u00d7 i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u00d7 c(\u03c6|e; f1 , e1 ) = \u03b4(aj , a)\u03b4(aj\u22121, a\u2032) j P\u02dc(a1 |f1 , e1 ) \u00d7 J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 \u03c6 \u03b4(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)\u03b4(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency. We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability. Although we can estimate the parameters by using 1 1  . The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.", "Recent work ##CITATION## de\u00ad scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es\u00ad timation", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Expectation Maximization Algorithm."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Recent work ##CITATION## de\u00ad scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es\u00ad timation", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.   applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "Recent work ##CITATION## de\u00ad scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es\u00ad timation", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "Following prior work   we augment the standard HMM with a fertility dis\u00ad tribution", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "Following prior work   we augment the standard HMM with a fertility dis\u00ad tribution", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "Following prior work   we augment the standard HMM with a fertility dis\u00ad tribution", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency   from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably.", "Following prior work   we augment the standard HMM with a fertility dis\u00ad tribution", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["Moore   also suggested adding multiple empty words to the target sentence for IBM Model 1. After we add I + 1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a : j \u2192 i, i = aj where j = 1, 2, . . . , J and i = 1, 2, . . . , 2I + 1. Words from position I + 1 to 2I + 1 in the target sentence are all empty words.", "I Pr ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion \u03bb(e) = s c(\u03c6| e; f , e ) s c(k|e; f (s), e(s)) (6) probability: the distortion probability is 0 if fertility where s is the number of bilingual sentences, andis not consistent with alignments, and uniform oth c(f |e; f J 2I +1 \u02dc J J 2I +1 erwise. The total number of consistent fertility and 1 , e1 ) = P (a1 |f1 , e1 ) \u00d7 J alignments is J ! . Replacing 1 with a1 \u03c6\u01eb ! J i ! \u03c6\u01eb ! J i ! (2I +1)J \u03b4(fj , f )\u03b4(ei, e) J ! , we have: c(a|a\u2032; f J , e2I +1) = j P\u02dc(aJ |f J , e2I +1) \u00d7 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n \u03bb(ei)\u03c6i e\u2212\u03bb(ei ) \u00d7 i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u00d7 c(\u03c6|e; f1 , e1 ) = \u03b4(aj , a)\u03b4(aj\u22121, a\u2032) j P\u02dc(a1 |f1 , e1 ) \u00d7 J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 \u03c6 \u03b4(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)\u03b4(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency. We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.", "I Pr ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence. Therefore, we assume that \u03c6\u01eb follows a Poisson distribution with parameter I \u03bb(\u01eb).", "I Pr ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model  . There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "I Pr ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.   applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "I Pr ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e). Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words. Table 1, Figure 1, and Figure 2 shows the AER results for different models.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter \u03bb(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter \u03bb(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)).", "Prior work addressed this by using the single parameter Pois\u00ad son distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I \u03c6i 1 \u03bb(ei ) summing over fertilities and alignments: n \u03bb(ei) e\u2212 \u00d7 P (f J |e2I +1) = \"\u00a3 I J P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1). i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence.", "Prior work addressed this by using the single parameter Pois\u00ad son distribution forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10\u22128), so they never become too small. We run both models for 5 iterations.", "The prior work compared Viterbi with a form of local search  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["i=1 \u03c6i! 1 1 \u03c61 ,\u03c6\u01eb ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable \u03c6i, and we assume \u03c6i follows a Poisson distribution Poisson(\u03c6i; \u03bb(ei)). The sum of the fer (I \u03bb(\u01eb))\u03c6\u01eb e\u2212(I \u03bb(\u01eb)) \u03c6\u01eb! \u00d7 J tilities of all the empty words (\u03c6\u01eb) grows with the length of the target sentence. Therefore, we assume that \u03c6\u01eb follows a Poisson distribution with parameter I \u03bb(\u01eb). Now P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (\u03c6I |e2I +1)P (\u03c6\u01eb|\u03c6I , e2I +1) \u00d7 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f j\u22121, aj\u22121, e2I +1, \u03c6I , \u03c6\u01eb) j=1 1 1 1 1 P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1) = n \u03bb(ei) e\u2212\u03bb(ei ) 1 1 1 I \u03c6 1 \u03bb(e ) \u03c6i! \u00d7 = n \u03bb(ei) i e\u2212 i i=1 (I \u03bb(\u01eb))\u03c6\u01eb e\u2212I \u03bb(\u01eb) \u03c6\u01eb!", "The prior work compared Viterbi with a form of local search  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Fertility Hidden Markov Model."], ["We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (\u03bb(e), P (a|a\u2032) and P (f |e)) by adding a small value (10\u22128). We run both models for 5 iterations.", "The prior work compared Viterbi with a form of local search  ", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, \u00b7 \u00b7 \u00b7 aj\u22121, aj+1 \u00b7 \u00b7 \u00b7 aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, \u00b7 \u00b7 \u00b7 , aj 1, a , \u00b7 \u00b7 \u00b7 , a , f J , e2I +1) \u2212 j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"\u00a3 J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.", "The prior work compared Viterbi with a form of local search  ", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tar get language. However, IBM Model 4 is so complex that most researches use the GIZA++ software package  , and IBM Model 4 itself is treated as a black box.", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["get sentence are a set Bi, such that each element in Bi is aligned with i, and all alignments of i are in Bi. Inverted alignments are explicitly used in IBM Models 3, 4 and 5, but not in our model, which is We denote as aJ = a1, a2, . . . , aJ the alignments one reason that our model is easier to understand. between f J and eI . When a word fj is not aligned 1 1 with any word e, aj is 0. For convenience, we add an empty word \u01eb to the target sentence at position 0 (i.e., e0 = \u01eb).", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Statistical Word Alignment Models."], ["We consider words that appear less than 10 times as infrequent words. Table 1, Figure 1, and Figure 2 shows the AER results for different models. We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM. The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM. Figure 3 show the training time for different models.", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Experiments."], ["A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand.", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model  .", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model  . There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "The sequence-based model is easier to implement and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models Lopez and Resnik Liang et al DeNero and Klein ##CITATION## Bansal et al", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "ABSTRACT"], ["Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here. Al ig n m en t M o d e l P R A E R e n \u2192 c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.", "Our Gibbs sampler is similar to the MCMC algorithm in ##CITATION##   but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Gibbs Sampling for Fertility HMM."], ["Due to space constraints, we are unable to provide details for IBM Models 3, 4 and 5; see Brown et al.   and Och and Ney  . But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency   from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (\u03c6I , \u03c6\u01eb, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.", "Our Gibbs sampler is similar to the MCMC algorithm in ##CITATION##   but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Statistical Word Alignment Models."], ["We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596\u2013605, MIT, Massachusetts, USA, 911 October 2010. Qc 2010 Association for Computational Linguistics estimation. Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion  . Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4.", "Our Gibbs sampler is similar to the MCMC algorithm in ##CITATION##   but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree", 0, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility \u03c6i of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, \u03c6i = j=1 \u03b4(aj , i) which has nice probabilistic guarantees. DeNero et al.   applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "Our Gibbs sampler is similar to the MCMC algorithm in ##CITATION##   but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree", 1, "A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC", "Introduction"], ["The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place. (b) After they were released...", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition Inkpen and De\u00b4silets optical character recognition Wick et al co-reference resolution ##CITATION## or malapropism detection Bolshakov and Gelbukh", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition Inkpen and De\u00b4silets optical character recognition Wick et al co-reference resolution ##CITATION## or malapropism detection Bolshakov and Gelbukh", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition Inkpen and De\u00b4silets optical character recognition Wick et al co-reference resolution ##CITATION## or malapropism detection Bolshakov and Gelbukh", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition Inkpen and De\u00b4silets optical character recognition Wick et al co-reference resolution ##CITATION## or malapropism detection Bolshakov and Gelbukh", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases. This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics. In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used. 7 Acknowledgements. This work was supported in part by the National Science Foundation under grant IRI9704240.", "Recently ##CITATION##  have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction Their experiments were conducted on collections of texts in two topic areas terrorism and natural disasters ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["In this section, we describe how contextual role knowledge is represented and learned. Section 2.1 describes how BABAR generates training examples to use in the learning process. We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents. Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples. 2.1 Reliable Case Resolutions.", "Recently ##CITATION##  have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction Their experiments were conducted on collections of texts in two topic areas terrorism and natural disasters ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["Given a document to process, BABAR uses four modules to perform coreference resolution. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer  , and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.", "Recently ##CITATION##  have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction Their experiments were conducted on collections of texts in two topic areas terrorism and natural disasters ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released. The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context.", "Recently ##CITATION##  have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction Their experiments were conducted on collections of texts in two topic areas terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.", "Recently ##CITATION##  have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction Their experiments were conducted on collections of texts in two topic areas terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["4 Evaluation Results. 4.1 Corpora. We evaluated BABAR on two domains: terrorism and natural disasters. We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuter\u2019s text collection8 that had a subject code corresponding to natural disasters. For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X \u2229Y =S 1 \u2212 ) m1 (X ) \u2217 m2 (Y ) m1 (X ) \u2217 m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).", "Recently ##CITATION##  have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction Their experiments were conducted on collections of texts in two topic areas terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Figure 2 shows examples of lexical expectations that were learned for both domains. collected too. We will refer to the semantic classes that co-occur with a caseframe as the semantic expectations of the caseframe. Figure 3 shows examples of semantic expectations that were learned. For example, BABAR learned that agents that \u201cassassinate\u201d or \u201cinvestigate a cause\u201d are usually humans or groups (i.e., organizations).", "the dependency from the event head to an event argument depij  our model instead emits the pair of event head and dependency relation which we call a caseframe following ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags. This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain. Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected. However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse. For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent.", "the dependency from the event head to an event argument depij  our model instead emits the pair of event head and dependency relation which we call a caseframe following ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The belief value that would have been assigned to the intersection of these sets is .60*.70=.42, but this belief has nowhere to go because the null set is not permissible in the model.7 So this probability mass (.42) has to be redistributed. DempsterShafer handles this by re-normalizing all the belief values with respect to only the non-null sets (this is the purpose of the denominator in Equation 1). In our coreference resolver, we define \u03b8 to be the set of all candidate antecedents for an anaphor. Each knowledge source then assigns a probability estimate to each candidate, which represents its belief that the candidate is the antecedent for the anaphor. The probabilities are incorporated into the DempsterShafer model using Equation 1.", "the dependency from the event head to an event argument depij  our model instead emits the pair of event head and dependency relation which we call a caseframe following ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "the dependency from the event head to an event argument depij  our model instead emits the pair of event head and dependency relation which we call a caseframe following ##CITATION## ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Another source of inspiration is the work by ##CITATION##  They use contextual roles ie the role that an NP plays in an event for extracting patterns that can be used in coreference resolution showing the relevance of verbs in deciding on coreference between their arguments However they use a very small corpus two domains and do not aim to build a dictionary  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place. (b) After they were released...", "Another source of inspiration is the work by ##CITATION##  They use contextual roles ie the role that an NP plays in an event for extracting patterns that can be used in coreference resolution showing the relevance of verbs in deciding on coreference between their arguments However they use a very small corpus two domains and do not aim to build a dictionary  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too. an event. For IE, the system must be able to distinguish between semantically similar noun phrases that play different roles in an event. For example, management succession systems must distinguish between a person who is fired and a person who is hired. Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime.", "Another source of inspiration is the work by ##CITATION##  They use contextual roles ie the role that an NP plays in an event for extracting patterns that can be used in coreference resolution showing the relevance of verbs in deciding on coreference between their arguments However they use a very small corpus two domains and do not aim to build a dictionary  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released. The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context.", "Another source of inspiration is the work by ##CITATION##  They use contextual roles ie the role that an NP plays in an event for extracting patterns that can be used in coreference resolution showing the relevance of verbs in deciding on coreference between their arguments However they use a very small corpus two domains and do not aim to build a dictionary  ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.", "Another source of inspiration is the work by ##CITATION##  They use contextual roles ie the role that an NP plays in an event for extracting patterns that can be used in coreference resolution showing the relevance of verbs in deciding on coreference between their arguments However they use a very small corpus two domains and do not aim to build a dictionary  ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data. Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics.", "Another source of inspiration is the work by ##CITATION##  They use contextual roles ie the role that an NP plays in an event for extracting patterns that can be used in coreference resolution showing the relevance of verbs in deciding on coreference between their arguments However they use a very small corpus two domains and do not aim to build a dictionary  ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", " and unsupervised approaches eg Cardie and Wagstaff  ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Table 1 briefly describes the seven syntactic heuristics used by BABAR to resolve noun phrases. Words and punctuation that appear in brackets are considered optional. The anaphor and antecedent appear in boldface. 1. Reflexive pronouns with only 1 NP in scope..", " and unsupervised approaches eg Cardie and Wagstaff  ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", " and unsupervised approaches eg Cardie and Wagstaff  ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PAPER"], ["Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions. 3.1 General Knowledge Sources.", " and unsupervised approaches eg Cardie and Wagstaff  ##CITATION## ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.   also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners  . These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions.", "Since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning Soon et al Ng and Cardie manually-defined coreference patterns to mine specific kinds of data ##CITATION## Bergsma or accepted the noise inherent in unsupervised schemes Ge et al Cherry and Bergsma", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, the passive voice pattern \u201c<subject> were kidnapped\u201d and the active voice pattern \u201ckidnapped <direct object>\u201d are merged into a single normalized pattern \u201ckidnapped <patient>\u201d.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, \u201c<agent> kidnapped\u201d or \u201ckidnapped <patient>\u201d), and (2) predicate-argument relations associated with both verbs and nouns (e.g., \u201ckidnapped for <np>\u201d or \u201cvehicle with <np>\u201d). We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus. The learned patterns are then normalized and applied to the corpus. This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.", "Since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning Soon et al Ng and Cardie manually-defined coreference patterns to mine specific kinds of data ##CITATION## Bergsma or accepted the noise inherent in unsupervised schemes Ge et al Cherry and Bergsma", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data. Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics. BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions.", "Since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning Soon et al Ng and Cardie manually-defined coreference patterns to mine specific kinds of data ##CITATION## Bergsma or accepted the noise inherent in unsupervised schemes Ge et al Cherry and Bergsma", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Although these existential NPs do not need a prior referent, they may occur multiple times in a document. By definition, each existential NP uniquely specifies an object or concept, so we can infer that all instances of the same existential NP are coreferent (e.g., \u201cthe FBI\u201d always refers to the same entity). Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm   and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved. Table 1 briefly describes the seven syntactic heuristics used by BABAR to resolve noun phrases. Words and punctuation that appear in brackets are considered optional.", "Since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning Soon et al Ng and Cardie manually-defined coreference patterns to mine specific kinds of data ##CITATION## Bergsma or accepted the noise inherent in unsupervised schemes Ge et al Cherry and Bergsma", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["Ex: He was found in San Jose, where ... Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding.", "Since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning Soon et al Ng and Cardie manually-defined coreference patterns to mine specific kinds of data ##CITATION## Bergsma or accepted the noise inherent in unsupervised schemes Ge et al Cherry and Bergsma", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["In this section, we describe how contextual role knowledge is represented and learned. Section 2.1 describes how BABAR generates training examples to use in the learning process. We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents. Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples. 2.1 Reliable Case Resolutions.", "##CITATION##   used bootstrapping to extend their semantic compatibility model which they called contextual-role knowledge by identifying certain cases of easily-resolved anaphors and antecedents They give the example \u201cMr Bush disclosed the policy by reading it\u201d Once we identify that it and policy are coreferent we include read:obj:policy as part of the compatibility model  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["Ex: He was found in San Jose, where ... Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding.", "##CITATION##   used bootstrapping to extend their semantic compatibility model which they called contextual-role knowledge by identifying certain cases of easily-resolved anaphors and antecedents They give the example \u201cMr Bush disclosed the policy by reading it\u201d Once we identify that it and policy are coreferent we include read:obj:policy as part of the compatibility model  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags. This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain. Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected. However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse. For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent.", "##CITATION##   used bootstrapping to extend their semantic compatibility model which they called contextual-role knowledge by identifying certain cases of easily-resolved anaphors and antecedents They give the example \u201cMr Bush disclosed the policy by reading it\u201d Once we identify that it and policy are coreferent we include read:obj:policy as part of the compatibility model  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "##CITATION##   used bootstrapping to extend their semantic compatibility model which they called contextual-role knowledge by identifying certain cases of easily-resolved anaphors and antecedents They give the example \u201cMr Bush disclosed the policy by reading it\u201d Once we identify that it and policy are coreferent we include read:obj:policy as part of the compatibility model  ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent. Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The learned patterns are then normalized and applied to the corpus. This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes. 2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too. an event.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, management succession systems must distinguish between a person who is fired and a person who is hired. Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime. We applied the AutoSlog system   to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. For example, kidnapping victims should be extracted from the subject of the verb \u201ckidnapped\u201d when it occurs in the passive voice (the shorthand representation of this pattern would be \u201c<subject> were kidnapped\u201d).", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime. We applied the AutoSlog system   to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. For example, kidnapping victims should be extracted from the subject of the verb \u201ckidnapped\u201d when it occurs in the passive voice (the shorthand representation of this pattern would be \u201c<subject> were kidnapped\u201d). The types of patterns produced by AutoSlog are outlined in  .", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["If these sets do not overlap, then the words cannot be coreferent. The semantic caseframe expectations are used in two ways. One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves. Given an anaphor and candidate, BABAR checks (1) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate, and (2) whether the semantic classes of the candidate intersect with the semantic ex pectations of the caseframe that extracts the anaphor. If one of these checks fails then this knowledge source reports that the candidate is not a viable antecedent for the anaphor.", "##CITATION##  present a system called BABAR that uses contextual role knowledge to do coreference resolution They apply an IE component to unannotated texts to generate a set of extraction caseframes Each caseframe represents a linguistic expression and a syntactic position eg \u201cmurder of <NP>\u201d \u201ckilled <patient>\u201d From the case- frames they derive different types of contextual role knowledge for resolution for example whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes or whether they are substitutable for each other in their caseframes   ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer  , and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions.", " or Wikipedia Ponzetto and Strube and the contextual role played by an NP see ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics. BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions. In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned. Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features. Our coreference resolver also incorporates an existential noun phrase recognizer and a DempsterShafer probabilistic model to make resolution decisions.", " or Wikipedia Ponzetto and Strube and the contextual role played by an NP see ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor. If either case is true, then CFLex reports that the anaphor and candidate might be coreferent. 2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations. Semantic expectations are analogous to lexical expectations except that they represent semantic classes rather than nouns. For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe.", " or Wikipedia Ponzetto and Strube and the contextual role played by an NP see ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place.", " or Wikipedia Ponzetto and Strube and the contextual role played by an NP see ##CITATION## ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.", "##CITATION##  proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution ##CITATION## learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "##CITATION##  proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution ##CITATION## learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The learned patterns are then normalized and applied to the corpus. This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.", "##CITATION##  proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution ##CITATION## learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "##CITATION##  proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution ##CITATION## learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot.", "##CITATION##  proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution ##CITATION## learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["If no candidate satisfies this condition (which is often the case), then the anaphor is left unresolved. One of the strengths of the DempsterShafer model is its natural ability to recognize when several credible hypotheses are still in play. In this situation, BABAR takes the conservative approach and declines to make a resolution. 4 Evaluation Results. 4.1 Corpora.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts caseframes ##CITATION## and FrameNet frames Baker et al", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases. This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics. In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used. 7 Acknowledgements. This work was supported in part by the National Science Foundation under grant IRI9704240.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts caseframes ##CITATION## and FrameNet frames Baker et al", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Our results show that BABAR achieves good performance in both domains, and that the contextual role knowledge improves performance, especially on pronouns. Finally, Section 5 explains how BABAR relates to previous work, and Section 6 summarizes our conclusions. In this section, we describe how contextual role knowledge is represented and learned. Section 2.1 describes how BABAR generates training examples to use in the learning process. We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts caseframes ##CITATION## and FrameNet frames Baker et al", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Learning Contextual Role Knowledge."], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts caseframes ##CITATION## and FrameNet frames Baker et al", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.   also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners  . These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions.", "Finally several coreference systems have successfully incorporated anaphoricity determination modules eg Ng and Cardie  and ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["ments contained 322 anaphoric links. For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links. In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998). We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships. Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in \u03b8 is correct, so eliminating all of the hypotheses violates this assumption.", "Finally several coreference systems have successfully incorporated anaphoricity determination modules eg Ng and Cardie  and ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Finally several coreference systems have successfully incorporated anaphoricity determination modules eg Ng and Cardie  and ##CITATION## ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Finally several coreference systems have successfully incorporated anaphoricity determination modules eg Ng and Cardie  and ##CITATION## ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["(S2) The burglar fired the gun three times and fled. \u201cThe gun\u201d will be extracted by the caseframe \u201cfired <patient>\u201d. Its correct antecedent is \u201ca revolver\u201d, which is extracted by the caseframe \u201ckilled with <NP>\u201d. If \u201cgun\u201d and \u201crevolver\u201d refer to the same object, then it should also be acceptable to say that Fred was \u201ckilled with a gun\u201d and that the burglar \u201cfireda revolver\u201d. During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor.", "The DempsterShafer rule Dempster which combines the positive and negative pairwise decisions to score a partition is used by Kehler  and ##CITATION##  to identify the most probable NP partition", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Consequently, we cut their evidence values in half to lessen their influence. 3.2 The DempsterShafer Decision Model. BABAR uses a DempsterShafer decision model   to combine the evidence provided by the knowledge sources. Our motivation for using DempsterShafer is that it provides a well-principled framework for combining evidence from multiple sources with respect to competing hypotheses. In our situation, the competing hypotheses are the possible antecedents for an anaphor.", "The DempsterShafer rule Dempster which combines the positive and negative pairwise decisions to score a partition is used by Kehler  and ##CITATION##  to identify the most probable NP partition", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The two knowledge sources that use semantic expectations, WordSemCFSem and CFSemCFSem, always return values of -1 or 0. -1 means that an NP should be ruled out as a possible antecedent, and 0 means that the knowledge source remains neutral (i.e., it has no reason to believe that they cannot be coreferent). The CFLex and CFNet knowledge sources provide positive evidence that a candidate NP and anaphor might be coreferent. They return a value in the range [0,1], where 0 indicates neutrality and 1 indicates the strongest belief that the candidate and anaphor are coreferent. BABAR uses the log-likelihood statistic   to evaluate the strength of a co-occurrence relationship.", "The DempsterShafer rule Dempster which combines the positive and negative pairwise decisions to score a partition is used by Kehler  and ##CITATION##  to identify the most probable NP partition", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions. 3.1 General Knowledge Sources.", "The DempsterShafer rule Dempster which combines the positive and negative pairwise decisions to score a partition is used by Kehler  and ##CITATION##  to identify the most probable NP partition", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "However the use of related verbs is similar in spirit to ##CITATION##\u2019s  use of patterns for inducing contextual role knowledge and the use of semantic roles is also discussed in Ponzetto and Strube ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.", "However the use of related verbs is similar in spirit to ##CITATION##\u2019s  use of patterns for inducing contextual role knowledge and the use of semantic roles is also discussed in Ponzetto and Strube ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections. Table 5: Individual Performance of KSs for Disasters (e.g., \u201cthe mayor\u201d vs. \u201cthe journalist\u201d). We also performed experiments to evaluate the impact of each type of contextual role knowledge separately. Tables 4 and 5 show BABAR\u2019s performance when just one contextual role knowledge source is used at a time. For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision.", "However the use of related verbs is similar in spirit to ##CITATION##\u2019s  use of patterns for inducing contextual role knowledge and the use of semantic roles is also discussed in Ponzetto and Strube ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "However the use of related verbs is similar in spirit to ##CITATION##\u2019s  use of patterns for inducing contextual role knowledge and the use of semantic roles is also discussed in Ponzetto and Strube ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship.", "Caseframes do not consider the dependents of the semantic role approximationsThe use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution ##CITATION## and information extraction Chambers and Juraf- sky where they serve the central unit of semantic analysis", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot. conceptual relationship in the discourse.", "Caseframes do not consider the dependents of the semantic role approximationsThe use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution ##CITATION## and information extraction Chambers and Juraf- sky where they serve the central unit of semantic analysis", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Caseframes do not consider the dependents of the semantic role approximationsThe use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution ##CITATION## and information extraction Chambers and Juraf- sky where they serve the central unit of semantic analysis", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "Caseframes do not consider the dependents of the semantic role approximationsThe use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution ##CITATION## and information extraction Chambers and Juraf- sky where they serve the central unit of semantic analysis", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.", "In addition BABAR ##CITATION## used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs not the NPs themselves ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Table 2 shows BABAR\u2019s performance. We measured recall (Rec), precision (Pr), and the F-measure (F) with recall and precision equally weighted. BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters. We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus. Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added.", "In addition BABAR ##CITATION## used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs not the NPs themselves ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "In addition BABAR ##CITATION## used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs not the NPs themselves ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "In addition BABAR ##CITATION## used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs not the NPs themselves ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PAPER"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.", "In addition BABAR ##CITATION## used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs not the NPs themselves ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["4 Evaluation Results. 4.1 Corpora. We evaluated BABAR on two domains: terrorism and natural disasters. We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuter\u2019s text collection8 that had a subject code corresponding to natural disasters. For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X \u2229Y =S 1 \u2212 ) m1 (X ) \u2217 m2 (Y ) m1 (X ) \u2217 m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).", "In addition BABAR ##CITATION## used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs not the NPs themselves ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information such as gender and number Ge et al or contextual role-knowledge ##CITATION##", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PAPER"], ["First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer  , and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information such as gender and number Ge et al or contextual role-knowledge ##CITATION##", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections. Table 5: Individual Performance of KSs for Disasters (e.g., \u201cthe mayor\u201d vs. \u201cthe journalist\u201d). We also performed experiments to evaluate the impact of each type of contextual role knowledge separately. Tables 4 and 5 show BABAR\u2019s performance when just one contextual role knowledge source is used at a time. For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information such as gender and number Ge et al or contextual role-knowledge ##CITATION##", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information such as gender and number Ge et al or contextual role-knowledge ##CITATION##", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "##CITATION##  used information extraction patterns to identify contextual clues that would determine the compatibility between NPs", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes. 2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too. an event. For IE, the system must be able to distinguish between semantically similar noun phrases that play different roles in an event.", "##CITATION##  used information extraction patterns to identify contextual clues that would determine the compatibility between NPs", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["conceptual relationship in the discourse. For example, co-occurring caseframes may reflect synonymy (e.g., \u201c<patient> kidnapped\u201d and \u201c<patient> abducted\u201d) or related events (e.g., \u201c<patient> kidnapped\u201d and \u201c<patient> released\u201d). We do not attempt to identify the types of relationships that are found. BABAR merely identifies caseframes that frequently co-occur in coreference resolutions. Te rro ris m Na tur al Dis ast ers mu rde r of < NP > kill ed <p atie nt > <a ge nt > da ma ged wa s inj ure d in < NP > <a ge nt > rep ort ed <a ge nt > add ed <a ge nt > occ urr ed cau se of < NP > <a ge nt > stat ed <a ge nt > add ed <a ge nt > wr eak ed <a ge nt > cro sse d per pet rat ed <p atie nt > con de mn ed <p atie nt > dri ver of < NP > <a ge nt > car ryi ng Figure 1: Caseframe Network Examples Figure 1 shows examples of caseframes that co-occur in resolutions, both in the terrorism and natural disaster domains.", "##CITATION##  used information extraction patterns to identify contextual clues that would determine the compatibility between NPs", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "##CITATION##  used information extraction patterns to identify contextual clues that would determine the compatibility between NPs", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.", "##CITATION##  used information extraction patterns to identify contextual clues that would determine the compatibility between NPs", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship.", "It has shown promise in improving the performance of many tasks such as name tagging Miller et al semantic class extraction Lin et al chunking Ando and Zhang coreference resolution ##CITATION## and text classification Blum and Mitchell", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution.", "It has shown promise in improving the performance of many tasks such as name tagging Miller et al semantic class extraction Lin et al chunking Ando and Zhang coreference resolution ##CITATION## and text classification Blum and Mitchell", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Te rro ris m Na tur al Dis ast ers mu rde r of < NP > kill ed <p atie nt > <a ge nt > da ma ged wa s inj ure d in < NP > <a ge nt > rep ort ed <a ge nt > add ed <a ge nt > occ urr ed cau se of < NP > <a ge nt > stat ed <a ge nt > add ed <a ge nt > wr eak ed <a ge nt > cro sse d per pet rat ed <p atie nt > con de mn ed <p atie nt > dri ver of < NP > <a ge nt > car ryi ng Figure 1: Caseframe Network Examples Figure 1 shows examples of caseframes that co-occur in resolutions, both in the terrorism and natural disaster domains. The terrorism examples reflect fairly obvious relationships: people who are murdered are killed; agents that \u201creport\u201d things also \u201cadd\u201d and \u201cstate\u201d things; crimes that are \u201cperpetrated\u201d are often later \u201ccondemned\u201d. In the natural disasters domain, agents are often forces of nature, such as hurricanes or wildfires. Figure 1 reveals that an event that \u201cdamaged\u201d objects may also cause injuries; a disaster that \u201coccurred\u201d may be investigated to find its \u201ccause\u201d; a disaster may \u201cwreak\u201d havoc as it \u201ccrosses\u201d geographic regions; and vehicles that have a \u201cdriver\u201d may also \u201ccarry\u201d items. During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent.", "It has shown promise in improving the performance of many tasks such as name tagging Miller et al semantic class extraction Lin et al chunking Ando and Zhang coreference resolution ##CITATION## and text classification Blum and Mitchell", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "It has shown promise in improving the performance of many tasks such as name tagging Miller et al semantic class extraction Lin et al chunking Ando and Zhang coreference resolution ##CITATION## and text classification Blum and Mitchell", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX pages 60\u201368 Beijing Augustrecent work on anaphora resolution Dagan and Itai  ##CITATION##  Yang and Su  and Ponzetto and Strube  all explored this task ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX pages 60\u201368 Beijing Augustrecent work on anaphora resolution Dagan and Itai  ##CITATION##  Yang and Su  and Ponzetto and Strube  all explored this task ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["Tables 4 and 5 show BABAR\u2019s performance when just one contextual role knowledge source is used at a time. For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision. For pronouns, however, all of the knowledge sources increased recall, often substantially, and with little if any decrease in precision. This result suggests that all of contextual role KSs can provide useful information for resolving anaphora. Tables 4 and 5 also show that putting all of the contextual role KSs in play at the same time produces the greatest performance gain.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX pages 60\u201368 Beijing Augustrecent work on anaphora resolution Dagan and Itai  ##CITATION##  Yang and Su  and Ponzetto and Strube  all explored this task ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent. Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX pages 60\u201368 Beijing Augustrecent work on anaphora resolution Dagan and Itai  ##CITATION##  Yang and Su  and Ponzetto and Strube  all explored this task ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u2019s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source. For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus.", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX pages 60\u201368 Beijing Augustrecent work on anaphora resolution Dagan and Itai  ##CITATION##  Yang and Su  and Ponzetto and Strube  all explored this task ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.", "##CITATION##   present a system which uses contextual role knowledge to aid coreference resolution They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge They got substantial gains on articles in two specific domains terrorism and natural disasters  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "##CITATION##   present a system which uses contextual role knowledge to aid coreference resolution They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge They got substantial gains on articles in two specific domains terrorism and natural disasters  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "ABSTRACT"], ["Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us. Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.", "##CITATION##   present a system which uses contextual role knowledge to aid coreference resolution They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge They got substantial gains on articles in two specific domains terrorism and natural disasters  ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["The problem of coreference resolution has received considerable attention, including theoretical discourse models  ), syntactic algorithms  ), and supervised machine learning systems  . Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "##CITATION##   present a system which uses contextual role knowledge to aid coreference resolution They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge They got substantial gains on articles in two specific domains terrorism and natural disasters  ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "Introduction"], ["The scoping heuristics are based on the anaphor type: for reflexive pronouns the scope is the current clause, for relative pronouns it is the prior clause following its VP, for personal pronouns it is the anaphor\u2019s sentence and two preceding sentences, and for definite NPs it is the anaphor\u2019s sentence and eight preceding sentences. The semantic agreement KS eliminates some candidates, but also provides positive evidence in one case: if the candidate and anaphor both have semantic tags human, company, date, or location that were assigned via NER or the manually labeled dictionary entries. The rationale for treating these semantic labels differently is that they are specific and reliable (as opposed to the WordNet classes, which are more coarse and more noisy due to polysemy). KS Function Ge nde r filters candidate if gender doesn\u2019t agree. Nu mb er filters candidate if number doesn\u2019t agree.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus. For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NP\u2019s caseframe. For example, if X and Y are coreferent, then both X and Y are considered to co-occur with the caseframe that extracts X as well as the caseframe that extracts Y. We will refer to the set of nouns that co-occur with a caseframe as the lexical expectations of the case- frame. Figure 2 shows examples of lexical expectations that were learned for both domains. collected too.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the \u03c72 table to obtain a confidence level. The confidence level is then used as the belief value for the knowledge source. For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent. 3 The Coreference Resolution Model. Given a document to process, BABAR uses four modules to perform coreference resolution.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 0, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Ex: Mr. Cristiani, president of the country ... Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABAR\u2019s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["BABAR uses the log-likelihood statistic   to evaluate the strength of a co-occurrence relationship. For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the \u03c72 table to obtain a confidence level. The confidence level is then used as the belief value for the knowledge source. For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent. 3 The Coreference Resolution Model.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Given a document to process, BABAR uses four modules to perform coreference resolution. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer  , and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus. Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Table 3 shows BABAR\u2019s performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us. Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.", "##CITATION##   used high-precision hand-coded rules to identify coreferent mention pairs which are then used to acquire role pairs that they refer to as Caseframe Network features They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters ", 1, "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "PPs containing \u201cby\u201d and a gerund followed by \u201cit\u201d."], ["Once the scenario had been identified, the ranked retrieval method was used, and the ranked list was sampled at different points to collect approximately 200 relevant and 200 nonrelevant articles, representing a variety of article types (feature articles, brief notices, editorials, etc.). From those candidate articles, the training and test sets were selected blindly, with later checks and corrections for imbalances in the relevant/nonrelevant categories and in article types. From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks. The selection was again done blindly, with later checks to ensure that the set was fairly representative in terms of article length and type. Note that although Named Entity, Coreference and Template Element are defined as domain-independent tasks, the articles that were used for MUC6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task.", "Named Entity evaluation began as a part of recent Message Understanding Conferences MUC whose objective was to standardize the evaluation of IE tasks ##CITATION##", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well. The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors. Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies. The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.", "Named Entity evaluation began as a part of recent Message Understanding Conferences MUC whose objective was to standardize the evaluation of IE tasks ##CITATION##", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "ACKNOWLEDGEMENTS"], ["Note also that even the best system on the third event was unable to determine that the succession event was occurring at McCannEfickson; in addition, it only partially captured the full title of the post. To its credit, however, it did recognize that the event was relevant; only two systems produced output that is recognizable as pertaining to this event. One common problem was the simple failure to recognize \"hire\" as an indicator of a succession. Two systems never filled the OTHER_ORG slot or its dependent slot, REL OTHER_ORG, despite the fact that data to fill those slots was often present; over half the IN_AND_OUT objects in the answer key contain data for those two slots. Almost without exception, systems did more poorly on those two slots than on any others in the SUCCESSION_EVENT and IN_AND_OUT objects; the best scores posted were 70% error on OTHER_ORG (median score of 79%) and 72% error on REL_OTHER ORG (median of 86%).", "Named Entity evaluation began as a part of recent Message Understanding Conferences MUC whose objective was to standardize the evaluation of IE tasks ##CITATION##", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations. Other sources of excitement are the spinoff efforts that the NE and CO tasks have inspired that bring these tasks and their potential applications to the attention of new research groups and new customer groups. In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure. Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well.", "Named Entity evaluation began as a part of recent Message Understanding Conferences MUC whose objective was to standardize the evaluation of IE tasks ##CITATION##", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision. The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode.", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS Av \u00a9Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd\u00ad ing Conferences MUC6 and MUC7 ##CITATION## Chinchor which also included evaluations of systems on the so-called coreference task a subtask of which is the resolution of definite descriptions ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well. The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a \"community\" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors. Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies. The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS Av \u00a9Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd\u00ad ing Conferences MUC6 and MUC7 ##CITATION## Chinchor which also included evaluations of systems on the so-called coreference task a subtask of which is the resolution of definite descriptions ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "ACKNOWLEDGEMENTS"], ["Test abstract The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS Av \u00a9Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd\u00ad ing Conferences MUC6 and MUC7 ##CITATION## Chinchor which also included evaluations of systems on the so-called coreference task a subtask of which is the resolution of definite descriptions ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years. The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS Av \u00a9Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd\u00ad ing Conferences MUC6 and MUC7 ##CITATION## Chinchor which also included evaluations of systems on the so-called coreference task a subtask of which is the resolution of definite descriptions ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The example passage covers a broad spectrum of the phenomena included in the task. At one end of the spectrum are the proper names and aliases, which are inherently definite and whose referent may appear anywhere in the text. In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus. On the periphery of the central phenomena are markables whose status as coreferring expressions is determined by syntax, such as predicate nominals (\"Motor Vehicles International is the biggest American auto exporter to Latin America\") and 100 90 80 70 60 50 40 30 20 10 0 0 10 20 30 appositives (\"MVI, the first company to announce such a move since the passage of the new international trade agreement\"). At the far end of the spectrum are bare common nouns, such as the prenominal \"company\" in the example, whose status as a referring expression may be questionable.", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic * Univcrsidade do Vale do Rio dos SinosUNISINOS Av \u00a9Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd\u00ad ing Conferences MUC6 and MUC7 ##CITATION## Chinchor which also included evaluations of systems on the so-called coreference task a subtask of which is the resolution of definite descriptions ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["As defined for MUC6, the ST task presents a significant challenge in terms of system portability, in that the test procedure requ~ed that all domain-specific development be done in a period of one month. For past MUC evaluations, the formal run had been conducted using the same scenario as the dry run, and the task definition was released well before the dry run. Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels. However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1. The standardized template structure minimizes the amount of idiosyncratic programming required to produce the expected types of objects, links, and slot fills.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example in the corefercnce annotation experiments for MUC6   relations other than identity were dropped due to difficulties in annotating them ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Frequently, at least one can be found in close proximity to an organization's name, e.g., as an appositive (\"Creative Artists Agency, the big Hollywood talent agency\"). Nonetheless, performance is much lower on this slot than on others. Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot? One reason for low performance is that an organization may be identified in a text solely by a descriptor, i.e., without a fill for the ORG_NAME slot and therefore without the usual local clues that the NP is in fact a relevant descriptor. It is, of course, also possible that a text may identify an organization solely by name.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example in the corefercnce annotation experiments for MUC6   relations other than identity were dropped due to difficulties in annotating them ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The TYPE slot, however, is a more difficult slot for ENAMEX than for the other subcategories. It involves a three-way distinction for ENAMEX and only a two-way distinction for NUMEX and TIMEX, and it offers the possibility of confusing names of one type with names of another, especially the possibility of confusing organization names with person names. Looking at the document section scores in table 3, we see that the error score on the body of the text was much lower than on the headline for all but a few systems. There was just one system that posted a higher error score on the body than on the headline, the baseline NMSU CRL configuration, and the difference in scores is largely due to the fact that the system overgenerated to a greater extent on the body than on the headline. Its basic strategy for 96.42 0 95.66 0 0 7 7 94.92 0 0 8 8 94.00 0 0 20 9 93.65 0 2 16 10 93.33 0 4 38 9 92.88 0 0 18 10 92.74 0 0 22 11 92.61 100 0 18 9 91.20 0 0 30 13 90.84 3 11 19 14 89.06 3 4 28 18 88.19 0 0 22 20 85.82 0 6 18 21 85.73 0 44 53 21 84.95 0 0 50 21 Table 3.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example in the corefercnce annotation experiments for MUC6   relations other than identity were dropped due to difficulties in annotating them ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example in the corefercnce annotation experiments for MUC6   relations other than identity were dropped due to difficulties in annotating them ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns. CO Results on Some Aspects of Task and on \"Walkthrough Article\" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way.", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example in the corefercnce annotation experiments for MUC6   relations other than identity were dropped due to difficulties in annotating them ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns. CO Results on Some Aspects of Task and on \"Walkthrough Article\" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way. Two useful attributes for the equivalence class as a whole would be one to distinguish individual coreference from type coreference and one to identify the general semantic type of the class (organization, person, location, time, currency, etc.).", "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example in the corefercnce annotation experiments for MUC6   relations other than identity were dropped due to difficulties in annotating them ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.. The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA. The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration. The changes occurred only in performance on identifying organizations. BBN conducted a comparative test in which the experimental configuration used a larger lexicon than the baseline configuration, but the exact nature of the difference is not known and the performance differences are very small.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system Appelt et al  and prod uced one of the top scores a recall of 59% and precision of 72% in the MUC6 Coreference Task which evaluated systems' ability to recog nize coreference among noun phrases Sund heim ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision. The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system Appelt et al  and prod uced one of the top scores a recall of 59% and precision of 72% in the MUC6 Coreference Task which evaluated systems' ability to recog nize coreference among noun phrases Sund heim ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["\") as a monetary value rather than ignoring it as a weight. In addition, a number of errors identifying entity names were made; some of those errors also showed up as errors on the Template Element task and are described in a later section of this paper. COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe. The variety of high-frequency phenomena covered by the task is partially represented in the following hypothetical example, where all bracketed text segments are considered coreferential: 428 [Motor Vehicles International Corp.] announced a major management shakeup .... [MVI] said the chief executive officer has resigned ....", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system Appelt et al  and prod uced one of the top scores a recall of 59% and precision of 72% in the MUC6 Coreference Task which evaluated systems' ability to recog nize coreference among noun phrases Sund heim ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers. EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.  Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.  Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence from anywhere in the text.  Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system Appelt et al  and prod uced one of the top scores a recall of 59% and precision of 72% in the MUC6 Coreference Task which evaluated systems' ability to recog nize coreference among noun phrases Sund heim ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The following breakdowns of overall scores on NE are computed:  by slot, i.e., for performance across tag elements, across TYPE attributes, and across tag strings;  by subcategorization, i.e., for performance on each TYPE attribute separately;  by document section, i.e., for performance on distinct subparts of the article, as identified by the SGML tags contained in the original text: <HL> (\"headline\"), <DD> (\"document date\"), <DATELINE>, and <TXT> (the body of the article). NE Results Overall Fifteen sites participated in the NE evaluation, including two that submitted two system configurations for testing and one that submitted four, for a total of 20 systems. As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites. On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well. It was also unexpected that one of the systems would match human performance on the task.", "For example the best F-score in the shared task of BioNER in COLINGJNLPBA Kim et al was 7255% Zhou and Su 1 whereas the best performance at MUC6 in which systems tried to identify general named entities such as person or organization names was an accuracy of 95% ##CITATION##", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["descriptor identify or the SCENARIO TEMPLATE A Scenario Template (ST) task captures domain-and task-specific information. Three scenarios were defined in the course of MUC6: (1) a scenario concerning the event of organizations placing orders to buy aircraft with aircraft manufacturers (the \"aircraft order\" scenario); (2) a scenario concerning the event of contract negotiations between labor unions and companies (the \"labor negotiations\" scenario); (3) a scenario concerning changes in corporate managers occupying executive posts (the \"management succession\" scenario). The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation. One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion.", "For example the best F-score in the shared task of BioNER in COLINGJNLPBA Kim et al was 7255% Zhou and Su 1 whereas the best performance at MUC6 in which systems tried to identify general named entities such as person or organization names was an accuracy of 95% ##CITATION##", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one. The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks. The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task. The association of shortened forms of the name with the full name depends on techniques that could be used for NE and CO as well as for TE. The real challenge of TE comes from associating other bits of information with the entity.", "For example the best F-score in the shared task of BioNER in COLINGJNLPBA Kim et al was 7255% Zhou and Su 1 whereas the best performance at MUC6 in which systems tried to identify general named entities such as person or organization names was an accuracy of 95% ##CITATION##", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["(The reverse is not the case, i.e., ORG_COUNTRY may be filled even if ORG_LOCALE is not, but this situation is relatively rare.) Since a missing or spurious ORG_LOCALE is likely to incur the same error in ORG_COUNTRY, the error scores for the two slots are understandably similar. 5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision.. ((XI 90  ,,v 80   04 ~) 5O 4O 20 10 0 .. 0 10 220 30 41) 50 60 7(1 80 91) Recall I(X) Figure 4.", "For example the best F-score in the shared task of BioNER in COLINGJNLPBA Kim et al was 7255% Zhou and Su 1 whereas the best performance at MUC6 in which systems tried to identify general named entities such as person or organization names was an accuracy of 95% ##CITATION##", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Statistically, large differences of up to 15 points may not be reflected as a difference in the ranking of the systems. Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]). Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal \"key to response\" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Also, the descriptor is not always close to the name, and some discourse processing may be requ~ed in order to identify it --this is likely to increase the opportunity for systems to miss the information. A third significant reason is that the response fill had to match the key fill exactly in order to be counted correct; there was no allowance made in the scoring software for assigning full or partial credit if the response fill only partially matched the key fill. It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys. TE Results on \"Walkthrough Article\" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems. Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting. Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\"", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\" As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["This period comprised the \"evaluation epoch.\" As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants. The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query. It can also be used to do unranked, Boolean retrievals. The Boolean retrieval method was used in the initial probing of the corpus to identify candidates for the Scenario Template task, because the Boolean retrieval is relatively fast, and the unranked results are easy to scan to get a feel for the variety of nonrelevant as well as relevant documents that match all or some of the query terms.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["All the participating sites also submitted systems for evaluation on the TE and NE tasks. All but one of the development teams (UDurham) had members who were veterans of MUC5. Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant. Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object. The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT \" Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7.", "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret ##CITATION## Interannotator agreement was measured on 30 texts which were examined by two annotators It was found to be 83% when one annotators templates were assumed to be correct and compared with the other  ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["All the participating sites also submitted systems for evaluation on the TE and NE tasks. All but one of the development teams (UDurham) had members who were veterans of MUC5. Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant. Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object. The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT \" Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7.", "The test corpus consists of 100 Wall Street Journal documents from the period Januaryto June 54 of which contained management succession events  ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\" As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.", "The test corpus consists of 100 Wall Street Journal documents from the period Januaryto June 54 of which contained management succession events  ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting. Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the \"evaluation epoch.\"", "The test corpus consists of 100 Wall Street Journal documents from the period Januaryto June 54 of which contained management succession events  ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation. One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion. The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure. At the top level is the TEMPLATE object, of which there is one instantiated for every document.", "The test corpus consists of 100 Wall Street Journal documents from the period Januaryto June 54 of which contained management succession events  ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion. The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure. At the top level is the TEMPLATE object, of which there is one instantiated for every document. This object points down to one or more SUCCESSION_EVENT objects if the document meets the event relevance criteria given in the task documentation.", "The test corpus consists of 100 Wall Street Journal documents from the period Januaryto June 54 of which contained management succession events  ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites. On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well. It was also unexpected that one of the systems would match human performance on the task. Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC. This test measures the amount of variability between the annotators.", "It is not clear what resources are required to adapt systems to new languages\" It is important to mention that the F-measure for the human performance on this task is about 96% ##CITATION## ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The document section results show 0% error on Document Date and Dateline, 7% error on Headline, and 6% error on Text. The subcategory error scores were 6% on Organization, 1% on Person, and 4% on Location, 8% on Date, and 0% on Money and Percent. These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates. Analysis of the results shows that some Date errors were a result of simple oversight (e.g., \"fiscal 1994\") and others were a consequence of forgetting or misinterpreting the task guidelines with respect to determining the maximal span of the date expression (e.g., tagging \"fiscal 1993's second quarter\" and \"Aug. 1\" separately, rather than tagging \"fiscal 1993's second quarter, ended Aug. 1\" as a single expression in accordance with the task guidelines). NE Results on \"Walkthrough Article\" In the answer key for the walkthrough article there are 69 ENAMEX tags (including a few optional ones), six TIMEX tags and six NUMEX tags.", "It is not clear what resources are required to adapt systems to new languages\" It is important to mention that the F-measure for the human performance on this task is about 96% ##CITATION## ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Human performance was measured in terms of variability between the outputs produced by the two NRaD and SAIC evaluators for 30 of the articles in the test set (the same 30 articles that were used for NE and CO testing). Using the scoring method in which one annotator's draft key serves as the \"key\" and the other annotator's draft key serves as the \"response,\" the overall consistency score was 93.14 on the F-measure, with 93% recall and 93% precision. TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5. Figure 6 indicates the relative amount of error contributed by each of the slots in the ORGANIZATION object. It is evident that the more linguistic processing necessary to fill a slot, the harder the slot is to fill correctly.", "It is not clear what resources are required to adapt systems to new languages\" It is important to mention that the F-measure for the human performance on this task is about 96% ##CITATION## ", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC. This test measures the amount of variability between the annotators. When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%. The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%. In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1.", "It is not clear what resources are required to adapt systems to new languages\" It is important to mention that the F-measure for the human performance on this task is about 96% ##CITATION## ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%. In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1. Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74..  It represents just one style of writing \"the Chrysler division\" (currently, only \"Chrysler\" (journalistic) and has a basic basic toward financial news and a specific bias toward the topic of the Scenario Template task.  It was very small (only 30 articles).", "It is not clear what resources are required to adapt systems to new languages\" It is important to mention that the F-measure for the human performance on this task is about 96% ##CITATION## ", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["TE Results on \"Walkthrough Article\" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems. Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing. There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations. Errors of these kinds result in a penalty at the object level, since the extracted information is contained in the wrong type of object. Examples of each of these types of error appear below, along with the number of systems that committed the error.", "In an article on the Named Entity recognition competition part of MUC6##CITATION##  remarks that \"common organization names first names of people and location names can be handled by recourse to list lookup although there are drawbacks\" ##CITATION## 16", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Nearly half the sites chose to participate in all four tasks, and all but one site participated in at least one SGML task and one extraction task. The variety of tasks designed for MUC6 reflects the interests of both participants and sponsors in assessing and furthering research that can satisfy some urgent text processing needs in the very near term and can lead to solutions to more challenging text understanding problems in the longer term. Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem. Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns. The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting.", "In an article on the Named Entity recognition competition part of MUC6##CITATION##  remarks that \"common organization names first names of people and location names can be handled by recourse to list lookup although there are drawbacks\" ##CITATION## 16", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks. The selection was again done blindly, with later checks to ensure that the set was fairly representative in terms of article length and type. Note that although Named Entity, Coreference and Template Element are defined as domain-independent tasks, the articles that were used for MUC6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task. The manually filled templates were created with the aid of Tabula Rasa, a software tool developed for the Tipster Text Program by New Mexico State University Computing Research Laboratory. NAMED ENTITY The Named Entity (NE) task requires insertion of SGML tags into the text stream.", "In an article on the Named Entity recognition competition part of MUC6##CITATION##  remarks that \"common organization names first names of people and location names can be handled by recourse to list lookup although there are drawbacks\" ##CITATION## 16", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["They can also be quite long and complex and can even have internal punctuation such as a commas or an ampersand. Sometimes it is difficult to distinguish them from names of other types, especially from person names. Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.. The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA. The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration.", "In an article on the Named Entity recognition competition part of MUC6##CITATION##  remarks that \"common organization names first names of people and location names can be handled by recourse to list lookup although there are drawbacks\" ##CITATION## 16", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem. Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns. The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting. Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "INTRODUCTION"], ["Commercial systems are available already that include identification of those defined for this MUC6 task, and since a number of systems performed very well for MUC6, it is evident that high performance is probably within reach of any development site that devotes enough effort to the task. Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one. The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks. The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task. The association of shortened forms of the name with the full name depends on techniques that could be used for NE and CO as well as for TE.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4. Highest P&R F-Measure scores posted for MUC6 and MUC5 ST tasks Note that table 4 shows four top scores for MUC5, one for each language-domain pair: English Joint Ventures (EJV), Japanese Joint Ventures (JJV), English Microelectronics (EME), and Japanese Microelectronics (JME). From this table, it may be reasonable to conclude that progress has been made, since the MUC6 performance level is at least as high as for three of the four MUC5 tasks and since that performance level was reached after a much shorter time. ST Results on Some Aspects of Task and on \"Walkthrough Article\" Three succession events are reported in the walkthrough article. Successful interpretation of three sentences from the walkthrough article is necessary for high performance on these events.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei", 0, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]). Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed. Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."], ["Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the \"key\" and the other annotator's templates were treated as the \"response\". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed. Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations. MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4.", "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable objective way of measuring relative task difficulty has not been adequately addressed [Sundhei", 1, "OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION", "Miscategorization  of  entity  as."]]