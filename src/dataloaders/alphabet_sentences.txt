Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (φ, θ) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively.
P St = n. β T VARIABLES ψ Y W : Word types (W1 ,.
.., Wn ) (obs) P T : Tag assigns (T1 ,.
The P (W |T , ψ) term in the lexicon component now decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions are not modeled by the standard HMM, which = n  n P (v|ψTi f ) instead can model token-level frequency.
i=1 (f,v)∈Wi
t(i).
 .
 .
 .
La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1.
1 1 0.
1 2 3.
8 1 2.
8 1 8.
C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3.
2 M 5 5 9 M NA N T S 9 4 2 1 6 7 2 5.
2 M 5 0 7 M AC QU A I N T 1 03 3 46 1 2 1.
where wgt(w, r, wt ) is the weight function for relation (w, r, wt ).
This consists of five passes over each sentence that first identify noun and verb phrase heads and p(w, r, wt ) − p(∗, r, wt )p(w, ∗, ∗) p(∗, r, wt )p(w, ∗, ∗) (2) then collect grammatical relations between each common noun and its modifiers and verbs.
The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length.
• A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}.
J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).
2.
3.
Given the parameters α, B, and the English part E, the joint conditional distribution of the topic-weight vector θ, the topic indicators z, the alignment vectors A, and the document F can be written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) where the hyperparameter α is a K -dimension vector with each component αk >0, and Γ(x) is the Gamma function.
5.
To approximate: p(θ, z, A|E, F, α, B), the joint posterior, we use the fully factorized distribution over the same set of hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· from a topic-based language model β, instead of a N Jn (7) uniform distribution in BiTAM1.
exp ( ) ) ϕdnji log Bf ,e ,k (9) j i j=1 i=1 K ( This gives rise to our BiTAM3.
In the M-step, we update α and B so that they improve a lower bound of the log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].
Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).
(14) i∈[1,Idn ] icon’s strength.
IBM 1 H M M IBM 4 p( Ch ao Xi an (Ji!
$) |K ore an) 0.
� )|K ore an) 0.
�).
45 8 15 .7 0 6.
82 2 17 .7 0 6.
92 6 18 .2 5 6.
5).
Note that, the boosted BiTAM3 us SE T TI N G IBM 1 H M M IBM 4 B I T A M 3 U D A BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E D ( % ) U N I O N ( % ) I N T E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N I S T B L E U 7.
5 9 19 .1 9 7.
7 7 21 .9 9 7.
8 3 23 .1 8 7.
1.
4.
B is also a square matrix such that eachentry B(i, j) is proportional to sim(i, j).
Table 5.
rπ(p) ranges from [0,1].
The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: pmi(i, p) = log x, p, y x,*, y *, p,* where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard.
Table 1.
Table 2.
Table 3.
SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† RH 02 5 7 , 5 2 5 2 8 . 0 % 5 . 3 1 RH 02 2 5 5 6 2 5 . 0 % 3 . 7 6 PR 04 1 , 5 0 4 4 7 . 0 % 0 . 2 3 PR 04 1 0 8 4 0 . 0 % 0 . 2 5 ES P 4 , 1 5 4 7 3 . 0 % 1 . 0 0 ES P 2 0 0 8 5 . 0 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances.
Table 5.
SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† RH 02 1 2 , 8 2 8 3 5 . 0 % 4 2 . 5 2 RH 02 1 1 , 5 8 2 3 3 . 8 % 5 8 . 7 8 ES P 1 3 2 8 0 . 0 % 1 . 0 0 ES P 1 1 1 6 0 . 0 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances.
Table 7.
SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† RH 02 4 9 , 7 9 8 2 . 0 % 3 6 . 9 6 RH 02 6 , 0 8 3 3 0 % 5 3 . 6 7 ES P 5 5 4 9 . 0 % 1 . 0 0 ES P 4 0 8 5 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances.
segmentation (Table 2).
phrase (markContainsVerb).
 .
For Arabic we M o d e l S y s t e m L e n g t h L e a f A n c e s t o r Co rpu s Sent Exact E v a l b L P LR F1 T a g % B a s e l i n e 7 0 St an for d (v 1.
95 B a s e li n e ( S e lf t a g ) 70 a l l B i k e l ( v 1 . 2 ) B a s e l i n e ( P r e t a g ) 7 0 a l l G o l d P O S 70 0.7 70 0.801 278 0.7 52 0.794 278 0.7 71 0.804 295 0.7 52 0.796 295 0.7 75 0.808 309 77.
60 ( P e tr o v, 2 0 0 9 ) all B e r k e l e y ( S e p . 0 9 ) B a s e l i n e 7 0 a l l G o l d P O S 70 — — — 0 . 8 0 9 0.839 335 0 . 7 9
0 . 8 3 1 0.859 496 76.
3.
Various verbal (e.g., �, .::) and adjectival.
Formally, for a lexicon L and segments I ∈ L, O ∈/ L, each word automaton accepts the language I∗(O + I)I∗.
(1) CEO of McCann . . .
”).
2.
3.
2.
Lexical: k t b =Root C V C V C =Template a + =Voc Surface: ^[ k t b .m>.
C V C V C .<m. a + ^] Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: ^[ k t b .m>.
C V C V C .<m. u * i ^] Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: ^[ d r s .m>.
The expression [A .<m. B] represents the same merge operation as [B .m>.
A].
C V V C V C .<m. u* i As we have defined them, .<m. and .m>.
C t V C V C .<m. a+ produces the desired result, ktatab, without any additional mechanism.
5 Conclusion.
1. Enumerate all sets S = {c1, ...
2.
,en}, define an.
1.
2.
Second, since the translation map­ pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve • c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.
y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect.
par t. y e s y e s y e s y e s y e s ye s ye s ye s y e s y e s y e s y e s y e s y e s y e s ind. acti on loc at.
This dictio #Retrieved #Relevant n #Retrieved Recall(R) = #Relevant (1) (2) nary, was called D6 and contains 254 expressions.
For each dictionary, an index was created in 1 M AP (Q) = |Q| mj 1 P (R ) (3) the IR system.
2.
3.
6.
7.
8.
10 11 B C N 0.
09 39 W C N 0.
12 24 G S 0.
23 93 D T 0.
11 93 M 1 0.
12 62 M 2 0.
Po sit io n D o c u m e n t S c o r e P 1 L A 04 30 94 02 30 0.
B STATEMENT N C State.
5% I can imagine.
2% <Laughter>, < Throat_clearing> 2% Yes.
1% No. 1% Oh, okay.
<.1% I'm sorry.
Applying Bayes' rule we get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U Here P(U) represents the prior probability of a DA sequence, and P(EIU ) is the like- Table 4 Summary of random variables used in dialogue modeling.
~ T Ui ~ ....
Table 6).
A1 Ai An T 1 1 Wl Wi W,, T T t <start> -, 0"1 , ...---* U/ , ...
Assign probability p to the first production (A ~ AA) and q = 1 -p to the second (A ~ a).
In general, Sh+l = q + pSi.
Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi.
Consequently: f(A ---+AA;wi) < l[f(A ~ AA;~i) + f(A ~ a;wi)] for each wi, and ~ < .
(1) orE(rUT)* s.t.
(2) AEV a s.t. i=1 The function p : R ~ [0,1] subject to (1) that maximizes (2) satisfies: 6 AAp(A ~ a) + f(A ~ a;cai)logp(A ~ a) = 0 AEV   i=1 (A~o~)ER V(B ~/3) E R where {AA}AEV are Lagrange multipliers.
Denote the maximum-likelihood estimator by fi: n B AB q- ~i=lf( --+ /3;ca;) = 0 V(S ~ /3) E R f,(B +/3) Since ~ fi(B+/3)=l) fl sA.
Y(wn) (Y(wi) E T* for each 1 < i < n).
The likelihood is substantially more complex, since p(Y(w)) is now a marginal probability; we need to sum over the set of w E f~ that yield Y(w): p(Y(w)) = E p(Y(w')).
Hopcroft & Ullman [1979], Theorem 4.4).
Define a sequence of probabilities, ~n, by the iteration ~n+i(B ~ fl) = ~i'=1E~,[f(B ~ fl;w)iw E fly(w,)] (5) ~ ~" ~7=, E~,[f(B ~ a;wliw E aY(o,D] (8~a)ER The right-hand side is manageable, as long as we can manageably compute all possible parses of a sentence (yield) Y(w).
Given a context-free grammar G = (V, T, R, S), let f2 be the set of finite parse trees, let p : R ~ [0,1] be a system of production probabilities satisfying (1), and let wl, w2,.
, w,.)
For any A E V: qA = p(UBEv U ..... t~,~ U i s,t. oti=B{Olifails to terminate}) -~ Z p(U a s.t. B~c, [-Ji s.t. o~i=B {Oli fails to terminate}) (A~c,)ERBEV = Z Z fi(A ~ c~)~(Ui s.t. ozi=B{Olifails to terminate}lA ~ ol) BEV a s.t. B~c, (A~c~)ER < ~ ~ ~(A~cz)nB(eOqB BEV a s.t. aEc~ { Y]~, .~, nB(a) Y'~i__,f(A ---* ol;wi) } .... n s, " a B*V E(A~,e a ~i=lf( ----r OGWi) { ~in=l E .....
,~,~ nB(ol)f(A --'~ Ol;Wi) } qB n BEV Ei=I Z ,Z~'i~af(A ~ Gwi) { ~in=l ~ ~.'.
"E~ nn(ol)f(A "---~ Ol;Wi) } Z (A~cQER = qB n BEV Zi=I F(A;0i) yt qA Z F(A; wi) Z qB ~ Z nB(OOf(m ~ OGCdi) i=1 BEV i=1 a s.t. Be~ (A~o,)ER Computational Linguistics Volume 24, Number 2 Sum over A E V: E qA E F(A;wi) AEV i~-1 _< E qB ~ E E nB(ol)f(A ~ BEV i=1 AEV c~ s.t. B~a (A~c~)ER c~;wi) n = ~ qB ~l:(B;wi) BEV i=1 i.e./ H qA E(Ie(A;wi) -F(A;wi)) ~ 0 AEV i=1 Clearly, for every i = 1,2,...,n F(A;wi) = F(A;wi) whenever A # S and F(S;wi) < F(S; wi).
Proof Almost identical, except that we use (5) in place of (3) and end up with: n E qA EEG_1[F(A;wi) -F(A;wi)lw C fly(w,)] ~ 0.
Furthermore, F(A; w) and F(A; ~) satisfy the same conditions as before: I:(A; w) = F(A; w) except when A = S, in which case F(A; w) < F(A; w).
2.
3.
<expe > ::= <ref> <expt > ::= <drs> <ref>∗ <drs> ::= <condition>∗ <condition> ::= <basic> | <complex> <basic> ::= <sym1 >(<expe >) | <sym2 >(<expe >,<expe >) | <named>(<expe >,<nam>,<sort>) <complex> ::= <expt > | <expt >⇒<expt > | <expt >∨<expt > | <ref>:<expt > DRSs are structures comprising two parts: 1) a set of discourse referents; and 2) a set of conditions constraining the interpretation of the discourse referents.
UDRSs are either ordinarly DRSs, DRSs conjoined by the merge (for which we use the semicolon), or NP/N: A N/N: record N: date λq.λp.( x ;q@x;p@x) λp.λx.( y record(y) nn(y,x) ;p@x) λx. date(x) [fa] N: record date y λx.( record(y) nn(y,x) ; ) date(x) . . .
[merge] y λx. record(y) nn(y,x) date(x) [fa] NP: A record date y λp.( x ; record(y) nn(y,x) date(x) ;p@x) . . .
bin/boxer --input working/step/text2.ccg --semantics drs --box --resolve --roles verbnet --format no %%% %%% | x0 x1 x2 | | x3 x4 x5 | | x6 x7 | | x8 x9 x10 x11 | | x13 x14 x15 x16 x17 | %%% |------------| |--------------| |--------------| |------------------------| |---------------------| %%% (| thing(x0) |+(| cancer(x3) |+(| know(x6) |+(| lead(x8) |+| researcher(x13) |)))) %%% | neuter(x1) | | cervical(x3) | | time(x7) | | vaccine(x9) | | look(x14) | %%% | neuter(x2) | | cause(x4) | | event(x6) | | seem(x10) | | agent(x14,x13) | %%% | | | virus(x5) | | theme(x6,x0) | | proposition(x11) | | cancer(x15) | %%% | event(x4) | | for(x6,x7) | | event(x10) | | | %%% | theme(x4,x3) | | | | event(x8) | | | | | %%% | by(x4,x5) | | agent(x8,x1) | | |----------| | %%% | | | agent(x10,x9) | | | | x15 = x3 | | %%% | theme(x10,x11) | | | | | %%% | to(x8,x9) | | cause(x16) | %%% | | | virus(x17) | %%% | | x12 | | | event(x16) | %%% | x11:|---------------| | | theme(x16,x15) | %%% | | prevent(x12) | | | by(x16,x17) | %%% | | event(x12) | | | for(x14,x15) | %%% | | agent(x12,x9) | | | event(x14) | %%% | | theme(x12,x2) | | | | %%% | | | | %%% | | Attempted: 3.
Completed: 3 (100.00%).
(Is that reality?).
(1) context prob.
By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) ∗ p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) ∗ p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) ∗ p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) ∗ p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the ∗ p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) ∗ p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1.
p(ADJA | ART, PART.Zu) ∗ p(Pos | 2:ART, 1:PART, 0:ADJA) ∗ p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) creases the uncertainty about the class.
a threshold of 1.
de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7.
2 8 9 7.
1 7 97.26 97.51 9 7.
4 5 84.11 89.14 8 5.
∏ P(tc tc∈C ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word.
We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.
P(e | c)directly, in (i.e., Chinese in our task).
For transliteration, we estimate P(e | c) as follows: P(e | c) = P(e | pinyin) = ∑ P(e, a | pinyin) a For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995.
Table 1.
Table 3.
‘Cont.
com §Cambridge, UK Email: nc201@eng.cam.ac.uk © 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.
3.
4.
2.
'Malaysia.'
2.
3.
(a) IDictionary D I D:d/0.000 B:b/0.000 B:b/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cps:nd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.
£ : _ADV: 5.88 If:!
:zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 £: _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I •=- :il: .;ss:;zhangt • '-:.
!!:\ :yu2 e:_nc [::!!:zen3 l!f :moO t:_adv il!:shuot ,:_vb i i i 1 • 10.03 13...
and f,.
J:j:l :zhongl :0.0 ;m,Jlong4 :0.0 (mHHaryg9tltHBI) £: _ADV: 5.98 ¥ :hua2:o.o E :_NC: 4.41 :mln2:o.o mm : guo2 : 0.0 (RopubllcofChlna) .....,.
wo rd => na m e 2.
0 X u} "' o; .2 X X><X X XX X X X X X X x X X X X X x X V X X X X .;t'*- XXX:OX X X X X X X 9 x X X XX XX X X X X X X X XXX:< X X>O<XX>!KXX XI<>< »C X X XX :X: X X "' X X XX >OO<X>D<XIK X X X X X X --XX»: XXX X X»C X X«X...C:XXX X Xll< X X ><XX>IIC:liiC:oiiiiCI--8!X:liiOC!I!S8K X X X 10 100 1000 10000 log(F)_base: R"2=0.20 (p < 0.005) X 100000 Figure 6 Plot of log frequency of base noun, against log frequency of plural nouns.
 .
JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14.
2.
3.
2.
"c' 0 + 0 "0 ' • + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y • Taiwan 0 ·;; 0 c CD E i5 0"' 9 9 • Mainland • • • • -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.
 .
An example is in (i), where the system fails to group t;,f;?"$?t!: lin2yang2gang3 as a name, because all three hanzi can in principle be separate words (t;,f; lin2 'wood';?"$ yang2 'ocean'; ?t!; gang3 'harbor').
Two sets of examples from Gan are given in (1) and (2) (:::::: Gan's Appendix B, exx.
(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'
gaolxing4 'happy' => F.i'JF.i'J Jl!
gaolbu4-gaolxing4 (hap-not-happy) 'happy?'
(b) F.i'JJI!
a classifier.
Citizen-Of”.
1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.
6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.
4 5 4 . 8 5 3 . 5 Reside nce 6 6 1 9 9 67.
9 2 8 . 8 4 0 . 4 N EA R 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 P A R T 1 6 4 1 0 6 3 9 73.
1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.
4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37.
8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84.
4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.
8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.
1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.
6 6 4 . 2 6 1 . 8 Memb er 2 2 4 1 0 4 3 6 74.
3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74.
1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33.
1 4 9.
5 4 5.
 .
 .
 .
1.
2.
3.
4.
5.
For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated.
node2 = Dereferencelnode2).
(sharedst, shareds2) = SharedArcs(nodel.arcs, node2.arcs).
complements1 = ComplementArcs(node|.arcs, node2.arcs).
complements2 = ComplementArcs(node2.arcs, nodel.arcs).
IF Equal?(arcnode, Bottom) ]HEN Return(Bottom).
AddArc(outnode, complement.label, newnode).
Return(outnode).
5 due to a change of node Y G3/<a c g>).
6).
ELSE node.copy-dependency = node.copy-dependency U {Cons(ancestor, arc)}.
Return(Nil_).
newarcs = {newarc} U newarcs.
1.
67 8 0.
70 0 0.
78 3 0.
71 0 0.
60 7 0.
68 2 0.
77 5 0.
67 4 0.
95 2 0.
9 4 3 0.
96 4 0.
95 0 O u r s 0.
95 1 0.
9 5 1 0.
97 1 0.
P(rule) =count(LHS root(rule)) 3.1 Hiero.
 .
, ein, which are the 10-best translations according to each of: h(e) · w B(e) + h(e) · w −B(e) + h(e) · w • For each i, select an oracle translation: (1) 4.1 Target-side.
e∗ = arg max (B(e) + h(e) · w) (2) e Let ∆hi j = h(e∗) − h(ei j).
For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9∗ 36.
4 37.6∗∗ Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8∗ 38 .7 39.9∗ 38.
 .
6 Analysis.
MIRA: . . .
3 MERT: . . .
MIRA: . . .
4 MERT: another thing is . . .
6 MERT: . . .
MIRA: . . .
7 MERT: . . .
MIRA: . . .
 ).
 ).
4.
7.
The resulting algorithm has a complexity of O(n!).
The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei􀀀1 = e0.
A modified language model probability pÆ(eje0; e00) is defined as follows: pÆ(eje0; e00) =  1:0 if Æ = 0 p(eje0; e00) if Æ = 1 : We associate a distribution p(Æ) with the two cases Æ = 0 and Æ = 1 and set p(Æ = 1) = 0:7.
2.
diesem 3.
4.
mein 5.
6.
besuchen 9.
= p(fj je) max Æ;e00 j02Cnfjg np(jjj0; J) p(Æ) pÆ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).
(f1; ;mg ; l) 2 (f1; ;mg n fl; l1g ; l0) !
(f1; ;mg n fl1g ; l) 3 (f1; ;mg n fl; l1; l2g ; l0) !
(f1; ;mg n fl1; l2g ; l) 4 (f1; ;m 􀀀 1g n fl1; l2; l3g ; l0) !
2.
The search starts in the hypothesis (I; f;g; 0).
The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Æ;e00 np(jjj0; J) p(Æ) pÆ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).
The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.
of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2.
5 84 0 its , it' s 19 .5 1 3.
, c_ 1, c1, ...
We then ignore a context word c if: L m; < Tmin or L (Af;- m;) < Tmin l i n l5i n where m; and A{ are defined as above.
Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords ± 3 ± 6 ± 1 2 ± 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 ..
5 7 5 0.575 0.585 0.498 0 . 7 5 9 0.697 0.671 0.586 0 . 5 3 0 0.530 0.521 0.557 0 . 6 9 5 0.526 0.516 0.558 0 . 7 5 4 0.705 0..574 0.574 0 . 7 2 6 0.623 0.557 0.466 0 . 2 9 0 0.290 0.290 0.435 0 . 4 5 5 0.2.50 0.364 0.318 A vg no.
P r u n e f e a t u r e s t h a t h a v e i n s u f f i c i e n t d a t a . o r a r e u n i n f o r m a t i v e d i s c r i m i n a t o r s . S o r t t h e r e m a i n i n g f e a t u r e s i n o r d e r o f d e c r e a s i n g s t r e n g t h . Store the list of features (and their associat ed statistics ) for use at run time.
Then reliability'(!)
47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.
U(xiy) measures how much additional information we get about the presence of the feature by knowing the choice of word in the confusion set.6 U(xiy) is calculated as follows: U(xiy) H(x) H(xiy) H ( x ) H ( x i y ) H ( x )-p(f)lnp(f)- p( .J)lnp(-.J) - Lp(w;) (p(flw;)lnp(flw;) + p( .Jiw;)lnp(•flw;)) The probabilities are ca.lculated for the population consisting of all occurrences in the training corpus of any w;.
50 C on fu si on se t Ba sel in e Cwords Collocs ± 3 2 Dl ist Dlist R el y U(xiy) w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0.902 0.931 0 . 8 8 6 0.914.
0.981 0 . 8 6 3 0.862 0.945 0 . 8 6 1 0.861 0.909 0 . 8 0 7 0.931 0.965 0 . 7 8 0 0.791 0.853 0 . 7 4 1 0.747 0.821 0 . 7 2 6 0.816 0.887 0 . 6 2 7 0.646 0.646 0 . 6 1 4 0.639 0.639 0 . 5 7 . 5 0..575 0.807 0 . 5 3 8 0.759 0.730 0 . 5 3 0 0.530 0.840 0 . 4 4 2 0.695 0.789 0 . 3 9 3 0.754 0.869 0 . 3 0 6 0.726 0.932 0 . 2 9 0 0.290 0.812 0 . 1 1 4 0.455 0.318 0.
93 5 0.829 0.
98 0 0.808 0.
93 1 0.805 0.
93 2 0.892 0.
96 7 0.961 0.
84 2 0.933 0.
82 1 0.654 0.
86 8 0.896 0.
62 9 0.667 0.
62 7 0.651 0.
0.
65 9 0.800 0.
84 0 0.840 0.
78 9 0.726 0.
85 2 0.836 0.
91 4 0.906 0.
81 2 0.841 0.
51 C on fu si on se t B as eli 11 e C w or ds Collocs Dlist Bayes ± 3 : : ; 2 R e l y R e l y Trigra ms w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 5 0 ..
5 3 8 0 ..
5 3 0 0 . 4 4 2 0 . 3 9 3 0 . : 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.931 0.93.5 0.935 0 . 9 1 4 0.981 0.980 0.985 0 . 8 6 2 0.945 0.931 0.942 0 . 8 6 1 0.909 0.932 0.924 0 . 9 3 1 0.96.5 0.967 0.973 0 . 7 9 1 0.853 0.842 0.869 0 . 7 4 7 0.821 0.821 0.827 0 . 8 1 6 0.887 0.868 0.901 0 . 6 4 6 0.646 0.629 0.662 0 . 6 : 3 9 0.639 0.627 0.639 0 ..
5 7 . 0 . 7 . 5 9 0.730 0.6.59 0.786 0 ..
5 3 0 0.840 0.840 0.840 O . G 9.
4 Evaluation.
 .
W  .
1.
We define two vertices s (source) and t (sink),.
2.
3.
90 Si milar to 6, 88 7 1, 61 4 0.
76 Al so se e 1, 03 7 33 7 0.
81 Do m ai n 4, 38 7 89 2 0.
83 Do m ain m e m be r 4, 38 7 89 2 0.
Wiebe&Mihalcea’s dataset.
2.
1998).
j hu.
I. 14.
1, 26.
1, 13.
2 3 5 A m us e 31.
1 1 3 4 Ru n 51.
2 5 6 C he at 10.
6 2 9 St ea l an d Re m ov e 10.
5, 10.
1 4 5 Wi pe 10.
1– 4 1 6 9 O bj ec t Dr op 26.
1, 26.
3, 26.
113, |)-721174Tfilfi,,ge,, (ler- ma.ny, {rig,King} g'~sfs.n phil.uni-I uebingen.de.
A disjunctive I"CI{.
(31:it i:leserw.;s i:erl;a.in (lmrhal)S other) fea,1;llres wil.h Vi, l.[ll(~S o[ certain (perha.ps other) khi,<ls lo::I exa~]nple, the following F(',|/.
[uluitively; the l, ypes fornla.lize I;lie notion ol" kinds +,j" objecl, t g: t,' ill' ca.oh oil|eel, of tyl>e t' i~<i Mso of l;Ylle L, il, ll(] Approp(l, f) = lI ill' (!;i('[I object oF type t deserves [eaA.urt~ f wil.]i :i. Vi./.]lle or type ft. ~@'e call S/IC]I it.
[Ol'tll;liiSlll i-i, ii ;I,])l)l"Opl']al, olio,~/ fOl'lllil]i~;lll.
(',iLl'- peliLel",s AI,F, and (,erdeliia.
i ;ill(| (i(~t,z's Troll are ex:-t.niples o| illilllenienl.a.Lions o| a,pF, ro]) ria, Loliess |or illa.[iSlil,s. l low an a.i)ln'oprhi.teness [orniaJisnl enco<les a conjunctive I:(',R is ob\.'i<>us~ bll(.
[(!a.[./ll'(!S f 'and .q, I)oth with boolea.I/ wdues a.ll(I ['lll'l,[lel'lllOF(~ that the va.hies of f aild g iil/lSl al~r(!e, [> is the disjunct]w! I"(111.
SUl)tyl)e ['()l' ea,ch disjuuct iu the cousequenl, of'p.
o| p wilhoul, the a.i[-Ol'-liot;hilig c(m(til.ion.
Furl.her|note> a ['eal, urt~ strut(tire is >l'lds exanll)h: I:(JR is, for eXlmsil.ory l)nrl)oses, quilt simph'.
As noted I)y Copcstakc.
ct al. [4], it.
for the type l, I.he path (fg) lakes a vahie subsuuied I)y .% one nlust tirst hll, ro ducc the chaiu Approp(/, f) = .,, Approp('a, g) = .~.
Silch ilil.crlllCdialc I.'~'l)lts COllid ll(!
hll.rodllced a.<-; part o[ a (onilli[al.iou sl.age..
4 Nob: I.hal.
Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,.
T$ is satisfial~le if[' 7~(F) 7 ~ (7).
4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t" a.nd 1'" 6 fS then "R ( F) tJ 1"(1"') = "R ( F tO F').
(!rty a.llows a. disjultctivo fesolv(,d featur(, structti re to I)e r(;l)rosetd,(~d more et[icieutly a,s ~t sitlgle untyl)(~d l'eatur(' st.l'll(:l.llfe plus a, sel; of d(;pondlmt node la.h(~liugs, which ca.n be further (;oml)a,(:t(~d using mi, Nie(l dis.
junction a.s in (',(~rdemann [(i], I)i'~['re t(: Fo]' exanH)le , SUl)l)OS(~ \v(~ I,,yl)(~ r(~solvc the [ea, l, urc st, ructure t[,f ; bool,fl; bool] using our encoding of p.
()he can (rosily see tha.t this fea.tur(~ strut:fur(, has only two I'e solwl, nts, which ca, n I)e colla.ps(~d iuto one fea,1;ure strlll:ttlro with llallV2d d]sjunci.ion a,s shown below: f:k , : :> f: (I t -) II'll;1} ["'"' ] 0:t-LU: J ,u: (I t ) We now ha,vo a, [;(mSolml)ly COml)a(:l l'q)-resentaJ;ion hi which t.ho l"(il{, ha.s lie(Hi tl';tllsl;I,t(~([ iul,o a. Ila, ill(!(I ([iS.]llll(:l.ioli.
Ih,w O,V(H'> (Hie should note tha, t fills dis.iun(: l;ion is only l)l'eSeUl; b(~(:aats(~ the ['oaJ, tli'O~i .f a,]l(l g ha>l)l)en 1:o I)o Fir(~s(HIt.
Tilt!S(!
I(,a tures would .eed l;o Im l)res(mt il w(~ wtwe enforchl<ej (Jaxpcnl,(H"s [:7] lcil, al w(ql i.yl)iug r(xluiroti]oilt ~ whhth ,qa,y's 1.1ial [(!al:ilr('s I. lial a,l:e a.llowed ilillSt 1)o pres,.ml., lllil.
Iol.a[ well I.yping is, hi fax:t> incoinl)a.lib]e ;villi lype resolul, ioli~ since I;hore lilil$' w(ql I)o all inli llit;(~ seL of tota, lly w(,ll iyl)od I'esolvalil.s of ;1 l'(;a, Lllr(J st]'llcttir('~, For (~xa.llipi(~, a.ll illi(lei'.- Sl)ocifiod list stl'u('tlir(' couhl be iT(~S()/v0(I 1.o ;~ list of length (L a. list of h:ngl.h 1, el.c, ,qhlce I, ota.I well I,yliin g is liOt i'(!quir(!([, we lm~y i~s well a.ctiwqy un[il[ r0(lulid;lnt ['0a, tlires, 5 ill this (!Xalli[)l(!> i[ t, li(' f ail(l (7 fo.a, tllrOS ;~l'e reliiovod, we a,lO lell wil, h lh(, simple, disjunction {if,/'~}: which is (!quiv- a,lent, to l;]le or(lillaJ'y l,Yl)(' l.(; Thus, iu lliis ca, so> ]lO (lisjtulcl, ion a.t all ix rc!(llliro(l 10 (!11" force the I"CIL All th',tt is requirc(I is tim ~qntuil, ively, [eat, ui'cs arc rodundaui it Ilwir val llCS art'.
eul,h'cl 5 predictaldc fl'oui ihc approluiaic .ross Sl>eCificatim,..%'c GStz [1)], (',cr,lemam, [7] k,r ;I. IIlOl;('.
[HXX:iHCforUllllalioii.
[n this casc, il.
/Snforl, unai,e, ly, llmvcvcr, this i~; l.>i ;ihvay~.
the (:asc, as C;lll |)(!
S(!t'II in the [ollowiug (!Xalll])lC: t{j: +] :> {C/: +]} ~ ~'.
asSUml)tion tha.t t will only be ext(mded I)y unil'ying il with a.lmther (t;Oml)a.ct(~d) m(mll)(!r o[' "l)']?.Jr,_c,.
It would not lmve I)('en i)os sihle to relnov(' tim fea.tur('s f ~tll(I g if thest~ 17,atu['es had I)oen involved iu re(m-tranci(+s of i[' tlt(,se lim.tures ha.d ha.d t:om- i)h+x va.lu('s, lu gt+tlera.I, howover, our eXl)e- ri(!ll(:(~ ha,s I)(~(ql that, eV(;l! wil, li very (:()tit pl('x type hi(~ra, rchi(~s a.nd |'(m, tur(; SLI'UC-l, lll'eS [()1" liPS(i, very i'ow named (lisjunc-lions a, re introdu('e(l. 7 q'hus~ uuilica.1;ion is e;(merally uo more (~xp(msive tha.n unifica.- li,:)H with unlylmd l(mt.ur(~ sl.fu(:l.ur('s.
\% havc~ sh,:Y, vu in this i~al),:~r tha.t the kind of consl raints ,:~Xl)r.t~ssihlo Ity api)Vol)rh~,l;or.~ss c~mdit.ions call he imlflemc'.nted iN a i.'actical .,.D, sle]n e,ul)loyinK typ,M featu r,:'~ st.ru(:t.uf(,s and utdlica.Lion a.s I.he I:,ritna.ry Ol)(U'a,t, ic:,n on t(>;t,l, ur<+ ,'-;t, ruct, ure~.
Of IIlOl'(' COIII[)I(~N l;yp(~ CC'IIH|,F.~LilI|,,q it~v'.)l',.' h~y; r(~enl;ram:ies': [ntro(IL~ciug reeJH.ra.ncies illl.
(;lea['ly the re ~olv;-~nl.~, o[ such a. recursiv(~ l.yl)(', could Not I)(~ l,reCOmlfiled a.s r,.~quiI'oxl in Troll.
Oue might, uew'rtholoss, considm' a l- [OWil]l[ f('(Hl(, f a, ll('y- ('OIls t f a hI| S oll llollrecursiv(qy defiltcd l.ypcs.
A ])ro/)leul still arises; nantcly, il lhe l'eSo[va.itts of a Frail, till't1 .qll'tlCI411"(~ ill(:ludcd sonic with a pa.rticu lar r(~onll'all(:y a.nd s()Tn(~ \viLh(',ul, then the (:,.)mliti()ll iliad, a.II resc)lva.uts ha.v(~ th,:~ same shal)(~ would m)lon~e[' hold.
()ue v.,ottkl l.her(q'or,.~ no(~(l i.o eml)loy a moue COml)l(,x vorsion .r ,a.med (lis.it, f,t:tio, (Ill], [12], tit)I).
ig (i,.L(~sti,.malfl(~ wh('thef such a.d ditional (:()mpl(~xit.y would I)e justified to 'Our CXl)ericl~(:c is derived l,'imarily flora test-i.I" Ihc 'l'loll system (m a tat, her lar<e,e e, ramul;G for (',(!l>lll;lll imfiial vcrh I>lHases, which was wiit-t('n I)y I'hhard Ilillrichs a.d Tsum:ko Na, kazawa aud iinl)lclncut,cd by I)clmar McuH_:J's.
For example, one can either say: ContainsX = Σ* X Σ*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 ∩ Σˆ<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29–32, Athens, Greece, 3 April 2009.
Qc 2009 Association for Computational Linguistics Operators Compatibility variant Function [ ] () [ ] () grouping parentheses, optionality ∀ ∃ N/A quantifiers \ ‘ term negation, substitution/homomorphism : : cross-product + ∗ + ∗ Kleene closures ˆ<n ˆ>n ˆ{m,n} ˆ<n ˆ>n ˆ{m,n} iterations 1 2 .1 .2 .u .l domain & range .f N/A eliminate all unification flags $ $.
˜ $ $.
void check_word(char *s) { 2.
fsm_t *network; 3.
fsm_match_result *result; 4.
result = fsm_match(fsm, s); 7.
if (result->num_matches > 0) 8.
Qc 2009 ACL and AFNLP Σ, T Trans : Σ ∪ {NULL} → 2T s = (s0 , . . .
, sn ) ∈ Σn t = (t1 , . . .
, tm ) ∈ Tm τs : {1, . . .
, n} → {0, . . .
, n} τt : {1, . . .
, m} → {0, . . .
, m} a : {1, . . .
2.
(Table 1 explains notation.)
That is, p(t, τt, a | s, τs) = exp{θTg(s, τs, a, t, τt)} plest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s ∈ Σ and t ∈ T. Phrase-to-phrase features generalize these, estimated as p(tl | sl) and p(sl | tl) where sl (respectively, tl) is a substring of s (t).
 ; they can overlap.5 Additionally, since phrase features can be any func g (s, a, t) = Pm i∈a(j) f lex (si , tj ) (3) tion of words and alignments, we permit features + P f (slast (i,j) , tj ) that consider phrase pairs in which a target word g (t) = P i,j:1≤i<j≤m Pm+1 phr first (i,j) i j lm N ∈{2,3} j=1 f N (tj−N +1 ) (4) outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, gsyn (t, τt ) = Pm j τ (j) , τt (j)) val t (j)) (5) 2007).
g (s, τs , a, t, τt ) = Pm m P i∈a(j) f dist (i, j) (6) Lexical translation features factor as in Eq. 3 (Tab.
2).
xj ) in sequence x = (x1 , . . .).
the target phrase; if k:i≤k≤j a(k) = ∅, no phrase i first (i, j) = mink:i≤k≤j (min(a(k))) and last (i, j) = feature fires for tj . maxk:i≤k≤j (max(a(k))).
2).
2).
Eq. 6 (Tab.
Grammars A quasi-synchronous dependency grammar   specifies a conditional model p(t, τt, a | s, τs).
(t∗, τ ∗ ∗ T s t t,τt ,aa Formally, for a parent-child pair (tτt (j), tj ) in τt, we consider the relationship between a(τt(j)) and a(j), the source-side words to which tτt (j) and tj align.
(“a(τt(j)) = τs(a(j))” corresponds to their “parent-child” configuration; see Fig.
2).
, m} → {0, . . .
p(t | s), marginalizing out senting possible translations.
übersetzen: ?:?
• A counter of uncovered source words: of the source sentence during translation: all parts f sunc (a) = �n δ(|a−1(i)|, 0).
form (t (i) , τ (i), s (i) , τ (i)), for i = 1, ..., T , max The solution is to introduce a set of coverageimum likelihood estimation for this model con 9 features gcov (a).
3).
 .
sag, 1975): p(t, τt | s, τs) ≈ p(t | τt, s, τs) × p(τt | t, s, τs) Plugging this into Eq. 9, we arrive at Eq. 10 (Tab.
3).
For reordering, we use a single absolute distortion feature f dist (i, j) that returns |i−j| whenever a(j) = i and i, j > 0.
2.
Step 2.
Figure 3 Figure 1.
Step 3.
Step 4.
and “H” represents “Hanson Plc”.
Step 1.
Step 2.
Step 3.
Step 4.
buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.
D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1.
D o m ai n Li n k ac cu ra cy W N c o v e r a g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.
3.
7 % 31.
, J and i = 1, 2, . . .
, 2I + 1.
, fJ and a f J , we have "£I φi + φǫ = J . target sentence eI 1 = e1, e2, . . .
−1 aligns with one ofP (f J |e2I +1) = "£ J P (aJ , f J |e2I +1).
The sum of the fer (I λ(ǫ))φǫ e−(I λ(ǫ)) φǫ!
Now P (φI , φǫ, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (φI , φǫ, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (φI |e2I +1)P (φǫ|φI , e2I +1) × 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f j−1, aj−1, e2I +1, φI , φǫ) j=1 1 1 1 1 P (φI , φǫ, aJ , f J |e2I +1) = n λ(ei) e−λ(ei ) 1 1 1 I φ 1 λ(e ) φi! × = n λ(ei) i e− i i=1 (I λ(ǫ))φǫ e−I λ(ǫ) φǫ!
× φ i=1 (I λ(ǫ))φǫ ! × e−(I λ(ǫ)) J n P (aj |f j−1, aj−1, e2I +1 I φǫ!
× J j=1 1 1 1 , φ1 , φǫ) × n P (aj | j=1 aj−1 , I )P (fj | eaj ) (2) P (fj |f j−1, aj , e2I +1, φI , φǫ)l 1 1 1 1 Superficially, we only try to model the length 1 |e2I +1probability more accurately.
Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = P (φI , φǫ, aJ , f J |e2I +1) = P˜ ′ aJ e 2I +1, f J ) log ′ P (aJ , f J | e2I +1) 1 1 ,φǫ 1 1 1 1 1 1 J 1 1 1 1 ≈ P (φI , φǫ, aJ , f J |e2I +1) × − ξ1(e)( P (f |e) − 1) 1 1 1 1 I  J  e f n δ  i=1 j=1 δ(aj , i), φi × − ξ2(a′)( a′ a P (a|a′) − 1)  2I +1 J  Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 δ  i=I +1 j=1 δ(aj , i), φǫ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, φǫ and each P (f |e) = "£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.
Because we only sum over fer tilities that are consistent with the alignments, we P (a|a′) = "£s c (a|a′; f (s), e(s)) (5)have "£f J P (f J |e2I +1) < 1, and our model is de "£ "£ a s c(a|a′; f (s), e(s)) 1 1 1 "£ (s) (s) ficient, similar to IBM Models 3 and 4  .
Replacing 1 with a1 φǫ ! J i ! φǫ ! J i !
(2I +1)J δ(fj , f )δ(ei, e) J ! , we have: c(a|a′; f J , e2I +1) = j P˜(aJ |f J , e2I +1) × P (φI , φǫ, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n λ(ei)φi e−λ(ei ) × i=1 (I λ(ǫ))φǫ e−(I λ(ǫ)) × c(φ|e; f1 , e1 ) = δ(aj , a)δ(aj−1, a′) j P˜(a1 |f1 , e1 ) × J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 φ δ(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)δ(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency.
, J ) is O(tI J ).
Al ig n m en t M o d e l P R A E R e n → c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.
8 4 3.
8 3 9.
0 3 7.
9 3 6.
2 3 4.
9 3 4.
5 c n → e n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 52 .6 55 .9 66 .1 68 .6 71 .1 71 .1 69 .3 53 .7 56 .4 62 .1 60 .2 62 .2 62 .7 68 .5 4 6.
9 4 3.
9 3 5.
9 3 5.
7 3 3.
5 3 3.
2 3 1.
We smooth all parameters (λ(e), P (a|a′) and P (f |e)) by adding a small value (10−8).
1.
2.
Se ma nti c (a) filters candidate if its semantic tags d o n ’ t i n t e r s e c t w i t h t h o s e o f t h e a n a p h o r .
6 Conclusions.
7 Acknowledgements.
0 10 220 30 41) 50 60 7(1 80 91) Recall I(X) Figure 4.
1.
& Puris.
(BBN system) Table 5.
2.
3.
&quot;speedometer&quot; from &quot;car&quot;).
&quot;speedometer&quot; from &quot;car&quot;).
The first is Dunning's [10] log-likelihood metric which measures how &quot;surprised&quot; one would be to observe the data counts w,P -'w,P I, I to,-' p I and I -'w,-19 I if one assumes that p(w = p(w).
In (J'applaudis a la decision I 1(1) applaud(2) the(4) decision(5)), a is generated by the empty cept.
Thus, we may also write the previous alignment as (J'applaudis a la decision e0(3) 1(1) applaud(2) the(4) decision(5)).
By definition, where Pr(aje, f) = Pr(f, ale)/ Pr(fle).
.
Obviously, we can find V„_)(fle; 1) and 17,,_1(f le; 2) quickly with a straightforward modification of the algorithm described above for finding V(f le; 1) and V(f le; 2).
The parameters of Model 3 are thus a set of fertility probabilities, n(cbl ei) E Pr(01011.-1, e); a set of translation probabilities, t(f ei) = Pr(Tik e); and a set of distortion probabilities, d(jli, m, 1) a- Pr (111k = jk1 74-1, 7-(1, 10(1), e).
By now, the reader will be able to provide his or her own auxiliary function for seeking a constrained minimum of the likelihood of a translation with Model 3, but for completeness and to establish notation, we write h(t,d,n,p, A, tt, v, Pr(fle) - E A, (f. t(f 1 e) - 1) - Eitina(Ed(j1i, m, 1) - 1) _Eve( n(01 e) - 1) - (po + pi -1).
The sequence of alignments a, b(a), b2(a) b(b(a)), .
.
With these preliminaries, we define S by S = Ar(r)(V(fl e; 2))) U V (fle; 2))).
(44) In this equation, we use b°°(V(f le; 2)) and lf° j(V,-1 (f le; 2)) as handy approximations to V(f le; 3) and Vi—j(f le; 3), neither of which we are able to compute efficiently.
For [i] > 0, we require that the head for cept i be r[]i and we assume that Pr (II[]i = /1741-1,71, Of), e) = - Oi-i IA(e[i-i]), 8(4)).
We expect d>1(2IB(pas)) to be relatively large compared with d>i (218( en)).
After training, we find that d>i(2I13(pas)) is 0.6847 and d>1(215( en)) is 0.1533.
Here, Pr(ale, f; 3) means Pr(ale, f) as computed with Model 3, and Pr(ale, f;4) means Pr(ale, f) as computed with Model 4.
We define S for Model 4 by N-(b&quot; ( V(f le; 2))) U (fie; 2))).
We assume that, for [ii > 0, = 7-(!,, 0(1), e) = (vilB(.6), voi_i, vin - + 1)(1 - 6(v1, vj-i))• (48) The number of vacancies up to j is the same as the number of vacancies up to j - 1 only when j is not itself vacant.
( V(f I e; 2) le, f; 4).
Specifically, we have assumed that Pr(M = m le) = (A Oine-Al I M!, with A equal to 1.09.
I Promises, promises.
Let w(e) be the sum of Pr(f I e) over well-formed French strings and let i(e) be the sum over ill-formed French strings. lit a deficient model, w(e) + i(e) < 1.
If Pr(failurele) 0, but i(e) > 0, then the model is spiritually deficient.
. mangeassent).
The log-likelihood of a sample of translations (f(s), e(s)), s = 1, 2, ... , S, is Here C(f, e) is the empirical distribution of the sample, so that C(f,e) is 1/S times the number of times (usually 0 or 1) that the pair (f, e) occurs in the sample.
We can compare hidden alignment models Po and Po using the relative objective function where P -0(a I f, e) = (a, f I e)/P0(f I e).
Note that R(P6, Po) = 0.
Thus, it could be that, even for the best 0, 11)(130) < 0(1)0).
We interpret 0(w) as the probability of the event w and c(w; a, f, e) as the number of times that this event occurs in (a, f, e).
We need only modify Equation E(m I 1) string length probabilities t(f I e) translation probabilities Here f E ,F; e E E or e = eo; 1 =- ...; and m = 1,2, .... Pe(f, a I e) = Po(m I e)Po(a I m, e)Po(f I a, m, e) (67) Assumptions.
E(m I 1) string length probabilities t(f I e) translation probabilities a(i I j,l,m) alignment probabilities Here i= 0, .
,1; and j = 1,. , m. General Formula.
The counts c(w; a, f, e) of Equation (58) are We obtain the parameter reestimation formulae for t(f I e) and a(i I j,l,m) by using these counts in Equations (62)—(66).
The counts c(o.
); a, f, e) of Equation (58) are We obtain the parameter reestimation formulae for t(f I e), a(j I i,1, m), and t(q5 I e) by using these counts in Equations (62)—(66).
If Pti satisfies ti(a If,e)=14 ( a; I , e), (105) j=1 as is the case for Models 1 and 2 (see Equation (82)), then this sum can be calculated explicitly for 'et-,(f e;f,e) and Eo(jI i;f,e): -ea(f I e;f,e) m E-6(j i;f,e) EEpo(i I], f, e)5(e, ei)6(f Jj), i=o 1=1 ijo(i Unfortunately, there is no analogous formula for Eo(0 I e; f, e).
For example, 0 = 5 has 7 partitions since 1 + 1 + 1 + 1 + 1 = 1 + 1 + 1 + 2 = 1 + 1 + 3 = 1 + 2 + 2 = 1 + 4 = 2 + 3 = 5.
If -y is the partition corresponding to 1 + 1 + 3, then = 2, 'y3 = 1, and -yk = 0 for k other than 1 or 3.
To obtain Equation (114), note that 0, = Em1 6(i, a) and so i= x4).' flr X6(j'al).
If k 1 then choose 7r1 according to the distribution A(e p,), B(m)).
Hypothesis 2.
 .
Note that only 6 of these 24 relation types are symmetric: “relative-location”, “associate”, “other-relative”, “other-professional”, “sibling”, and “spouse”.
“OTHER” became “OTHER-PART” and “OTHER-ROLE”).
Words , , , .
4.
 ).
(p < .05).
**: Better than baseline (p < .01).
+: Better than Chiang-05 (p < .05).
++: Better than Chiang-05 (p < .01).
V ... V (uconj.
Adi a jy, A ...A diejny).
/.
If desc.indefinite = 0, Then return (desc); Else begin; 2.
Let new-def = desc.definite (a DG).
Step 2.
Step 3.
2.
4.
In this graph, the edges (1, 4) and (3, 5) are crossing.
In Graph 3a, the projection of i is an interval ((2, 3, 4)), so i has gap degree 0.
In Graph 3b, 7ri = (2, 3, 6) contains a single gap ((3, 6)), so the gap degree of i is 1.
In the rightmost graph, the gap degree of i is 2, since 7ri = (2, 4, 6) contains two gaps ((2, 4) and (4, 6)).
* i.
“make something up”; PRTs, 2%) and clauses (SBARs, 2%, and Ss, 2%).
X Null elements 2.
(ADV "here/KB")) (?
(ADV "always/KB")) (AUX (TNS *)) (VP (VPKES "buck/VBP"))) (?
(S (NP (PRO *)) (AUX to/TNS) (VP (V "visit/VB") (NP (PNP "Mexico/NNP"))))) (?
(MID ",/,")) (?
always) (VP buck)) (?
(PP up (NP nervous newcomers))) (?
(PP with (NP the tale (PP of (NP the (ADJP first)))))) (?
(PP of (NP their countrymen))) (?
(S (NP *) to (VP visit (NP Mexico)))) (?
ashore) (NP 375 years)))) (?
their countrymen))) (NP Mexico)))) (NP a boatload (PP of (NP (NP warriors) (VP-I blown ashore (ADVP (NP 375 years) ago))))) *pseudo-attach*)))))))) ?
f la i l  ever .
The  tape  ind icates  t l l a t  shor t ly  a f te r  the  p lane  leve led ~ff  a t  i ts c ru i s ing a l t i tude  o f  as ,000  feet ,  t i le  cl~ief p i lo t  o f  t i le a i rc ra f t  left  the  p lane s cockp i t ,  l eav ing  one  o f  t i le  twc~ co-p i lo ts  nIol le t i lere as the  a i rc ra f t  began its descent .
= {topic,< ( t , ,w l ) , .
, ( t , , ,w , , )  >} (1) where topic is the target concet)t and .,d.q)zat~Lrc is a vector of related ternls.
Assuming the following two hyl)othe,~es: t typothes is  1 ( I f l ) :  t(~Pvlti) = P = P(PvltT/), i.e.
the r(.,lewmcy of a d()(:|lment is in(teI)en(hmt, of t i .
I  ]  [ypothes is  2 ( t t2) :  I(Pv[ti) == lh ~ 1)2 - t)(Pvlt, i), i.e.
:;(;n(:(~ of t i indi(:~Lt(.~.
; strong r(~levan(:y ~ssunling ]h >> 1)2 ?
ti o(:curring in the non- l  e leva i i t  seL.
-kssmning a l)inomial distril)ution: C;) b(~; ,,., :/.)
= :,:~(1 - .~:)(" ") (2 ) 5This assumes |ha l  the ratio is between the inaximuni like> [ihood est, im&t.(!
over a .qll})part of l;}l(!
i)alatlllCt(~r sl)a(:(~ ;tll(] l.h(!
lllaxillUllll likelihood (}sI.i|II}tlA~ ov(!r the (Hltill!
i)alaillt~tt!r si);t(:e. Set!
then the likelihood for HI is: L(H~) = b(Ot~; 0~ + Ou,,p)b(O:,~; 0:,, + Om,,p) and for //2 is: L(H2)  = D(OI 1; O11 Jr" ()12, Pl )b(O21; ()21 Jr- (,)22,1)2) The -2log, value is then computed ms follows: 1.
(f/1 ) m --21o 9 - - L( i t  2 ) b(O 11 ; O I  1 + O12,  P) I J (021 : O21 + 022 , P) - -21o 9 1((-)1 l ; ( )11  + O1-),  P I )h (O21 ; O21 q- ( )22  , P2 ) : - -2 ( (O l l  +021) lo r_ Jp+( ( )12+022) lo9(1 - - l~) - -  (,~1) (? )
l l l o  JP l+Ol21og( l  " t 1 )+0211ogp2-~0221o0(1- f~2) ) ) -- .2.,~ x (~  i (7~) -  ;~(~19- ) )  (4 ) = 2,v x Z(P~;  T )  (5 ) whel e  N = O l t  -F O12 -1- O21 -I- 022 is the  to ta l  l lum-.
to aecon l lnod~te  more  i l l l l la Ies zlt pena l  fac i l i t ies ,  e .g .
,  doub l i l l g tip, Ile~y COllStructlon?
Q,I ~,Vhat measures  have  been  taken/planned/rec~mnlel,ded (etc .}
498 Top ic I :ll h~l al l l  -21,~gX  ] l i~ la l l I  -21,,9X j a i l  t)3L I)1,1 e()tH~t 2, ja i l  Dit) 27:1 c+,l l l l l} .IIJN ~21 eae ly  le+]+.~lSt ?
N,~ :{t;] , )v , . )
, ~ , ,wd ln~;  :?12:1.
, ,n  7.1 R72 i l l ln?lt ," 2 : t l  7d5  s ta l , "  1,) i~, ,n, .
l i f  [  IF, I .
i l o  ,1:~ 3 fill," l ; l  t(;2") s ta le  151 9t t~ ia i l  r l%l  lctr~%vI] l r ld I;1 ~[ i I}l l~t l l i l  l  I I  ~" I ";~ C(,tlt I + , l , i " t  t{ll.O!
)l} i+tl-s,,rl 1,17, 3t),i h .
.a l  j a i l  56  t i t+ C l )y  133177 p l l .
, )D  ( )vcy lc l , , i v th l l~  55 37:  +, , , v , .
r , ,wd ,+d 12N I)t)S i-(*lllt :l[ fac i l i t  3 52 9o9 10 S ignat t t ro  Torms o f  Tup ic  151  Ovorcrowdod Pr i sons "II I~I al I l  -21,,~11 f - , I t .
l~t l  c , ,u l~ <, l t t , .
l  -I., :),;11 C, , l l l p  ]y  c+,ll~(lll ,]+c[+++" 3,5 12L +l,.kali+ ii)iI i,[~ +h,  l l  [  [  [15 121 ~,11  i,) t l ; ,nk  :;.5 L21 j , )o t l ; l l l k  IH ) l i5  :~.,.121 pl l~C, l l ,  r  c+)l l : l l~ la i l  :~5 121 91: i f , .   ]
) l l~t ) l l  i21) l l l t l~ ~N t).l[] t put  pl is+, l l  .2t~ :t-II c+~uuly  jaiL ~l ; l l , .
2 t~31 l h,,hl l,~e~l ja i l  2d  :l I I Top ic  10  S ig l ln t t l re  Tern ls  o f  Top ic  257  - ( l igar~t t~ Co| l s l l l ) l | ) t lo | l l :n i<r tun  21ogX I+i ~.rarll -- 2 / , , f / .
i r i4~am - 21, ,~A c lgrt l , .
I t?+ .171; [}:iN ~tlb:xtc+) LIt(  ] l l~l l~ ~il 7)iN I ,h i l ip  I I l , ) l l i+ t j l  2.~ ~DSI l ( )ht lcc~) : l l ; l  017  hn  t - lg / l l , - l l t -  t ;7 t2}I I r ) l  ] i l I l a l l s  beDs~, l l  h<.([~f.
211 ~)t~[l s I I IOk i l l~  28.t  19~ ph i l ip  t l l ( , l  [ i~  5t  ()7;~ [1111~ Likll(e[ d + [ l l l l  22 21. t ~n l~,ke  15913.1  clarxl<t1, :  %, at  t80 .
t5  q t t  iri l ln cl l~ .21 I IS I ,~ lh l l l a l l?
)375 to lh l l l l l l l~  i t l Y ,  l I l a t l t ) l lgk l  -t.t .13.1 qt l  q t t  f i~ ln  21 - I lS , ,~ha  I .
, )  elll()k.~ 112){}I  bll  b[i bl l  20  22t i s ,~i la 12)i .121 ~il pat r i ck  t0 .
t55  c+)l lst l l l l l} l lo l l  bn  c lgar , .
l te  2022d Illtll 113 ~+1~) c l~at+ l l~"  c~l l lpa l lV  :ID [$1)D ~[t+gtt a l l l .
r  [ iCtt l l  ~llI,lk?
(}llt 20226 al l l , )k(  l  10.1 I i0  (el l l  l l l a lk+ l  36223 [ l l l l~ Ca l l t e  [  ht:gl[ I  2(~ 22{i b[~t 79 .90:1  ?~IN illt+ll+il++t :1t;.22:1 i l l a  [ay~ia l l  +illk~[tl>,ll+e t4)l l lpi l l IV 2( I .22t  ] Top ic  I0 S ignatur .
"I~r)ns of Top ic  258 -- Co)nputor  Secur i ty I ~llial /lilt "2Ionia I t  i+/,r al l l  21,QIX "I1 i~ratn  --21o9, X (:+lll l l)l ltOr 115!~ :151 C4, l l lp I l l l  t  ae l  t l l l l y  213331 )e l  I l t t /p l l  [~i() l l   ] l th t )h l t ( l l y   [~  ~5.t v i rus  927.G7-1 ~[ ; idt lgt l , "  s l t l  [ t l  l l l  17~ 5NN I l lh .
l l  I lilt) 9R 85, t hacker  867 .377 FOl l lp t l t ,  t  +yS le l l l  1 -16.32~ C,+ltl++]l I l l l iVet~il~,  ~ lad l l  [ t le  7}) IJNI in,) l  rl+ +i+;+~ 2i13.! )
l , .~+-arch c,+ulte[  l ; i2  .l I :i l awte l l c l "  b ,  rk ,   *j~ lal l , ) l  al+,l  ,,.
79.0N [ c , , rn ,  l l  3P+5 6+4 c , : ,ml ,u t ,  r  wrus  12~k033 I~+,++, je t  p tO l , t l L+ io t+ 79 .0~1 un lv+ l?
i ty  31)5 .95~ corne l i  U lX iVe le i ty  1(1~4 7-t l  U l ; iV ,q+i )y  ~; radu lx t , .
lll 79U~1 +ysl+ l l l  290 .3"17  Iltl(:l,P;ll %t++npl)ll 107 .283 lawt l l l e ,+ l i v+: r tn ( .
te  I igt l i () l lal  i][) l[I;~ I / tb , .
ra lL : ) ly  2N7 521 in i l i ta ry  t  ( , l l lp , l l .
: r  106 .522 l iv~qll l ,) l?"
i lu) i ,maL  lubora lo ry  {59195 [ab  225 .51) ;  v i tu~ plo~t< l l l l  1U6 522 c,) l l lp l l I (~r S ,~CUl i ly  eXpet l  66 .19G mecLa ly  128 .515 %vesl ~et l l l a l l  82  2  [0  ~ecu [ i t?
,   cenl{~[  13ethesda  -19423 Top ic  10  S ignature  Ter lns  o f  Top ic  271  So lar  Power I  l l i g ta ln  - -21oqX t i ig t  ~ltn --2logX "Ir i ~;r hi l l  - -21o!~A so la r  -1S- l .315 e,~la~ e l te t l4y  2{Di 521 d iv i~ i ,m Inu l l ip l ,~  acress  31 3-17 ) t lazd i t  :10Pt 0IY) s<,lal l , t lw ,  t  9,1 210  n l , )b i l , :  l , , l , -ph , ,n , .
#c iv ic , ,  313 .17 le,) 271; .932  ( h r i~t ia l l  a id  8 f i .211  b l i l l sh  It .cl l l l i l l l )R} g  [ , , l l p  23510 it JtLi It l l l  2.5N.71):"+ l++,a S3Sl,*III 711 5:{5 el l l I} l  he iNht  llXile 23+5111 pax+lh , ,n  2133 81 I ill++tlllt.
Ie i t  j ) l l l ) l le  (115;l+i IillllllCilll I lack i l l+;  I l Jd l l l l l l  22i+51(1 i)(~tltld 12 / ,121  i t i , l i un l  p l , , j , .
c l  112.697 ~l,~l lal  In r )h i l ,  + sa l ,  l l i te  23  511J t , lw~r  12G.35:1 lei l i  <+, , ,d -  61.~111 ha l ld l le ld  IIled~il," t ,  l eph , , l l , :  23510 [ , , , , k , ,u t  125 .ll3t; scie.
l lc, ,  pa lk  ~>.1 NS{) i l l ( ,h i le  ~ate l l l l .
v>tetn  23  510 i i l  [ l l l i lSRl  1O9728 ~()llkl t  i l l l t  l  l l t l i l I l , l  51t ~5{} I l l l l t l l lvl i l l  i g id i l ln l  I> l , l j ec t  23 ,510 hc ,ydsh , t l  7N :173 l)p s l l la l  ?+1 ; /17  act iv t -  s+,la[ *ys tern  15673 Tattle 2: Top 10 signat.me t.erm.~; of mfigram, bigram, and trigram for fore" TREe  t.opics.
(~ the (d[+:ct.iv(,im.~s nf l(>l)i(: .~dgna- l;lll(~S llS(~(] ill SlllllIIN/ly (~Xtlit(:t;iOll, W{,  ~ CtIllll)~ll(~ +flit!
VC III(+~}/SIlI(+ + l;h(; l)crfl)rmanc(~ using a c()ml)ined umasure of lncall (I~) and pr(~cisi(m (P), F. F-score is defined by: I " - -  (1 +H2)Il?
where /3-P + I~ t ) 2 7 . )
tim model ,s.mn)?lr!l # of  sc+lt(!ncc,s i11 tim nlo,h:l .~um.tav!l # of  ,s(./Itclwcs c:rlv?lclcd t)1,t ll*c .Sll.Slcln rclaticc iml,ortancc of  l~ aml 1: (6) (7) Ve as.~um(~ (,(lual importance of re(:all iIIld preci- sion aim set H to 1 in our (+,Xl)(+rimtml;s. The Imselitm (I)ositi(m) module scores (at:h S(!llt(:llC{} hy its I)osi- ti(>n in the text.
This s(:ol( 3 indical.es l;h(~ l(!l(wall(:(~ of l.h(; S(!llt.t~n(:(!
original l;(}xI.
.&ass 0 50000 n ~ .~ .
1,* +  .~  *+-  .
; -5; , :~:;  .
:~.7~.~ ~ ~ ^ - -~- .
o 400OO f ,  " +- ~-" + ~ -, "~2x-+, [ ?
: [ - ...... ; ""7 ........ 2,=_ ~ 0 =0000 j   +J" J j  1" " .,::iff "4.
-a  + -a--  -#.
0 ~o00o d-;9~7~ -7 + 5~:7~:=-+: ; :  ~ .
=-~++:7:: ~ -:~ +--~ ....... " ~5_~Ztt::~:ll;: ; i I " , ; .
A  / , -?~-  <F" ~. "
I__ I - - - - - -~~g-  lO% I ___~o~a -ao~~a--- -  4o~ I ~0%- -  [  ~o~ [ ~,o~ ~o% I 9o~ I lOO% I [ ~.+,_~.,~dl .... i .
,~:c -  I ...ao=, [ __2 :~r  I~  o.ara oar , ;  I .
s i~  I -2 ,7d  -2 .19- - [ 257-h , , * , , l i  .... r--- (1.1-}98 {~.15.__.5 I c,,, ,,.is., ".~L I o.,~~--F--,~.~,, I - -o  t,l o.1~, I ?.
!s~ [ _,.~r_,a,,r [ -55.11- -38.56 I -".5U ~"> ~".0;   " +   I S ~:    I ~ ~ " " "  I +r  0 ~t  .
(h.~] [_ 25u_h~,~.,li .
L_  o l  tk_ o 270 I "4-2 ~ *~:~ I ,, ~,r_, L_  "47t_ J  .4 r , ,  1 - - ~ -  1  o.~,__,+~ o s_,Z._J [ 271_l,aseli .
I <,at tT_.._,~.
:,,~; T--,Ta77--- ..a:~ _L ,, :s:,r, .L .... ~~~~:~- i~-  T -  o.ae~ ] , ) .
lO  j _ _  + 4 ~ _ ~ ~ s .
Values in the 1)aselin(,.
Vahms in the tfidf and tot)i(: signatur(~ rows arc i)(.rformmlc(~ increase or (h,.crease divide(l by their (:orr(.,sI)ontling baseline scores and shown in I)er(:(mtag(!.
Kathh~(m M(:K(!own and l)rag(mfir R. I ladev.
I I (  ra t ,  i l l g  S l l l l l l l l ; l l  i ( : s  o f  I l t l l l t ,  i  [ ) l  [~  l l ( !~vs  articles.
http://www.mu(:.sai(:.(:om.
), locations (city, country, continent, etc.
1990).
1997).
.
.
.
43).
Harman [1993, 1995]).
4.
5.
Each sentence, S, has a single backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S).
However, (charge X) shares many arguments with (accuse X), (search X) and (suspect X) (e.g., criminal and suspect).
Thus, each event slot (v, d) for all d E D„ belongs to a chain c E C in the schema.
4 0 1.00 0.1203 0.2019 I)yn Adjs (1 ,_"; of L19.
Ib/+: positive polarity, l b l - :  negalive polarity.
Natural Language E, gineering, 6(2).
Sema,tics, volume 1.
Coml;tttatioltal Lin,~?uistics, 19(2):313-330, June.
()n grading: A study ill semantics.
1.
5,7).
The conditional probabilities have the following log-linear form: p(y|x) = 1 Z(x)e ? i ?i fi(y,x) (1) where fi is a feature, ?i is the corresponding weight, and Z(x) is a normalisation constant.
= L(?)
?G(?)
(2) = log m ? j=1 P?(d j|S j) ? n ? i=1 ?2i 2?2The data consists of sentences S 1, . . .
, dm.
L(?)
= 0.1).
2.
3.
4.
AS = {(wi, wj) E A|wi, wj E 51.
? TLINK.
(1) {A before B, A before C, B equals C} (2) {A after B, A after C, B equals C}Scoring (1) at 0.33 precision misses the interde pendence between the temporal relations.
For example, for the Brown Corpus sentence: Miss Xydis was best when she did not need to be too probing, consider the candidate parse: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WI[ADV when))) ( ie -s  (PRO she)) (VP ((VPAST did) (NEG ,tot) (V need)) (VP((X to) (V be)) (ADJP(ADV too) (ADJ probing))))))(?
After step-one rasures, this becomes: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WIIADV wheu))) (NP-s (PRO she)) (VP((VPAST) (NEG) (V need)) (VP((X)  (V be)) (ADJP(ADV too) (ADJ probing)))))) (?
(S (N (N Sue)) (V (V sees) (N (N Tom))))  = : ( ( (Sue)  ) ( ( sees) ( (Tom)  ) ) )e tc .
(NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) ( ))); NOTA BENE Example 2 : ("The lawyer with whom I studied law"): (NP (DET The ) (N lawyer) (S-REL (PP (P with) (NP whom ) ) (NP I) (VP (V studied) (NP (N law)) (PP 0)))) NOTA BENE - ?
(NP (DET The) (N lawyer ) (S- REL (PP (P with) (NP whom) ) (NP I) (VP (V studied) (NP (N law)) (PP ) ) )) NOTA BENE (e) Possess ive endings ( s,  ) E.g. "
I Ineer-do-welll 308 2.
Original parse (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST did ) (MEG not ) (V need )) (vP ((x to ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (?
Parse with all erasures performed except those of const i tuentdel imiters (parentheses): (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST ) (MEG ) (V need )) (vP ((x ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (?
Example: (will probably have) (seen Milton) -> ( probably ) (seen Milton) -> (probably seen Milton) 3.
Average Recall = (3 /4  + 7 /8 + 2/4  + 518 + 314)  / 5 = .700 Average Precision = (3 /5 + 7 /10  + 2 /5  + 5 /10 + 315)  / 5 = .560 311
Algorithm 5  .
 .
Similarly, S(z) denotes the string in z.
2004AA114010).
We collected counts for all configurations Lm.T.Rn for m E {0, 1, 2, 3, 41, n E {0, 1, 2, 3, 41 that occurred in the data.
P (wjr, pron r) = P(wjpron r).
 ).
Given a POS feature, for example, we could choose noun = 1, verb = 2, adjective = 3, and adverb = 4.
 ,  ,  ,  , (Pedersen and Bruce, 1997a)).
For the Naive Bayes model with 3 observable features A, B, C and an unobservable classification feature 5, where 0 = {P(a, s), P(b, s), P(c, s), P(s)} , the E and M-steps are: where s, a, b, and c denote specific values of S, A, B, and C respectively, and P(s1b) and P(s1c) are defined analogously to P(sla).
Bruce, 1997a)).
 ).
 ,  ),  ).
5.
4.
Our m-aspect ranking model contains m+1 components: ((w[1], b[1]), ..., (w[m], b[m]), a).
3More precisely (taking into account the possibility of ties): y[i] = min1E{1,..,k}{r : scorei(x) − b[i],.
For each possible prediction r = (r[1], ..., r[m]) this criterion assesses the level of grief associated with the ith-aspect ranking model, gi(x, r[i]).
For each training instance (xt, yt), each aspect i  1...m, and each rank r  1...k, define an auxiliary variable y[i]t r with y[i]t r = −1 if y[i]t  r and y[i]t r = 1 if y[i]t > r. In words, y[i]t r indicates whether the true rank y[i]t is to the right or left of a potential rank r. Now suppose that a training set (x1, y1), ..., (xT , yT) is perfectly rankable for each aspect independently.
That is, for each aspect i  1...m, there exists some ideal model v[i]∗ = (w[i]∗, b[i]∗) such that the signed distance from the prediction to the rth boundary: w[i]∗ · xt − b[i]∗r has the same sign as the auxiliary variable y[i]tr.
In other words, the minimum margin over all training instances and ranks, γ = minr�t{(w[i]∗ · xt − b[i]∗r)y[i]tr}, is no less than zero.
Consider any model (w, b = (b)).
Note that w · x1 < b and w · x2 > b together imply that w3 < 0, whereas w · x3 > b and w · x4 < b together imply that w3 > 0.
23(1):13–31.
Step 1.
For both Examples 3 and 4, 2P(A) − 1 = 0.8.
Thus (1+2+1)!/(1+1)!
We c ? 2008.
yes, ?on?
yes, ?juicy?
=, where N = ( )??
We therefore multiplied miw,c with a discounting factor: ( ) ( ) ( ) ( ) ( ) ( ) 11 +???
Figure 1.
2.
3.
dist(C, A) is the number of operations performed using the above transformation rules on C. a b e c d e a c d b e b a c d e a b c d e A) B) C) D) E) Figure 2.
Table 3.
(cf.
*Mary began a rock. b.
?
That is, it has as its type, ([N Telic], N).
The projective conclusion space, P('DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicate Q: as: P(43R) = {(P(Q1)P(Q)) I(Qi, • • • , Qn) 430.
The outcome probabilities are: where t = sub(c) o sub(c).
Figure 2(a) and (b) are tree-equivalent.
Figure 2(b) is binary.
The model is a conditional probability p(f|T(e)).
.L....:=Iret.urn(nRatord_form(1,!
See <http://www.cogs. susx.ac.uk/lab/nip/carroll/morph.html>.
4.
5.
Thus, .
(2) and in Eqn.
In order to discard incorrect instances, they learn WordNetbased selectional restrictions, like “X(scene#4)’s Y(movie#1)”.
“Canada, country”) and added to I.
.
[Australia] [has] [dipl. rels.]
We set b = 30, R = 1 for X cells, b = 15, R = 1 for S cells, and b = 100 for rules except where noted in Table 5.
Here, |sum2||sum1 |>n |sum2|).
5), and Viterbi parses (Sec.
6).
1.
Formally, for the net in (a), we can write P(a, b) = P(a)P(b|a).
For (b) we write P(a, b) = P(b)P(a|b).
However, in (c), the nodes A and B carry the information P(a|b) and P(b|a) respectively.
However, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1) = 3/4 × 3/4 = 9/16, while the score of 33 is 1.
.
.
• . hn-1).
.
= o otherwise.
(S[dcl]\NP)/PP or S[dcl]\NP).
The function can be defined as follows: S(u;?, ?, ?) = ? ?
0 for u ? ?
2(u??
)2 for?
u ? ?
1?
2(u??
)2 for ? ?
u ? ?
1 for u ? ?
+ ?)/2 = 8.
.
2.
4.
.
.
If there is a genuine association between x and y, then the joint probability P(x,y) will be much larger than chance P(x) P(y), and consequently I(x,y) » 0.
If there is no interesting relationship between x and y, then P(x,y) P(x) P(y), and thus, I(x,y) 0.
If x and y are in complementary distribution, then P(x,y) will be much less than P(x) P(y), forcing I(x,y) « 0.
First, joint probabilities are supposed to be symmetric: P(x, y) = P( y, x), and thus, mutual information is also symmetric: I(x, y) = I(y, x).
In contrast, when /(x, y) = 0, the pairs are less interesting.
Alternatively, one could make estimates of the variance and then make statements about confidence levels, e.g. with 95% confidence, P(x, y) > P(x) P(y).)
If /(x, y) « 0, we would predict that x and y are in complementary distribution.
Then, P(x) = P(y) = 10-5 and chance is P(x) P(x) = 10-1°.
.
.
.
.
.
.
.
.
Set off occurs nearly 70 times in the 7.3 million word corpus [P(x, y) = 70/(7.3 x 106) » P(x) P(y)].
.
.
151-152).
Using Sinclair's estimates P(set) 250 x 10-6, P(off ) 556 x 10-6, and P(set, off) 70/(7.3 x 106), we would estimate the mutual information to be I(set; off) = log2 P(set, off )1 (P(set) P(off )) 6.1.
.
.
.
.
.
.
.
.
.
.
.
. from.
.
.
(3) s(x) —> np(y), vp(x, y).
(4) r: run(r), past(r), argl(r, j), name(j, John) Furthermore it is a complete sentence.
(5) vp(x) —> vp(x) adv(x) (6) r: run(r), past(r), fast(r), argl(r, y) agenda now contains the entries in Words Cat Semantics John ran s(r) r: run(r), past(r), arg I (r, j), name(j, John) ran fast vp(r, j) r: run(r), past(r), fast(r), argl(r, j) ran quickly vp(r, j) r: run(r), past(r), fast(r), arg 1 (r, j) Assuming that adverbs modify verb phrases and not senthere will be no interactions when the ran is moved to the chart. the edge for fast moved, the possibility of creating the phrase fast quickly well as fast. are rejected, however, on the grounds that they would involve using a predicate from the original semantic specification more than once.
1.
2.
3.
4.
5.
(11) dog(d), def(d), saw(s), past(s), cat(c), def(c), argl(s. d), arg2(s, c).
(12) s(x) np(y) vp(x, y) (13) vp(x, --> v(x, Y, z) np(z) (14) np(x) ---> det(x) n(x) (15) Words Cat Semantics cat n(x) saw z) x: see(x), past(x), argl(x, y), arg2(x,z) dog n(x) the det(x) The procedure will be reminiscent of left-corner parsing.
(16) Vert Words Cat Semantics d the det(d) d: def(d) the np(d)/n(d) d: def(d) dog n(d) d: dog(d) s saw v(s, d, c) s: see(s). past(s), d), arg2(s, c saw vp(s, d)/np(c) r: see(s), past(s), argl(r, j) the det(c) c: def(c) the np(c)/n(c) c: def(c) cat n(c) c: dog(c) (17) Vert Words Cat Semantics d the dog np(d) d: dog(d), def(d) saw the cat vp(s, d)/np(d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) c the cat np(c) c: cat(c), def(c) s saw the cat vp(s, d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) Among the edges in (16), there are two interactions, one at vertices c and d. They cause the first and third edges in (17) to be added to the agenda.
203 (18) Vert Words Cat Semantics s The dog saw the cat s(s) dog(d), def(d), see(s), past( s),argl(s , d), arg2(s, c), cat(c), def(c).
The grammar is consulted only for the purpose of creating active edges and all interactions in the chart are between active and inactive pairs of edges incident from the d the dog np(d) d: dog(d), def(d) saw the vp(s, d)/np(d) s: see(s), past(s), cat argl(s, d), arg2(s, c), cat(c), def(c) c the cat np(c) c: cat(c), def(c) s saw the vp(s, d) s: see(s), past(s), cat argl(s, d), arg2(s, c), cat(c), def(c) Among the edges in (16), there are two interactions, one at vertices c and d. They cause the first and third edges in (17) to be added to the agenda.
For Expectation Maximization training, we compute lexicalized inside probabilities Q(X(e/f),l, m, i, j), as well as unlexicalized inside probabilities Q(X,l, m, i, j), from the bottom up as outlined in Algorithm 1.
Let INS(l, m, i, j) denote the major factor of our Model 1 estimate of a cell’s inside probability, Ht∈(i,j) Es∈{0,(l,m)} t(ft  |es).
One can map F to matrix f of size W x d, where d << C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions.
Specifically: L(x) = max(0, 1− s(x) + s(˜x)).
pages).
3.
Any opinions, findings and conclusions or rectitle auth. pubnum date abs. aff. addr. email deg. note ph. intro k.w. web title 3446 0 6 0 22 0 0 0 9 25 0 0 12 0 author 0 2653 0 0 7 13 5 0 14 41 0 0 12 0 pubnum 0 14 278 2 0 2 7 0 0 39 0 0 0 0 date 0 0 3 336 0 1 3 0 0 18 0 0 0 0 abstract 0 0 0 0 53262 0 0 1 0 0 0 0 0 0 affil.
19 13 0 0 10 3852 27 0 28 34 0 0 0 1 address 0 11 3 0 0 35 2170 1 0 21 0 0 0 0 email 0 0 1 0 12 2 3 461 0 2 2 0 15 0 degree 2 2 0 2 0 2 0 5 465 95 0 0 2 0 note 52 2 9 6 219 52 59 0 5 4520 4 3 21 3 phone 0 0 0 0 0 0 0 1 0 2 215 0 0 0 intro 0 0 0 0 0 0 0 0 0 32 0 625 0 0 keyword 57 0 0 0 18 3 15 0 0 91 0 0 975 0 web 0 0 0 0 2 0 0 0 0 31 0 0 0 294 ommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
In summary, a DCG grammar rule will be encoded as the clause node(( syno )1 (semo), PO-P) ---> [node((sym) I (semi), PO-P1), .
.
.
, node((syn„) I (sem„)), P„_1-P].
.
.
.
.
For example, here is the rule for sentences: s(Form, GO-G, Store)/quant(Q,X,R,S) ---> (8) s(Form, GO-G, [qterm(Q,X,R)IStore])/S.
The term quant(Q,X,R,S) represents a quantified formula with quantifier Q, bound variable X, restriction R, and scope S; qterm(Q,X,R) is the corresponding store element.
&quot;burned/burnt&quot;); and variants such as &quot;traveller/traveler,&quot; realize/realise,&quot; etc.
Computational Linguistics, 27(3):351–372.
For example, if , where and , then , , , ,and .
.
2.
(cf.
2.
3.
Nilsen  ).
.
Given a source (’English’) sentence e = eI1 = e1, ... , ei, ..., eI and a target language (’French’) sentence f = fJ1 = f1, ..., fj, ..., fJ.
4.
A gain is defined as follows: exp[EMm= 1 λmhm(a, e, f)] where l = (i, j) is a link added to a.
Experimental 3 E → C (MEC); A2: Model 3 C → E (MCE); A3: POS E → C (PEC); A4: POS C → E (PCE); A5: Dict (normalized such that E5m_1 Am = 1). results show that log-linear models for word alignment significantly outperform IBM translation models.
2004AA114010).
Collins  , Charniak  ).
First, we note that the feature functions factor over edges, i.e., fu(T) = (i,j)k∈ET fu(i,j, k).
Furthermore, we set 0(i) = 1 for each i E Vx.
We can reduce the 3D-matching problem to the first-order vertical Markov parsing problem by constructing a graph G = (V, E), such that L = AUBUC,V = {00,0} U A U B U C and E = {(i, j)k  |i, j E V, k E L}.
This graph simply consists of the edges (0, a)b, (a, b)c, and (b, c)c, for all (a, b, c) E S0, plus an arbitrarily labeled edge from 00 to 0.
4.
We write K = (K, ®, ®, 0,1) for a semiring with elements K, additive operation ®, multiplicative operation ⊗, additive identity 0, and multiplicative identity 1.
We are given a function p : D → R>0, which decomposes multiplicatively over component hyperedges e of a derivation d ∈ D: that is, p(d) def = eEd pe.
For example, in the real semiring (Il2, +, x, 0, 1), we define p* = (1 − p)−1 (= 1 + p + p2 + ...) for |p |< 1 and is undefined otherwise.
Define ke = (pe, pere).
Then Figure 2 will return (Z, r).
Define ke = (pe, pere, pese, perese).
Then the algorithm of Figure 2 returns (Z, r, s, t).
With a valid semiring, we then simply observe that Figure 2 returns the total weight ®dED ®eEd ke = ®dED (p(d), p(d)r(d)) = (Z, r).
It is easy to verify the second equality from the definitions of ®, Z, and r. The first equality requires proving that ®eEd ke = (p(d), p(d)r(d)) from the definitions of ®, ke, p(d), and r(d).
The main intuition is that ® can be used to build up (p(d), p(d)r(d)) inductively from the ke: if d decomposes into two disjoint subderivations d1, d2, then (p(d), p(d)r(d)) = (p(d1)p(d2),p(d1)p(d2)(r(d1) + r(d2))) = (p(d1),p(d1)r(d1)) ® (p(d2),p(d2)r(d2)).
In particular, one mainly needs to show that ®e∈d ke = (p(d), p(d)r(d), p(d)s(d), p(d)r(d)s(d)).
We will show how to speed it up with an “inside-outside algorithm.” In general, for P, R, 5, T, we can define the first-order expectation semiring EP,R = (P × R, ⊕, ⊗, 0,1) and the second-order expectation semiring EP,R,S,T = (P × R × 5 × T, ⊕, ⊗, 0, 1), using the definitions from Tables 1–2.
Here (ke,xe) def = (pe, pere), and the algorithm returns (ˆk, ˆx) = (Z, r).
Then lift this a second time to obtain the “nested” first-order expectation semiring EK,X = E(EP,R),(SxT), where we equip Xdef = 5 x T with the operations (s1, t1) + (s2, t2) def = (s1 + s2, t1 + t2) and (p, r)(s, t) def = (ps, pt + rs).
The resulting first-order expectation semiring has elements of the form ((p, r), (s, t)).
Table 4 shows that it is indeed isomorphic to EP,R,S,T, with corresponding elements (p, r, s, t).
For example, to speed up Section 3.2, we may define (ke, xe) = ((pe,pere), (pese,perese)) for each hyperedge e. Then the inside-outside algorithm of Figure 4 will compute (ˆk, ˆx) = ((Z, r), (s, t)), more quickly than the inside algorithm of Figure 2 computed (Z, r, s, t).
In Sections 3.2 and 4.1, we saw how our semirings helped find the sum Z of all p(d), and compute expectations r, s, t of r(d), s(d), and r(d)s(d).
The KL divergence to p from q can be computed as KL(p II q) = H(p, q) − H(p).
We define µ(si, ti) = sim(�ti, s')− sim(si, s'), where sim(·, ·) is simply computed as the count of overlapping words.
The topics in the “not useful” category are “appeal”, “execute”, “fired”, “pardon”, “release” and “trial”.
(B.umelhart, McChdland, t98,1)).
 .
l;igu,e 2: Structure.
11 f  @ @ ?...?
@ @ @...@ @...?
3) is returned.
noun, verb, adjective).
Nakamura, M., I(.
8 ).
“the gunman”.
The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y.
For example, f(k)=ak – b when k >= 0, and a, b > 0.
8 ).
PROTEIN, DNA, etc.).
“BMLF1 transcript”).
The axiom (V x, y)part(y, D nn(x, y) allows interpretation of compound nominals of the form &quot;<whole> <part>&quot;, such as &quot;filter element&quot;.
If we superscript the atoms in the above logical form by their assumability costs, we get the following expression: (3e, x, c, kl, k2, y, a, o)Past(e)&quot; A disengage' (e, x, c)&quot; A compressor(c)&quot; A after(ki, k2)&quot; A event(k2 )S1° A rel(ki,y)sss A y E {c, e} A event( k2)11° A rel(k2, a)s&quot; A alarm(a)55 A nn(o, a)82° A lube-oil(o)55 While this example gives a rough idea of the relative assumability costs, the real costs must mesh well with the inference processes and thus must be determined experimentally.
Abduction is the process by which, from (lx)p(s) D q(r) and q(A), one concludes p(A).
One can think of q(A) as the observable evidence, of ( z)p(s) D q(x) as a general principle that could explain q(A)'s occurrence, and of p(A) as the inferred, underlying cause of q(A).
35.)
While the axiom (V x)pressure(x) D ,lube-oil(x) is certainly true, the axiom (V ei, x)pressure(ei, x) D e2)lube-oir(e2, x) would not be true.
This section discusses the structure of f(x,y).
.
.
Table 1)1.
“Dreaming”.
(1) Rare morphs.
The tags are A (adjective), ACT (active voice), ADV (adverb), CMP (comparative), GEN (genitive), N (noun), PCP2 (2nd participle), PL (plural), PTV (partitive), SG (singular), V (verb), and <DER:ly> (-ly derivative). mentation with the desired morphemic label sequence (cf.
Figure 4).
ML), and Linguistica (Ling.).
(1)).
>s: x/y liz y liz --+ x liz can be maintained outside the parse chart, to serve 11 f g Az. f(z)(g(z)) b.
Example 2 - Repetition.
Example 3 also illustrates this change from specific (1, 3, 5) to general (7).
(A).
(B).
Note that Ri,j > i for all i, j.
The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j].
Given training data E = {(x, y)}, a maximum entropy model gives conditional probability p(y|x) as follows.
A feature forest Φ is a tuple (C, D, r, -y, b), where: We denote a feature forest for x as Φ(x).
Ω(n) is defined recursively.
For a terminal node c E C, obviously Ω(c) = {{c}}.
Given a feature forest Φ = (C, D, r,γ, 6), a set Ω(n) of unpacked trees rooted at node n E C U D is defined recursively as follows.
Hence, Ω(n) is properly defined.
We map the tuple (em, el, er), which corresponds to (m, l, r), into a conjunctive node.
2005).
and their translations (m-grams) t?, along with associated translation probabilities p(s?|t?) and p(t?|s?).
Phrases are limited to 8 tokens in length (n,m ? 8).
= argmax t p(t|s) ? argmax t,a p(t,a|s),where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar get phrases such that t = t?1...t?K ; s?k are sourcephrases such that s = s?j1 ...s?jK ; and s?k is the trans lation of the kth target phrase t?k. To model p  to maximize the system?s BLEU score   on a development corpus.
according to p(t?|s?) are retained.
where s?
We then define: C(s?, t?)
and t?.
and t?
C(s?, t?)
C(s?)?
C(s?, t?)
C(s?) C(t?)?
C(s?, t?)
N ? C(s?)?
C(t?) + C(s?, t?)
N ? C(s?) C(t?) N ? C(t?) NIn particular, Fisher?s exact test calculates probabil ity of the observed table using the hypergeometric distibution.
ph(C(s?, t?)) = ( C(s?) C(s?, t?)
)( N ? C(s?) C(t?)?
C(s?, t?)
p-value(C(s?, t?)) = ??
Note that the count C(s?, t?)
can be larger or smaller than c(s?, t?)
with t?
On the other hand, C(s?, t?)
2.4 C(s?, t?)
= 1.
Similarly, ? ?
The phrasetables were then used as a list of n,m-grams for which counts C(s?, t?), C(s?), and C(t?) were obtained.
= log(N)).
= 1 14?
4.5 Comment about C(s?, t?)
= 1.
= 1.
= 1.
2.
3.
4.
5.
2.
3.
1.
2.
(phrase) f?
4.
5.
6.
2
A translation rule is written X ? ?/?.
... a c a c b a d c a d ... a c a d b a a d b d ... a d d b a a d a b c ... a d d b d a a b b a ... a d d b d d c a a a ...
1).
3).
For example, the maximum likelihood estimate for a is a = k, giving the model M = {a} = {kJ.
 ).
“grasp his hand”).
In 28(3).
As a result, I missed my appointment (CAUSE)).
 ).
 ).
The statistics about the lexicalized examples are as follows: N-N (30.01%), Adj-N (0%), s-genitive (0%), of-genitive (0%), adjective phrase (1%).
Q.
Q.
1.
2.
3.
4.
5.
Examples are: lists, narrative structures, and various binary structures (&quot;A but B&quot;, &quot;A because B&quot;, etc.).
7.
Operators include &quot;connectors&quot; like &quot;and&quot;, &quot;or&quot;, &quot;because&quot;, as well as &quot;discourse markers&quot; like &quot;well&quot;, &quot;so&quot;, &quot;incidentally&quot;.
- The Interaction specifies referents for indexicals like &quot;I&quot;, &quot;you&quot;, &quot;here&quot;, &quot;now&quot;.
7.
Operators include &quot;connectors&quot; like &quot;and&quot;, &quot;or&quot;, &quot;because&quot;, as well as &quot;discourse markers&quot; like &quot;well&quot;, &quot;so&quot;, &quot;incidentally&quot;.
- The Interaction specifies referents for indexicals like &quot;I&quot;, &quot;you&quot;, &quot;here&quot;, &quot;now&quot;.
: lists, narratives).
- structures formed by a binary operator, such as &quot;A because B&quot;, &quot;If A then B&quot;.
&quot;If A then B&quot; is a clear example.)
2.
(Cf.
Verkuyl, 1972.)
(Cf.
i=1 p(ti|ti?1o).
i=1 p(ti|ti+1o).
They correspond to the following equations: (a) P (t1...t3|o) = P (t1|o)P (t2|t1o)P (t3|t2o) (5) (b) P (t1...t3|o) = P (t3|o)P (t2|t3o)P (t1|t2o) (6) (c) P (t1...t3|o) = P (t1|o)P (t3|o)P (t2|t3t1o) (7) (d) P (t1...t3|o) = P (t2|o)P (t1|t2o)P (t3|t2o) (8)(a) and (b) are the standard left-to-right and rightto-left decompositions.
Note that for each local clas sification, the function chooses the appropriate local function bestScore() { return bestScoreSub(n+2, ?end, end, end?, ?L,L?); } function bestScoreSub(i+1, ?ti?1, ti, ti+1?, ?di?1, di?)
{ // memorization if (cached(i+1, ?ti?1, ti, ti+1?, ?di?1, di?)) return cache(i+1, ?ti?1, ti, ti+1?, ?di?1, di?); // left boundary case if (i = -1) if (?ti?1, ti, ti+1?
= ?start, start, start?)
return 1; else return 0; // recursive case P = localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?); return maxdi?2 maxti?2 P?
bestScoreSub(i, ?ti?2, ti?1, ti?, ?di?2, di?1?); } function localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?)
In this section we present a greedy version of the decoding method for bidirectional inference, which 469 function bestScore() { return bestScoreSub(n+3, ?end, end, end, end, end?, ?L,L, L, L?, ?L,L?); } function bestScoreSub(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?) { // to avoid cycles if (di?1 = di & di != d?i) return 0; // memorization if (cached(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?) return cache(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?); // left boundary case if (i = -2) if (?ti?2, ti?1, ti, ti+1, ti+2?
= ?start, start, start, start, start?)
return 1; else return 0; // recursive case P = localClassification(i, ?ti?2, ti?1, ti, ti+1, ti+2?, ?d?i?1, di?1, di, d?i+1?); return maxd?
i?2 maxdi?3 maxti?3 P?
1.
2.
(P (ti|o)).
Method Accuracy (%) Dep.
1.
We will write p(y|x; w, v, q) to refer to the conditional distribution under parameter values w, v, q.
.
. qk from step (2) fixed.
.
Specifically, we define to the dj-dimensional feature vector rj(x, h, m, l).
3.
1 and 2.
We define the initial parameters (w0, v0) = arg max,,v L(w, v; q0).
4.
8.
It’s a stiff!
2.
3.
4.
6.
pen 2 1.
3.
4.
2.
3.
4.
goat 1.
3.
a lecherous man. 4.
7.
2.
6.
page 2 1.
3.
4.
5.
6.
7.
3 391 Figure 2.
392 4 Figure 3.
In what follows, x = x1 ? ?
(i,j)?y s(i, j) = ?
We represent the generic directed graph G = (V,E) by its vertex set V = {v1, . . .
, vn} and set E ? [1 : n]?
[1 : n] of pairs (i, j) of directed edges vi ? vj .Each such edge has a score s(i, j).
Since G is di rected, s(i, j) does not necessarily equal s(j, i).
Chu-Liu-Edmonds(G, s) Graph G = (V, E) Edge weight function s : E ? R 1.
Let M = {(x?, x) : x ? V, x?
s(x?, x)}.
2.
Let GM = (V, M).
4.
5.
Let GC = contract(G, C, s).
6.
Let y = Chu-Liu-Edmonds(GC , s).
7.
Find a vertex x ? C s. t.
(x?, x) ? y, (x??, x) ? C. 8.
return y ? C ? {(x??, x)} contract(G = (V, E), C, s) 1.
Add a node c to GC representing cycle C. Add edge (c, x) to GC with s(c, x) = maxx??C s(x?, x) 4.
For x ? V ? C : ?x??C(x, x?)
E. Add edge (x, c) to GC with s(x, c) = maxx??C [s(x, x?)
s(a(x?), x?)
+ s(C)] where a(v) is the predecessor of v in C and s(C) = Pv?C s(a(v), v) 5.
w0 = 0; v = 0; i = 0 2.
for n : 1..N 3.
for t : 1..T 4.
min ? ?
w(i+1) ? w(i) ? ?
s.t. s(xt, yt) ? s(xt, y?)
L(yt, y?), ?y? ? dt(xt) 5.
v = v + w(i+1) 6.
i = i + 1 7.
The resulting online update (to be inserted in Figure 4, line 4) would then be: min ? ?w(i+1) ? w(i) ? ?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?) where y?
s.t. s(l, j) ? s(k, j) ? 1 ?(l, j) ? yt, (k, j) /?
2.
3.
To compensate for this, Shaw and Hatzivassiloglou   propose to compute the transitive closure of the ordering relation: if a � c and c � b, then a � b. Malouf   further proposes a back-off bigram model of adjective pairs for choosing among alternative orders (P((a,b) {a,b}) vs. P((b,a) {a,b})).
Emotions, such as “:(“ symbolizing sad, “:)” symbolizing smiling, “:()” symbolizing shocked, are representations of body language.
Takako w r u?
(ihin;sha.w and I{..lackendoff.
:cs ((vp 2 :mood to-infinitlve :subject 1)) :features (:raising subject) :gs (:subject () :comp 2) :ex "they seemed to wilt.")
using ILl)IlL _ _ m 1 + 2 100% 1 + 3 97% 1 + 4 96% 2 + 3 99 % 2 + 4 95% 3 + 4 97% 2-elf av I !
Second, we are dewq.pinp; a IIIO1(~ balanced C(HplIH for t]lo dv,,s to , , ,~ , i l t .
i~)~c,.,,,, st,,di~,s (e4;., [1]) co,lh., , , our {)])serv;ttions I,h:d, I(:d, ures SllCh as sub(atego- rizati()n patter l l s  luay di[il!r sui)stantiatly betweell corpora.
[41 [5] [2] Michael Brent.
[3] l)onald ltindle and Mats l{.ooth.
Morgan l(aufmaml.
[8] P. Proctor, editor.
[9] Naomi Sager.
Addison-Wesley, I{.eading, MA, 1981.
References [1] Douglas Biber.
It contains two pairs of paraphrases: (“burst into tears”, “cried”) and (“comfort”, “console”).
.
For example, when , the contextual feature for the paraphrase pair (“comfort”, “console”) from Figure 1 sentences is left =“VB TO ”, (“tried to”), left =“VB TO ”, (“tried to”), right =“PRP$ , ”, (“her,”) right context$ =“PRP$ , ”, (“her,”).
Japanese).
&quot;place&quot;) mass.
 ).
This function is f(z) = log (1 + e—z), f(z) = e—z, or f (z) = Qz < 01 for LogLoss, ExpLoss, and Error, respectively.
As z Y oo, the functions f (z) = e—z and f (z) = log(1 + e—z) become increasingly similar, because log (1 + e—z) Y e—z as e—z Y 0.
Thus our training data consist of a set of parses, {xi,j : i = 1, ... , n, j = 1, ... , niJ, together with scores Score(xi,j) and log-probabilities L(xi,j).
.
.
, n, j = 2,. .
.
˜hk ˜pk(¯a').
), (VP, PP, VBD!, NP), (VP, VBD!, NP, NP), (VP, NP, NP, SBAR), and (VP,NP, SBAR, STOP) (! is used to mark the head of the rule).
As an example, say the nonterminal dominating the example rule is S. The example rule would contribute (Left, S, VP, VBD, PP, adj = 1), (Right, S, VP, VBD, NP, adj = 1), (Right, S, VP, VBD, NP, adj = 0), and (Right, S, VP, VBD, SBAR, adj = 0).
This relation would then generate features (VP, VBD, SBAR, = dist), and (VP, VBD, SBAR, < x) for all dist < x < 9 and (VP, VBD, SBAR, > x) for all 1 < x < dist.
Table 4 shows the value of Savings(a,b) for various values of (a,b).
Savings(n)(y-axis) versus n(x-axis).
2002).
The running time for these methods is therefore O(f x (p + 1)1k).
(DP2).
Given a string s, where s o D , we want to return argmaxw P(w  |s)P(w  |context) .
With generic edits, we have to examine all cells (a,b) where a<_ i and b <_ j.
The prior has 0 mean and diagonal covariance: A ∼ N(0, diag(σ2 i)).
o1: COLING-92, NAN rJ,S, A[Jo.
7 However, the financial 6.
Guthile, J., L Guthrle, Y.
Hanks, Patrick (ed.)
(Special Issue).
Then, cuts in G are defined as follows: Definition 1 A cut (S, T) of G is a partition of its nodes into sets S = {s} U S0 and T = {t} U T0, where s ∈� S0, t ∈� T0.
The and ind2(si) = 1 − ind1(si).
Copestake, A., T. Briscoe, 1 ).
I-ON n I-UPON ], 1 L9 -q,] .
Farwell, D., 1,.
Machine 7Yanslaliou, 8(3).
Fillmore, C.,I.
[lolt, l{,inehart, a11d Winston, p~ges 1 88.
,l~u;kendoff, R,.
l,onsdale, I)., T. Mitamura, and F,.
WOIU)NF, T: A /)ictiouary lh:owser.
l,onginan, l,ondon.
Wilks, Y., 71).
Machine 7ranslalion, 5(2):99 154.
P]al,e, and B.M.
I,mge-Seale Automatic Ex- traction o[ an l,]nglish-Chinese, Translation l,exieon.
Machine Translalion, 9.
Some examples are “X is hit” or “Y’s residents”.
Input: “ ” (I live in Metropolis of Tokyo .)
In other words, y = ((w1, t1), ... , (w#y, t#y)) where #y is the number of tokens in the path y.
A normalization constant is then given by Zx = α(weos,teos)(= β(wbos,tbos)).
).
Computational Linguistics, 27(2): 199-229.
 .
, .).
Suppose we have a precedence rule: VB → (nsubj, 2, NORMAL), (dobj, 1, NORMAL), (self, 0, NORMAL).
As shown in Figure 3, “��dl P, ” (“with a bat”) is actually between “ L” (“John”) and “� ��” (“the ball”).
It corresponds to “4ol-01 (a bat) (with)”.
2 days).
2 weeks).
The rightmost-path of the tree t is (a(c(b))), and occurs at positions 1, 4 and 6 respectively.
The implicit mapping defined by tree kernel is given as: Φ(x)=(I(t1 ⊆ x), ... , I(t|F |⊆ x)), where tj∈F, x ∈ X and I(·) is the indicator function 1.
A.
B.
2!
Table 2.
3!
.
2.
4.
5.
(organization name).
as shown in Figure 1(e).
For example, ?John?s wife?
and ?John?s wife?
as shown in Figure 1(e).
and ?John?s wife?
3) Calculate 1 2( , )n nD recursively as: ? = D+=D )(# 1 2121 1 )),(),,((1(),( nch k knchknchnn l (2) where )(# nch is the number of children of node n , ),( knch is the k th child of node n andl (0< l <1) is the decay factor in order to make the kernel value less variable with respect to different sub-tree sizes.
D= m i NnNn ii C iiii nnTTK 1 ]2[]2[],1[]1[ 11 1111 ])2[],1[(])2[],1[( (3) Where ? ][1 jN i is the set of root node paths with length i in tree T[j] while the maximal length of a root node path is defined by m. ? ])[...(][ 211 jnnnjn ii = is a root node path with length i in tree T[j] , which takes into account the i-1 ancestral nodes in2 [j] of 1n [j] in T[j].
732 3) Calculate ])2[],1[( 11 ii nnD recursively as: ? = D+= D ])1[(# 1 11 11 1 ))],2[(),],1[((1( ])2[],1[( inch k ii ii knchknch nn l (4) where ])],[( 1 kjnch i is the k th context-sensitive child of the context-sensitive sub-tree rooted at ][1 jn i with ])[(# 1 jnch i the number of the con text-sensitive children.
with degree d=2, i.e. 2( , ) ( ( , ) 1)pK K?
 ).
For example “X acquired Y” entails “X owns Y”.
‘X s+_ prevent � Y’).
‘X s+_ prevent � Y’).
Assuming |T |= O(n), the algorithm is O(n4).
In general we assume that L(x, y, ˆy) = 0 for y = ˆy.
L(xi, yi, y) is large.
2.
Parts of the first type are single constituent tuples (A, s, e, i), consisting of a non-terminal A, start-point s and end-point e, and sentence i, such as r in figure 1(b).
Parts of the second type consist of CF-ruletuples (A —* B C, s, m, e, i).
This would lead to L(x, y, ˆy) tracking the number of “constituent errors” in ˆy, where a constituent is a tuple such as (A, s, e, i).
Another, more strict definition would be to define l(x, y, r) to be 0 if r of the type (A —* B C, s, m, e, i) is in the derivation y and 1 otherwise.
3.
We used the constituent loss in our experiments. marginals as Qm(µ(α)), where µ(α) is the vector with components µi,r(αi), and Qm(µ) is defined as: where li,r = l(xi, yi, r), φi,r = φ(xi, r) and Ii,r = I(xi, yi, r).
from(x, atlanta) ? to(x, denver)?
airline(x, delta air lines) ? flight(x)?
to(x,washington) ? month(x, april)?
day number(x, 22) ? during(x, night)?
679 following phrase to create a new category of typeN : flights to boston N (N\N)/NP NP ?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston > (N\N) ?f.?x.f(x) ? to(x, boston) < N ?x.flight(x) ? to(x, boston) The top-most parse operations pair each word with a corresponding category from the lexicon.
w ? f(x, y) . Assuming sufficiently local features3 in f , search fory?
The first new combinators we consider are the relaxed functional application rules: A\B : f B : g ? A : f(g) (&) B : g A/B : f ? A : f(g) (.)
A second set of new combinators are the relaxed functional composition rules: A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B) B\C : g A/B : f ? A\C : ?x.f(g(x)) (.
?f. argmax(f, ?x.flight(x) to(x,washington) ?y.depart time(y)) .B NP\N ?f. argmax(?x.f(x)?
Set ? = ?0.
Algorithm: ? For t = 1 . . .
T, i = 1 . . .
n : Step 1: (Check correctness) ? Let y?
= argmaxy?GEN(xi;?)
w ? f(xi, y) . ? If L(y?) = zi, go to the next example.
Step 2: (Lexical generation) ? Set ? = ? ?
GENLEX(xi, zi) . ? Let y?
= argmaxy?GEN(xi,zi;?)
w ? f(xi, y) . ? Define ?i to be the set of lexical entries in y?.
Set lexicon to ? = ? ?
?i . Step 3: (Update parameters) ? Let y?
= argmaxy?GEN(xi;?)
w ? f(xi, y) . ? If L(y?) 6= zi : ? Set w = w + f(xi, y?)
f(xi, y?)
Then, any class assignment c = c(s1), c(s2), ... , c(sn) can be assigned a cost where c(s) is the “opposite” class from c(s).
2(b) is non-local.
4).
4).
.
.
.
.
.
.
.
.
.
.
.
.
Section 4).
E,R (simple past) John left.
62. a.
'selectional restrictions').
1
We write the parameter associated with a trigram hx; y; zi as  x;y;z, and the param eter associated with a tag/word pair (t; w) as  t;w. A common approach is to take the param eters to be estimates of conditional probabilities:  x;y;z = logP (z j x; y),  t;w = logP (w j t).
For convenience we will use w [1:n]as short hand for a sequence of words [w 1 ; w 2 : : : w n ], and t [1:n] as shorthand for a taq sequence [t 1 ; t 2 : : : t n ].
In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i
Boguraev & Briscoe, 1987).
 ).
 .
Boguraev & Briscoe, 1987).
1
We use a two-layer neural network to compute the global context score, scoreg, similar to the above: a1 (g) = f(W (g)1 [c;xm] + b (g) 1 ) (5) scoreg = W (g) 2 a (g) 1 + b (g) 2 (6) where [c;xm] is the concatenation of the weighted average document vector and the vector of the last word in s, a1(g) ? Rh (g)?1 is the activation of the hidden layer with h(g) hidden nodes, W (g)1 ? Rh (g)?(2n) and W (g)2 ? R 1?h(g) are respectively the first and second layer weights of the neural network, and b(g)1 , b (g) 2 are the biases of each layer.
i=1 k?
j=1 p(c, w, i)p(c?, w?, j)d(?i(w), ?j(w ?)) (8) where p .
49.8 C&W Wiki.
Three actions, &quot;shift 3&quot;, &quot;shift 4&quot;, and &quot;shift 5&quot;, are to be executed.
2.
4.
For instance, subsequent states could be either: (x1,p(2,(3,4))) or (p(1,2),p(3,4)).
2, we simply set: p = p ||p||.
For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).
A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs, es): s = 1,. .
.
Hence, we also include a dependence on the word positions in the lexicon model p(e  |f, i, j): Here, [(i', j) ∈ A] is 1 if (i', j) ∈ A and 0 otherwise.
The sequence of K = 6 alignment templates in Figure 5 corresponds to the following sum of seven jump distances: 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6.
1994).
Figure 1).
.
Let S = (C, T, cs, Ct) be a transition system.
Let S = (C, T, cs, Ct) be a transition system for dependency parsing.
An oracle for a transition system S = (C, T, cs, Ct) is a function o : C - 4T.
Basis: If ∆(p) = 0, then i and j are adjacent and Π(p, i, j) holds vacuously.
Inductive step: Assume that Π(p, i, j) holds if ∆(p) < q (for some q > 0) and that ∆(p) = q + 1.
There are three cases: Case 1: If tp = RIGHT-ARCsl (for some l), then there is a node k such that j < k, (j,l, k) E Acp, and cp−1 = (σ|i|j, k|β, Acp− I(j,l,k)}).
Because ∆(p − r) = ∆(p) − r < q, we can use the inductive hypothesis to infer Π(p − r, i, j) and hence Π(p, i, j).
Case 2: If tp = LEFT-ARCl (for some l), then there is a node k such that i < k < j, (j, l, k) E Acp, and cp−1 = (σ|i|k,j|β, Acp− I(j,l,k)}).
Because ∆(p − 1) < q, we can use the inductive hypothesis to infer Π(p − 1, k, j) and, from this, Π(p, k, j).
Moreover, because there has to be an earlier configuration cp−r (r < ∆(p)) such that cp−r = (σ|i,k|β,Acp−r) and ∆(p − r) < q, we can use the inductive hypothesis again to infer Π(p − r, i, k) and Π(p, i, k).
Π(p, i, k), Π(p, k, j) and (j,l, k) E Acp together entail Π(p, i, j).
This means that there is a node k such that i < k < j, (i, l, k) E Acp, and cp−2 = (σ|i,k|j|β,Acp − I(i,l,k)}).
Because ∆(p − 2) < q, we can again use the inductive hypothesis to infer Π(p − 2, i, k) and Π(p, i, k).
As before, Π(p, i, k), Π(p, k,j) and (i, l, k) E Acp together entail Π(p, i, j).
Basis: If JxJ = 1, then the only projective dependency forest for x is G = ({0},0) and Gcm = Gx for C0,m = (cs(x)).
Inductive step: Assume that the claim holds if JxJ < p (for some p > 1) and assume that JxJ = p + 1 and Gx = (Vx,Ax) (Vx = {0, 1,...,p}).
Basis: If ∆(p) = 0, then i and j are adjacent, which entails Π(p, i, j).
Inductive step: We assume that Π(p, i, j) holds if ∆(p) < q (for some q > 0) and that ∆(p) = q + 1, and we concentrate on the transition tp that results in configuration cp.
Case 1: If tp = LEFT-ARCl (for some l), then there is a node k such that i < k < j, (j, l, k) E Acp, and cp−1 = (6|i|k,j|P,Acp− I(j,l,k)}).
Because ∆(p − 1) < q, we can use the inductive hypothesis to infer Π(p − 1, k, j) and, from this, Π(p, k, j).
Moreover, because there has to be an earlier configuration cp−r (r < ∆(p)) such that cp−r = (6|i,k|P,Acp−r) and ∆(p − r) < q, we can use the inductive hypothesis again to infer Π(p − r, i, k) and Π(p, i, k).
Π(p, i, k), Π(p, k, j) and (j,l, k) E Acp together entail Π(p, i, j).
Case 2: If the transition tp = REDUCE, then there is a node k such that i < k < j, (i, l, k) E Acp, and cp−1 = (6|i|k, j|P, Acp ).
Because ∆(p − 1) < q, we can again use the inductive hypothesis to infer Π(p − 1, k, j) and Π(p, k, j).
Moreover, there must be an earlier configuration cp−r (r < O(p)) such that cp−r = (σJi,kJβ,Acp−r) and O(p − r) < q, which entails II(p − r, i, k) and II(p, i, k).
As before, II(p, i, k), II(p, k,j) and (i, l, k) E Acp together entail II(p, i, j).
Basis: If JxJ = 1, then the only projective dependency forest for x is G = ({0},0) and Gcm = Gx for C0,m = (cs(x)).
Inductive step: Assume that the claim holds if JxJ < p (for some p > 1) and assume that JxJ = p + 1 and Gx = (Vx, Ax) (Vx = {0, 1,...,p}).
■
[2, 3, 4] = [0, 1, 2, 3, 4].
Basis: If JxJ = 1, then the only dependency forest for x is G = ({0}, 0) and Gcm = Gx for C0,m = (cs(x)).
Inductive step: Assume that the claim holds if JxJ < p (for some p > 1) and assume that JxJ = p + 1 and Gx = (Vx,Ax) (Vx = {0, 1,...,p}).
As in Proof 1, we may now assume that there exists a transition sequence C0,q for the sentence x' = (w0,w1,wp−1) and subgraph Gx, = (Vx − {p},A−p), but the terminal configuration now has the form cq = (λcq, [],[],A−p), where λcq = [0, 1,... , p − 1].
Remark 1).
Computational Linguistics Volume 34, Number 4 graph non-projective only if there is a node k such that i < k < j or j < k < i and neither i →* k nor j →* k. Let C0,m be a configuration sequence for x = (w0, w1, ... , wn) and let Π(p, i, j) (for 0 < p < m, 0 ≤ i < j ≤ n) be the claim that, for every k such that i < k < j, i →* k or j →* k in Gcp.
(If cp = (λ1 |i,λ2,j|β,Acp) and Π(p,i,j), then Π(p', i, j) for Z p' such that p < p', because in cp every node k such that i < k < j must already have a head.)
Basis: If ∆(p) = 0, then i and j are adjacent and Π(p, i, j) holds vacuously.
Inductive step: Assume that Π(p, i, j) holds if ∆(p) ≤ q (for some q > 0) and that ∆(p) = q + 1.
Case 1: If tp = LEFT-ARCpl (for some l), then there is a node k such that i < k < j, (j,l, k) ∈ Acp, cp−1 = (λ1|i|k, λ2, j|β, Acp− {(j,l,k)}), and cp = (λ1|i, [ ],j|β,Acp).
Because ∆(p − 1) ≤ q, we can use the inductive hypothesis to infer Π(p − 1, k, j) and, from this, Π(p, k,j).
Moreover, because there has to be an earlier configuration cp−r (r < ∆(p)) such that cp−r = (λ1|i, λ21, k|β, Acp−r) and ∆(p − r) ≤ q, we can use the inductive hypothesis again to infer Π(p − r, i, k) and Π(p, i, k).
Π(p, i, k), Π(p, k,j), and (j, l, k) ∈ Acp together entail Π(p, i, j).
Case 2: If the transition tp = NO-ARCp, then there is a node k such that i < k < j, (i, l, k) ∈ Acp, cp−1 = (λ1|i|k, λ2, j|β, Acp ), and cp = (λ1|i, k|λ2, j|β, Acp ).
Because ∆(p − 1) ≤ q, we can again use the inductive hypothesis to infer Π(p − 1, k, j) and Π(p, k, j).
Moreover, there must be an earlier configuration cp−r (r < ∆(p)) such that cp−r = (λ1|i,λ21,k|β,Acp−r) and ∆(p − r) ≤ q, which entails Π(p − r, i, k) and Π(p, i, k).
As before, Π(p, i, k), Π(p, k,j), and (i, l, k) ∈ Acp together entail Π(p, i, j).
,p − 1].
■
2007).
2007).
).
Consider the following example: ( , ) = (powell, pauel).
For example, in the dependency structure of figure 1(b), the dependencies are {(ROOT, fell), (fell, payrolls), (fell, in), (in, September), (payrolls, Factory)}.
For example, consider figure 4(a).
V M = 2·h·c h+c .
H(S).
Lebanese violate warplanes Israeli airspace A l T A } r A t A l H r b y P y l y P A l A s r A } t n t h k A l m j A l A l j w y A l l b n A n y Abstract In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.
3.
1).
1.
Commercial).
Here, Kˆ(•,•) is the normalizedK(•,•), Kp(•,•) is the polynomial expansion of K(•,•) with degree d=2, i.e.
Kp(•,•) = (K(•,•)+1)2 , and α is the coefficient.
1) with “NP”.
(2) and the rule (2) in calculating ∆(n1,n2) (see subsection 3.1).
226-227).
212-213).
Sentences (4), (5) and (6) also have twin interpretations.
A: &quot;I bought an interesting book.&quot; B: &quot;Who is the author?&quot; (ibid.).
[[shepherdl, usel, ti 1.
[itl, shepherdl, sheepin).
[n ode2 [[agent, [[agent, [preference. animalljj, [preference, animall 1], [object, (object, [preference, foodijljjj).
[preference, drinkl1M1).
What is needed is extra processing that interprets the semantic relation(s) in a later dependency with respect to the semantic relation(s) established in an earlier [1, Manima11, drink1, drink1j, [cart, use2, gasolinelth], [distInctive_larget, [10, [[bounds1, distinctl], [extentl, three_dimensionall], [behaviourl, solidi], [compositionl, metall], [animacyl, nonlivinglj, [cart, rolll, [on3, landith [driverl, drivel, cart[, [cart, hovel, [4, whee111], [carl, havel, anginal], [cart, carryl, passengerl]]]]]], [non_relevant, [[same, [3, Eboundsl, distinctl], [bounds1, distinctl]], [[extentl, three_dimensionallj, [extent1, three_dimensionall]], [[behaviourl, solidi], [behaviourl, solidl]]]], [sister, [2, [[[compositionl, fleshl], [compositionl, meta111], [[animacyl, livingl], [animacyl, nonlivingi][]], [distinctive_source, [2, [[animall, earl, foodl], [biologyl, animall]]]], [distinctive_target, [5, [(cart rolll, [on3, land1]], [driven, drivel, car1], [Carl, havel, [4, wheell]], [cart havel, enginel], [cart, carryl, passengerl]]]]]]]]]] Vector statement of match of relevant cell from animall against cells of carl Vector statement of match of relevant cell from drinkl against cells of gasolinel (noun senses) one.
2.4.2 A Single Distribution: π.
2.6.1 Krippendorff’s α.
From this table we can calculate Ao = 1 − 2E and Ae = 1 − 2(δ + E) + 2(δ + E)2, as well as π and κ.
2006).
[id(disk&quot;drive,X), bad(X), de r(c), that is, X was referred to with a full, definite noun phrase, full_np (X)] rather than a pronoun or indefinite noun phrase.
(x, y) (y, x).
We also set i (x , y) to 0 if f (x y) = 0.
(wi, w'2) and (wi, wz)).
The right context similarity, simR(wi,w2,w), is defined equivalently, for /(wi , w) and /(w2, w)4.
EwElexicon WL(W1, 11/21 w) WR(W15 W2) 1-0) Et, cieicon min(/(w, wi), /(w, w2)) min(/(wi, w), /(w2, w)) Ew Etexicon max(/(w, /pi), /(w, w2)) max(/(wi /(w2, w)) tic and exhaustive search produce nearly the same results.
.
.
, x,,,) and a bracket is any sub-sequence (xi,.
.
.
The depth of a word x E U under a bracket B E 13 (x E B) is the maximal number of brackets X1, ... , X,,, E 13 such that x E X1 C ... C X,,, C B.
Such a set R is a representative subset of (see figure 1(b)).
The set of labels L(W) = W × {0, 1} consists of two labels based on every word w: a class label (w, 0) (denoted by [w]) and an adjacency label (w, 1) (denoted by [w ] or [ w]).
Based on this count, I also define a normalized version of Awi : Awi (l) = Awi (l)�#(Aw i ).
A label l is a matching label between Ax i and AySign(−i) if Axi(l) > Axi (Stop) and either l = (y, 1) or AySign(−i)(l−1) > 0.
The best matching label l = (w, S) from x to y can be either a class (S = 0) or an adjacency (S = 1) label at Axi .
), Number(x, n!
), Gender(x, g!
), where x can be either a cluster or mention, e E {Person,Organization,Location,Other}, n E {Singular,Plural} and g E {Male, Female, Neuter}.
We then add the rule Appo(x, y) ==> (InClust(x, c) <---> InClust(y, c)) which ensures that x, y are in the same cluster if y is an appositive of x.
LCP of the text T = {w1,• • •, wN} is a sequence {c(Si),• • •, c(SN)} of lexical cohesiveness c(Si).
P(Si) is produced by activating each node w E Si with strength s(w)2/ s(w).
For example: c(&quot;Molly saw a cat.
Setting p2(x) = 0.5 to simulate uniform smoothing gives ploglin(x) = p1(x)α/(p1(x)α + q1(x)α).
Evelyn: I see.
.
5.
In detecting a CS whose KB is the nth bunsetsu (B„), we consider only a partial matrix (denoted An) that is the upper right part of B, (Figure 2): For specifying candidate pre- and post-conjuncts and measuring their similarity, we define a path in A, (Figure 2): path ::-= (a(pi,m),a(p2,m —1), ... ,a(p„,_„,n + 1)), wheren+1 <m< 1,a(pi,m)00, 131 = n, pi > pi+1(1 <i<m—n-1).
4.
5.
KORERA-NO 0 0 2 0 0 0 0 0 0 0 0 0 0 (these) AIMAISEI-WO 0 0 2 5 0 2 0 5 0 2 2 2 (ambiguities) KAISHOU-SURU-TAME-NI-WA, 0 0 0 8 0 2 0 5 0 0 2 (in order to solve) SONO 0 0 0 0 0 0 0 0 0 0 (the) SUBblb-NO 2 0 2 0 2 7 2 (all) KANOUSEI-WO 2 2 2 (possibility) , a>HYOUKA-SHI, 0 4 6'40 0 2 (evaluate) SAITEKI-TO 0 2 0 2 2 2 (to be optimum) KB/ OMOWA-RERU 0 2 0 0 0 (be thought) KAI-WO 0 2 2 2 (the answer) DOUSHUTSU-SURU 0 o 0 (derive) KOTO-MO 2 2 (that) HITOTSU-NO 2 (one) HOUHOU-DEARU.
Among these omitted modifiers, the ones that depend on the EB do not have to be recovered, because a remaining modifier that depends on the KB is treated as depending on the CS node, which means that the Sadao Kurohashi and Makoto Nagao Syntactic Analysis Method MOCHIRON, 0 0 0 0 0 0 0 MONDAI-NO 2 0 2 0 0 2 DAI-BUBUN-WA, 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (of course) 2 0 0 2 5a 2 0 5 2 0 2 0 2 0 2 0 (of the problem) 2 0 0 5 2 2a0 2 2 0 2 0 2 0 2 0 (a major part) ARU00800000008a000000000(acertain) GENSHOU-WO 0 0 2 5 00 22 2 0 2a2 02 0 20 2 0(phenomenon) SHIRABERU-NONI 0 0 0 0 6 0 0 0 0 0 0a2 0 2 0 2 0 2 (to check) DONNA 0 0 0 0 0 0 0 8 0 0 Oa 0 0 0 0 0 0 (what) ARGORITHM-GA 2 0 0 5 2 7 0 2 2 0 2a 0 2 0 2 0 (algorithm) HITSUYOU-KA-WO 0 0 2 2 2 0 2 2 0 2 Oa 2 0 2 0 (be necessary) SEIKAKU-NI 0 0 0 0 0 0 0 0 0 0 Oa 0 0 0 (accurately) MISADAMERU 0 0 0 0 0 0 2 0 2 0 2a 0 2 (to ascertain) a> KOTO-DE-ARU-GA, 0 0 0 0 0 2 0 2 0 2 Oa 2a (be, but) .„%toillit-Tia:1,10-... 0 5 2 0 2 0 2 0 2 0 (of a computer) An example of analyzing a long sentence into a dependency structure. remaining modifier also depends on the EB (Figure 14c).
• ARE-BA(sometimesbe),]- [SAMATAGE-NI(an obstacle).
semantic structure: ?y?
syn = argmax y F (x, y) that maximizes a scoring func- tion F .
Algorithm 1 The Online PA Algorithm input Training set T = {(x t , y t )} T t=1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for (x t , y t ) in T let y?
t = argmax y F (x t , y) + ?
(y t , y) let ?
t )?F (x t ,y t )+?
(x,y t )??(x,y?
JMLR, 2006(7):551?585.
JMLR, 2008(9):627?650.
(1) The corpus.
(4) The syntactic parser.
We then classified the samples into four categories: “A”, “B”, “C”, and “D”.
“C” means that alignment was not included in “A”,“B” or “D”.
There were 89 A’s and 11 D’s.
not require reordering, for example, ??(Sue) (?s)*l(friend)?
One exception is when the NP is a pronoun (PN), e.g., ?(he) (?s) ? i(name),?
(Winter Olympics) DEC  (DEC) NPB NR {I (French) NPB NN ?L?
0.
7
Capitalization also appears (as a ‘prefix’), such as C< c in English, D<d in German, and V<v in Dutch.
Then, Cost(X—Y)=Cost(B1SE1—B2SE2)=Cost(C 1 —C 2).
Hence, we can assign Consider Table 2 which is a sample of PPMVs Prorth(X<Y) = 1-Cost(X<Y).
(2) signatures.
6).
5) using a log-linear model (Eq.
3.
A total of 110 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German, and Spanish news sites:2 Czech: aktualne.cz (4), Novinky.cz (7), iHNed.cz (4), iDNES.cz (4) French: Canoe (5), Le Devoir (5), Le Monde (5), Les Echos (5), Liberation (5) Spanish: ABC.es (6), Cinco Dias (6), El Periodico (6), Milenio (6), Noroeste (7) English: Economist (4), Los Angeles Times (6), New York Times (4), Washington Post (4) German: FAZ (3), Frankfurter Rundschau (2), Financial Times Deutschland (3), Der Spiegel (5), S¨uddeutsche Zeitung (3) The translations were created by the professional translation agency CEET.3 All of the translations were done directly, and not via an intermediate language.
Working this back into P(E)’s definition, we have P(A > B) = P(A < B) = 5, and therefore P(E) = 0.36 rather than 0.333. and 50 below for a detailed breakdown by language pair.
Words in a sentence are denoted by v (v').
Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s'): Features are thus defined over (t, a, t') triples; we discuss specific features below.
For any node n E t (n' E t'), the inside span i(n) (i(n')) comprises the input tokens of s (s') dominated by that node.
NUMCHILDREN(|c(n)|, |c(n')|) = 1 For each feature above (except monolingual features), we create label-specific versions by conjoining the label pair (`(n), `(n')).
Let c(n, `) select the children of n with la- = arg max Ea exp(w>φ(g, a, g0)) bel `. w � �a exp(w>φ(t, a, t0)) (2) (t,t') CHILDLABEL(`, `') = |c(n, `) |· |c(n', `')| Note that the corresponding “self labels” feature is not listed because it arises in the next section as a typed variant of the bias feature.
1990)), a hand-built lexical thesaurus.
Journal of Le~xieography, 3(4):235-244.
Computational Lzngmstics, 17(1):21-48.
[N Eastern Airlines N] [N 'creditors N] [v have begun exploring v] [N alternative approaches N] [N to a Chapter 11 reorganization N] [v because v] [N they N][v are unhappy v] [N with the carrier N] [N 's latest proposal N] .
?Cc k c cXYkkXZ XYP f ,,exp)( 1| ??
 .
 .
Table 4 present recalls (R), precisions (P), f scores (F) and recalls on both unknown (Roov) and known words (Riv).
(CABLE in AS) and ???
(LINE) and ?(CASE) are suffixes.
For example, ??
(3rd year) and ??
Sometimes, ?8?
This feature checks whether the current character is a punctuation symbol (such as “。”, “-”, “,”).
Four type classes are defined: numbers represent class 1, dates (“日”, “月”, “年”, the Chinese character for “day”, “month”, “year”, respectively) represent class 2, English letters represent class 3, and other characters represent class 4.
The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure (2RP /(R + P) ).
(assuming “k” was tagged as PN).
We also added additional features (d) − (f).
For example, “0V,” means “knowledge”, “3'u0” means “ignorant”, “0-8” means “well-known”, etc.
1.
The number of permutations of the sequence 1, 2, ..., n, which avoid the subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large Schr¨oder number Sn−1.
2.
3.
Therefore, we introduce a threshold pruning parameter q, with 0 < q < 1.
6.
The optimal segmentation maximizes the joint probability, p(X, z|θ0) = p(X|z, θ0)p(z).
In the M-step, we maximize p(θ0|X, z) ∝ p(X|θ0, z)p(θ0).
Formally, qling(z'  |z') a (1 + num-cue(z'))q(z'  |z).
2006).
, driving, car's, .
.
.
Chain 2.1, {afflicted, darkness, .
.
.
Chain 3, {married, wife, .
.
.
Chain 5, {virgin, pine, .
.
.
Chain 6, {hand-in-hand, matching, .
.
.
.
.
}.
 ).
= OTHER, ?George?
= PERSON-BEGIN, ?Her bert?
= PERSON-MIDDLE, ?Bush?
= PERSON-END, ?said?
= OTHER, ?Clinton?
= PERSON-SINGLE, ?is? = OTHER.
|l}~ {G to map + u ) to a probability-like value.
Polynomial kernels of degree 1, 2, and 3 resulted in 83.03%, 88.31%, F-measure (%) ? ?
RG+DT ? ?
ME ? ?
SVM 0 20 40 60 80 100 120 CRL data ???E? ?^??:???
?SVM?
Suppose ?* G[Z\#^]_ G[Z `a] hasonly ?
 Z\#^]_ ?  Z `] is given by ? fi ? 1) G[Z??
 Z??
] . Hence, I#!??D?7  ? *?#!W? fi 0 ? 1) G?Z??
 Z???]!? fi 0 ? 1) G?Z??
 Z???] ?  We can rewrite +-) as follows.
fi 0 ? 1) _?  Z??
]?G[Z???]?!m? ? Z???]?G[Z???]
fi.?
 0 ? 1) fi 0 ? 1 ??
 ???rZ??? B@]?G[Z??
]?G?Z?B@]_ where ? ?
/ ?1) 3 ? ??Z??
/ ?1) 3 5? Z??
]_ ? ?
Z??
]?* ? / ?1) 3 ??p8Z??
]??% ?P?rZ?? B@]?* ? ?
/ ?1) 3  ?  Z??
0 ??,?9?
?l? 1) _?C?  Z???]
0 ?-?,????%?
?9?
1) ? ?
Z?? B@]  where ? ?
 Z???]?* ?  Z???]
!m? ? Z???]Y* ? 0  ???5?
?l? 1) 3   ???9Z??? B@]?* ? 0  ?,???_? ?l? 1 ?????
1) 3   Now, +?) can be given by summing up ? ?
 Z???]
for every non-zero element G?Z??
] and ? ?
Z?? B@] for every non-zero pair G?Z??
]?G[Z?B@] . Accordingly, we only need to add #W!???!??j?R?# z%?
 and ???
Its initialization phase took 2.13 seconds and all vectors in the training data were classified in 11,488.13 ( *=#9#%X?%Q??9????x?#p? ) sec onds.
XQK makes the classifiers faster, but mem ory requirement increases from ? ? / ?1) ?  to ? ? / ?1) ?  ?
 and ???
} 8???
Z??
]?f???
} ? ?????rZ??? B@]?f ??
} ? ???P?rZ?B- ?]??
K???
XQK can be easily extended to a more generalquadratic kernel BG? ?*??vl??!?v  G ?
In the training time, calculation of B???Dr   B??$Dr ?  B??D@  for various  ? s is dominant.
Then, for each non-zero G[Z??
] , the counters are incremented for all 7   fi2si Z???]
] for non-zero G[Z??
]?*?Q ?8Z???]??*YQ orG[Z???]W?*?Q ? ?Z???]?*yQ . Therefore, TinySVM?s clas sifier is faster than other classifiers.
Then, B??D   B???D  can be efficiently calculated because ??
(boxes marked with A in Figure 2) B.
(C„,hi(n v) is defined analogously.)
For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y E Y(x) decomposes into a set of “parts” r E y.
One technical difficulty is that D(p II p') is not defined when p' (x) = 0 but p(x) > 0.
 .
Senses are classified as S(ubjective), O(bjective), or B(oth).
Thus, applying the term <q x r(x) > to a wff containing that complex term, say, p( <q x r(x) > ), yields the quantified wff Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 49 Jerry R. Hobbs and Stuart M. Shieber An Algorithm for Generating Quantifier Scopings q(x, r(x), p(x)).
For instance, if p were a predicate opaque in its only argument, then, for the wff p(s( <q x r(x) > )), pullopaque-args would generate the wff p(q(x, r(x), s(x))) or the unchanged wff p(s( < q x r(x) > )).
Thus, from restriction (10), apply will construct the quantified wff (13) some(r, every(d, and(dept(d), in(d, <most c and(rep(r), of(r, d))), see(r, <a-few s samp(s)>)) In apply-terms, the tail recursion turns the remaining complex terms into quantifiers with wide scope.
% No more arguments. pull_opaque_args(_Pred,_ArgIndex,H,H) !.
;;;.
Lemma 2 If a is a wff and 0 is a subexpression of a and p(a) > 0, then p(P) < p(a) Lemma 3 If a is a wff and /3 is a subexpression of a and p(a) = 0, then p(i3) = 0.
Condition 4: Similarly, and p(a) < p(a) trivially.
Lemma 4 For all expressions a such that p(a) = 0, pull(a,x) and pull-opaque-args(a) terminate with result a.
Condition 1: We must show that pull-opaque-args(a) terminates with result (3 such that p(13) < p(a).
Now /3, is either a, or pull(a,false).
In the first case, p(1t) < p(a,) trivially.
Also by the induction hypothesis, p(f11) < p(a,).
Thus, we see that in either case, p(A) < p(a,).
So Condition 2: We must show that for all terms t in a, apply(t,a) terminates with result p such that p(I3) < p(a).
Then = aPP1.YRq v r>, a) = q(v, y, 8) where y = pull(r, false) and 8 = subst(v, <q v r>, a).
Now, let p(r) = m. By Lemma 2, m <n.
So Finally, by definition of p we have p(fl) < 1 + p(y) + p(8)<1+m+n— 2 —m=n-1<n.
Let t = applicable-term(a) and y = apply(t,a).
By the second condition just proved above, the latter computation terminates with p(y) < p(a) — 1 < n. Now let E = apply-terms(y, true).
= true, we return c as g, so p(P) = 0 as required.
Condition 4: We must show that apply-terms(a false) terminates with result 13 such that p([3) < p(a).
Let t = applicable-term(a) and y = apply(t,a).
By the second condition just proved above, this computation terminates with p(y) < p(a) — 1 < n. Now let E = apply-terms(y false).
= false, we return E or a as p. In either case, p(I3) < p(a) as required.
Let y = pull-opaqueargs(a).
Now, let E = apply-terms(y,true).
= true, we return E as 0, so p(P) = 0 as required.
Condition 6: We must show that pull(a false) terminates with result /3 such that p([3) < p(a).
Let y = pull-opaqueargs(a).
Theorem 2 For all expressions such that U(a) = u = , um' and V(a) = v = , v,j, and for b E {true, falsel and for 0 any of gen(a), pull(a, b), pull-opaque-args(a), apply-terms(a, b), and apply(applicable-term(a), a), u(g) = u and V(P) = v. Proof: Again, the proof is by induction on p(a), but we will be less formal in demonstrating the well-foundedness of the induction.
The previous proof of termination provides the well-foundedness of this proof. apply(applicable-term(a),a): We must show that if t= <qxr> is an applicable term in a and U(a)=u and V(a) = v then U(apply(t,a)) = u and V(apply(t,a)) = v as well.
Note that u = u, U u,.
So U(r) = {x} U u„.
(If x does not occur in r, a similar argument shows that U(r) = u,.)
Let r' = pull(r, false) and s = subst(x,t,a).
By the induction hypothesis, U(r') = fx1 U ur.
Since s does not include t (which binds x) but does include x, U(s) = 1x1 U u.
Again, v = v, U v,.
Trivially, V(r) = v„.
By induction, V(I) = v, also.
Since s does not include t, V(s) = v,.
V(I3) = V(r) U V(s) = v unless the quantification of x in 13 is vacuous.
If p is a quantifier, then let a = p(x,r,$).
The output 13 then is wff(p, pull-opaque-args(x), pullopaque-args(r), pull-opaque-args(s)).
Similarly, U(s) = {x} U u, or U(s) = u,.
By the induction hypotheses U(I) = U(r) and U(s') = U(s).
If the quantification of x in a is not vacuous, then x occurs free in either r or s (and by induction in or s') so u(p) = {x} U u, U u, — {xl = u.
Then V V(a) = {x} U v, U v. By the induction hypothesis, V(I) = V(r) = v, and V(s') = V(s) = v,.
Therefore, the quantification of x in p is also vacuous and V(13) = {x} U v,U v, = V(a).
&quot;not-found words&quot;).
: ( aircraft carrier 7 -t• 7 ).
Our OCR observes: -q :/ j.
: :/ • 7°12— (jyon.buroo), • y -q 7l (arhonsu.damatto), and -q • Y.
7 (maiku.dewain).
‘Edwin Hubble’, ‘Missouri’, ‘born’), linked by directional dependencies (e.g.
‘pred’, ‘lex-mod’), as in Figure 1.
8.
We computed Recall (R) as correct I (correct + missing), and Precision (P) as (correct duplicate) I (correct + duplicate + mislabeled + spurious).
]~{ECOGNI:ZING ]:F:XT GENII.ES Wl r l l  S:lb,/l:ll,I,; ~/~I,;II/I(~S USING DISCII .
I)is(:rimina.ut analysis makes it possibh~ tl,qC it, la,rge l l l l l l lber of  l).~Xla-Ill(:l,(:rs Lh;tL llHl,y 1)(!
I y I)eS, ;rod of v;trying quality.
(:(+rl.aitl topics ula,y only occur iu (:(;rt;ailt g(!tll(!s, alt(] {.exl.s ill eertaiu ge.nres may only t.rea.t c(ql.ain topics; mosl.
()m: a im is 1,o t;~ke ~ set of texts that.
t;yl)e , aud Experiment I I",xperiment 2 l~xperiment 3 .
( l]~?w~Lc at e g ?zies) .. [.
Informa.tive 1.
Press: report;tge B.
Press: editoriaJ (L Press: reviews 4.
Mis(: - l )~  Ileligion- " I,,.
I)olml~u: Lore C. Belles ],cttr(s, cl.c.
[,estr n(.d II.
[magin;ttivu 3.
~ Wes{elll P. tloma.nce }i: ii i ,h.; i .
374 16 (4 %) I I .
40 + I I I 20 + I I I X .
X I 11 I 111 + 1111 I I i i i i  I 11111111 I 211111111 + 11111111111 2 ] 11111111111112212 2 2 22 [ 11111111111111111112222222222221 x .
x -2 .0  0 .0  2 .0 Cent ro ids  : * * F igure  1: D is t r ibut ion ,  2 Categor ies Category J ~  Errors 2.
Non-Iiction 28 (25 %) 3.
12(; I ~ (~ %) 4.
/ 176 I 68 (47 %) focal L%~ ?
+ I 223 [ I 23 I [ 233 I + + 22433 + + [ 244433 [ I 224 44333 I 1 244 44433 * ] I * 224 44333 I I 244 44433 I + 0.0  + 224 + 4433+ + ] 2244 * 44333 [ [ 2444 44433 [ I 22211444444444444444433 ] I 221111111111111111111443333 I I 2211 * 111111333[ + 22211 + + 1113+ I 22111 i i I I 2211  I I 22211 ] + .
The project uses categories uch as " l tuery"  ~ lCCOI I l l l | e l l t ) l  1 l l kL l l I l ( ) | l | lC (~ l l le l l t l l  1 "FAQ", a l l ( l  so [orth, categorizing theui I,sing paranieters such ;is dif- ti~rent ypes of length tneanurcs, form word content, quote level, ]lereentage quoted text and other USEN I;T News Sl)ecific parameters.
Press: reportage B.
+ [ -4  -2  L J J  0 2 JHH [ [ LL J  J JH  [ [ LLP J J  JH  [ + + + LLLPKF J J+  + +JHH + [ LLLPKKKFF J J J  J Jn  [ ] LLLPKKKKFFFFF J J J  * * JHH [ [ * LLLPKKK KF FFF J J  J JH  [ [ L**LNPRK KF FF J J J  JH  [ [ LLLLNNNKKK*  KKF *FF J J J  J I I t t  I + LLLLNNNNKKK*  + KFF  + *FFF J J+  + J JH  + [ LLLLNNNNNRKK KF  * FFF J J J J  JHH [ {LLLLNNNNNNNKK KKF  *FFFGGGGGJ J J  J J t l  [ ] LNNNN NNNKKK KK*RFFFFFFFFFFGGGG GGGJ J J J  JH ] INN NNKKK KKRRRBBBBBBB*BBBBBGGGGGGGGGJJJJ JHH [ [ NNNKK KKKRR RB * BBBBBBGGGGGGGJJ J J  JJll I + NNNKKK + KKKRRR RRB + + BBAAAAAAAAAJJ J JHH + ]NNKKK KKKRRR RBB * BBA AAAAJJHH] {NKK KKRRR RB BBAA AAAAHI [KK KKKRR RRB BAA AA{ [ KKKRRR RBB BBA [ { KKKRRR RRB BBHBBBAAAAAA [ + KKRRR +RBBBBBBBBBBHBBBCCCCCCCCCCAAAAAAAAAAAAAA+ { KKKRR RRBBBCCCCCCCCCCCC CCCCCCCCCCCCCCC [ [ KKKRRR RRCCCCC [ IKKRRR RRCC I + .
(hns(m, an(1 l.N.n(~h)l)e S ibun  1992.
~Scatl.e,/(~ather: A (Jh,sl.cr-lmst~d Al)l)roa(:h to Browsing [,arge ])ocument (2olhx:l.ions" Irocs.
I n tF i l te r  199:1.
ThcMcasurcmcntoft~adabi l i tg, [owa.
Sept>o Mus l ,onmi  1965.
The ,5/,5.b Ib:[ercncc (;~+id+:, (.qdca+go: ,qP,q5 I I IC+ 7075
Consider sentence (4).
&quot;A, B, and C do D, E, and F, respectively&quot;) of various natural languages  .
Let us consider a constraint dependency grammar G =< E, R, L,C > with arity=2 and degree=k.
1990).
A string-to-dependency grammar G is a 4-tuple G =< R, X, Tf, Te >, where R is a set of transfer rules.
A string-to-dependency transfer rule R E R is a 4-tuple R =< 5f, 5e, D, A >, where 5f E (Tf U {X})+ is a source string, 5e E (Te U {X})+ is a target string, D represents the dependency structure for 5e, and A is the alignment between 5f and 5e.
We say the category of di..j is (C, −, −) if j < h, or (−, −, C) otherwise.
(1) holds.
',� phrase alignments, where source phrase P ?
',� �is e under alignment3 A, and D, the dependency structure for P m,n e , is well-formed.
Let (Pi,j f , Pm,n e , D1, A) be a valid rule template, and (Pp,q f , Ps,t e , D2, A) a valid phrase alignment, where [p, q] C [i, j], [s, t] C [m, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned.
8.
Then algebraic simplification using these approximations and previous definitions gives GΛ(g, µ) = where Zo(i)(A, g, µ) (with non-bold o) is simply s PΛ(s|o(i))exp(µg(s,o(i),t(i))).
As a shorthand, y(M(t)i) = y(Mi),tMi.
That is, we replace H−1(θ(k)) in (5) with a local approximation of the inverse Hessian B(k): with B(k) a symmatric, positive definite matrix which satisfies the equation: where y(k) = G(θ(k)) − G(θ(k−1)).
Thus we have the alignments (beaucoup d’, e) and (e, quite often).
The function ` : (V i U E) → L labels each internal node or edge; q E Q is the root state, and s : V f → Q assigns a frontier state to each frontier node (perhaps including r).
This operation replaces T with (T.V U t.V − {d}, T.V i U t.V i, T.E' U t.E, T. B U t.�, T.q, T.s U t.s − {d, t.q}).
An elementary tree pair t is a tuple (t1, t2, q, m, s).
Let m¯ denote m U {(d1, null) : d1 is unmatched in m} U {(null, d2) : d2 is unmatched in m}.
parser  , model 2.
epJ.stemi~nss, c s. uc l .
Suppose U has N elements, where N > I.
72 i ndez  = z ~tat t~ = .
Grosz, Barbara J.
2.
3.
8.
The vector does not arg maxy,z p(y, z|xi; 0).
 .
A simplified example of a syntactic rule is np: [wh=ynq, pers_num=N] , s: [sentence_type=ynq, form=tnsd, gapsin=np: [pers_num=N] , gapsout=null]] ) .
Janssen [1990].)
We define a generative model over (1) a source lexicon, (2) a target lexicon, and (3) a matching between them (section 2).
.
.
+ tn).
Thus, H(ci ) is -0.83.
For Agent A, P(A) = .78, P(E) = .265, and frc = .70.
Another class, (n = 7), are possessives.
6, [Car87]).
.
• , TN) * Pr(Wi, W2/ • / WN T1, T2/ • .
7.
1.
2.
(2) He buys a notebook.
(3) I read a boo]~ on international polilics.
correspondence links For example, in Prolog, ewd e(  [e l ,  [buy ,v  ] , [e2,[he,pron]], [e3, [notebook,n], [e4,   [a ,det  ]  ]  ]  ] )  .
%% He buys a notebook.
jwd_e(   [ j I ,  [kau,v] , [ j2 ,   [ha ,p  ]  , [ j3,[kare,pron]]], [ j4 ,  [wo,p] , [ j5 ,  [nouto,n]]]]).
c l inks([  [el , j l  ] ,  [e2, j3] ,  [e3, j5] ]) .
el,  e2, e3, j l ,  j3, j5.
e l -e2,  e l -e3,  e l -e2-e3,  j l - j3 , j l - j5 ,  j l - j a - jS .
Matching expression(ME) is de- fined as the following: <HE> : :=  [<ID>I<ME-Commands>] <ME-Commands> : : = [] or [<ME-Command> I <ME-Commands>] <ME-Command> : := [d, < ID>] or [r,<ID>, <ME>] or [a,<ID>,<ME>] %% delete <ID> %% rep lace  <ID> %% with <ME> %% add <ME> as a %% ch i ld  of  root %% node o f  <ID> Every ID in an ME should be translatable.
ewd_e( fell, freud,v] , [el2, [I ),prOn]] , [el3, [book,n] , [el4, [a,det] ] , [elb, Ion,p] , [el6, [politics,n] , felT, [international, adj] ]  ]  ]  ]1 ) .
Y,Y, I read a book on international %% politics.
jwd_e([j l l ,  [yomu,v] , [j12, [ha,p] , [j13, [watashi,pron] ]] , [ j14 ,  [wo,p] , [j15, [hon,n] , [ j16 ,   [ ta ,  aux] , [j17, [reru,aux] , [j18, [kaku,v] , [j19, [nitsuite,p] , [j20, [kokusaiseij i,n] 1]  ]11]  ]  ] ) .
%% Watash i  ha kokusa ise i j i  n i t su i .
cl inks([el l ,  ] l l  ] ,  [e12, j13] ,  [e13, j15] , [e16 , j20  ]  ] ) .
(a)  [ [buy,v1 , [  [he ,pron  ]  ]  , [[book,hi , [  [a ,det  ]  ]  , [ Ion,p], [[politics,n] , [ [ international,adj]]] ] ] ] %% He buys a book on international Y,Y, polit ics.
For example, SWD = [[buy,v], [ [he,pron] ] , [ [book,n] , [  [a,det]  ]  , [[on,p], [ [politics ,n], [ [international, adj] ] ] ] ] ] SME =  [e l ,  [ r ,ea ,  [e l3  ]  ]  ] The main tasks in this step are to retrieve translation units and compare the source WD with retrieved translation units.
TME =  [ j l ,  [ r , j5 ,   [ j lS  ]  ]  ] TWD = [[kau,v], [ [ha,p] , [ [kare,pron] ] ] , [[wo,p], [ [hon ,n] , [[ta, aux], [ [tern, aux], [ [kaku, v] , [ [nitsuite,p] , [ [kokusaiseiji,n] ] ] ] ] ] ] ] ] ~,~.
For example, in the above target word- dependency tree, [v, [p,p] ] , [p,  [prom ], [p, [n] ], In, [aux] ] .
3 249 z" / 7t2 L 1 " / ~ l I I , 7/5 71,7 .2 .
250 4 s im([book,n] ,   [notebook,n] ,O.8).
sire( [buy,v] , [read,v] ,0.5) .
sire( [hon,n] , [nouto,n] ,0.8).
s im(  [kau,v  ] ,  [yomu,v  ] ,O .5) .
SME,  TME,  TWD)  = ~,n i~( seo,~( S ME.
(5) Acid eats metal.
***  T r&ns la t ion  Source  *** [  [eat ,  v]  , [ the,pron] ] , [ [potato,n] ] ] Y,Y, He eats potatoes.
I (Score = 0.5889) [ [taberu, v] , [ [ha,p], [ [kare,pron] ] ] , [[wo,p], [ [ jagaimo,n] ] ] ] No.
2 (Score = 0.4S56) [ [okasu, v], [ [ha, p], [ [kare,pron] ] ] , [ [~o,p], [ [ jagaimo,n]] ]] *** Trans lat ion  Source *** [[eat,v] , [[acid,n] , [  [sulfur ic,adj] ]  ] , [[ iron,n]]] %% Sulfur ic  acid eats iron.
I (Score = 0.5500) [ [okasu, v], [ [ha, p] , [ [ryuusan,n] ] ] , [ [wo,p], [  [ te tsu ,n  ]  ] ]] No.
[Jeliuek 1990]).
NANTES, 23-28 AOLr 1992 8 5 6 I)ROC.
(71: COLING-92, NANTES.
All input string is ml nquple of elements of L: (l,t,._,Ln) ~ I, n. A Collckttellatio[l ~ Gill big defined OI( sllil(gS US usual: (;l,...,b),~(c,...,ll) = (a,...,b,c,...,d).
~/i-ee-9~ mid a tuuction le;tves: ~l?ee~L n by for n_>O, root((L,(tl,...,tn))) = I, rot n>O, le,~ves((L,(tt,...,l~t))) ~ l?,lves(tl)*... ~le~lves(tn) torn--O, leaves((L,O)) = (L) Corpus A corpus C is a multiset of trees, ill file ~nse  that ally tree can  occur zero, nile or more times.
Construction8 Ill order to define the Constowtions of a tree, we need two additional notions: Subffees and l~tttems, Snbtrees((L,(tl,...,t~))) = n [(L,(tl,..,t~))} u (~ Snhtrees?ll)) i=~ Pattems((L,(t 1,..., In))) = {(L,O) 1 ty {(l,(ul,...,no.)
) / Vi~11,,l: nid~attenls(ti)l Constructions(T) = {t / 3beSubtrecs(1): teP,tttenls(u)} We Slulll use tile lbllowing notation for a constnlction of a tree in a corpus: tee =tier ~nc()" tc(.imstmctionsO0.
The set of par~s of s will( respect to C, P,use(s,C), is given by I,use(S,C) = (1 eTive / lcaves(T)=s A 3tl,...,t .
23-28 AOt~:f 1992 8 5 7 l)mlc.
OF COI,ING-92, NANTES, AU?}.
I992 and t/mh Occur (hall,k]).
[Harris 1966]): k P(L/ Ei) = X P(Ei) - X l(ldi1~Li2) + X P(hit,~Ei2~Ei3) i=l i i1<i2 i1<i2.~i3 - .... +/- P (E I~E2~ ... c~lS k) We will use Bayes decomposition formula to derive the conditional probability of "1) given s. Let 7/~ and Tj be parses of s; the conditional probability of T i given s, is illen given by: P(Ti)P(sFI" i) P(r)P(srl~) V(7)ts)  .
P(s) z~j P(Tj)P(slTj) Since P(slTj) is 1 for all j, we may write P(T) P(Tils ) .
23-28 AOt,q" 1992 8 S 8 PROC.
[Hanmlersley 1964]:  Hauunersley, J.M.
[Jelinek 1990]: Jelinek, F., l.afferty, J.D.
Thus we write a nonterminal as X(x), where x = (w,t), and X is a constituent label.
(3) Does the string contain 0, 1, 2 or > 2 commas?
(where a comma is anything tagged as &quot;,&quot; or probability P(R3(r3) I P, H, h, distance, (2)).
In (1) the probability of generating both &quot;Dreyfus&quot; and &quot;fund&quot; as subjects, P(NP-C(Dreyf us) I S , VP , was) * P(NP-C(fund) I S ,VP , was) is unreasonably high.
(2) is similar: P(NP-C (bill) , VP-C (funding) I VP , VB , was) = P(NP-C (bill) I VP , VB , was) *'P(VP-C (funding) I VP , VB , was) is a bad independence assumption.
Prc(RC I P,H,h).
We specify a parameter PG(G I P, h, H) where G is either Head, Left or Right.
Note that we decompose P L(L,(1ws, Its) I P, H, w, t, A, LC) (where /wi and /t, are the word and POS tag generated with non-terminal Ls, A is the distance measure) into the product 'PLI(Li(itz) I P,H,w,t,A,LC) x PL,2(hoi I Ls, its, P, H, w, t, A, LC), and then smooth these two probabilities separately (Jason Eisner, p.c.).
The probability for any sentence can be estimated as P(S) = ET P(T, s), or (making a Viterbi approximation for efficiency reasons) as P(S) P(Tbest, S).
The ?V:subj:N:joke?
2) Molybdenum is a metal.
Consider two strings a(1, n) and b(1, m) of lengths n and m re spectively.
Let a1(1, n) and a2(1, n) be the level 1 (lexical level) and level 2 (POS level) representa tions for the string a(1, n).
Similarly, let b1(1, m) and b2(1, m) be the level 1 and level 2 representa tions for the string b(1, m).
Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.
X, or Y X, _DT Y _(WDT|IN) Y like X and X, (a|an) Y X, _RB known as Y _NN, X and other Y X, Y X ( Y ) Y, including X, Y, or X Y such as X Y, such as X X is a Y X, _RB called Y Y, especially X 774relationships.
Table 4.
and ?What is X??
or ?Incorrect (I)?
Table 7.
P r  Sense l  Co l loca~ion i )  ratios.
Precision (Prec.)
SEAL (5).
Seal (s).
1991).
• • xi) = P(xo )/P(xo • .. xi).
.
. x,_1 can be computed as the sum P(X xk...k_i) = E -yickx —+ A.) i:kx- A.
Then =-- a 7 Rationale.
Then i: j1( //.
[a&quot;, -y&quot;] i: kX XY.ti [a', 71 j kX —> A.Yit [a,7] j a' += a += 7.7&quot; (12) Note that a&quot; is not used.
Each R(X Y) is defined as a series where we use the delta function, defined as S(X, Y) = 1 if X = Y, and 6 (X, Y) =0 if X Y.
The 1)-1 factors are a result of the left-corner sum 1 ± q q2 + • • • = (1 - q) 1.
> k2X2 —> A2X3.
-> &quot; > k,X, —> A,X,+1 • be a completion cycle, i.e., ki A1 =-- Ac, X2 = Xc+1.
: jy. v. [arr, i: kX AZ.A [ctr,,yri kX A.Zit [a, 'y] for all Y, Z such that R(Z Y) is nonzero, and Y v is not a unit production or v e E).
Thus, v x).
1 i : —> AY.? j: k for all Y.
Let Y = Y .
The logical form of sentence (1) is JOHN, ZEUS) H N) but not Exist(Z S).
Sentence (6) is encoded as (8) and (7) as (9): (8) (3x)believe(J , spy(x)) (9) believe(J,(3x)spy(x)) If one adopts this notation and stipulates what the expressions mean, then there are certainly distinct ways of representing the two sentences.
In particular, the dicto of be represented by something like believe(J, P) S) J , Q) , S,T) the next table.
I said. is P) P, S) J , S • T) us represent the reading as .1 Apy'l P. Exi.0)(,)) A ,IP(y.
13 J. common (12) (1:') t he I.
16.)
17.)
Conversely, (Ve, , x.
).Ezist(e) A pi(e, xi , r,) D p(x, , That is. if e is the condition of p's being true of xt,.... x,,, and e exists, then p is true of xl,...,x„.
18M U )A Pasi( E.1)Arevette( Kn.
Then if E is such that plE, X), we define the interpretation of E to be < 1(p), 1(X) >.
I have said. is (14hba)t is common to (12) and (1:') are t he conprict, helleIH I.
11. spyl P, 5) and uti(Q.
5.7').
In addition, ri) includes the conjuncts knout'( J, R) and ivhs(R.S) - line (13b). sentence 5?
By axiom (3), married(S) is really just an abbreviation for marrierr(E,S) A Ezist(E).
(10).
(11).
Then, we could get: For a specific atom sequence A, P(A) is a constant and P(W,A)= P(W).
Mr).
CityBank).
Let a = {(s(1), t(1)), ... , (s(n), t(n))} be an alignment where s(i) is a word in the source sentence xs (not necessarily the ith word) and t(i) is similarly a word in the target sentence xt (again, not necessarily the ith word).
The notation (s(i), t(i)) E a indicates two words are the ith aligned pair in a.
For each child symbol s': A.
Variational inference transforms this problem into an optimization problem where we try to find a distribution q(B, z) from a restricted set Q that minimizes the KL-divergence between q(B, z) and p(B, z  |x): KL(q(B, z) k p(B, z  |x)) Thus F is a lower bound on likelihood.
= arg max e M ? m=1 ?mhm(e, f) (1) where {hm(e, f)} is a set of M feature functions and{?m} a set of weights.
Let wL1 = (w1, . . .
(3) While intuitively appealing, Eq.
#ids for j = 0 ..
3).
1.
2.
Definition 1 (Mutual Information) The mutual information of two terms x and y is defined as: )()( ),(log),(),( yPxP yxPyxPyxI = where P(x,y) is the co-occurrence probability of x and y, and P(x) and P(y) denote the occurrence probability of x and y, respectively.
3.
Here, a regular event ?Mother?s day?
and ?chinaphillippine?.
and ?8 typhoons?
.
, Tn.
Table 1 shows qi, 17, the ratio qi (x)/13(x), and the weighted point divergence /3(x) ln(f 9(x) /q1(x)).
.436) = (1 /2, 1 /2, 2/3, 1 /3, 1 /2, 1 /2).
That is, the score for f is D(Igold) — D(311q0/).
.
Set A 6,,A, for all i, and solve equation (3) again.
(See Appendix 2.)
The sampler for p(.) proposes a new item y.
In brief, the acceptance probability of y is A(y I xn) = min(1, 1/r).
If r < 1 then A(y I x) = 1.
Otherwise, writing 0(x) for the &quot;field weight&quot; ni 0{,(x), we have:
The acceptance function then reduces to min(1,71-(y)/ir(x)), which is min(1,q(y)/q(x)) in our notation.
This permits us to compute F;(6) and F;(6).
     ).
SYNONYMY, GLOSS); and (b) shorter paths.
For this purpose we assign the following weights to each relation considered: W(SYNONYM) = 1.0; w(IS-A) = 0.9; w(GLoss) = 0.9; w(IN-GLoss) = 0.3; w(HAs-PART) = 0.7; w(MoRPHo-DERIVATION) = 0.6; and W(COLLIDESENSE) = 0.5.
.
.
(B) αj, b E R, αj > 0.
Suppose a feature set F = {1, 2, ... , N} and training examples Xj(j = 1, 2, ... , L), all of which are subsets of F (i.e., Xj C_ F).
Kd(x, y) = Kd(X, Y ) = (1 + |X n Y |)d, (3) where d = 1, 2,3, ....
(i.e., 0 : F —* Fd).
In this section, we introduce two fast classification algorithms for the Polynomial Kernel of degree d. Before describing them, we give the baseline classifier (PKB): where 3 X ´ cd(r) will be referred as a subset weight of the Polynomial Kernel of degree d. This function gives a prior weight to the subset s, where |s |= r. Example 1 Quadratic and Cubic Kernel Given sets X = {a, b, c, d} and Y = {a, b, d, e}, the Quadratic Kernel K2(X, Y ) and the Cubic Kernel K3(X, Y ) can be calculated in an implicit form as: K2(X,Y ) = (1 + |X n Y |)2 = (1 + 3)2 = 16, K3(X,Y ) = (1 + |X n Y |)3 = (1 + 3)3 = 64.
Using Lemma 1, the subset weights of the Quadratic Kernel and the Cubic Kernel can be calculated as c2(0) = 1, c2(1) = 3, c2(2) = 2 and c3(0)=1, c3(1)=7, c3(2)=12, c3(3)=6.
In addition, subsets Pr(X n Y ) (r = 0, 1, 2, 3) are given as follows: P0(X n Y ) = The complexity of PKB is O(|X |· |SV |), since it takes O(|X|) to calculate (1 + |Xj n X|)d and there are a total of |SV  |support examples.
Here we take the following strategy: Definition 2 w': An approximation of w An approximation of w is given by w' = (w'(s1), w'(s2), ... , w'(s|Γd(F)|)), where w'(s) is set to 0 if w(s) is trivially close to 0.
(i.e., σneg < w(s) < σpos (σneg < 0, σpos > 0), where σpos and σneg are predefined thresholds).
The weight w(s) is calculated by w(s) = ω(s) x cd(|s|)/Cd, where ω(s) is a frequency of s, given by the original PrefixSpan.
I: non-initial word.
Let s = c1c2 · · · cm be a sequence of Japanese characters, t = t1t2 · · · tm be a sequence of Japanese character types 3 associated with each character, and yi ∈ {+1, −1}, (i = (1, 2,...,m− 1)) be a boundary marker.
 ).
Lincoln).
DISTANCE).
 ).
 ).
(2003; 2008b).
2.
1).
€bdcillh&quot;).
; cirdjyk yrt ydzt&quot; (a metathesis of k and n).
et al, 2004), Basque  , Catalan, (Mart??
2.
2.
table 1).
table 1).
2.
3.
6.
2.
3.
Task 1.
Task 2.
Task 3.
1.
 .
??wand ??v are weight vectors.
Algorithm 1: Training ? baseline algorithm ? = {(xi, yi)}Ii=1 // Training data??w = 0,??v = 0 ? = E ? I // passive-aggresive update weight for i = 1 to I tss+e; extract-and-store-features(xi); tes+e; for n = 1 to E // iteration over the training epochs for i = 1 to I // iteration over the training examples k ?
(n? 1) ? I + i ? = E ? I ? k + 2 // passive-aggressive weight tsr,k; A = read-features-and-calc-arrays(i,??w ) ; ter,k tsp,k; yp = predicte-projective-parse-tree(A);tep,k tsa,k; ya = non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average dren ?h,d,g where h, d, g, and s are the indexes of the words included in xi.
A = {??w ? ??f h,d,??w ? ??f h,d,s,??w ? ??f h,d,g} The parsing algorithm uses the weight arrays to predict a projective dependency structure yp.
 .
= ?(yi, yp) // number of wrong labeled edges if ? > 0 then ??u ?
(?(xi, yi)?
?(xi, yp)) ? = ??(F (xt,yi)?F (xi,yp))||??u ||2??w ? ??w + ? ?
??u ??v ? ~v + ? ?
??u return ??w , ??v al., 2006).
(n? 1) ? I + i ? ?
E ? I ? k + 2 // passive-aggressive weight tse,k; A?
extr.-features-&-calc-arrays(i,??w ) ; tee,k tsp,k; yp?
predicte-projective-parse-tree(A);tep,k tsa,k; ya?
h1 ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|3 h2 ? |(l xor ((l >> 13) ? 0xffffffffffffe000) xor ((l >> 24) ? 0xffffffffffff0000) xor ((l >> 33) ? 0xfffffffffffc0000) xor ((l >> 40) ? 0xfffffffffff00000)) % size | vector size h1 #(h1) h2 #(h2) 411527 85.67 0.41 85.74 0.41 3292489 87.82 3.27 87.97 3.28 10503061 88.26 8.83 88.35 8.77 21006137 88.19 12.58 88.41 12.53 42012281 88.32 12.45 88.34 15.27 115911564?
l represents the label, h the head, d the dependent, s a sibling, and g a grandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance.
data-list ?{(w1, w2)} c?
A ? collect-result(Tt) return A // array-thread T d?
[b (s)] =6 C [s]).
We note that the condition and the formula s' 2 B (r) and b (s') 2� B  ).
These formulae offer us a method of generating the rules r which are influenced by the modification s' —� b (s'): (a) If p (b (s')) = false then decrease good (r), where r is the rule created with predicate p s.t. target T [s'];
Case II: C [s'] =� C [b (s')] (b does change the classification of sample s').
It is easy to notice that C [s'] =� C [b (s')] and s' E B (r) implies that b (s') E� B (r); indeed, a necessary condition for a sample s' to be in a set C [b (s)] = tr in formula (1) and removing the test altogether for case of (2).
For all samples s that satisfy C [s] = T [s] generate all predicates p s.t. p (s) = true; for each rule r s.t. pr = p and tr =� C [s] increase bad (r).
2.
(2).
For all S’ ❑ S, if it can be parsed using G, we assume P(S’|FT) = 1, and tagged S’ as a factoid candidate.
南 瓜 ‘pumpkins’) cannot.
(2) Statistical filtering.
(3) Linguistic selection.
In principle, the ON class model recovers P(S’|ON) over all possible C: P(S’|ON) = ∑CP(S’,C|ON) = ∑CP(C|ON)P(S’|C, ON).
Since P(S’|C,ON) = P(S’|C), we have P(S’|ON) = ∑CP(C|ON) P(S’|C).
2.
A features.
 .
Figure 2).
Table 2).
We parameterize the distortion c(·) using a multinomial distribution over 11 offset buckets c(<_ −5), c(−4),.
.
.
.
We first replace the asymmetric alignments a with a set of indicator variables for each potential alignment edge (i, j): z = {zij ∈ {0, 1} : 1 ≤ i ≤ I,1 ≤ j ≤ J}.
Sequence-based models p(a  |e, f) induce a distribution over p(z  |e, f) by letting p(z  |e, f) = 0 for any z that does not correspond to any a (i.e., if z contains many-to-one alignments).
.
.
Consequently, if some property Pi in L is made redundant by later additions (i.e., when ([[P1]] n · · · n [[Pi − 1]] n [[Pi + 1]] n · · · n [[Pn]]) C [[Pi]]), then Pi is retained as a member of L nevertheless.
Like D&R, D&RAtt has linear complexity.
Then a proof by induction over van Deemter Generating Referring Expressions i shows that [[Desi]] c [[Q1]] n · · · n [[Qi]], for all i < m. (Consider the basic case, where i = 1, and assume that Q1 E� [[Des1]].
It follows that [[Desm]] c [[Q1]] n · · · n [[Qm]] = {r}.
But r E [[Desm]], so [[Desm]] = {r}.
.
.
, Vi,an, So, either (i) r E� BV or (ii) there exists x =� r for which x E [[Vi,a1]] n · · · n [[Vi,an]] n BV .
But Case (i) contradicts the definition of FindBestValue (see Section 2); Case (ii), on the other hand, implies that hence, x E [[BV ]] while x E� [[Vi,j]].
But r E [[BV]] n [[Vi,j]], so [[BV]] and [[Vi,j]] are not disjoint.
Consequently, by Assumption (1), Case (ii) implies that [[Vi,a1]] n · · · n [[Vi,an]] n [[Vi,j]] is a real subset of [[Vi,a1]] n · · · n [[Vi,an]] n [[BV ]], contradicting the fact that FindBestValue prefers a more general Value (i.e., BV ) over a more specific one (i.e., Vi,j) only if it removes the same distractors.
.
.
For example, if a set of players—say, a, b, and c—are to be characterized as a collection (e.g., to say that they won as a team of three), then S = {{a, b, c}}; if they are to be characterized distributively (e.g., to say that each of them has the flu), then S = {{a, b, c}, {a, b}, {a, c}, {b, c}, {a}, {b}, {c}}.
Consider a KB whose domain is a set of animals (a, b, c, d, e) and whose only Attributes are TYPE and COLOR: TYPE: DOG ({a, b, c, d, e}), POODLE ({a, b}) COLOR: BLACK ({a, b, c}), WHITE ({d, e}) (All domain elements happen to be dogs.)
D&RB.,e� Phase 1.
Phase 2.
Phase 3.
Abbreviating B = BLACK, D = DOG, P = POODLE, and W = WHITE, we have P = (B, D, P, W, B, D, P, W).
Now, if S = {c, d, e} and S = {c} (as before) are to be characterized, nothing eventful happens.
The situation gets more interesting if S = {a, b, d, e}, which triggers Phase 2.
If we modify the example by letting [[BLACK]] = {a, c} (rather than {a, b, c}) and S = {b, c, d, e} (rather than S = {a, b, d, e}), then the description L = {BLACK U POODLE} is found.
As a result, one may generate descriptions like X n (Y U Z) (e.g., white (cats and dogs)) in a situation where Y U Z (e.g., cats and dogs) would have sufficed (because (Y U Z) C_ X).
Theorem 3: Completeness of D&RBOOk.
Also, [[cp]] C [77]].
To see this, suppose the algorithm finds 0 such that [[0]] = [[cp1]] n · · · n [[cpm]].
Then [[x]] n [[0]] = [[cp]]; but [[cp]] C [77]] C [[x]], therefore also [[77]] n [[0]] = [[cp]].
Grosz [1977], Reichman [19851).
Section 7).
 .
  3.
S = sentences.
C' = characters, T = word types).
1).
(b) feature window.
(c) kernel features.
Thus we can define two states (j, S) and (j′, S′) to be equivalent, notated (j, S) — (j′, S′), iff. j = j′ and f(j, S) = f(j′, S′).
For example, for the full model in Table 1(a) we have where d = 2, f2(x) = x.t, and f1(x) = f0(x) = (x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).
3.
3, if c < c′.
4).
3.5) using feature templates (1)-(3) in Tab.
6a).
6b).
4.1).
 .
y p?(y) log p?(y) H(T ) = ? ?
t p?(t) log p?(t) I(Y, T ) = ? y,t p?(y, t) log p?(y, t) p?(y)p?(t) H(Y |T ) = H(Y )?
I(Y, T ) 297 H(T |Y ) = H(T )?
I(Y, T ) VI (Y, T ) = H(Y |T ) +H(T |Y ) As Meila?
The precise form of the model we investi gated is: ?y | ?y ? Dir(?y) ?y | ?x ? Dir(?x) yi | yi?1 = y ? Multi(?y) xi | yi = y ? Multi(?y)Informally, ?y controls the sparsity of the state-to state transition probabilities while ?x controls thesparsity of the state-to-observation emission proba bilities.
P(x,y|?, ?)P(?|?y)P(?|?x) d?
d? ? ?
Q(y, ?, ?) log P(x,y, ?, ?) Q(y, ?, ?) dy d?
Specifically, the E-step for VB inference for HMMs is the same as in EM, while theM-step is as follows: ??(`+1)y?|y = f(E[ny?,y] + ?y)/f(E[ny] + s?y) (4) ??(`+1)x|y = f(E[nx,y] + ?x)/f(E[ny] +m?x) f(v) = exp(?(v)) ?(v) = (v > 7) ? g(v ? 12) : (?(v + 1)?
1)/v g(x) ? log(x) + 0.04167x?2 + 0.00729x?4 +0.00384x?6 ? 0.00413x?8 . . .
(5) 301 P(yi|x,y?i, ?) ?
and ??
Take, for example, the sen- rouge: ( l ) Buy a ear \[p,o with a steering wheel\].
UNANNOTATI{D \] "I'I~X'I' 1NH'\[AI, l STATE ANNOTATliD TEXT TI~.I j'\['l l , ~ e , N El( ~-~ RUI ,I-S Figure \[: Transfonm~tion-I~ased Error.-Driven l,earlfiUg.
\[n this e?periment (as in (\[II~,9\], I\]l{93)), tim at- tachment choice For l)repositional i)hrases was I)e- I,ween the oh.iecl~ mmn and l,he matrix verb.
79,00 t 77.00 !--R . . .
/ - -F . . .
%oo!1 / I 74:001 . . .
_ _ t ....
1200 1 2 4 5 (3 7 8 9 10 II 12 :13 \]4 15 \[6 17 \[8 119 2()_ Change Att{:~ehment Location l"r~m~ To ( ;oMit ion N1 V P is at N\ ] \ / P is as N1 V I ) is iulo N:I \/ P is ,l}'om N:I V P is with N\] V N2 is year N 1 V P is by I?
|;o it WOl:(\[ g i l t \ [ { l i ly O\[' its features.
1207 (~lml~.ge \] At tachment , / Location / # li'rom t 'Fo \[ Condition 1 N1 V N2 is \[time\]
4 N1 V P is into 5 N 1 V P is from 6 N1 V 1 ) is wilh 7 N1 V P is of P is in and NI is 8 N 1 V \[measure, quanlily, amou~l\] P is by all.el 9 N1 V N2 is \[abslraclion\] I 0 NI V P is lhro'ugh 1) is in and N I is 11 NI V \[group,group.in.g\].
See figure 5 h)r a sun> mary of results.
(\]iv{;\]\] a eh{}ice of features, they train ;t prol}abi/islie model For I)r(Sitclcoutext), and in {.esl.ing choose Site :-: v oP Site = n l a~ccordi\]lg I;o which has {he higher eomlitional probal)i\]ity.
I}iguity.
I{e(;a.llSe these papers deseril)e results ol)- tained on different corpora, however, it is (lifIicull; to II~,:'tl,:.{; a. 1)(;r\['(}rllla, iic{!
(D(xi)·(D(xj) = k(xi, xj) to another, usually a higher dimensional, feature space.
In the experiments, we use so-called 1-norm soft margin formulation described as: subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L, ei ≥ 0, i = 1,··· , L. (weighted) conjunction of d features in the original sample.
A family of O(n) global factors.
Crucially, however, F(A) only depends on the values in A of F’s its neighboring variables N(F).
A[V]=v a sum over (6)’s products of incoming messages.
Since Q`[s, t] = Q`[s, t−1]+�qLte, we can compute each entry in O(1) time.
The crossing sum is respectively Q`[0, i−1]+Q`[j+1, n], Q`[i+ 1, j − 1], or 0 according to whether E ∈ (i, j), E ∈� [i, j], or E = i.21 The non-crossing sum is Q`[0, n] minus the crossing sum.
Handling the subfactors in parallel, (3)–(4), would need O(n) iterations.
Replace each factor Fm(A) with Fm(A)1/T , where T > 0 is a temperature.
3).
 .
- I N T E R R U P T I O N :  It is something new though urn.
A: uh 15 thou (ASSERTION, but response) 4.
Interrupt/Abdicate 2 1.
A: yeah itll be 20% (ASSERTION, but response) 4.
B: um hm (PROMPT) - - - - -ABDICATE SHIFT BACK TO A- 5.
(ASSERTION) A: Okay (PROMPT) B: Yeah (PROMPT) -ABDICATE SHIFT TO A- - A: All right.
In Neil V. 77 [Kid85] [LA90] [LCN90] [McK85] [Nic76] [oc89] [PH90] [PHW82] [Po186] Smith eds.
[Rob86] [Sch82] [Sid79] [Sid83] [SSJ74] [Wa189] [Web86] [Web88] [ws88] Craige Roberts.
6(93–107).
For equation (8), and E[log C] is found similarly.
.
(See Section 4.)
.~ ~ R T .~.
256 4 % I S V NP~\] 1 hates NP V NP~ I politician hates mm F \ j - -T '~..
F”give”,”IN”.
<deliver, Arg0>, <deliver, Arg1 > and <deliver, ArgM>.
A(nx, nz) = A and respectively.
 .
CArg0 = 0.
Thus we write a nonterminal as X(x), where x = (w, t) and X is a constituent label.
“???
The next child, R3(r3), is generated with probability P(R3(r3) I P, H, h, distancer(2)).
The resulting model has a cleaner division of subcategorization: Pl,({NP-C}  |S, VP, verb) ,: 1 and Pl,({NP-C}  |SG, VP, verb) = 0.
For example, (NP-C, NPB, PP, R) is replaced by (NP, NPB, PP, R).
As noted previously, whereas our parameters are PL2(lwi  |Li, lti, c, p, P, H, w, t, ∆, LC), Charniak’s parameters in our notation would be PL2(lwi  |Li, P, w).
.
.
, dn) and (e1,.
.
.
, em), respectively.
1993).
J 12.
J 13.
J 14.
(a) resp.
2.(b).
For the latter, step 0 assigns Δ(cjr, cjr) = 0.
Q(BOW) + A(PT,BOW) resp.
Q(PT) + A(PT,BOW) combinations).
Q(BOW) + A(PT,BOW).
Q(BOW) + A(BOW), exceeds 3 points.
The Q(BOW) + A(PT,BOW,PASN) model is not more effective than Q(BOW) + A(PT,BOW,PAS).
(elj I , would take the value 1.0 if e’= e, and 0 otherwise 2 .
In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6).
3 (b).
 .
Ruge,  ).
Sahlgren, 2006).
784–787).
Table 1.
Table 2.
DT CD VBP NNS IN NNP NP NNS VBG 3 2 2 1 7-8 4 4 5 9 1 2 3 4 5 6 7 8 9
(a) S(x0:NP, x1:VP, x2:.)
x0, x1, x2 (b) NP(x0:DT, CD(7), NNS(people)) ? x0, 7?
(c) DT(these) ??
(d) VP(x0:VBP, x1:NP) ? x0, x1 (e) VBP(include) ?-?
(f) NP(x0:NP, x1:VP) ? x1,?, x0 (g) NP(x0:NNS) ? x0 (h) NNS(astronauts) ??*,X (i) VP(VBG(coming), PP(IN(from), x0:NP)) ?e?, x0 (j) NP(x0:NNP) ? x0 (k) NNP(France) ???
(l) .(.)
962 OR NP (x0 :NP , x1 :V P) ! x1 ,!
, x0 VP (x0 :VB P, x1: NP ) ! x0 , x 1 S(x 0:N P, x1: VP , x 2:.
) ! x0 , x 1, x 2 NP (x0 :DT CD (7) , N NS (pe opl e)) ! x0 , 7 .(.)
DT (th ese ) ! # VB P(i ncl ude ) ! $%  NP (x0 :NP , x1 :V P) ! x1 , x0 NP (x0 :NP , x1 :V P) ! x1 , x0 VP (V BG (co mi ng) , PP (IN (fro m), x0 :N P)) ! ( , x0 , ! VP (V BG (co mi ng) , PP (IN (fro m), x0 :N P)) ! ( , x0 NP (x0 :NN S) ! x0 NP (x0 :NN S) ! !, x0 NP (x0 :NN P) ! x0 , ! NN P(F ran ce) ! )* NN S(a stro nau ts) ! +,, OR OR N NS (as tro nau ts) !!
,+ ,, OR NP (x0 :NN P) ! x0 NP (x0 :NN P) ! x0 NN P(F ran ce) ! )* , ! NP (x0 :NN S) ! x0 VP (V BG (co mi ng) , PP (IN (fro m), x0 :N P)) ! ( , x0 co min g fro m NN S IN NN P NP VP NP VB G PP NP 7-8 5 7-8 5 7-8 4 4 5 4 5 6 7 8 4 4 4-5 4-5 4-8 NN P(F ran ce) !) *, ! NP (x0 :NN P) ! x0 , ! VP (V BG (co mi ng) , PP (IN (fro m) , x0: NP )) ! ( , x0 , ! NN S(a stro nau ts) ! !
, + ,, NP (x0 :NN S) ! !
pictured in (a).
E that maximizes:4 e?
= argmax e?E { Pr(e) ? Pr(f |e) } (1) Pr(e) is our language model, and Pr(f |e) our translation model.
= argmax e?E { Pr(e) ? ?
and use p(?)
964 Xa Y b a?
b? c?c (!,f 1 ,a 1): X a Y b b?
Figure 3 contains a sample corpus from which four rules can be extracted: r1: X(a, Y(b, c)) ? a?, b?, c?
r2: X(a, Y(b, c)) ? b?, a?, c?
r3: X(a, x0:Y) ? a?, x0 r4: Y(b, c) ? b?, c?
tributed across two possible output strings a?b?c? and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 + p3 ? p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).It is relatively easy to prove that the probabil ities of all derivations that correspond to a givendecomposition ?i ? ?
The first one is the relative frequency estimator conditioning on left hand sides: p(rhs(r)|lhs(r)) = f(r) ? r?:lhs(r?)=lhs(r) f(r ?)
given that the normalization factor |?| is 2, both probabilities p(a?b?c?|pi) and p(b?a?c?|pi) are 0.5.
x0 x1 x2 .845 (b) NP-C(x0:NPB) ? x0 .82 (c) NPB(DT(the) x0:NNS) ? x0 .507 (d) NNS(gunmen) ??K .559 (e) VP(VBD(were) x0:VP-C) ? x0 .434 (f) VP-C(x0:VBN x1:PP) ? x1 x0 .374 (g) PP(x0:IN x1:NP-C) ? x0 x1 .64 (h) IN(by) ??
.0067 (i) NP-C(x0:NPB) ? x0 .82 (j) NPB(DT(the) x0:NN) ? x0 .586 (k) NN(police) ?f? .0429 (l) VBN(killed) ???
.0072 (m) .(.)
.981 . The gunm enw ere killed by the polic e. DT VBD VBN DT NN NP PP VP-C VPS NNS IN NP . ! #$ %  Best composed-rule derivation (C4) p(r) (o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .(.)) ??K x0 . 1 (p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ??
x1 x0 0.00724 (q) NP-C(NPB(DT(the) NN(police))) ?f? 0.173 (r) VBN(killed) ???
The probability P(S3|S2) will be calculated by taking the product of P(h|e), P(h |f), P(h|g), P(i|e), P(i |f), and P(i|g).
So, P(h|e) will be 0.16 given that f(h,e) is one and f(e) is six (see the normalization in (5)).
They are represented as triples, consisting of a head (leftmost element, e.g., say, name), a modifier (rightmost element, e.g., company, its) and a relation (e.g., subject (V:subj:N), object (V:obj:N), modifier (N:mod:A)).
2003).
NE1(n) = w1n and NE2(m) = w2m.
Given: NESet = {NE1, .
Examples of each include (1) ‘explode’, (2) ‘explosion’, and (3) ‘explode:bomb’.
We only score a document if its coverage, cvg(d, c), is at least 3 words (or less for tiny clusters): ir(d, c) = r avgm(d, c) if cvg(d, c) > min(3, jcj/4) l 0 otherwise A document d is retrieved for a cluster c if ir(d, c) > 0.4.
 .
2).
2.
3.
3), Annotators (3.1) and Rearguard (4.1).
2.
All”.
For simplicity, assume two labels {Y, N}.
We allow two types of binary-valued questions: The model expected value is, Emf~ = ~(h)p(d lh ) f i (h ,  d) h,d 1.
A feature is defined as follows: -.~(h,d) d~_f ~1, i f fd=OandVq6 Q~,q(h)= 1 O, otherwise.
Head Verb (V) 2.
Head Noun (N1) 3.
Head Preposition (P) .
B i t  m o f  Verb  == 0, B i t  m o f  Verb  ==I The feature search then proceeds as follows: 1.
The WSJ treebank indicates the accuracy rate of our training data, the human performance indicates how much information is in the headwords, and the ME model is still a good 12 4 the key is N,V,N,N,V, N,N,N,N,V,V,N,V,N,N,N,V,N,V percentage points behind.
2.
Table 1.
The 10 synsets are mapped to 6 supersenses: “artifact”, “quantity”, “shape”, “state”, “plant”, and “act”.
Three similar senses (2), (7) and (9), and the probably related (8), are merged in the “artifact” supersense.
 ).
Algorithm 1).
 ).
“Savannah” (city/river) is distinguished from “savannah” (grassland).
Table 3).
 ).
E.g., I word = mot l, I word = proposl.
Initially set C(e~,f) = 0 for words et.
Then increment by 1 the counter C(e,,?f).
Then increment he counters C(e,,,f), C(e,,,f) ..... C(o,o,f) by tire fraction 1/n.
P(e~) is given by Pie i) = ~f P(e i l f  ) r ( f  )  = ~f P(e~ l f  )M( f  ) /M  (3.i) where M is the total length of the French text, and M(f  )  is the number of occurrences o f f  t in that text (as before).
The fraction P(e, I f )  / P(e,) is an indicator of the strength of association of e, with f, since P(e, I f )  is normalized by the frequency P(e,) of associating e~ with an average word.
We can then think of estimating the probabilities P(e, l f ) ,  R(k l e,), and Q(e:l?)
Estimate the model probabilities by nornmlizing the correspnndiug counters, i.e., P(e,]f) = CP(ei, f ) /CP( f )  where CP(f) = ECP(e, f ) i R(k] e i) = CR(k, ei)/CR(e,) where CR(ei) = E CR(k, ei) k Q(ejl e i) = CQ(e 1, e,)/CQ(e i) where CQ(e,) = ECQ(ei, e,) J The problem with the above algorithm is that it is circular: ila order to evalnate P(E,$ ] F) one needs to know the probabilities P(e, I)c), R(kl e,), and Q(ejle,) in the first place!
More precisely, given any specification of the probabilitics P(e, l f ) ,  R(k l e,), and Q(%le,) , we compute the probabilities P(E,$ [ F) needed in step 1, and after carrying out step 4, wct, sc the freshly obtained probabilities P(e, I f ) ,  R(k ]e,), and Q(e, I e,) to repeat the process fiom step I again, etc.
73 g. l.et R(I  [e,) = 0 .8 ,  R(01?)
= 0.1, R(2 I<)  = R(31<) = R(4 I g) = R(5 I e,) = 0.025 for all words e, except he null word ell l,et R(0 le0) = 1.0.
(ii) Compute the mutual information values l(e,; f )  by formula (Y2), and for each f find the 20 words e, for which I(e,;f) is largest.
(iii) I.ct P(<~I./) = P(< l f )  = ( I /21)  - e for all words<on the list obtained in OiL whEre e is some small positive number.
[i~,) PD(k l h,m,n) RL(2 l eiA) RR(I  e~a) QL(G_:~, 3 I G) QL(% ,,11%) QR(e,~+2,2lei ,) (5.2) Obviously, other distortion formulations are possible.
(e]e) = EQR(e  ,  i l e) i WE can then establish an experimentally appropriate threshold 71, and iuchulc in the glossary all pairs (e, e) and (e, e) whose mutual information exceeds 7.
Lccch and (].P,.
Reproduced in: I,ocke.
Friedtnall, R.A. Olshen, and ( J .
A subsequence wz · · · wj is called a span, and is denoted [i, j].
Then we update P by adding [2, 5] and removing [2, 2] and [4, 5], which are covered by [2, 5].
Example sentences are shown in figure 1(a) and (b).
The quantity p(|x |= `) is simply ˆZ`(θ)/ ˆZ(θ).
? Ref.
Input fh(f p Ref.
, (xN, yN).
If f# (x, y) is constant (f* (x, y) = M for all x, y, say) then AA, is given explicitly as If f# (x, y) is not constant, then AA, must be computed numerically.
Here al -= 1, a2 = 2, a3 = a4 = 3, a5 = 4, and a6 = 5.
IV.T features of templates 2, 3, 4, and 5.
7 Preparat ion  process Round 1: Reso lu t ion  o f  SemEva l The  committee,  had l)ropos(;(t a ve.ry anl l ) i t ious I ) rogrmn of cvahu~l;ions.
(:h of t;he SemEva l  tasks.
* For  corefcren(e., ther(, were prob lems i(hull;i[y- ing i)art-whoh~ and sei;-sul)s(?
e For  p red icate -argumeal t  sl;ru(;l;llr(,, pracl; ical ly every new CoIIS[;Ill(;l; 1)(;y()lI(l s imple clauses and noun l)hrases r;tise(l new issues which had I;o t)e toilet:l ively r(:solve(l. Beyon(l  th(;se in(l ividuM t)rolflenls, il; was fell: l;hal; l;he menu was s imply  (;oo anfl)il, ious, mM l;hal; w(.
In i/arl;, this r(~tl(w.l;est a feel- ing that  the t)rol)lems wi@ Lh(, (:()refl,ren(:(~ Sl)(X> ili(:a.I;ion w(.re l:he mosl; mn(mable l:o solul i (m, lilt, also re.fle(:i;ed a.
(:onvicl;ion I;hal; (:or(ff(r(m(:(~ idea> t:i l ication had 1)een, &nd would re, main,  cr i t ical 1;o success ill inforina.t;iou cxl;r~mi;ion, au(1 st) it wgts [IIlpor[;~l,ll[; 1;o (?llC()llrtl,~(~ a, dvtl~ltc(;s in  (:or(,[ k m(;nc(;, tin contrasl;,  mosl; (;xt, rat:l;ion sysl;ems did nol; buil(t fltll t)redi(:ate-atrgument sl;ru(:l;ures, and word-sense ( l ismnbigual; ion p layed a relal;iv(ly stnal l  role ill exl;ra(:l;ion (par t i cu lar ly  since (;xl;l;t(> l;ion sysl;ems o l )erated in a nar row domain) .
Phe (:or(~ha(;n(:(~ task,  like.
the nam(xl  entil;y l;ask, was a.nnotal;ed us ing SGMI ,  n()tal;i()tl.
Aga in  a sl;alwa.rt grtml) of vt)l- uui;e(w a.nn()i;alx)rs was assenfl) led; 7 each was 1)to - vide(l with 25 mt;i(:lcs f rom 1;h(.
was SOlUe over lap b(!Lween t;hc arLi(:les assigned,  s() t, haL we could IIIO&Slll(!
A major  role o[ the.
mmol;aLion l)ro(:e.ss was Lo i( lemify and res()lv(~ l)r(fl)h!ms wil;h l;he task Sl)(X> ifi(:a.tions.
For COI(~[(~I(;I/(,(;, it proved r(,- markat) ly  (lifli(:ult to f()rmutat;e guitl(,l ines which were reasonal) ly  comI/lel;(~ a, nd <:onsist, ent.. s RomM 3: d ry  rml ()nee the t;ask sl)e(:ifica.l;ions eemed r(~asonably stM)l(b N lbd)  ()rg;ufiz(~(l a "(lry run" a full-s(:al(~ r(~hearsal for MUC-6 ,  I)ul; wi th all result:s r(4)ori;ed a.nouymously.
The dry  run Ix)ok t)l;u:e in Apr i l 1995, wil;h a s(:enario iuvolving labor  union (:()n. l,ra,c.t; n(~gotia.l:i(ms. ()f  0m sil;es whi(:h we.re in- volved in t;he annot;at ion l/r()(:(~s,q, t;en 1)arl:i(:ipatx~(l in lhe dry  run.
The s(:(;nario (l(~finil;ion w;L,q dis- t, ribuIxxt at, t;he t)egimling ()[ S(q)tember I l;he test data  was disiaibut, cd four we(.ks late.r, wi th re sult;s due by (,he end ()[ th(; w(.ek.
The ,qcena.rio involv(M (;h;l,II~O,S ill COI|)OF;I,I;(~ (LK(~CIII;iv(; II],%II;/,~C- m(.n(; p(a,~onn(~l.
(;valua.1;i(m reel; mmly  ()I 1;t1(.
f~oals which had /)(~en set, 1)y th(~ iniLial p lann ing (:onfer(mt:e. in l)e(:emlmr ()f 1993.
There  were (;va]u;Lti(ms for [our t, asks: 1HIIII(RI entit;y, (:orel(.re.n(:e, 1;eml)lat(, c, lt!inenI;, }l, l l(t s(;c-- nmio  I;e, mt~lm;(u Ttmre w(r(; 16 t)mti(;ipmfl;s; 11.5 1)arti(:it)al;e(l in the nmne(l  ent, it, y task,  7 in (,oref- (~lO, ll(~(~,, ] 1 ill t(,ml)lat;(; elemenl;, an(l 9 in s(:enari() l,(;mi)lal;(,,.
Name( l  en i ; i ty  was inl;(mdcd to b(; a siml)h~ task  on whi(:h syst, ems coul(t ( lernoustrat, e a high level of 1)(!rforumn(:e ... high enough for imme(l i - m;e use.
Our  su(:(;(;ss iu I;his t, ask (~x(:(;(~(le(l our >lhe annol;;)A;ion groups were from BBN, Brall(t(fis Univ., t~he Univ.
of l)(mnsylwmia, SAIC (San /)iego), SRA, SR[, the Univ.
SAs exl)e, rienced (:Oml)ut~tional linguists, we 1)rol)- ably should ha,re kuown 1)el;l;(,r l;han to l;hink this wa.s an easy t~ask.
Dooner</COREF>.
 .
1.
H(y, y').
(1). beled set.
5.
First, we claim that defining d(y, 1C(x)) to be the Hamming distance is superior to using a binary value, d(y, 1C(x)) = 0 if y E 1C(x) and 1 otherwise.
(See Figure 2.)
(See Figure 3.)
(See Figure 4.)
The penalty will then decrease linearly with Size(A)+Size(B) so long as k < Size(A)+ Size(B) < 2k.
The four ranges of segment sizes were (20, 30), (15, 35), (10, 40), and (5, 45).
Thus, WD essentially solves Problems 1, 2, and 3.
Thus, we get p = a2 + 2b + a2 = a + b2.
1995).
1994).
).
Believe(OCP, Supports (p, qiA...Aq.)))
 ).
 , and Mitkov  ).
 ).
Three training instances will be generated for He: i({Monday}, He), i({secretary of state}, He), and i({Barack Obama, his}, He).
Lexical (7): Tokens in a window of 7: {wi−3, ... , wi+3}.
Grammatical (1): The part-of-speech  .
Semantic (1): The named entity  .
&quot;father(X,Y)&quot; denotes the relation 'father' between X and Y.
8.
* relative(6,9,X,X) * open(6,6,x(gap,nt,trace,x(gap,nt,close,[])), x(gap,nt,close,x(gap,nt,trace, x(gap,nt,close,[])))) * rel_marker(6,7,x(gap,nt,close,x(gap,nt,trace, x(gap,nt,close,[]))), x(gap,nt,trace,x(gap,nt,close, x(gap,nt,trace,x(gap,nt,close,[]))))) The nodes of the analysis fragment, for the relative clause &quot;that likes fish&quot;, are represented by the corresponding goals, indented in proportion to their distance from the root of the graph.
L2/L2, L2/L3).
In eqn (1) and (2), P .
A transliteration unit correspondence < e, c > is called a transliteration pair.
.
Langley & Sage, 1994).
Storage requirements are proportional to N (compare 0(N * F) for IB1).
Stanfill & Waltz, 1986, Cost & Salzberg, 1994).
2morrow “tomorrow”).
@twitter), hashtags (e.g.
4 “for”).
Cat * * 3 = 6 parses.
388-389, 531-533).
Let x' be an abbreviation for multiplying x x • ... • x, i times.
The first few Catalan numbers are 1, 1, 2, 5, 14, 42, 132, 469, 1430, 4862.
 .)
8.
(42) VP = V NP (1 + PP)ADJS = V (N E Cati(P N)')(E Cati(P (E adji) and simplify the convolution of the two Catalan functions (43) VP = V (N E Cati+i(P N)i)(E adj') so that the parser can also find VPs by just counting coccurrences of terminal symbols.
 .
OPTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a.
DECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) > 1.
EXPECTATION, £: Given a weighted sentence pair (e, f, φ) and indices i, j, k,l, compute Ea φ(a) over all a E A such that (eij, fkl) E a.
SUM, S: Given (e, f, φ), compute EaEA φ(a).
The existence of a polynomial time algorithm for £ implies a polynomial time algorithm for S, because A = U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}.
Then, we set φ(·, ·) = 0 everywhere except: Proof.
If (v, C) is satisfiable, then wsp(v, C) has an alignment a with 0(a) = 1.
First, we introduce binary indicator variables ai,j,k,l denoting whether (eij, fkl) ∈ a.
The objective function is log 0(a) for a implied by {ai,j,k,l = 1}.
SMF-20041076).
2.
4.
5.
6.
7.
9.
 .
 .
attached S S A S S A S A A S A DEPRELS for punc.
(2007a), while the systems called ?Duan (1)?
and ?Duan (2)?
(Turkish).
.
2, r defines the local context).
3.
2).
(j — j ± 1)2.
For a given expression h, we will need to find expressions for f and g such that either h = f(g) or h = Ax.f(g(x)).
For example, T(tex) = e and T(Ax.state(x)) = (e, t).
For example Q(e, t)) = S|NP and Q(e, (e, t))) = S|NP|NP.
4.
The distributions p(y|x, z; B, A) and p(y, z|x; B, A) are defined by the log-linear model, as described in Section 3.3.
Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: O(p,a,i) for the predicate-argument relation; and O(p,T(a),i) for the predicate argument-type relation.
2.
3.
haus ? house, home, building, shell 2.
NN|plural-nominative-neutral ? NN|plural, NN|singular 3.
{ ?|house|?|?, ?|home|?|?, ?|building|?|?, ?|shell|?|?
} 2.
{ ?|house|NN|plural, ?|home|NN|plural, ?|building|NN|plural, ?|shell|NN|plural, ?|house|NN|singular, ...
} 3.
i=1 ?ihi(e, f) (1) Z is a normalization constant that is ignored in practice.
For a translation step component, each feature function hT is defined over the phrase pairs (f?j , e?j) 871 given a scoring function ? : hT(e, f) = ? j ?(f?j , e?j) (3) For a generation step component, each feature function hG given a scoring function ? is defined over the output words ek only: hG(e, f) = ? k ?(ek) (4) The feature functions follow from the scoring functions (?
1(b).
1(b).
= argmax e { p(e|f) } (1) = argmax e { J ? j=1 ? jh j(f,e) } (2) h j(f,e) are J arbitrary feature functions over sentence pairs.
b i . . .
b i ..
fs = ?ni=1 log p(oi = S| . . .)
fd = ?ni=1 log p(oi = D| . . .).
u, . . .
2(a).
Specifically, orientation is set to oi = M if (s? 1,u? 1) contains a word alignment and (s?1,v+1) contains no word alignment.
It is set to oi = S if (s?1,u?1) contains no word alignment and (s?1,v+1) contains a word alignment.
3.
include shift (S), reduce (R), and accept (A).
and ?.?
Similarly to C-E, we provide results for two orientation sets: {M,S,D} and {M,S,Dl,Dr}.
Having approval, we queried: (1) onelook.com, (2) acronymfinder.com, and (3) infoplease.com.
These can either include ({X } n) or exclude QX* } n ) C’s counts.
If g(a,b) were defined to be (a-b)/b , and if h(a,b)=ab/N and `I'(X)=fX , we essentially get Zscores.
1).
Q(J + 1, $) is the probability of the optimum translation.
The resulting algorithm has a worst-case complexity of O(J · M · Ve · E).
The in-set(Z,Y) fact is deleted.
2.
3.
This structure was produced by the following set of transfer rules, where var refers to the indices in the representation of the f-structure: rtrace(r13,keep(var(98),of)), rtrace(r161,keep(system,var(85))), rtrace(r1,del(var(91),set,by)), rtrace(r1,del(var(53),be,for)), rtrace(r20,equal(var(1),and)), rtrace(r20,equal(var(2),and)), rtrace(r2,del(var(1),hope,and)), rtrace(r22,delb(var(0),and)).
2.
5 and 6.
4.
4.
For instance: [P1] associates with [P2], [P1] binding to [P2], [P1], inhibitor of [P2].
For instance: [P1] - [P2] association, [P1] and [P2] interact, [P1] has influence on [P2] binding.
 ).
3.
• • in (Tt.
1 aaw 2 llary a ( i=  2) and the other to: 0 John  1 saw 2 that  3 Mary  4 le f t  5.
The grammar can be thought as a five-tuple (VN, ~, O, S, Lex) where: ?
--+ 2?(finite).
R e f e r e n c e s Abeilld, Anne, 1988.
Schabes, Yves and Joshi, Aravind K., ]988 (b).
3.
parseReviews(R); E?
findExplicitFeatures(R?, C); O?
findOpinions(R?, E); CO? clusterOpinions(O); I?
findImplicitFeatures(CO, E); RO?
suffixes).
Extraction Rules Examples if ?(M,NP = f)?
po = M (expensive) scanner if ?(S = f, P,O)?
po = O lamp has (problems) if ?(S, P,O = f)?
po = P I (hate) this scanner if ?(S = f, P,O)?
Rule Templates Rules dep(w,w?) m(w,w?) ?v s.t. dep(w, v), dep(v, w?)
?v s.t. m(w, v), o(v, w?)
Notation: dep=dependent, m=modifier, o=object, v,w,w?=words.
3.
At iteration m, let q(w,L)(m) denote the support function for label L of w and let P (l(w) = L)(m) denote the probability that L is the label of w. P (l(w) = L)(m+1) is computed as follows: RL Update Equation   P (l(w) = L)(m+1) = P (l(w) = L)(m)(1 + ?q(w,L)(m)) P L?
P (l(w) = L ?)(m)(1 + ?q(w,L?)(m)) where L?
If so(w) > 0, then w is positive, otherwise w is negative.
The P (l(w) = L|Ak)(m) term quantifies the influence of a particular label assignment to w?s neighborhood over w?s label.
In the following, we formalize this map ping.Let T denote the type of a word relationship in R (syn onym, antonym, etc.) and let Ak,T represent the labelsassigned by Ak to neighbors of a word w which are con nected to w through a relationship of type T . We have Ak = ? T Ak,T and P (l(w) = L|Ak)(m) = P (l(w) = L| [ T Ak,T )(m) For each relationship type T , OPINE defines a neighborhood feature fT (w,L,Ak,T ) which computes P (l(w) = L|Ak,T ), the probability that w?s label is L given Ak,T (see below).
Balanced growth of (2, 2), (5, 5), (10, 10) and (15, 15) examples and imbalanced growth of (1, 5), (5, 1) examples are compared in the figure.
100%).
:-( Happy thanks giving everybody :-)”, sarcasm, e.g.
(3) ?We don?t hate+ the sinner,?
?well,?
?reason,?
= 1).
 .
The arc (A, B) yields a category {A, B, C, D}, and the arc (A, C) yields a category {A, C, B, E}.
Second, given two categories Q, R, we merge them iff there’s more than a 50% overlap between them: (|Q n R |> |Q|/2) n (|Q n R |> |R|/2).
We used 5%.)
Additional patterns include ‘from x to y’, ‘x and/or y’ (punctuation is treated here as white space), ‘x and a y’, and ‘neither x nor y’.
However, the normal form requires the 2nd to be [1, 3, 2, 4], not [4, 3, 2, 1], so must be listed last.
[pi.]
X[h] Y[g]) Z [h] 0 \Z [h] + X[h] Z [II]) 0 X[h](v)[v + The rules in Section 3.2 are simple examples of B.ID1 and B.LP1.
However, the normal form requires the 2nd permutation to be [1, 3, 2, 4], not [4, 3, 2, 1], so EZE must be listed last.
I.e., L(G) = {Render(Q) : $ z Q}.
E.g., if v = (1, 3; 8, 9) and a = (7,8), then v + o- = (1, 3; 7, 9) and v 0 o- = [1], [2, 1].
A.C: in 0 (C (G)).
Thus, the time complexity of Parser A is in 0 (C (G) INI3Dn3c(G)).
We therefore approximate P(x|W) with a simple function Q(x; 0).
Littman’s results.
1 x 107 words), 76.06% for a medium-sized corpus (approx.
2 × 109 words), 82.84% for a large corpus (approx.
1 x 1011 words).
They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative.
:).
For example, given the tree (EW (JJ) (great) (POS)), the PT kernel will use (EW (JJ) (great) (POS)), (EW (great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)), (EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ), (great), and (POS).
The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f) and p(f|e)), (b) two word translation probabilities (both p(e|f) and p(f|e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model.
The frequent words overlap greatly between the Palestinian and Israeli perspectives, including “state,” “peace,” “process,” “secure” (“security”), and “govern” (“government”).
“arafat”), countries (“international” v.s.
“america”), and actions (“occupation” v.s.
“settle”) are mentioned.
2.
3.
1999).
For example, the sequence (abc) consists of the bag {c, a, b} and the ordering relation {(b, 2), (a,1), (c, 3)1.
For most translation models, this method produces suboptimal results, however, when e,(u) > 1 and J(v) > 1.
Similarly for cooc( NULL, v).
Given estimates for A+ and A-, we can compute B(links(u, v)lcooc(u, v), A+) and B(links(u, v)icooc(u, v), A-) for each occurring combination of links and cooc values.
1999).
Here, the translation probability φ(0  |e') and notranslation probability p(f  |e').
 .
 .
The resulting algorithm has a complexity of O(J!).
For the initialization the costs for starting from city 1 are set: D({k}, k) = d1k for each k E {2,..., |C|}.
The DP equation is evaluated recursively for each hypothesis (e', e, C, j).
The search starts in the hypothesis (Initial, {∅}, 0).
{1,. .
.
?kernel?
The relative partition p(S) is then {A B} {C} and {D} .
, c(S) = (ISI -1) ?
, m(S) c(S) Recall in turn i s _ ISI ?
Response Links: <A-C> The key generates a single equivalence class S : (A B C}.
m(S ) c(S) (ISI ?
That is , E(ISiI ?
We then have : c(S) = (ISI ?
1)  size of spanning tree for S m(S) = (Ip(S)I ?
1)  number of missing links RT = 1(IS;I ?
1) 48 PT 1(ISil ?
Ip  (S i) I ) c(S)?m(S ) P ?
(Ip(S)I -1) ISI -1 Precision ISI - 1 ISI ?
Response : <A-B B-C D-E E-F G-H-H-I > The key establishes a single equivalence class, while the response defines three : {A B C}, {D E F}, {G H I} .
A Evaluating for recall : R _ ISI ?
Ip  (S  i) I ) PT = E(ISil -1 ) (3?2)+(3?2)+(3?2 ) (3?1)+(3?1)+(3?1) = 3/6, or 50% Before closing, let us consider an even more complex example with multiple sets in bot h the key and the response .
Ip(Si)I ) E(ISiI ?
I p  (S i) I ) PT = 1(ISil ?
1 ) (2?1)+(2?2)+(3?2) (2?1)+(2?1)+(3?1 ) = 2/4, or 50 % It is delightful that the formula yields exactly what intuition would dictate in this case .
?~ 2 = ( ad - be) 2 (a + b) (a + c) (b+ d) (c + d) 02 is bounded between 0 and 1.
A Near M iss /  I~-,~,L.t~.,~ .
Recall the mutual information I(x;y) is computed by Prob(x,y) l?g2 Prob(x)Prob(y) where Prob(x,y) = a/N, Prob(x) = (a + b)/N, and Prob(y) = (a + c)/N.
We use the following reasoning: var(~ 2) = vat(a) + vat(b) 2 2 where var(a) = a, var(b) = b, var(c) = c and var(d) = a + b + c. A direct calculation of this is valid when ~2 is small: vatreal(02) = + "- (a + b)(c + a)(a + c)(b + d) .
~.2, 1 + c+var (d ) + r .
Each local feature is either a state feature s(y, x, i) or a transition feature t(y, y, x, i), where y, y are labels, x an input sequence, and i an input position.
The expectation Ep,,(Y |.
For a given x, define the transition matrix for position i as Let f be any local feature, fi[y, y] = f(y, y, x, i), F(y, x) = Ei f(yi−1, yi, x, i), and let  denote component-wise matrix product.
For any label y = c'c, c(y) = c is the corresponding chunk tag.
For example, c(OB) = B.
 ).
= argmax t { M?
m=1 ?mhm(s Fs 1 , t Ft 1 ) } (1) t?
fn) = p(f1) n?
i=2 p(fi|f(i?1)) where f is a factor (eg.
M? m=1 ?mhm(sw, tw) + N?
M? m=1 ?mhm(swc, tw) + N?
or ?\?
?eats?
t? ? M? m=1 ?mhm(swc, tw) + ?
( N?
.
.
.
.
We have two (inverted) first-order alignment models: p=1(∆j  |· · ·) and p>1(∆j  |· · ·).
Using an interpolation parameter β, the fertility distribution is then computed as � p'(φ  |e)β0 (e)) · p(φ  |e) + 0 (e) · p(φ  |g(e)) (38) Here, n(e) denotes the frequency of e in the training corpus.
This linear interpolation ensures that for frequent words (i.e., n(e) » β), the specific distribution p(φ  |e) dominates, and that for rare words (i.e., n(e) « β), the general distribution p(φ  |g(e)) dominates.
In general, this takes about (I + J) operations.
I did this for n in {1, 2, 3, 5, 10, 20}.
AML = arg maxΛ pΛ(D), where D = {(a, e, f)} are the training data.
.
.
, fi|fi|), where each fij E 11, ... , m}.
3.
Formally, our probabilistic model places a distribution over (r, f, c, w) and factorizes according to the three stages as follows: p(r, f, c, w  |s) = p(r  |s)p(f  |r)p(c, w  |r, f, s) The following three sections describe each of these stages in more detail.
In the E-step, we compute expected counts according to the posterior p(r, f, c  |w, s; 0).
To evaluate a learned model, we compute the Viterbi segmentation and alignment (argmaxr,f,c p(r, f, c  |w, s)).
3.
Cm, j  ]  B  -+ Dr ."
"Dn 0 = mgu(B ,B  ) [j, BO ~- DxO.. .
The highest-priority initialization items are added first, s [O,S-~ ,NP  VP,  O] " [0, N P .--+ castillo ?, 1] Castillo By Completion, the item [0, S ---* NP  ?
VP  XI 1] " [1, vp - -+ .
9 [5, AdvP .--+ yesterday.
,  6] yesterday (2) [I, VP ---+ VP  AdvP .
,  6] was shot yesterday [4, V P ---* V P AdvP  .
, 6] shot yesterday [1, VP --* VP .AdvP ,  6] said Sonny was shot yesterday (4) [0, S ~ NP  VP .
,  6] Castillo said Sonny was shot yesterday [3, V P --* V P ?
Adv P, 6] was shot yesterday (5) [2, S --* NP  VP?
,6  ]  Sonny was shot yesterday [4, V P ~ V P ?
AdvP, 6] ~said Sonny was shot yesterday (7) [0, S --* NP  VP .
,  6] was shot yesterday [3, VP ---* V P ?
Adv P, 6] was shot yesterday (5) [2, S --~ NP  VP  ?, 6] Sonny was shot yesterday (6) [1, VP-+ VP  S ?, 6] said Sonny was shot yesterday [1, VP --~ VP  ?
AdvP, 6] said Sonny was shot yesterday (7) [0, S ~ NP  VP  ?, 6] Castillo said Sonny was shot yesterday (8) [2, S---* NP  VP  ?, 5] Sonny was shot [1, VP ---+ VP  S ,,  5] said Sonny was shot [1, VP --+ VP  ?
AdvP, 5] said Sonny was shot (2) [1, VP ~ VP  AdvP .
AdvP, 6] said Sonny was shot yesterday (4) [0, S ~ NP  VP?
-~ sonny , ,  O] Sonny (2) [0, NP.+ kait ,,13] Knit [0, V -?
,  O] to [0, V -* was ?, O] was [% v - ,  were .
,  O] were [0, V -+ loves *, 0] loves [0, V -+ love , ,  0] love [0, V -* loved , ,  0] loved [0, AdvP .--* passionately ,, O] passionately (3) [0, S ~ ?
VP  AdvP, 0] " [0, w -~.
[0, VP ~ V .,  O] to [0, V P --.
, 0] is [O, VP  -* V.,0] was [0, VP -~ V. ,  O] were (5) [0, vP  -~ v .
,  0] loves [0, VP -~ V. ,  O] love [0, VP ~ V .
,  0] love [0, VP ---* V .
[0, VP .-~ VP  ?
AdvP, 0] loved [0, VP --, V P AdvP  .
[0, vP  - - , .
VP  X,O] " Eventually, this item is completed with items (5) and (2).
[0, VP ---, V P ?
N P, 0] loves [0, VP --~ VP  NP  ,,  0] loves Knit The remaining items generated are [0, VP ---* VP  , AdvP, 0] loves Knit [0, V P ---, V P Adv P o, 0] loves Knit passionately [0, S ---* NP  VP .
For simplicity we sometimes write P(C,), i = 1, , (k + 1) for P(C, I v, r).
BIRD bug bee insect f(C) 8 0 2 0 Cl 4 1 1 1 P(C) 0.8 0.0 0.2 0.0 P(n) 0.2 0.0 0.2 0.0 where for simplicity we write P(n) for Pm(n I v, r).
Since we assume here that every tree cut has an equal L(F), technically we need only calculate and compare L'(M, S) = L(O F) L(S F, e) as the description length.
.
.
•
G: OK?
F: Right.
G: Right, okay.
G: Mmhmm.
Agreement on move classification was K = .69 (N = 139, k = 4).
The semantically smoothed probability of a pair (v, n) is defined to be: The joint distribution p(c, v, n) is defined by p(c, v , n) = p(c)p(vic)p(nic).
We are given: (i) a sample space)) of observed, incomplete data, corresponding to pairs from V x N, (ii) a sample space X of unobserved, complete data, corresponding to triples from Cx V x N, (iii) a set X(y) = {x E X I x = (c, y), c E CI of complete data related to the observation y, (iv) a complete-data specification po(x), corresponding to the joint probability p(c, v, n) over Cx V x N, with parametervector 0 = (Oc,Ov,OncIc E C, v E V, n E N), (v) an incomplete data specification P0(Y) which is related to the complete-data specification as the marginal probability P0 (Y) = Ex(y)po(x). '
Let x = (c, y) for fixed c and y.
9.
4.
2.
3.
4.
5.
(See Figure 2).
.
.
.
.
7.
8.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Sentences (1)—(2) and (3)—(4) compare the word associations comparison with/to with association with/to.
The role of determiner can be played by several classes of items: articles, (e.g., &quot;a,&quot; &quot;the&quot;), possessives (e.g., &quot;my,&quot; &quot;your&quot;), indefinite adjectives (e.g., &quot;some,&quot; &quot;many,&quot; &quot;few,&quot; &quot;certain&quot;), demonstratives (e.g., &quot;this,&quot; &quot;those&quot;), numbers, etc.
Theorem 1.
Because p0 − p ≥ 1, the right hand side is at most −(u + 1)|E|.
(6)
P (fJ1 , v R 1 , d K 0 , c K 0 , y K 1 , x K 1 , u K 1 ,K, e I 1) = P (eI1)?
Source Language Model G P (uK1 ,K|e I 1)?
Source Phrase Segmentation W P (xK1 |u K 1 ,K, e I 1)?
Phrase Translation and Reordering R P (vR1 , d K 0 , c K 0 , y K 1 |x K 1 , u K 1 ,K, e I 1)?
Target Phrase Insertion ? P (fJ1 |v R 1 , d K 0 , c K 0 , y K 1 , x K 1 , u K 1 ,K, e I 1) Target Phrase Segmentation ?
We make the following conditional independence assumption: P (yK1 |x K 1 , u K 1 ,K, e I 1) = P (y K 1 |x K 1 , u K 1 ).
In the first case, bk ? {0,+1,?1} while in the second case, bk ? {0,+1,?1,+2,?2}.
Under this restriction, the probability of the jump bk (Eqn 5) can be simplified as P (bk|xk, uk, ?(b k?1 1 )) = (6)?
?1(xk, uk) bk = +1, ?k?1 = 1 1 ? ?1(xk, uk) bk = 0, ?k?1 = 1 1 bk = ?1, ?k?1 = 2.
There is a single parameter jump probability ?1(x, u) = P (b = +1|x, u) associated with each phrase-pair (x, u) in the phrase-pair inventory.
The score on a self-loop with labels (u, x) is P (x|u) ?
(1 ? ?1(x, u)); on a 2-arc path with labels (u1, x1) and (u2, x2), the score on the 1st arc is P (x2|u1) ? ?1(x2, u1) and on the 2nd arc is P (x1|u2).
This latter condition implies that for an input sequence {a, b, c, d}, we disallow the three output sequences: {b, d, a, c; c, a, d, b; c, d, a, b; }.
The jump probability of Eqn 5 becomes P (bk|xk, uk, ?k?1) = ? ????
?1(xk, uk) bk = 1, ?k?1 = 1 ?2(xk, uk) bk = 2, ?k?1 = 1{ 1 ? ?1(xk, uk) ??2(xk, uk) bk = 0, ?k?1 = 1 (7) { ?1(xk, uk) bk = 1, ?k?1 = 2 1 ? ?1(xk, uk) bk = ?1, ?k?1 = 2 (8) { 0.5 bk = 0, ?k?1 = 3 0.5 bk = ?1, ?k?1 = 3.
(9) { 1 bk = ?2, ?k?1 = 4 (10) { 1 bk = ?2, ?k?1 = 5 (11) { 1 bk = ?1, ?k?1 = 6 (12) We note that the distributions (Eqns 7 and 8) are based on two parameters ?1(x, u) and ?2(x, u) for each phrase-pair (x, u).
Suppose the input is a phrase sequence a, b, c, the MJ-2 model (Fig 5) allows 6 possible reorderings:a, b, c; a, c, b; b, a, c; b, c, a; c, a, b; c, b, a. The distri bution Eqn 9 ensures that the sequences b, c, a andc, b, a are assigned equal probability.
??1(x, u) = Cx,u(0,+1) Cx,u(0,+1) + Cx,u(0, 0) .
(13) Cx,u(?, b) is defined for ? = 1, 2 and b = ?1, 0,+1.
If a phrase-pair (x, u) is never seen in the Viterbi alignments, we back-off to a flat parameter ?1(x, u) = 0.05.
(ALL=02+03+04).
Charniak (C), Minipar (M) and Chunker (CH).
The graph-based model, MSTParser, learns a scoring function s(i, j, l) E R over labeled dependencies.
= 5.9).
The large majority of responses (80%) fall within the bars for N = 0 (47.8%), N = 1 (23.0%), and N = 2 (10.0%), forming a rapidly descending curve.
= 92%; min .
= 82%).
= 80%; min.
The value is +sentence.final.contour if &quot;.&quot; or &quot;?&quot;, -sentence.final.contour if &quot;,&quot;.
Recall is 92% (a = .008; max = 1; min = .73), precision is 18% (a = .002; max = .25; min = .09), fallout is 54% (a = .004; max = .65; min = .45), and error is 49% (a = .004 max .61; min = .41).
Passonneau [19941).
Rotondo [1984], Swerts, [1995]).
.
.
n is q, q is n, q become n, n coord q, n have q, n use q, n with q, n without q q coord n, q have n, n in q, n provide q, q such as n
This remains tractable as long as features factor by edge, f(x, z) = &;Ez f(x, z), because that ensures that q(z) will have the same form as pθ(z | x).
.
1.
1).
1.
1.
2.
1.
3 approximately.
1.
3.
6.
6.
LCC-WSD ? ?
NUS-ML ? ?
NUS-PT ? ?
PU-BCD ? ?
RACAI-SYNWSD ? ?
SUSSX-C-WD ? ?
SUSSX-CR ? ?
SUSSX-FR ? ?
TKB-UO ? ?
UOFL ? ?
UPV-WSD ? ?
USYD ? ?
1 word window).
For example, the model finds distinct senses for “medication” (Sense 1 and 7) and “illegal substance” (Senses 2, 4, 6, 7).
Figure 2(a)).
FORM —* state(x1) in Figure 2(a).
This is possible because some of the operators used in the logical forms, notably the conjunction operator (,), are both associative (a,(b,c) = (a,b),c = a,b,c) and commutative (a,b = b,a).
Step 1.
Step 2.
Step 3.
2.
[smallest [state] [by area]] in Figure 3.
 ).
“America” in “Bank of America”), (2) relative pronouns, and (3) gerunds, but allows (4) nested nouns (e.g.
“union” in “union members”).
2.
3.
2 Wikipedia ? A Wiki Encyclopedia.
2.
Consequently, a named entity e is included in d:E if and only if d = e:title, d 2 e:R, or d2e:D.
More exactly, if q:E contains n named entities e 1 , e 2 , ..., e n , then the dataset will be augmented with n pairs hq; e k i represented as follows: hq; e k i = [?(e k ; q:e) j q:T j e k :title] The field q:T contains all words occurring in a limit length window centered on the proper name.
2.
minimize: V (w; ) = 1 2 w
w+ C P  q;k subject to: w ((q; q:e) 
Conditional likelihood es timation using a log-linear model P (y | x) = 1 Z w  .
w that predict correct alignments on the train ing data: y i = arg max ?y i ?Y i wf(x i , y?
(y i , y?
(y i , y?
i ), given by max ?y i ?Y i [wf i (y? i ) +
i (y? i ) ? wf i (y i )], where
i (y? i ) =
(y i , y?
i ), and f i (y? i ) = f(x i , y?
i ).
i max ?y i ?Y i [wf i (y? i ) +
i (y? i)].
i (y? i ) = ? jk [ c-y i,jk (1 ? y? i,jk ) + c+y? i,jk (1 ? y i,jk ) ] = ? jk c-y i,jk + ? jk [c+ ?
(c- + c+)y i,jk ] s.t. ? j z i,jk ? 1, ? k z i,jk ? 1, 0 ? z i,jk ? 1.
Hence, without any approximations, we have a continuous optimization problem instead of a combinatorial one: max ?y i ?Y i wf i (y? i )+
i (y? i ) = d i +max z i ?Z i (wF i +c i )z i , where d i = ? jk c-y i,jk is the constant term, F i is the appropriate matrix that has a column of features f(x i,jk ) for each edge jk, c i is the vector of the loss terms c+ ?
(c- + c+)y i,jk and finally Z i = {z i : ? j z i,jk ? 1, ? k z i,jk ? 1, 0 ? z i,jk ? 1}.
max z?Z ? i wF i z i + c i z i ? wF i y i , (2) where z = {z 1 , . . .
, z m }, Z = Z 1 ? .
.?Z m . In-.
The gradient of the objective in Equation 2 is given by: ? i F i (z i ? y i ) (with respect to w) and F i w + c i (with respect to each z i).
as P ?
(w) = ?w/max(?, ||w||).
(wt + ? k ? i F i (y i ? zt i )); z?t+1 i = P Z i (zt i + ? k (F i wt + c i )); and correction: wt+1 = P ?
(wt + ? k ? i F i (y i ? z?t+1 i )); zt+1 i = P Z i (zt i + ? k (F i w?t+1 + c i )), where ? k are appropriately chosen step sizes.
The original edges jk have a quadratic cost 1 2 (z? i,jk ? z i,jk )2 and capacity 1.
(y i , y?
(a) Dice and Distance, (b) With Orthographic Features.
.
.
.
.).
Below is a sample query with its English gloss: answer(A,count(B,(city(B),loc(B,C), const(C,countryid(usa))),A)) ?How many cities are there in the US??
?RRB??null ?LRB?
Each production LHS ) RHS in the PCFG is in the form: P (h)!L n (l n ):::L 1 (l 1 )H(h)R 1 (r 1 ):::R m (r m )where H is the head-child of the phrase, which in herits the head-word h from its parent P . L 1 :::L n and R 1 :::R m are left and right modifiers of H . Sparse data makes the direct estimation ofP(RHSjLHS) infeasible.
label H: P h (HjP; h).
2.
subcat frames LC and RC: P l (LCjP;H; h) and P r (RCjP;H; h).
3.
ing the left and right modifiers: Q i=1::m+1 P r (R i (r i )jH;P; h;
i
(9) a.
Analogously, define cv(p) = E, c(v, p, true) and cv = Ep cv (P)• The counts c(n,p,true) and c(v,p,true) are from the extracted head word tuples.
 .)
Cfl: [TAROO, BOOK] SUBI OBJ b.
(Ziroo) invited (Taroo) to a movie.
contra-indexing,?
I Cf: [TAROO, DATA] 0 yatto hanbun yari-owarimasita.
Cb: [71 [ Cf: [TAROO, ZIROO] Itiniti-zyuu, kanzen-ni 0 0 musi-simasita.
Cb: [TARO0] Cf: [TAROO, ZIRO0] b. Itiniti-zyuu, kanzen-ni 0 0 musi-simasita.
1).
1).
Z(�B) (the partition function) is chosen so that E(x,y) p(x, y  |0) = 1; i.e., Z(B) = E(x,y) u(x, y | 0). u is typically easy to compute for a given (x, y), but Z may be much harder to compute.
1) are available.
5, both set Ai = {(xi, yz )}.
5, A = {xi}×� and B = N(xi)×Y.
1) is a special case where N(xi) = X, for all i, and the denominator becomes Z(~θ).
1a).
1b).
3.
(1) by relative frequencies: p(CIC&quot;) := n(C1C1')In(C'), p(wIC) = n(w)In(C).
 : The function h(n) is a shortcut for n • log(n).
(6) we have to model the monolingual a priori probability p(ef1E) and the translation probability p(f lel; e, F).
(1).
(6).
(9) with ng,1 = n9,2.
(4).
(10).
(12)).
Let L = P U R be the set of labels with P and R disjoint (i.e., P fl R = 0).
If G = (V, E) is a graph and e = (v, l, w) an edge (with l E L), then the extension of G with e, denoted as G + e, is the graph (V U {v, w}, E U {e}).
Moreover, with EG(v, w) we refer to the set of edges in EG from v to w; that is, EG(v, w) = {e E EG  |e = (v, l, w), for l E L}.
When H is isomorphic to some subgraph of G by an isomorphism 7r, we write H C,r G. Given a graph H and a vertex v in H, and a graph G and a vertex w in G, we define that the pair (v, H) refers to the pair (w, G) iff H is connected and H C,r G and 7r.v = w. Furthermore, (v, H) uniquely refers to (w, G) (i.e., (v, H) is distinguishing) iff (v, H) refers to (w, G) and there is no vertex w' in G different from w such that (v, H) refers to (w', G).
Thus, for each v E VH and for each e E EH: cost(v) = cost(e) = 1.
Formally, (v, H) uniquely refers to (W, G) iff H is connected, and for each w E W there is a bijection π such that H Cπ G, with π.v = w and there is no w' E G\W such that (v, H) refers to (w', G).
(+-ni, n-+j).
R C_ X x Y can be represented as a matrix, namely one that has as row-bases x E X and as column-bases y E Y , with weight cxy = 1 where (x, y) E R and 0 otherwise.
In the transitive case, 5 = N ® N, hence →−s t = →−n i ® →−n j.
The p scores are on a [−1, 1] scale.
3.
?which?
is ?WHNP?.
1 returns ?SBAR?
The ?Prob.?
4.
5.
The probability of a completion operation can be calculated as P (com|seg) = Y bw:s BDFT (bw|s) Y w:s Y dep:w CFT (dep).
7.
?calender?
and ?0?
3.
4.
8).
4.
s = argmax s P (s|c)P (s) (4) We combine the parts described in the previous sections to get the direct translation model: P (s|c) = ? ?:Str(?(c))=s (P (seg|c)P (com|seg) (5) ? node P (dp|node)P (ro|node)P (sub|node) ? w (sub|w)).
6) to calculate P (s|c).
3).
7.
Example 2.
and ?making?.
?Genetic?
(Moses): The same as (SW).
9.
?PE?
6).
PPL(text) = P (w1w2...wN )?
1.
Sibling parts (h, m, s) can thus be obtained from Figure 3(b).
Clearly, grandchild parts (g, h, m) can be read off of the incomplete g-spans in Figure 4(b,d).
Specifically, a grand-sibling is a 4-tuple of indices (g, h, m, s) where (h, m, s) is a sibling part and (g, h, m) and (g, h, s) are grandchild parts.
For example, in Figure 1, the words “must,” “report,” “sales,” and “immediately” form a grand-sibling part.
In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras   algorithm by “demoting” each third-order part into a second-order part: SCOREGS(x, g, h, m, s) = SCOREG(x, g, h, m) SCORETS(x, h, m, s, t) = SCORES(x, h, m, s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively.
1.
 .
06/21/98).
&quot;economic impact&quot;.
As a result, (return V:subj:N forecast) satisfied the condition (3).
Computational Linguistics, 20(4):563-596.
Computational Linguistics, 19(1):61-74, March.
.
. xi+l, yi−m .
.
5)Art Work: novels, books, paintings, operas, plays.
6)Films: films, telenovelas, shows, musicals.
7)Songs: songs, singles, albums.
8)Events: playoffs, championships, races, competitions, battles.
(JAIR), 15:31–90.
2.
3.
Let ei1(i) = ai(i) and define and 1 .
= —(Y1(001(0.
Equation 5 is already only 0(N).
 ).
If a set R = {r} of rules in G collide as the rule r' in G', we give r' the probability P(r') = maxIER P(r).
Therefore, we know that αG(e, s) < αG1(e, s).
Similarly, B is U (SXMLR, SIXLR).
 ,  ,  ,  ,  .
2.
1.
2.
3.
4.
7.
16-25, 26-35, >35) 8.
80-100%, 90-100%) 9.
Percent of JJ, N, V, RB (0%, 1-100%, 50-.
100%, 80-100%) 10.
Positive and negative word counts ( ? 1, ? 2,.
3, ? 4, ? 5, ? 6) 12.
Resulting pa rameters were: ? = 1.1, ? = 0.5, ? = 5, ? = 1.0, S = 0.5.
Resulting parameters were: ? = 1.2, ? = 0.9, ? = 4, ? = 1, S = 0.5.
(2a) E/NE: ?Ah,?
For example, the probability that &quot;I&quot; is a pronoun, Prob(PPSS I &quot;I&quot;), is estimated as the freq(PPSS I &quot;I&quot;)/freq(&quot;I&quot;) or 5837/5838.
So we could jointly model the parses t', t and the alignment a between them, with a model of the form p(t, a, t' I w, w').
Our models are thus of the form p(t w, w', t', a) or, in the generative case, p(w, t, a w', t').
In conditional training, the exponentiated scores s are normalized by a constant: Z = Et exp[s(t, t', a, w, w')].
 .
Figure 2).
Pedersen  ).
Given an input string W, the parser is initialized to ((), W, (), ()), and terminates when it reaches a configuration (S, (), (), A).
Right1 in a configuration (s1|S, n|I, T, A), adds an arc from s1 to n and pops s1 from the stack, producing the configuration (S, n|I, T, A∪{(s1, r, n)}).
Insert in a configuration (S, I, s1|T, A), pops s1 from T and pushes it to the stack, producing the configuration (s1|S, I, T, A).
A node n is in the frontier set iff complement span(n) n closure(span(n)) = 0.
1.
2.
3.
4.
 .
(d) I borrowed a saw from Jane.
We define the set of possible labelvalues as ?S?, ?T?, ?-?, where ?S? is the first to ken (or Start) of a source, ?T? is a non-initial token (i.e., a conTinuation) of a source, and ?-?
For eachsentence x, we define a non-negative clique poten tial exp( ?K k=1 ?kfk(yi?1, yi, x)) for each edge, and exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...)
Following Lafferty et al  , the conditional probability of a sequence of labels y given a sequence of tokens x is: P (y|x) = 1Zx exp ? X i,k ?k fk(yi?1, yi, x)+ X i,k ??k f ?k(yi, x) ?
(1) Zx = X y exp ? X i,k ?k fk(yi?1, yi, x) + X i,k ??k f ?k(yi, x) ?
(x,y)?D P (y|x).
(e.g., ?bless?
 .
357 1.
Possessives (e.g., ?Clinton?s idea?)
2.
In (2), ?spoken out?
or ?realize?.
2002).
Figure 4.
Q: What is a prism?
Consider 1.
2.
definition, reason,?
The coarse classifier determines a set of preferred labels, C 1 = Coarse Classifier(C 0 ), C 1  C 0 so that jC 1 j  5.
preferred labels, C 3 = Fine Classifier(C 2 ) so that C 3  C 2 and jC 3 j  5.
After ranking the classes in the decreasing order of density values, we have the possible class labels C = fc 1 ; c 2 ; : : : ; c n g, with their densities P = fp 1 ; p 2 ; : : : ; p n g (where, P n 1 p i = 1, 0  p i 1, 1  i  n).
As dis cussed earlier, for each question we output the first k classes (1  k  5), c 1 ; c 2 ; : : : c kwhere k satis fies, k = min(argmin t ( t X 1 p i  T ); 5) (1) T is a threshold value in [0,1].
Figure 2 is a gray-scale map of the matrix D[n,n].
Hence, we map the tuple (eM, el, er), which corresponds to (m, 1, r), into a conjunctive node.
 .
Formally, a hypergraph is a pair 7-1 = (V, £) consisting of a vertex set V and a set of hyperedges £ C_ V* x V. Each hyperedge e E £ connects a head vertex h(e) with a sequence of tail vertices T(e) = {v1, ..., v,,,}.
E.g. if X1 = {c, cd, d} and X2 = {f, g}, then a total of six sequences will result.
I don’t think it’s worth it.
Here the path <a b c> folds back onto itself, that is, <a> = <a b c>.
= {(AC) c) .
A side effect of A becoming A' = render: tem] number: sg case: nom is that C' simultaniously reduces to {(B C)).
C&quot; = {(B C)} = [number: Lease: aco As the example shows, once C is unified with (A B), A and B acquire a &quot;positive constraint.&quot; All later unifications involving them must keep at least one of the two pairs (A C), (B C) unifieable.
For example, unification of {A B} with {C DI yields {(A C) (A D) (B C) (B D)) assuming that the two values in each tuple are compatible.
The result of unifying {(A C) (B C)) with {(D F) (E F)} is {(A CD F) (A C E F) (B CD F) (B C E F)), again assuming that no alternative can initially be ruled out.
let + 3rd - - conversant:
P = ? ai:i?A ? res?ai freqres |ai| |Hi| |A| (1) R = ? ai:i?T ? res?ai freqres |ai| |Hi| |T | (2) Mode P = ? bgi?AM 1 if bg = mi |AM | (3) Mode R = ? bgi?TM 1 if bg = mi |TM | (4) A system is permitted to provide more than one response, just as the annotators were.
2.
3.
4.
(As before, P(E) is 0.333.)
For example 0(A → B C, i, k, j) is a weight for the rule hA → B C, i, k, ji.
We assume a trigram tagger, where a tag sequence is represented through decisions h(A, B) → C, ii where A, B, C ∈ T, and i ∈ {3 ... n}.
Rule productions take the form hA(a) → B(b) C(c), i, k, ji where b ∈ {i ... k}, c ∈ {(k + 1) ... j}, and a is equal to b or c, depending on whether A receives its head-word from its left or right child.
Each such rule implies a dependency (a, b) if a = c, or (a, c) if a = b.
The integrated parsing problem is then to find where R = {(y, d) : y ∈ H, d ∈ D, y(i, j) = d(i, j) for all (i, j) ∈ Ifirst} This problem has a very similar structure to the problem of integrated parsing and tagging, and we can derive a similar dual decomposition algorithm.
2 and 3.
4.
4; see figure 3.
2).
By theorem 5.2, this is equivalent to solving To formulate our approximation, we first define: Q' = {(µ, v) : µ ∈ conv(Y), v ∈ conv(Z), µ(i, t) = v(i, t) for all (i, t) ∈ Iuni} The definition of Q0 is very similar to the definition of Q (see Eq.
9.
The dual L(u) is convex.
u(k+1) , u(k) where g(k) is the subgradient of L at u(k) and > 0 is the step size of the update.
The following convergence theorem is well-known (e.g., see page 120 of Korte and Vygen  ): Theorem 6.1 If limk,,,,, = 0 and = oo, then limk,,,,, L(u(k)) = L(u).
Recall that the constraints for 20 specify that µ E conv(Y), v E conv(i), and µ(i, t) = v(i, t) for all (i, t) E Zuni.
Let µ be the convex combination of the following two parses, each with probability 0.5: (X(A w1)(X(A w2)(B w3))) and (X(A w1)(X(B w2)(A w3))).
It can be verified that µ(i, t) = v(i, t) for all (i, t), i.e., the marginals for single tags for µ and v agree.
Thus, (µ, v) E 20.
For the tagging model, set 0(AA → A, 3) = 0(AB → B, 3) = 0, with all other parameters having a negative value.
For the parsing model, set 0(X → A X,1,1, 3) = 0(X → A B, 2, 2, 3) = 0(X → B A, 2, 2, 3) = 0, with all other rule parameters being negative.
Thus, (µ, v) E� conv(2).
w,~ and a sequence of tags T = t i ts .
,  t,, of the same length.
P(W,T) = P(ql#)P(wtltt)P(t,.
l#, tl)P(w21t~) ~I P(tilti_2,ti_l)P(willi)P(#[t,,_l,?,,) (2) i=3 where "#" indicates the sentence boundary marker.
N(ti_2, ti-1, tl) P(tifti-2ti-t) = f(qltl-2ti-x) - iV ( t i _ .
, , t i _ , ) (3) P(wilti) = f(wilt,) -- N(w,t) (1) N(t) where f indicates the relative frequency, N(w, t) is t!,e number of times a given word w appears with tag l, aid N(li_2,ti-l,tl) is the number of times that sequer~ce l~ i _2t i _ l l  i appears in the text.
Wi we have P(W,r) = 1].
t l , F(w~,~) = ?
)(w,_, ,~-, )P(~d,.
for iffil to length(s t r ing)  do foreach parse in get -parse - l i s t ( i )  do foreach word ill l e f tmost -subat r ings(a t r ing , i )  ,I(7 poa-ngrma : -  append(parse.nth-order-stato , l i s t  (word.poa)) if (traneprob(poe-ngrtm) > O) then new-parse :.
Functions i n i t ia l - s tep  and f ina l - s tep  treat [be  t l a l iS l t  [ons  I%L sltl i l l~llce ] ,Ol l l |dl l l  ieg, 203 ing the last element, equals that of tile previous parse, ignoring the first element.
Let C = cic~.., c,~ denote the sequence of n charac- ters that constitute word zv whose part of speech is t. We approximate the probability of the word given part of speech P(wlt ) by tile trigram probabilities, p(,,,Iz) = P,(C) - -  f,(~,l#, #)~,(~1#, <) u IX P,(~,lc+-=, ~ -~)r,(#1c.._l, ..,) i=3 (9) where special symbol "#" indicates ttle word boundary marker.
> (get -lef tmo st-subst riags-uit h-word-model ( ( i~ 4) -~M 2.519457597358691E-7) (~ ~;~f:~"~j~ 2.3449215070189967E-8) (~ ~tlfj/~l~i] 7.02439907471337451{-9) (~]i,~, 1  2.375650975098567E-9) (,l~J~ "4.
7  ~ ~  [90.7% [ 0.007 [ I :~[ os.,~:~ [ 8 a .
s ~  ] 84.a% [ o.m2 ] I ~  9a.2% I 7 s .
~ / o  I 79.6% I o.o15 I k 5 I <~lii?
[7] [8] [9] [10] [11] [13] Re ferences [14] [1] Black, E. et al.
I1$3, Vol.24, No.l, pp.40-46, 19811 (in Japanese).
(5) X:tiP, ReXPron Y, Z --* X:tiP Z. X:tiP Y.
This ~no(M (the I)SM) is based on a sinq)le dependency tel)r(> sentation provided l)y I,exicalized Tree.
relative clauses, sentential adjuncts, a(Iw.rbials).
For a more R)rmal and (le- taile(I (lescription of l,lA(]s see (Schabes et M., J988).
() grant DAAH04-94-G0426.
Table 3).
Results show higher stability compared to the full annotation scheme (K = .83, .79, .81; N = 1,248; k = 2) and higher reproducibility (K = .78, N = 4,031, k = 3), corresponding to 94%, 93%, and 93% agreement (stability) and 93% (reproducibility).
Figure 7).
We measured stability at K = .9, .86, .83 (N = 100, k = 2) and reproducibility at K = .84 (N = 200, k = 3).
Figure 9).
Sentences occurring in parts 1, 2, 3, 4, 19, and 20 receive the values A, B, C, D, I, and J, respectively.
Table 7).
A total of 136 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German, Hungarian, Italian and Spanish news sites:2 Hungarian: hvg.hu (10), Napi (2), MNO (4), N´epszabads´ag (4) Czech: iHNed.cz (3), iDNES.cz (4), Lidovky.cz (3), aktu´alnˇe.cz (2), Novinky (1) French: dernieresnouvelles (1), Le Figaro (2), Les Echos (4), Liberation (4), Le Devoir (9) Spanish: ABC.es (11), El Mundo (12) English: BBC (11), New York Times (6), Times of London (4), German: S¨uddeutsche Zeitung (3), Frankfurter Allgemeine Zeitung (3), Spiegel (8), Welt (3) Italian: ADN Kronos (5), Affari Italiani (2), ASCA (1), Corriere della Sera (4), Il Sole 24 ORE (1), Il Quotidiano (1), La Republica (8) Note that Italian translation was not one of this year’s official translation tasks.
 .
= argmaxa n ? i=1 ?ifi(a, e, f) where the fi are features and the ?i are weights.
We compute LLR scores using the following formula presented by Moore  : LLR(f, e) = ? f??{f,?f} ? e??{e,?e} C(f?, e?)
and e?
are variables ranging over these values,and C(f?, e?)
and e?.
For example, in the word abode, pronounced [ a b o d ], the letter a produces a null phoneme (E).
Consider the word abomination [ a b n m i n e f a n ]: the first six letters and phonemes are aligned the same way by both aligners (abomin- [ a b n m i n ]).
(8)).
(See Figure 1.)
We calculate this quantity as follows: 1VaL(1\li,k)0(Nli,k).
We again make the independence assumption that p(t),k I NjA, to,i, ti) 13(NJ,k)* Additionally, we assume that p(Mk,to,i) and p(to,k) are independent of p(tk,n), giving: The denominator, p(to,k), is once again calculated from a tritag model.
Next, we define the vector of document’s labels to be X(d) = {k|Λ(d) k = 1}.
For example, suppose K = 4 and that a document d has labels given by A(d) = {0,1,1, 0} which implies X(d) = {2, 3}, then L(d) Then, 0(d) is drawn from a Dirichlet distribution with parameters α(d) = L(d) × α = (α2, α3)T (i.e., with the Dirichlet restricted to the topics 2 and 3).
 ).
.
.
15-17).
(n factorial) is equal to 1 x 2 x • • • x n. For example, tossing a fair coin three times (n = 3, p = 1/2) will result in 0, 1, 2, and 3 heads with probability 1/8, 3/8, 3/8, and 1/8, respectively.
.
.
.
.
In the Brown Corpus, it happens that f (strong, enough) = 11, f (strong) -= 194, f (enough) = 426, and N = 1, 181, 041.
The next step now is to assutne a sort O\['l,airwise inter- act, ion between tim French word f j an(l each, F,n- glish word ci, i = 1, ... l . These dep('ndencies are captured in the lbrm of a rnixtnre distritmtion: 1 p(J)le{) = ~_.p(i, fjlc I) i=1 I = ~_~p(ilj, l).p(fjle~) i=1 Putting everything together, we have the following mixture-based ntodel: J l r,'(fi!l~I) = p(JIO ' H ~_~ \[~,(ilJ, l).
~,(j)led\] (1) j= l i=t with the following ingredients: ? sentence length prob~d)ility: P(J l l); ? mixture alignment probability: p( i l j , I); ? translation probM)ility: p(f\[e).
To train the translation probabilities p(J'fc), we use a bilingual (;orpus consisting of sentence pairs \[:/ ';4"1 : ', . , s Using the ,,laxin,ul , like- lihood criterion, we ol)tain the following iterative L a equation  : / ) ( f ie) = ~ - will, $' A(f,e) = ~ 2 ~5 , that there is only one optinnnn and therefore the I,',M algorithm (Baum, 1!)72) always tinds the global optimum.
1).
837 DAYS BOTH ON EIGHT AT IT MAKE CAN WE IF THINK I WELL + + + + + + + + +j~ + + + + + + + + + ~J ~+ + +++++++/+?+.
We now assume a first-order depen- dence on the alignments aj only: Vr(fj,aslf{ -~, J-* a I , e l ) where, in addition, we have assmned that tile translation probability del)ends only oil aj and not oil aj-:l. Putting everything together, we have the ibllowing llMM-based model: a Pr(f:i'le{) = ~ I-I \[p(ajlaj - ' , l).p(Y)lea,)\] (4) af J=, with the following ingredients: ? IlMM alignment probability: p(i\]i', I) or p(a j la j _ l , I ) ; ? translation probabflity: p(f\]e).
Using a set of non-negative parameters {s ( i - i')}, we can write the IIMM alignment probabilities in the form: 4 i - i') (5) p(ili', i ) = E ' s(1 - i') 1=1 This form ensures that for each word position i', i' = 1, ..., I, the ItMM alignment probabilities satisfy the normMization constraint.
Note the similarity between Equations (2) and (5).
As with the IBM2 model, we use again the max- imum approximation: J Pr(fiSle~) "~ max\]--\[ \[p(asl<*j-1, z)p(fj l<~,)\] (6) a ' / .ll.
Thereibre, we have to re- sort to dynainic programming for which we have the following typical reeursion formula: Q(i, j ) = p(f j lel) ,nvax \[p(ili', 1) . Q(i', j - 1)\] i =l , . , , I Here, Q(i, j ) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 197@.
 .
' l 'able \] gives the details on the size of tit<; cor- pora a, ud t;\]t<'it' vocal>ulary.
Tall)le, I: (,orpol :L (,o~pt s l,angua.ge Words Voc.
(confer model 4 in  ).
5 Conclusion.
6 Acknowledgement.
HR001106-2-001 (Dyer).
(pp.
(pp.
(pp.
For example the gap between sentences 2 and 3 gets assigned a score of 8 computed as 2 * 1 (for A) +1 * 1 (for B) +2 * 1 (for C) +1 * 1 (for D) +1 * 2 (for E).
Figure 3(b) illustrates.
.
.
, (3) M1R11M1R1.
 .
“I gave him 15 photographs,” REVERB extracts (I, gave, him).
2.
3.
We observe: 1.
2.
3.
4.
1986).
 .
Formally, we have a CFG G, which consists of  : (i) a set of terminals {wk},k = 1,...,V; (ii) a set of nonterminals {Nk},k = 1,...,n; (iii) a designated start symbol ROOT; and (iv) a set of rules, {ρ = Ni —* ζ j}, where ζ j is a sequence of terminals and nonterminals.
1.
2.
We define the word representations as (vector, POS) pairs: ((a, A), (b, B), (c, C)), where the vectors are defined as in Sec.
6.
5.
6.
There are two sets that meet the requirements, namely {(1,1),(42)} and {(2, 1), (4, 2)}.
Totally unrelated words are associated in a few instances, as in &quot;Observatory&quot;/&quot;diesem&quot; (24), &quot;detectors&quot; /&quot;primare-&quot; (36), and &quot;bright-&quot;/&quot;Astronona-&quot; (48).
.
. w1_1.
To give a trivial example, if in the CKY recognition algorithm we had written chart[s, A, s+1] := chart[s, A, s+1] v chart[s, B, s+t] A chart[s+t, C, s+1]; instead of the less natural chart[s, A, s +1] := chart[s, A, s +1] V chart[s,B, s+t] A chart[s+t,C, s +1] A TRUE; larger changes would be necessary to create the inside algorithm.
Rather than, for instance, a chart element chart [i, A, j], we will use an item [i, A,]].
.
.
Consider the instantiation i -= 1, k = 2, j = 3, A = X, B = X, C = X, We use the multiplicative operator of the semiring of interest to multiply together the values of the top line, deducing that [I, X, 3] = 0.2 x 0.8 x 0.8 = 0.128.
Thus, [1, S. 4] = 0.2048.
We assume the addition of a distinguished nonterminal S' with a single rule S' S. An item of the form [i, A —> a /3,j] asserts that A => a/3 4 w, .
.
.
We also assume that x = x = 0 for all x.
.
,em.
We consider the following grammar: tion is S • aAA AGIA aaa, which has value R(S AA) 0 R(A a) 0 R(A AA) 0 R(A a) 0 R(A ---+ a).
Also, if D a„ .
.
.
.
.
.
.
,Dak).
.
.
Since corresponding items are iso-valued, for all i, V(E) = V(DO.
For instance, for CFGs, {(X —> YZ, Y —> y)} is a valid element, as is {(Y —> y, X —> x)}.
(a2 .
.)
.
.
.
(ak .
.)).
Call this set inner<g(x,B).
We can define the <g generation value of an item x in bucket B, V<g(x,B): Intuitively, as g increases, for x E B, inner<g(x,B) becomes closer and closer to inner(x).
Notice that for items x e B, there will be no generation 0 derivations, so V<0(x, B) = 0.
A formula for V<g(x,B) is useful, but what we really need is specific techniques for computing the supremum, V(x) = supg V<g(x,B).
, ak.
..,j-2,j-1,j+1,j+ 2, .. k— 1,k.
We denote such sequences by 1, k. By extension, we will also write f(1), (k) to indicate a sequence of the form f (1),f (2), ,f(j — 2),f(j — 1) , f(j + 1), f(j + 2),.
,f (k — 1) , f (k).
For w-continuous semirings, Z<g(x,B) approaches Z(x) as g approaches oo.
23).
(TΣ(∅) = TΣ.)
For p ∈ pathst, rankt(p) is the number of children, or rank, of the node at p in t, and labelt(p) ∈ The paths to X in t are pathst(X) ≡ {p ∈ pathst  |labelt(p) ∈ X}.
For example, the tree t = S(NP, VP(V, NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0).
A weighted regular tree grammar (wRTG) G is a quadruple (E, N, S, P), where E is the alphabet, N is the finite set of nonterminals, S ∈ N is the start (or initial) nonterminal, and P ⊆ N ×TΣ(N)×R+ is the finite set of weighted productions (R+ ≡ {r ∈ R  |r > 0}).
The tree S(NP(DT(the), N(sons)), VP(V(run))) comes out with probability 0.3.
The weight of a becoming b in G is wG (a, b) Eh:(a,())⇒c∗(b,h) w(h), the sum of weights of all unique (leftmost) derivations transforming a to b, and the weight of t in G is WG(t) = wG(S,t).
The weighted regular tree language produced by G is LG ≡ {(t,w) ∈ TE × IIB+  |WG(t) = w}.
What is sometimes called a forest in natural language generation   is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅.
Then the weight of (i, o) in X is WX(i,o) ≡ wX(Qi(i),o).
The weighted tree transduction given by X is XX ≡ {(i, o, w) ∈ TE × To × R+|WX(i, o) = w}.
Given a wRTG G =  .
Input: xR transducer X = (E, A, Q, Qi, R) and observed tree pair I ∈ TΣ, O ∈ TΔ.
Output: derivation wRTG G = (R, N ⊆ Q × pathsI × pathsO, S, P) generating all weighted derivation trees for X that produce O from I.
Returns false instead if there are no such trees. if anyrule? then N ← N ∪ {(q, i, o)} return anyrule?
Finally, given inside and outside weights, the sum of weights of trees using a particular production is γG((n, r, w) ∈ P) ≡ αG(n) · w · βG(r).
Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg.
1 if di,o = false then warn(more rules are needed to explain (i,o)) compute inside/outside weights for di,o and remove all useless nonterminals n whose βdi,o(n) = 0 or αdi,o(n) = 0 itno ← 0, lastL ← −∞, δ ← ǫ for r = (q, pat, rhs, w) ∈ R do wr ← w
: - q.TOP.S x → i x, r x - q.TOP.S x → r x, i x - q.TOP.S x → r x - q.NP.NN x → i x, r x - q.NP.NN x → r x, i x - q.NP.NN x → r x State i means “produce a Japanese function word out of thin air.” We include an i production for every Japanese word in the vocabulary, e.g.
'Ltd:), titles, etc.
 .
 .
'Ltd:), titles, etc.
 .
 .
.
Synchronization: if f(w) =- v and v €, then f(g(w)) = h(o), and f' (v) = w. Similarly, if f' (v) w and w e, then f'(h(v)) = g(w), and f(w) = v. Phrase contiguity: The image under f of the maximal substring dominated by a headword w is a contiguous segment of the target string.
Suppose there is a feature L which depends only on yields: f2(w) = fz(y(w)).
2.
.
4.
 .
A dependency parser receives a sentence x of n to kens, and outputs a labeled dependency tree y. In the tree, a labeled dependency is a triple ?h, m, l?, where h ? [0 . . .
m ? [1 . . .
n] is the index of the modifier token, and l ? [1 . . .
The first-order model scores a factor as score1(w,x, ?h, m, l?)
A first-ordermodel considers only ?h, m?.
The score of a factor is: score2(w,x, ?h, m, l, ch, cmi, cmo?)
= ?1(x, h, m) ? wl1 + ?2(x, h, m, ch) ? wlh + ?2(x, h, m, cmi) ? wlmi + ?2(x, h, m, cmo) ? wlmoNote that the model uses a common feature func tion for second-order relations, but features could be defined specifically for each type of relation.
Open structures: For each span from s to e and each label l, the algorithm maintains a chart entry [s, e, l]O associated to the dependency ?s, e, l?.
if h < m, and ?left?
The definition of ?2(x, h, m, c) is: ? dir ? cpos(xh) ? cpos(xm) ? cpos(xc) ? dir ? cpos(xh) ? cpos(xc) ? dir ? cpos(xm) ? cpos(xc) ? dir ? form(xh) ? form(xc) ? dir ? form(xm) ? form(xc) ? dir ? cpos(xh) ? form(xc) ? dir ? cpos(xm) ? form(xc) ? dir ? form(xh) ? cpos(xc) ? dir ? form(xm) ? cpos(xc)
1997).
.
Let x{u,i1 be a {0,1}-variable, defined to be 1 if and only if variable u is labeled i, where u ∈ E, i ∈ GE or u ∈ R, i ∈ GR.
(6), (7), and (8) are the integral constraints on these binary variables.
4).
1.
(10a) Forward Functional Application (FFA) value -~ functor argument <va lue> = <functorva l> <argument> = <functorarg> <functor d i r> = Right.
/ funct?
191 (23) c ~ ..
  or  ).
2.
3.
4.
2.
3.
“IBM executives” and “Microsoft executives”).
Merialdo  ).
Consider m problems indexed by ` E { 1 each with nt samples (Xti; Yt) indexed by i E {1; ::: ; nt}.
•Fixmpredictors{ut}, and find(O,{vt})that minimizes the joint empirical risk (3).
Ando  ).
We summarize this semisupervised learning procedure below. f = arg min f E ��1 L(f(o Xi)>Yi) + ~11w1122, where f (O; x) = wTx + vTOx as in (1).
From a c-way classification problem, c!=(c — k)! binary prediction problems can be created.
O-- - -cat - -S  0  [cat  S] - i .
ca t - -NP  1  [cat  < 2 .
ca t - -VP  2  [cat  VF;]I a r i ty - -2  ar  i t,y 2 I t l abe ls  I I - -NP  labe ls  NP - -2 - -VP  V specification is a two-item list of the form ( { a t t r ibute  I path ] [ path  [ va lue  } ).
(S NP VP ((0 f se t ) (2  f se t ) ) ((1 f set )  (2 f se t  sub ject ) ) ((2 f se t  form) f in i te ) ) The first specit~cation is an analogue of an I.F~; T = annotat ion;  the S node inher i ts  its feature set from the VP.
~cat - -S O--  - .
A It is permissible in D p,.
Fur thermore,  A = A, B = B, and C = C. For D PATR, the s i tuat ion is more 78 complicated.
-~-~ x Y {opt> ?
?= n i iiiiiiiii twtwtpttwtwwpTWp 1 111111 )..|()...|(),( by replacing the approximation: )|()..|( )|()...|( 1211 1111 ????
= = iiiiiiii iiiii tttptwtwtp twptwtwwp with the approximation: )|()..|( )|()...|( 1211 111111 ????
), LOC (Location), WEA (Weapon) and VEH (Vehicle).
In this relation example R, arg-, is ((“areas”, “NNS”, “area”, dseq), “LOC”, “Region”, “NOM”), and arg-,.dseq is ((OBJ, areas, in, 1), (OBJ, areas, controlled, 1)). arg2 is ((“troops”, “NNS”, “troop”, dseq), “ORG”, “Government”, “NOM”) and arg2.dseq = ((A-POS, troops, Syrian, 0), (SBJ, troops, controlled, 1)). path is ((OBJ, areas, controlled, 1), (SBJ, controlled, troops, 0)).
In terms of (4), PRO defines where (x)+ = max(0, x).
C = 0.01.
1.
2.
5.
(So N(A) = ? a?AN(a).)
More precisely, for each u ? N(A)\A, let the affinity between u and A be given by the ratio |N(u) ?N(A)| |N(u)| . The best new node b ? N(A) \ A is the node which maximises this affinity score.
 ).
, vn?1, vn} such that (vj , vj+1) ? E for 1 ? j < n. ? Connectedness is an equivalence relation.?
These include idioms (3-4), light verb constructions (5), small clauses (6).
(see Table 5.)
: (will) TR (very) MSA (be successful)” and “&A (the discussion meeting) TR (very) MSA (be successful)”.
-)� (business)”.
The (balanced) F-measure is 2pr/(p + r).
Compare, for example, sentences (1) and (4).
Brinton  ).
However, (3) and (4) produce a rather different result.
That is, S =< (to' , ti), (w2,t2)...(w7-4,4,) >.
Head-words 2.
So in this case, AF(5) = (0, < S >). and 1155 = announced, so AF(1) = (5, <NP ,S ,VP>).
D is now defined as the m-tuple of dependencies: D = {(AF (1), AF(2)...AF(m)} .
For example, C( (a, b) , (c, d) , 6.) is the number of times (a, 6) and (c, d) appear in the same sentence at a distance A apart.
This introduces an additional term, P(UP,S), into (20).
(1).
Section 7).
2.
Task 2.
P(j|i) = C(i,j) where MI(i, j) is the mutual information of (ei, ci) and (ej, cj).
Chrustjev.)
Let R = {r1, . . .
2.
?wn is a labeled directed graph D = (W,A), where (a) W is the set of nodes, i.e. word tokens in the input string, (b) A is a set of labeled arcs (wi, r, wj) (where wi, wj ? W and r ? R).
2.
4.
Unique label (wi r?wj ? wi r ? ?wj) ? r = r?
Termination ?S,nil, A?
Left-Arc ?wi|S,wj |I, A?
?S,wj |I, A ? {(wj , r, wi)}?
??wk?r?(wk, r?, wi) ? A Right-Arc ?wi|S,wj |I, A?
?wj |wi|S, I, A ? {(wi, r, wj)}?
??wk?r?(wk, r?, wj) ? A Reduce ?wi|S, I, A?
?S, I, A?
?wj?r(wj , r, wi) ? A Shift ?S,wi|I, A?
?wi|S, I, A?
 .
1999; Charniak, 2000).
Example confusion sets include: {principle , principal}, {then, than}, {to,two,too}, and {weather,whether}.
Consider the case n = 2.
:::+1 ) +,)>O1 yields better results.
309-317).
has a ?game?
Given grammar G and an input ti), we ask whether E T(L(G)).
For every q E Q, x,y E V, b E VT, de D, and q' E &(q, b, d), we specify that The reflexive and transitive closure of ha is written Ha*.
We define G = (VN, P,T[$]), where (iii) T fs(q), where we assume WLOG that Is is a singleton set {q}.
In general, G has p = 0(IVD13) = 0(t3).
H la exists if {x#y : (x, y) E L(Ha)} is regular (where # VT).
Collins's 0(n5)).
To do so, it is necessary to estimate i'(x), i'(y), and i' (xy), using only the base translation model.
The approximation begins with Assumption 3 implies that for all t E T Pr(xy, t) = Pr(x : RC = y, t) Pr(y : LC = x, t) (8) The approximation continues with Under Assumptions 3 and 4, we can estimate i'(xy) as shown in Figure 2.
All the terms in Equation 13 depend only on Pr(y, t), Pr(y : LC = x, t) and Pr(y : LC 0 x, t).
 .
1993).
2004).
Grid cells correspond to grammatical roles: subjects (S), objects (O), or neither (X).
The inter-subject agreement was r = .768 (p < .01.)
2004).
Given a source string fJ1 = f1, · · · , fj, · · · , fJ and a target string eI1 = e1, · · · , ei, · · · , eI, an alignment A of the two strings is defined as : A C {(j,i) : j = 1,···,J;i = 0,···,I} (1) in case that i = 0 in some (j, i) E A, it represents that the source word j aligns to an “empty” target word e0.
For each sentence pair i = 1, 2, · · · , N, assume two systems b and c have Viterbi alignment scores Sb, Sz .
Let R = {r1, . . .
2.
R): 1.
Left-Arc: In a configuration ?t|S,n|I,A?, if.
there is no arc (w, r, t) ? A, extend A with(n, r?, t) and pop the stack, giving the configu ration ?S,n|I,A?{(n, r?, t)}?.
2.
Right-Arc: In a configuration ?t|S,n|I,A?, if.
there is no arc (w, r, n) ? A, extend A with (t, r?, n) and push n onto the stack, giving the configuration ?n|t|S,I,A?{(t, r?, n)}?.
is an arc (w, r, t)?A, pop the stack, giving the configuration ?S,I,A?.
n onto the stack, giving the configuration ?n|S,I,A?.
TL.POS   ? TL.DEP . . .
T.POS T.LEX   ? TR.DEP . . .
TR.POS . . .
NL.POS   ? NL.DEP . . .
4 Experiments.
 .
2.
3.
4.
5 Conclusion.
Given an input sentence of words, w 1 , w 2 . . .
Consider sentence (4).
?NP 1 VBD 2 NP 3 , NP 1 VBD 2 NP 3 ? ?S, S?
?NP 1 VBD 2 NP 3 , NP 3 was VBN 2 by NP 1 ? ?NP, NP?
?he, him?
?NP, NP?
?he, he?
?NP, NP?
?he, Peter?
?VBD, VBN?
?sang, sung?
?NP, NP?
?a song, a song?
The decoder maximises over this space: y ? =argmax y:S(y)=x ?(y) (1) where ?(y) = ? r?y ??(r, S(y)), ??
(2) Here x is the source (uncompressed) tree, y is a derivation which produces the source tree, S(y) = x, and a target tree, T (y), 4and r is a gram mar rule.
5 This requires finding the maximiser of H(y) in one of: H s = (1?
??(y ? )??(y), ??)?(y ? ,y) H m = ?(y ? ,y)?
??(y ? )??(y), ??
This requires adapting the scor ing function, ?, in (2) to allow features over target ngrams: ?(y) = ? r?y ??(r, S(y)), ??+ ? m?T (y) ??(m,S(y)), ??
We have three Hamming loss functions over: 1) tokens, 2) ngrams (n ? 3), or 3) CFG productions.
(T) ?ADVP,RB?
(T) ?ADJP,JJ?
(P) ?S,S? ? ?S 1 and S 2 , S 2 and S 1 ?
(P) ?NP,NP?
?DT 1 NN 2 , DT 1 NN 2 ?
(S) ?NP,NP?
?DT 1 NN 2 , NN 2 ?
< 0.01).
< 0.01).
Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i.
Let D = (V, A) be the complete directed graph S+(v) , {hi, ji ∈ A  |i = v} denote its set of outgoing arcs.
9 by za ∈ B, a ∈ A.
We add valency indicators zval ik , ff(vi = k) for i ∈ V and k = 0,... , n − 1.
Conditions 1, 2, and 3&quot; can be encoded with O(n2) constraints.
[3 He] ...
(See Figure 1.)
1.
For each topic, k ? {1, . . .
,K}.
(a) For each synset s, randomly choose transition prob abilities ?k,s ? Dir(S?s).
2.
For each document d ? {1, . . .
, D}.
(a) Select a topic distribution ?d ? Dir(?)
(b) For each word n ? {1, . . .
, Nd} i. Select a topic z ? Mult(1, ?d) ii.
Specifically, given a document collection w1:D, the full posterior is p(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?)
?(?K k=1 p(?k |S?) ?D d=1 p(?d | ?) ?Nd n=1 p(?d,n |?1:K)p(wd,n |?d,n) ) , (1) where the constant of proportionality is the marginal likelihood of the observed data.Note that by encoding the synset paths as a hid den variable, we have posed the WSD problem asa question of posterior probabilistic inference.
p(?
The first term, the topic probability of the uth word, is based on the assignments to the K topics for words other than u in this document, p(zu = i|z?u) = n(d)?u,i + ?i ? j n (d) ?u,j + ?K j=1 ?j , (3) where n(d)?u,j is the number of words other than u in topic j for the document d that u appears in.
(4) 3.1 Transition Probabilities.
?Point,?
and ?center,?
?may,?
?shear,?
?coach,?
?incident,?
?fence,?
?bee,?
?balcony,?
?body,?
?arch,?
?floor,?
and ?corner.?
 .
Harmeling).
The tapes move under the heads from the right and then continue to the left No symbols are shown to the right on the bottom tape, because we are assuming that the operation chronicled in these diagrams is one in which a surface form is being 5 (a) V t b [1 (e) k t {b} CCVVCVC V VCCV V C C a a {al a [1 a ak t a (b) t b t [] V a a VVCVC [] VCCVC V V a ak tab (c) (g) V C VC VC VCCVC a a [] a •■■• [] ak t a b (d) k t [] (h) k t b V C C V a C VC V VCCVCVC a a i a k t a a ak t ab i b Table II generated.
The moves from (f) to (g) and from (g) to (h) are like those from (d) to (e) and (b) to (c) respectively.
A &quot;G&quot; transition causes a nondeterministic choice.
The eight step are labeled (a) - (h).
The moves from (f) to (g) and from (g) to (h) are like those from (d) to (e) and (b) to (c) respectively.
A &quot;G&quot; transition causes a nondeterministic choice.
Harrison, M. A.
</ttl> <person>James</person> </person> , <age><num>57</num> years old</age > </person> , is stepping down a s <post>chief executive officer</post> </post-out> [ .
<person><person>Mr.
retired-ttl(ttl) <?
successor((pers-a) succ) + job-out-in-context?
((succ, job-out) x-1) + job-out((pers-b, ttl, org) x-2 ) The mysterious-looking job-out-in-context?
References [1] Alshawi, H .
& Van Eijck, J .
[2] Appelt, D ., Hobbs, J ., Bear, J ., Israel, D., & Tyson, M .
SIGART Bulletin, 2(3),15-21.
[4] Borkovsky, A.
Seattle, 1 994 .
[9] Landman, F .
Linguistics and Philosophy, 12(3), 359-605 and 12(4), 723-744.
[to] Lehnert, W., McCarthy, J ., Soderland, S ., Riloff, E., Cardie, C., Peterson, J ., Feng, F ., Dolan, C., & Goldman, S .
Baltimore, Md ., 1993 .
[11] Vilain, M .
&quot;feature engineering&quot;.
According to their information gain values, features are ordered with decreasing importance as follows: 11,13, 10, 1, 2, 8, 12, 9, 6 , 4 , 7 , 3 , 5.
According to their information gain values, features are ordered in decreasing importance as follows: 16, 15, 12, 14, 11, 2, 1, 19, 10, 9, 13, 18, 6, 17, 8, 4, 7, 3, 5.
+0.7%).
.
.
We seek a string e that maximizes P(e If), or equivalently maximizes P(e) • P(fle).
Given a string f of length m and a set of parameter tables (b, E, s), return a string e of length 1 < 2m that maximizes P(elf), or equivalently maximizes Given a string f of length m, a set of parameter tables (b, €, s), and a real number k, does there exist a string e of length / < 2m such that P(e) • P(fle) > k?
For every pair (i,j) such that 0 < i,j < n: Finally, we set k to zero.
Therefore, P(e) = 0.
If M1-DECIDE returns yes, then some decoding e with P(e) • P(fle) > 0 must exist.
We define &quot;A knows that P&quot;, written KNOW(A,P), as P A BA(P).
We let W(A,P) (usually written WA(P)) mean &quot;agent A wants P to be true&quot;.
6.
INFORMIF(speaker, hearer, P) prec: KNOWIF(speaker,P) A W(speaker,INFORMIF(speaker,hearer,P)) effect: KNOWIF(hearer,P) body: B(hearer,W(speaker,KNOWIF(hearer,P))) Similarly, it must be possible for A to plan for S to tell A the referent of a description, without A knowing the referent.
Example 7.
In Example 8, the possible body-action inference from (2) to REQUEST(A,S,INFORMIF(S,A,KNOW(S,P))) is downgraded because the embedded inference to (3) is possible.
For instance, the bodies of the acts INFORM(A, S. P) and INFORM(A, S, —P) are BsWA(P A Bs(P)), and BsWA(—P A Bs(—P)), respectively.
(PC.!)
WA(Bs(P)) =c=> WA(Bs(Q)), if BABs(Q P).
(PH) BsWA(Bs(Q)) =i=> BsWA(Bs(P)), if BsBABs(Q P).
The weight of an 1 defined for two random variables X and Y as (H(X)+H(Y)H(X,Y))/max(H(X),H(Y)) with H(X) entropy.
This framework independently models the grammaticality of s (with P(s)) and whether s is a good compression of l (P(l  |s)).
' 2 \ I t " "IP cpspe~..
, - /~ / \~ i AAI ~ I'P " NI i VI : 1 t . . ".,....
A ul , P ,, ,.
Ail il;em is a tr iplet thai; represe.nts a.(possibly intern plei, e) X-ba, r strltc- i>ll I'(?
'l~wo it;ores <\[ i , , j l \ ] , A , , S ,> a.nd <\ [ i2 , j2 \ ] , A,2, S~,> can I)e combilxed if ? ' " a,(Ijacent o each \] l, heir Slll'\[a.ce sl, riilgs Arc el, her: i7 - : jl-I-1.
2.
t i l e SOtlrc(~ lTxessa,~es COTHe Vii/~ d i f fe . rent Ii,,ks: li,,ks(,g,) r~ li,,ks(S,~) =-- (k, where links(,q) is a. I'illlC~iOlX {hal,> given i~ set o\[ nlessa.ges, returl is the sel; of l inks via which the iiicssa.ges a, rrived.
{l'he result o\[ I~ixe colnbinM;ion is a. \[leW il;Oll;l: <\[il,.i~\], ,mil 'y(A,, A2), S, U S.~>.
()l:herwise, ig is disca.rded.
V, and tensed I).
484 . . .
-No&~.
4.
<arg>) (<gunc-name> <arg> . . .
<art>) (<-June-name> <arg>.
<ar t>) ) For exanq)le, (acknowledge (subcat ((cat v)) (((cat i) -bare inf))) (subcat ( (ca t v ) ) ( ( ( ca t n) ( case acc ) ) ) ) (subcat ( (ca t v ) ) ( ( ( ca t c ) ) ) ) q'\]le f'/ltlctioII subcat t'eturt/s a stll)c&|,egoriz&- Lion frame of the word.
wj . . .
w,,/' is stored as a s t r ing "'Wh . . .
'tOn, 101 . . .
'U,~h_l."
5.
6.
co.l;(.xt-f,:ee gra,,~,nars, wl.',',' ~.
1998: 291).
to ? stop?).
and ?seized?.
and ?seized?)
seized ??
holding ??
protesters ??
seized ??
raided ??
warning ??
troops ??
raided ??
?station?.
Therefore, ?troops?
[ seized VBD Verb ] ? [?]?
or ?never?.
or ?nor?.
K(x, y) = { 0, m 6= n ?n i=1 c(xi, yi), m = n (1) where c(xi, yi) = |xi?yi| is the number of common word classes between xi and yi.
?his ? actions ? in ? Brcko?, and 2.
?his ? arrival ? in ? Beijing?.
x = [x1 x2 x3 x4 x5 x6 x7], where x1 = {his, PRP, PERSON}, x2 = {?}, x3 = {actions, NNS, Noun}, x4 = {?}, x5 = {in, IN}, x6 = {?}, x7 = {Brcko, NNP, Noun, LOCATION} 2.
y = [y1 y2 y3 y4 y5 y6 y7], where y1 = {his, PRP, PERSON}, y2 = {?}, y3 = {arrival, NN, Noun}, y4 = {?}, y5 = {in, IN}, y6 = {?}, y7 = {Beijing, NNP, Noun, LOCATION} Based on the formula from Equation 1, the kernel is computed as K(x, y) = 3?1?1?1?2?1?3 = 18.
?stations?
(S\NP )/NP 2 ?seized?
?stations?
(S\NP )/NP 1 ?seized?
?protesters?
Because predicates (e.g. ?seized?)
and adjuncts(e.g. ?several?)
and ?that?.
2003).
2003).
Three versions exist; we use the simplest, Ta, here: sign [(q(vi) — q(v2))(r(vi) — r(v2)) 2(11;1) where sign(x) is 1 for positive arguments, —1 for negative arguments, and 0 at 0.
For example, suppose the probability estimate E (2 — Li (q,r)) • r(v) (suitably normalized) performed poorly.
Note that when δ = 0, pe - pe.
2).
1.
2.
Experiment: Annealing β.
 ).
(1.5) ).
We use a linear model score(x) = w� · O(x), where O(x) is a feature representation and w� is a weight vector.
Here the distractor-related probabilities are independent of r, i.e., P(D|r) = P(D), P(a1|D, r) = P(a1|D), etc.
 .
1.
1).
1).
4.
This distribution in turn induces a parse distribution P(T′|G) = P(7r(T)|G) over (projected) trees with unsplit evaluation symbols, where P(T′|G) = ET:T′=,(T) P(T |G).
 .
3.
4.
Does B?
[(d) Then [the system resolves confiicts.
](e) [It confirms the enhancement with the user.[().)
A particular triple (q,r,Y) rules out (s, t, X) if there is no way that (s, t, X) and (q,r,Y) could both be in the same parse tree.
In particular, if the interval (s, t) crosses the interval (q, r), then (s, t, X) is ruled out and counted as an error.
Formally, we say that (s, t) crosses (q, r) if and only ifs<q<t<rorq<s<r<t.
(6) Consistent Brackets Tree Rate = 1 if C = NG.
Let us define a new function, g(s,t, X).
3.
3.
A
For any rule F, we write F` for the set of instances {x : F(x) = �}, or (ambiguously) for that set’s characteristic function.
We write F¯` for {x : F(x) =� t n F(x) =� +}.
To compute the expression on the righthand side of the last line, we require P(Y |G), P(Y ), P(G|F), and P(G).
The first has intrinsic features N:Bruce-Kaplan, C:Bruce, and C:Kaplan (“N” for the complete name, “C” for “contains”), and contextual feature M:president (“M” for “modified by”).
Let us define Y (x) = + if x is a “location” instance, and Y (x) = − otherwise.
The precision of F is P(Y |F).
The value r = a − b is the difference.
A complex rule (or classifier) is a list of atomic rules H, each associating a single feature h with a label t. H(x) = t if x has feature h, and H(x) = L otherwise.
That is: P(Y`, G¯`JF`) = P(Y¯`, G`JF`) Let us first consider a concrete (but hypothetical) example.
The consistency condition is simply that no two spans in a bracketing may overlap, where two spans (i, j) and (k, 1) overlap if eitheri<k<j<lork<i<1<j.
This can be seen from the following three facts about any full binary bracketing B of a string w: Thus, in equation (2) for instance, the number of spans (i, k) for which e(i,k) 0 0 is O(Icl), and there is a single j between i and k for which e(i, j) 0 0 and&quot;d(j, k) 0 0.
A similar argument applies to equations (4) and (5).
?according to?)
+ + = (3) Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and ? = Plcs/Rlcs when ?Flcs/?Rlcs_=_?Flcs/?Plcs.
For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: X: [A B C D E F G] Y1: [A B C D H I K] Y2: [A H B K C I D] Y1 and Y2 have the same ROUGE-L score.
Given two sentences X and Y, the recurrent relations can be written as follows: (1) If xi = yj Then // the length of consecutive matches at // position i-1 and j-1 k = w(i-1,j-1) c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) // remember the length of consecutive // matches at position i, j w(i,j) = k+1 (2) Otherwise If c(i-1,j) > c(i,j-1) Then c(i,j) = c(i-1,j) w(i,j) = 0 // no match at i, j Else c(i,j) = c(i,j-1) w(i,j) = 0 // no match at i, j (3) WLCS(X,Y) = c(m,n) Where c is the dynamic programming table, 0 <= i <= m, 0 <= j <= n, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table position, c(i,j).
For example, f(k)-=-?k ? ?
when k >= 0, and ?, ? > 0.
where -?
> 1.
= ? )( ),(1 mf YXWLCSf (4) Pwlcs ???
= ? )( ),(1 nf YXWLCSf (5) Fwlcs wlcswlcs wlcswlcs PR PR 2 2 )1( ? ?
Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram based F-measure as follows: Rskip2 )2,( ),(2 mC YXSKIP = (7) Pskip2 )2,( ),(2 nC YXSKIP = (8) Fskip2 2 2 2 22 2 )1( skipskip skipskip PR PR ? ?
2 Combinations: C(4,2) = 4!/(2!*2!) = 6..
2. NIST, PER, and WER.
3.
4.
5 Conclusion.
.
.
 .
1 is O(INT1I x INT21).
1).
 .
 .
87.1%6 in  .
3.
Kupiec, J.
 .
1.
.
.
In fact, one should be able to improve the estimate of a tree's likelihood via p(S I W) -,-- p(S I T) * p(T I W).
Using the semantic probabilities alone p' (X I P, 0) had poorer performance, a 34% error rate.
Thymine).
Then, the latent class model is: (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} .
5.
6.
A linear-chain CRF with parameters ? = {?1, ...}
= argmaxy P? .
The log-likelihood of training set {(xi, yi) : i = 1, ...M} is written L?
= ? i logP?(yi|xi) = ? i ( T?
t=1 ? k ?kfk(yt?1, yt,x, t)?
The gradient of the likelihood is ?P?(y|x)/??k = ? i,t fk(yt?1, y(i)t ,x(i), t) ? ?
L? = ? i logP?(yi|xi)?
1.
2.
4.
Feature function are represented as f(yt?2, yt?1, yt,x).
We 1http://www.mandarintools.com, ftp://xcin.linux.org.tw/pub/xcin/libtabe, http://www.geocities.com/hao510/wordlist noun (e.g.,?,?)
verb (e.g.,?)
adjective (e.g.,?,?)
adverb (e.g.,!,?)
auxiliary (e.g.,,?)
preposition (e.g.,?)
number (e.g.,,) negative (e.g.,X,:) determiner (e.g.,?,?,Y) function (e.g. ?,?)
letter (English character) punctuation (e.g., # $) last name (e.g.,K) foreign name (e.g.,?)
maybe last-name (e.g.,?,[) plural character (e.g.,?,?)
pronoun (e.g.,fi,?,?)
unit character (e.g.,G,?)
country name (e.g.,?,?)
organization name title suffix (e.g.,?,?)
title prefix (e.g.,,?)
date (e.g.,#,?,?)
For example, person name ?????
However, ?????
5.
7.
.
.
.
The negative examples are ((the carrier)9, (union)13), ((wage)io, (union)13), ((selective wage reductions)n, (union)13), and ((Feb. 3)12, (union)13).
.
5).
 .
(p + q)subsequential).
According to the previous theorem the condition is sufficient since: Conversely if f is p-subsequential, let T = (Q, i, F, E, L, 6, a, p) be a p-subsequential transducer representing f , where p = (p1,.
More formally, a string-to-weight transducer T is defined by T = (Q, E, I, F, E, A, p) with: One can define for T a transition (partial) function S mapping Q x E to 2Q by: V (q, a) E Q x E, (q , a) = fq' I 3x E (q, a, x, q') e El and an output function a mapping E to R. by: A path 7 in T from q E Q to q' c Q is a set of successive transitions from q to q': 7r = ((q0,ao,x0, qi), • • • , (qm—i,am_i, xm_i, qm)), with Vi E [0, M — 11, q,±1 E 6(q11at).
More formally a string-to-weight subsequential transducer T = (Q, i, F, E, (5, a, A, p) is an 8-tuple, with: A string w E E* is accepted by a subsequential transducer T if there exists f E F such that (5(i, w) = f .
The output associated to w is then: A + cr(i, w) + p(f).
Let T = (Q, i, F, E, 6,0•, A, p) be a subsequential transducer.
Hence n1 (t) q' , if t = (q, a, x, q') E Ei.
Notice that the input ab admits several outputs in pi: {1 + 1 = 2,1 + 3 = 4,3 + 3 = 6,3 + 5 -= 8}.
Using equations 15, it is accepted if 62(i2, w) contains a pair (q,c(q,w)) with q E Fi.
Lemma 1 Let T = (Q, E, I, F, E, A, p) be a string-to-weight transducer, 7r E p q a path in T from the state p E Q to q E Q, and 7r' E p' q' a path from p' E Q to q' E Q both labeled with the input string w E E*.
Assume that the lengths of it and 7r' are greater than We also define A and p by: V(ii, t2) E h x A(ii, i2) = A2(i2)), V(fj,,,f2) E Fi x F2, P(fi,f2) -= (Pi(fi ), P2(f2)).
We have: Vw E A, 62(i2,w) = {(q0, c(qo,w)), • • • , (qm, c(q,n,w))1 .
Let B C A be the infinite set of strings w for which c(q0,w) = 0.
Since the number of subsets {(go, c(go, w)), • • • (qm, c(gm, w))}, w E B, is infinite, there exists], 0 <j < m, such that c(qj,w) be distinct for an infinite number of strings w E B.
By definition of the subset construction we have: (A(i1) + cr(71-1)) — (A(io) cr(71-0)) = c(qi,w).
Assume that w I > 112112 — 1.
Hence: (A(ii)+0(7r)) — (.\(io)±(7(4)) = c(qi,w).
Since c(I11) — cr(II0) E R(q0, qi), c(qi,w) E R(q0, qi) and C is finite.
There exists (u, v) E E* such that: ({q, q'} c SW, u), q E 61(q,v),q' E 81(qcv)) and 01(q, v, q) 01(q' ,v, q').
A subset 62(i2, uv&quot;) contains the pairs (q, c(q, uvk)) and (q', c(q', uvk)).
Consider (u, v) E (E*)2 and (q, q') E l(2112 such that: {q, q'} c I (I, u), q c 61(q, v), q' E (q' , v).
Then either lul > Assume that I u I > 1(21 12 - 1.
7r' from i' E Ito q'.
This number is bounded by the number of states of any subsequential transducer realizing S. So V(u, v) E (E*)2, 6(i, u) = 6(i, v) = (uRsv).
Pushing introduces an equivalence relation on minimal transducers: T Rp T' if p(T) -= p(T'), where p(T) (resp. p(T')) denotes the transducer obtained from T (resp.
T') by pushing.
(5, wR)w is also subsequential.'
Since Ti is trim there exists w E E* such that Si (ii, w) = q, so Si(Si, w) E F'.
6'(S2, 7.1)) E F'.
Thus, q E S2.
5.1.2).
Each hyperarc e ? E is a triple 54 e = ?T (e), h(e), f (e)?, where h(e) ? V is its head andT (e) ? V? is a vector of tail nodes.
Definition 2.
R, then ?1 ? i ? m, if ai ?a?i , then f (a1, ? ?
, ai, ? ?
, am) ? f (a1, ? ?
, a?i , ? ?
Definition 3.
If |e| = 0, then f (e) ? R is a constant and wecall h(e) a source vertex.
Definition 4.
The backward-star BS(v) of a vertex v is the set of incoming hyperarcs {e ? E | h(e) = v}.
A derivation D of a vertex v in a hyper graph H, its size |D| and its weight w(D) are recursively defined as follows: ? If e ? BS (v) with |e| = 0, then D = ?e, ??
is a derivation of v, its size |D| = 1, and its weight w(D) = f (e)().
If e ? BS (v) where |e| > 0 and Di is a derivation of Ti(e) for 1 ? i ? |e|, then D = ?e,D1 ? ?
?D|e|?
is a derivation of v, its size |D| = 1 + ?|e|i=1 |Di| and itsweight w(D) = f (e)(w(D1), . . .
,w(D|e|)).
The ordering on weights in R induces an ordering on derivations: D ? D? iff w(D) ? w(D?).
Definition 6.
The k-best derivationsproblem for hypergraphs, then, is to find D(t) given a hy pergraph ?V, E, t,R?.
Definition 7.
A derivation with back-pointers (dbp) D?
of v is a tuple ?e, j?
such that e ? BS(v), and j ? {1, 2, . . .
, k}|e|.
j|e|)?
?e,D j1 (T1(e)) ? ?
?D j|e| (T |e|(e))?
Accordingly, we extend the weight function w to dbps: w(D?) = w(D) if D?
D??
iff w(D?) ? w(D??).
Computationally, then, the k-best problem can bestated as follows: given a hypergraph H with arity a, com pute D?1(t), . . .
For example, the weighted CKY algorithm given a context-free grammar G = ?N,T, P, S ? in Chomsky Normal Form (CNF) and an input string w can be represented as a hypergraph of arity 2 as follows.Each item [X, i, j] is represented as a vertex v, corre sponding to the recognition of nonterminal X spanning w from positions i+1 through j. For each production rule X ? YZ in P and three free indices i < j < k, we have a hyperarc ?((Y, i, k), (Z, k, j)), (X, i, k), f ? corresponding tothe instantiation of the inference rule C???????
The graph projection of a hypergraph H = ?V, E, t,R? is a directed graph G = ?V, E??
where E?
= {(u, v) | ?e ? BS (v), u ? T (e)}.
For example, for CKY it is sufficient to generate all items [X, i, j] before all items [Y, i?, j?]
when j?
i? > j ? i (X and Y are arbitrary nonterminals).
Definition 9.
  Given a hypergraph H = ?V, E, t,R?, a hyperpath piv of destination v ? V is an acyclic minimal hypergraph Hpi = ?Vpi, Epi, v,R? such that 1.
Epi ? E. 2.
v ? Vpi = ?e?Epi (T (e) ? {h(e)}) 3.
An (A? ?.B?, i, j) (A? ?.B?, i, j) (B? .?, j, j) ? ?
Note that item (A ? ?.B?, i, j) appears twice (predict and complete).
1: procedure V??????(k) 2: for v ? V in topological order do 3: for e ? BS(v) do . for all incoming hyperarcs 4: D?1(v)?
min?(D?1(v), ?e, 1?)
4.1 Algorithm 0: na??ve.
Let r = |e|.
enumerate the kr derivations {?e, j1 ? ?
jr? | ?i, 1 ? ji ? k}.
Time: O(kr).
2.
Time: O(kr log(kr)) = O(rkr log k).
3.
Time: O(k).
It is initialized to {?e, 1?}.
Then ?e, j?
| 1 ? l ? |e|} (see Figure 4.2 for an illustration).
At each iteration, there are one E??????-M??
and |e| I?????
It is integrated into the Viterbialgorithm (Figure 3) simply by rewriting line 4 of to in voke the function M???(e, k): 4: D?(v) ? merge?k(D?(v),M???(e, k)) 4.3 Algorithm 2: combine merge?k into mult?k. We can further speed up both merge?k and mult?k by a similar idea.
That is, we initialize C to {?e, 1?
Algorithm 1).
| 1 ? l ? |e|} (again, as in Algorithm 1).
57 2 2 ? 0 ? ?
1 ? 1 2 4 (a) 2 2 ? ?
3 ? 0 1 ? ?
2 ? 1 2 4 (b) 2 2 ? ?
3 ? ?
4 0 1 2 ? ?
Here k = 3, ? is the numerical ?, and the monotonic function f is defined as f (a, b) = a + b. Italic numbers on the x and y axes are ai?s and b j?s, respectively.
We want to compute the top 3 results from f (ai, b j) with 1 ? i, j ? 3.
and?)
1: function M???(e, k) 2: cand ? {?e, 1?}
initialize the heap 3: p?
empty list . the result of mult?k 4: while |p| < k and |cand| > 0 do 5: A?????N???(cand,p, k) 6: return p 7: 8: procedure A?????N???(cand, p) 9: ?e, j?
E??????-M??(cand) 10: append ?e, j?
to p 11: for i?
1 . . .
j + bi 13: if j?i ? |D?(Ti(e))| and ?e, j??
< cand then 14: I?????(cand, ?e, j??)
1: procedure F???A??KB???(k) 2: for v ? V in topological order do 3: F???KB???(v, k) 4: 5: procedure F???KB???(v, k) 6: G??C?????????(v, k) . initialize the heap 7: while |D?(v)| < k and |cand[v]| > 0 do 8: A?????N???(cand[v], D?(v)) 9: 10: procedure G??C?????????(v, k) 11: temp?
{?e, 1?
| e ? BS (v)} 12: cand[v]?
k? is the global k 2: if |D?(v)| ? k then . kth derivation already computed?
5: G??C?????????(v, k?)
initialize the heap 6: append E??????-M??(cand[v]) to D?(v) . 1-best 7: while |D?(v)| < k and |cand[v]| > 0 do 8: ?e, j?
D?|D?(v)|(v) . last derivation 9: L???N???(cand[v], e, j, k?)
update the heap, adding the successors of last derivation 10: append E??????-M??(cand[v]) to D?(v) . get the next best derivation and delete it from the heap 11: 12: procedure L???N???(cand, e, j, k?)
13: for i?
1 . . .
j + bi 15: L???K??B???(Ti(e), j?i , k?)
recursively solve a sub-problem 16: if j?i ? |D?(Ti(e))| and ?e, j??
< cand then . if it exists and is not in heap yet 17: I?????(cand, ?e, j??)
4.3).
94.9%.
We tested Algorithm 2 for k = 2i, 3 ? i ? 10, and Algorithm 3 (offline algorithm) for k = 2i, 3 ? i ? 20.
A translation lexicon T can be represented as a sequence of t entries, where each entry is a pair of words: T ((xi,y1), • • • , (x, y)).
SIMR uses a fixed chain size k, 6 < k < 11.
If segments (Xi, • • ,X) align with segments (y1, .. • , yn ), then (0(1, • • • ,X,), (y1, • • • is an aligned block.
If, instead of the point in cell (H,e), there was a point in cell (G,f), the correct alignment for that region would still be ((G,H), (e,f)).
For instance, if the input contains (G,e), (H,e), and (H,f), then GSA adds the pairing (G,f).
8.
The kernel function must be symmetric [K(x, y) = K(y, x)] and positivesemidefinite.
Here, Oi(x) = 1 if word i occurs in document x.
X OWNS Y).
From the example in Figure 1, t0[1] = t2, t0[I0,1}] = It1, t2}, and t1.p = t0.
Let d(a) = an − a1 + 1 and l(a) be the length of a.
Therefore, d(a) = l(a).
The contiguous kernel matches the following substructures: It0[0], u0[0]}, It0[2], u0[1]}, It3[0], u2[0]}.
morphology, syntax, semantics).
append( [XlL1] ,L2, [XlL3] ) :-  append(L1,L2,L3).
: []~L,ST, w: (~ .
1i~1)] : -- APPEND[F: ~.t-~, 13: [~], W:  [~ ] .
A full evaluation of APPEND[w:(A B}] produces a set of three FSs: i F: 0, ": ID(A B), w: ~]  v F: <~A.
: ~(B) ,  w: <t~.
:  r~0,~:  (@ ?
294 sentenee(s(NP, VP)) --+ noun_phrase(Num,NP), verb_phrmse(Num,VV).
noun_phrasetNum , np(Det,Noun)) -+ determiner(Nnm,Det),  noun(Nnm,Nonn).
noun_phrase(singular,np(Name)) --~ name(Name).
verb..phrase(Num,vp(TV,NP)) ~ trans_verb(Num,TV),noun_phrasc(N1,NP).
determiner(Num,det(W)) --~ [W], is_determiner(W,Num).
noun(Num,n(l~oot)) --+[W], is_noun(W,Num,Root).
name(name(W)) --* [W], is_name(W).
trans_verb(Num,tv(Root)) ~ [W], is_trans(W,Num,Root).
(Figure 2) is_determiner(all, p ural).
PI4RASAL_CATEGORY = S V NP V VP.
S = [NP: NP[AaR: [~NUM], vP: VP[*oR: [g3]].
[DET: DET[AGR: Z]NUM]I [NAME: PN] NP = Vo, .
:  N[,oR: ~ ]  / V L *aR: SG LAOR: []~] J VP = [V: TV[AoR: [~INUM], NP: NP: AGR: [1~]].
LEXICAL_CATEGORY : DET V N V PN V V. DET : ALL V EVERY V A V THE.
ALL : [WORD: all, AOR: PL].
N : MAN V WOMAN.
MAN = [WORD:man, AGR:SG] V [WORD:men, *GR:Pq.
MARY = [WORD: Mary].
L,KE = [WORD:,ike,, ,on:SG] V [WORD:,ike, hoR:PL].
SENTENCE == NOUN_PHRASE[sTm[Na: ~ LIST, C-STR:[n~] VERB_PHRASE[sTRINO: ~ ,  C-STm  [~J APPEND [P :~,  .
: ~ ,  w:~ ] NOUN_PHRASE =- [STmNO: ~.___~,  C-STR: NP[DI~T: [~], ,OUN: IK]] : -- DETERMINER[STmNQ: ~ ,  C-STm ~]] NOUN[sTmNO: ~ ,  c-s~.rm [Ell APPEND [F :~B:~,  w : ~  ] V [STR INO:~,  C-STR: PN[NAME: ~]  ]  : -- NAMErsrmNG: ~ ,  C-STm ~ ] VERB-PHRASE : [STRING: ~ ,  C-STR: VP[v: [~TV, NP: ~]  : -- TRANS_VERB[sTRINO: ~ ,  C-ST,R: [~1] NOUN_PHRASE[sTmNO: ~ ,  C-STR: ~ ] , APPEND [~:~ B: ~ ,  w : ~  ] LEXICAL-RULE = [STRING: ([~]), C-STR: [WORD: [~]].
NOUN = LEXlCAL.RULE[c-STR: N].
TRANS_VERB = LEXICAL-RULE[C-STR: TV].
(1 )  SENTENCE[sTRING: (Mary likes all men)] (2) SENTENCE NP: NP[NAME: MARY] Iv: LIKE ALL, o  : MANIJ (3) SENTENCE "sTR,NG: (lEMony []like, []a, Rime.)
[s.,.N: [..E.,,o: I-~-al] ] (7) HEAD_FP .--- L D~rps: [.EA~o"rp.
(8) SUBCAT-FP .---- ?
(9) CH-CO-FP ---- SYI,I: SU~BCAT: 01PH RASAL.SIGN [PHON" hJh_~.a~ DTRS: [HEAD-DTR: : . ]
(11) SIGN L,K   DrRS MAN,I   ]]J 1COMP-DTF~S: { [COMP-Dr.s: (ALL) ) (12) PItRASAL-SI(;N "PHON: (l-~"Mary" .
[2]( .[~]"likes".
[~]("all" "men"))) IIEAD-DTn: PHRASAL-SIGN |ItNAD-DTR: LEXlCAL-SIGN / /ftNAD: [DTRS: SYN: SUBCAT: !
L [ :)TRS: LCOMP-D!
"itS: {[~ PHRASAL.
:SIGN[PltON:[~] ...]) COMP-OrRs: ( I~PHRASAL_SIGN ton 1] y.: F  .Ex: Lsu cA : UM  ] Lexical entr ies AI,L = DET[sYN IIINAD: [LEX:"aII", NUM:pl]]] , MAN =: NOUN[~YN:IIn~A ): [bEX:"man", NUM:sg]V ]].
", NUM:pl] MARY = PNOUN [SYN:I),BAD: (Lt~X:" mary", NUM:sg]]].
LIKE = TRANS A (3RD-S GISYN: ha,~Ao:[u~x:" likes"]]] V).
3RD-SG :~ [sYN:rHI;AD:[PFmSON:3, ug:sg]]].
~RANS = [SYN: SVr~CAT:(ISYN:[m:AD:ICASI-:acc]]] )] .
Therefore, we focus on unmatched pairs of the form {(r, w) r E R − W, w E W − R}.
The probability of a given state, P(s), is referred to as α(s).
1, Fig.
A tree sequence translation rule r is a pair of aligned tree sequences r =< TS f j , two tree sequences, satisfying the following condition: `d (i, j) E A : i1 < i < i2 H j1 < j < j2 .
3).
.
.
Estimation Note that P(a  |A, o3) = 0 if A =� a.
Each pair was presented in random orientation (i.e., either as A → a or as a → A), and the labels included “simpler”, “more complex”, “equal”, “unrelated”, and “?” (“hard to judge”).
Finally, some examples of simplifications found by our methods: “stands for” → “is the same as”, “indigenous” → “native”, “permitted” → “allowed”, “concealed” → “hidden”, “collapsed” → “fell down”, “annually” → “every year”.
The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .
.
.
,S .
.
[NP , the , squabbling, [PP, within , [NP ,the, Organization, [PP , of , [NP , Petroleum, Exporting, Countries]]]]] .
[NP control] [B,[5,[NP,Mideast,politics],have,[VP,calmed, [PP,down]Thand,[S,INP,the,squabbling,DP,within, [NP,the,Organization,[PP,of,[14P,Petroleum,Exporting, Countries]]]]],[VP,seems,[PP,under,[NP,control]], [PP,for,now]]]],.].
.
.).
[PP , of , [NP,Phillips,Petroletuniii] ; c. the oboist, Heinz Holliger d. [nu), [NP,the,oboist] , [NP,Heinz,Holliger]] .
.
.
.
.
(45) 7.
.
.
.
.
.
), 160 titles (Dr., Mr., etc.
3.
238].)
Note that ((wi, ti) .
Let (W, T(k)), k = 1 .
.
.
.
.
N, only the quantity 0(w77-0)) = p(w, T(0) / N-,N1 ' p k/w7 T(k)) and its derivation(W,T(k)).
We then scan all the derivations in the &quot;development set&quot; and, for each occurrence of the elementary event (y(m) , x(m)) in derivation(W,T(k)) we accumulate the value 0(W, T(k)) in the C(m)(y(m), x(m)) counter to be used in the next iteration.
<patient> kidnapped).
Formally, the score is calculated as the following: where E is the set of all event pairs, B(i, j) is how many times we classified events i and j as before in Gigaword, and D(i, j) _ |B(i, j) − B(j,i)|.
For example, one cluster contains {“Cory does”, “Ben saw”, “I can’t lose”, .....}.
2008).
(in preparation).
2.
3.
4.
or ?subj?verb?obj?.
[E1:phrase] VP [E2:phrase] (See above.)
[E1:phrase] [wh-word] [E2:phrase] (See above.)
Src [distance] [x] [distance] Op, where x ? {by, of, from, for, between, among, and, have, be, will, not, ], ?, . . .
Toensure coherent assignments, we add equality con straints ?i, Oi + O?i = 1.
Relation variables and weights For each link relation, we add two variables Li,j and L?i,j , and a constraint Li,j + L?i,j = 1.
1) We define six coefficients cx and c?x, where x ? {O,S,L} to modify a group of weights as follows.
That is, ?i, wAi := cA.
14Available at http://mallet.cs.umass.edu 15co = 2.5, c?o = 1.0, cs = 1.5, c?s = 1.0, cL = 2.5, c?L = 2.5, cA = 0.2.
Figure 1).
, (CONDITION 1 DIRECTIVE 2 )) ⇒ (if TEAM 1 player UNUM 2 has the ball, DIR 3 .
, ((bowner TEAM 1 {UNUM 2 }) DIR 3 )) ⇒ (if our player UNUM 1 has the ball, DIR 2 .
, ((bowner our {UNUM 1 }) DIR 2 )) ⇒ (if our player 4 has the ball, DIRECTIVE 1 .
(pt our 4)).
TEAM —* (our, our), UNUM —* (4, 4).
A greedy procedure is employed that repeatedly removes a link a E a that would maximize v(a) — v(a\{a}) > 0, until v(a) cannot be further reduced.
figure 2) roughly proceeds its folk)ws: S tep  ].)
S tep  2) S tep  3) Step 4) Step 5) t:r:ee :-: compute  t ree(words  in  w indow) loop tree ::: compute  conc(~ptua] d i s tanco( t ree) concept  -= se].occt concept  w i th  llighest-._weigth(tree) J.f concept  :: null.
% w:30 II Cove,-.
For a document with b potential boundaries, b steps of divisive clustering generates {D(1), ...,D(b+1)} and {bD(2), oD(b+1)} (see figure 6 and 7).
Table 1 presents the corpus statistics. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6.
B(r,b) randomly selects b boundaries as real boundaries.
The terms p(iniss) awl p(fa) in equation 6 corresponds to p(samelk) and p(difflk) = 1 -p(samelk).
Equation 7, 8 and 9 gives the general form of p(samelk), B(r,?) and Berm, respectively'.
H94(e.7.) uses the recommended parameters k = 6, w = 20.
Experimental result (table 3) shows H94(,,d) and H94(,,) are more accurate than H94(j,,,).
[Church 1988]).
5).
.
.
21(4):543–565.
Com- 22(4):531–53.
22(3):377–404.
❳
1 1.
3.
2.
As can be seen in the concordances in Table 3, for K=10, the vector is <1, 1, 0, 1, 1,0, 1, 0, 0, 0>.
4.
In contrast, for fisheries ~ lections, prob ( V f, V p ) = O, prob(Vf) =0.5 and prob(Vp) = 0.4.
5.
Object1: (type, chihuahua), (size, small), (calour, black) ?
Object2: (type, chihuahua), (size, large), (colour, white) * Object3: (type, siamese-cat), (size, small), (colour, black) In other words, r = 0bject l  and (7 = {Objest2, Object3}.
Assume that P = {type, colour, size, .
Computational Intelligence, 7(4), 1991.
Computational Intelligence, 7(4), 1991.
preprocessing, ?
The operator =!!
E.g., a template "&NP" could be declared to contain the alternatives ((N)), ((A) (N)) ((DET) .
(N)) ((DET) (A) (N)), etc.
nominal heads: @SUB J, @OBJ, @I-OBJ, @PCOMPL-S (subj.
), @PCOMPL- O (obj.
), @ADVL (adverbial) .... ?
the premodifiers AN>, DN>, NN>, GN> (genitival).
Thus, a mapping statement is a triple <morphologi- cal feature(s), context condition(s), syntactic func- tion(s)>.
Consider Figure 1(a).
Therefore c(x) > bound.
(2006b).
2.
(am a teacher)
I writed a letter (wrote)
2.
3.
Top 3 examples: 1.
2.
3.
Top 3 examples: 1.
2.
3.
5 Evaluation.
10).
Following Berger et al.  , we approximate p(h,t) , the joint distribution of contexts and tags, by the product of r, (h), the empirical distribution of histories h, and the conditional distribution p(t I h): p(h,t) p(h) p(t I h) .
Table 4).
 ).
Given W = {w[1], , w[n]l, a set of nouns: for i and j = lion, with i <j v[i, j) = sim(w[i], w[j]) c[i, j] = the most informative subsumer for w[i] and w[j] fork = 1 to num_senses(w[i]) if c[i, j] is an ancestor of sense[i, increment support[i, k] by v[i, j] fork' = 1 to num_senses(wW) if c[i, j] is an ancestor of sense[i, k'] increment support[j, k'] by v Ei, increment normalization[i] by v[i, j] increment normalization[j] by vii, j] This algorithm considers the words in W pairwise, avoiding the tractability problems in considering all possible combinations of senses for the group (0 (ma) if each word had m senses).
total active = 0 active = 1 active > 1 unannotated corpus ] ?
), average length (???
More specif- ically, for a tree T , Score(t) = log(PPCFG-GP(T )) + ?
tn) = ?n i=1 P(ti |ti?1, ti?2) with appropriate treatment of boundaries.
2 3824 25.81% 74.03% ?
5 1789 9.66% 59.64% ?
Charniak, E., & Caroll, G.  .
Copestake, A., Lascarides, A., & Flickinger, D.  .
Oepen, S., & Callmeier, U.
, n} and a set of clusters, K = {ki|1, . . .
For a perfectly homogenous solution, this normalization, H(C|K) H(C) , equals 0.
Thus, to adhere to the conventionof 1 being desirable and 0 undesirable, we define ho mogeneity as: h = { 1 if H(C,K) = 0 1?
In the perfectly complete case, H(K|C) = 0.
 .
Q0(C,K) = H(C|K)+ 1 n |K| ? k=1 log (h(k) + |C| ? 1 |C| ? 1 )where C is the target partition, K is the hypothe sized partition and h(k) is the size of cluster k. Q2(C,K) = 1 n ?|C| c=1 log (h(c)+|C|?1 |C|?1 ) Q0(C,K) We believe that V-measure provides two significantadvantages over Q0 that make it a more useful diag nostic tool.
Another information-based clustering measure is variation of information  , V I(C,K) = H(C|K)+H(K|C).
classes; |Cnoise| < |C| ? ?
Error probability; ? = ?1 + ?2 + ?3.
classes and ?3.
|Ku|: 10 values: 2, 3,.
, 11 ? |Knoise|: 7 values: 0, 1,.
, 6 ? |Cnoise|: 7 values: 0, 1,.
, 6 ? ?1: 4 values: 0, 0.033, 0.066, 0.1 ? ?2: 4 values: 0, 0.066, 0.133, 0.2 ? ?3: 4 values: 0, 0.066, 0.133, 0.2 We evaluated the behavior of V-Measure, Rand, Mirkin, Fowlkes-Mallows, Gamma, Jaccard, VI,Q0, F-Measure against the desirable properties P1 P74.
This is due to an increased homogeneity (H(C|K)H(C) ) and a relatively stable completeness (H(K|C)H(K) ).
, nk.
, Tk.
40-41).
• • Tk.
(1992a).
.
.
, fn.
To capture this notion, for any word pair (ei, fj), if a link l(ei,, fj,) exists where i − 2 ≤ i' ≤ i + 2 and j − 2 ≤ j' ≤ j + 2, then we say that the feature fta(i−i', j−j', ei,) is active for this context.
For example, York-based was split into 3 segments: (1) York, (2) - and (3) based.
Acronyms used: D -syntactic dependencies, P - predicate, A - argument, I - identification, C -classification.
 ).
Lines (c) and (d), however, do just that.
2 (facing a shiftreduce decision).
3).
Dirichlet(?, . . .
, ?) [draw component probabilities] For each component z ? {1, . . .
,K}: ??z ? G0 [draw component parameters] For each data point i ? {1, . . .
, n}: ?zi ? Multinomial(?)
[choose component] ?xi ? F (?;?zi) [generate data point]The model has K components whose prior dis tribution is specified by ? = (?1, . . .
, ?K).
For each mixture component z ? {1, . . .
Each component (cluster) z has multinomial parameters ?z which specifies adistribution F (?;?z) over words.
GEM(?)
to mean that ? = (?1, ?2, . . .
, where each uz ? Beta(1, ?).
The stick-breaking weights ? are then defined in terms of the stick proportions: ?z = uz ? z?<z (1 ? uz?).
GEM(1).
length stick (Figure 1).
GEM(?)
[draw component probabilities] For each component z ? {1, 2, . . .
}: ??z ? G0 [draw component parameters] For each data point i ? {1, . . .
, n}: ?zi ? Multinomial(?)
[choose component] ?xi ? F (?;?zi) [generate data point xn] 2.3 HDP-HMM.
, Am, (??(A1), . . .
,??(Am)) (3) ? Dirichlet ( ???(A1), . . .
, ???(Am) ) , where ?(A) = ? k?A ?k. 1 The resulting ??
HDP-HMM ? ?
GEM(?)
[draw top-level state weights] For each state z ? {1, 2, . . .
}: ??Ez ? Dirichlet(?)
[draw emission parameters] ??Tz ? DP(?
?, ?) [draw transition parameters] For each time step i ? {1, . . .
GEM(?)
[draw top-level symbol weights] For each grammar symbol z ? {1, 2, . . .
}: ??Tz ? Dirichlet(?
T ) [draw rule type parameters] ??Ez ? Dirichlet(?
E) [draw emission parameters] ??Bz ? DP(?
B ,??T ) [draw binary production parameters] For each node i in the parse tree: ?ti ? Multinomial(?Tzi) [choose rule type] ?If ti = EMISSION: ??xi ? Multinomial(?Ezi) [emit terminal symbol] ?If ti = BINARY-PRODUCTION: ??(zL(i), zR(i)) ? Multinomial(?
GEM(?)
??T ?Bz ? DP(??
HDP-PCFG for grammar refinement (HDP-PCFG-GR) For each symbol s ? S: ??s ? GEM(?)
[draw subsymbol weights] ?For each subsymbol z ? {1, 2, . . .
}: ???Tsz ? Dirichlet(?
T ) [draw rule type parameters] ???Esz ? Dirichlet(?
E(s)) [draw emission parameters] ???usz ? Dirichlet(?
u) [unary symbol productions] ???bsz ? Dirichlet(?
b) [binary symbol productions] ??For each child symbol s?
S: ????Uszs?
DP(?
U ,?s?) [unary subsymbol prod.]
??For each pair of children symbols (s?, s??)
S ? S: ????Bszs?s??
DP(?
B ,?s??
T s??)
[binary subsymbol] For each node i in the parse tree: ?ti ? Multinomial(?Tsizi) [choose rule type] ?If ti = EMISSION: ??xi ? Multinomial(?Esizi) [emit terminal symbol] ?If ti = UNARY-PRODUCTION: ??sL(i) ? Multinomial(?
u sizi) [generate child symbol] ??zL(i) ? Multinomial(?
U sizisL(i)) [child subsymbol] ?If ti = BINARY-PRODUCTION: ??(sL(i), sR(i)) ? Mult(?sizi) [children symbols] ??(zL(i), zR(i)) ? Mult(?
B sizisL(i)sR(i)) [subsymbols] 2.6 Variational inference.
def = argmin q?Q KL(q(?, z)||p(?, z | x)), (4) where Q is a tractable subset of distributions.
K? z=1 q(?Tz )q(?
E z )q(?
B z ) } .
(5) We further restrict q(?Tz ), q(?
E z ), q(?
We constrain q(?)
693 counts: ?Bz (zl, zr) = C(z ? zl zr) C(z ? ??)
(6) For the variational HDP-PCFG, the optimal q(?)
B z ;?
B?zl?zr ) e?(C(z???)+?B) , (9) where ?(?)
Top-level symbol probabilities q(?): Recall that we restrict q(?)
= ???(?), so optimizing ? is equivalent to finding a single best ??.
Unlike q(?)
is as follows: L(??)
= logGEM(??;?)+ (10) K?
z=1 Eq[logDirichlet(?Bz ;?
| ?) is mi nor.
= 1, ?T = 1 (uniform distribution over unar ies versus binaries), ?E = 1 (uniform distribution over terminal words), ?u(s) = ?b(s) = 1N(s) , whereN(s) is the number of different unary (binary) righthand sides of rules with left-hand side s in the treebank grammar.
.
.
man N ~.. phty~ j J - y ,  .% (b) The ill __ ~ / .~dachshund It) gol f . )
(a) Ihe [nice of Ihc sRu:k 1%11 I)T NN IN I)1 NN VIII) (b) tile price uf .
Pr(link presences and absences I words, tags) (1) I-[ I t om(i + 1), twom(i + 2)).
I  ]  I two,.d(i), two,dO)) (e) l< i<n l <_i,j <n lv(tword(i) ] tword(i + 1), tword(i + 2)) ~ l,(tag(i) I tag(i + 1), tag(i + 2)).
P,(word(i) I tag(/)) (a) Pr(words, tags, links) c~ Pr(words, tags, preferences) =/  r (words ,  tags).
Pr(preferences ] words, t~gs) (4) ]-I l,.
(twom(i) I two d(i + 1), t o,d(i + 2)).
H I two,.d(i)) 1 <i<n t< i<n / 1 +#r ight -k ids( i )  ~ Pv(words, t+gs, links)= I I  { 1-[ P,.
If i and j are tokens, then tword(i) represents he pair (tag(i), word(i)), and L,j C {0, 1} i~ ~ ill" i is the p~m:nt of j. exactly one parent.
(c < 0 in(hxes l(ft children,) Fhis may bc thought o[" as a, non-linca.r l;rigrrmt model, where each t;agg(d woM is genera, l,ed 1)ascd on the l)a.r (nl, 1,~gg(:d wor(l and ;t sistxr tag.
lhe links in the parse serve Lo pick o,tt; t, he r(Jev;mt t,rit:;t+a,n~s, and a.rc chosen 1;o g(t; l,rigrams t, lml, ot)l, imiz(~ t, hc glohM t,a,gging.
ltt;tl; the liuks also ha.t)l)en t;o ;ulnot,;:d,(.
useful setnant;ic rela, tions is, from this t>crsl)ective, quil.e a(-cidcn{,a,l.
Note that  the revised v(,rsiol~ of ulo(h:t A uses prol)a, bilit, ics / "@ink  to chihl I child, I)arenl,, closer-(hihh:en), where n.)(le] (; uses l v( l ink 1,o child ] parent,, eloscr-chil(hen).
lhis is I)c(:;,.t~se model A assunw.s 1,lu~l, I,h(.
(:hild was i)reviously gencrat, ed I)y a lin(;a,r l)roc(ss, aml all t;hal, is nec- ess+uy is t,o l i .
Model (~ a, cl,ually g(,n(;ral,es t, he chihl in the process o[ l iuking to il,.
y  l)msing ;dg;oril, hnl: ~ novel dytmJni(.-l)rogr;mJndng m(.l,hod 1,o assetnhle l, he mosl, l>rol)a,ble+ i)a.rse From the bet,- tom Ul).
CI, :Y tcquit,;s ()(?,.
:t~ ~) t.i,,,, +utd O(,,.
:.~) sp+..,;, where n is the lenglih of 1,he s(mtcn(:c and ,s is a,n Upl)(;r bouiM on signal;ures 1)er subsl;ring.
l lowew, r, if a. o:/tst,il, cnt s ?
, %i.y -  - ....< ?
~o d I a (loll s}fl!Slm,,) Ji, b(_right subspan)  , I " ig l l l e 5 The  ass,:,mbly of a span c from two sm:LIIcr spaus (a a,nd b) ~tml a cove.ring link.
lca+sl, k of t,hcsc for a, s/ ibst;rhl,~ Of le..e;IJI k, whence Ge houn(t ,,~ :: t: = ~(u) ,  giving: ;i l, illm COml)lex- it,y of t l ( , s ) .
((~ollins, 19.%)uses t,his t~(,.
"-)a, lgo ril, lml (lireclJy (t,ogel,h(r wil, h l)runiug;).
\% I)rOl)OSe a,u aJl,ermtl, ive a,I)l)roa.
(h l, ha, I, I)re serves the OOP) hound, hls~ca(t of analyzing sul) st,ri.gs as lcxical t, rees that, will be l inked t, ogoJ,her in(,o la, rgcr h~xica, I l, rees, t, lic I)arsc, r wil l  ana, lyze I,hcln a,s uon-const,itm.nt, sl)a:n.s t;haJ, will he cou cat;cm~t,ed into larger spans.
A Sl)a,n cousisl;s el > :~ ;.t.i{.
:e,l<; words; l,;~gs Ior a,ll these words cx (:el)l, possibly the last; ;t list, of all de l .
No cychs, n,ull, iph l)a, rcnts, or (,ossi,tg liul.
im, uilAon is I, haJ, L]le.
117 sl)an a, ctMs on t,he saanc word i l;[ha, l, st,all,s span b, t,h(;n law I)a,rs(er tries l;o c(>ml>ine I,hc l, wo spans I)y cove, red - ( - (mvatcnat ion  (l"igur(; 5).
Any tlepcudcncy parse ca, n I)c built  Ill:) hy eovered-coitca, tena, i;ion.
The <o,,ered-(:oncaJ,cnal,ion of (+ a.nd b, Iornfing (, is 1)arrcd unh;ss it, tricots terra, in simple test;s: ?
343 H Pr(tword(i) I tword(i + 1), tword(i + 2)).
H Pr(i has peels that j satisfies I tword(i), tword(j)) (6) k<_i<g k<i,j<g with i,j linked H Pr(Lij ItW?rd(i) tword(j), tag(next-closest-kid(i))).
H Pr(LiJ ItW?rd(i) tword(j),...) (7) k<_i,j<g with i,j linked k<i<(, ( j<k  or ~.<j) ?
,~  r 8 .1S~, ,a .~ 47.3 ~l  r~ sA rr.~ I ~  1 ~ ~ - L 4 0 : , < ~ A _  - ~ ~_ l~d)le 2: ]{.csults of preli ininary (,Xl)crimcnts: Per.
calne the rchi(:i;ail(;e of tile [)areiit to a(:(-el)t uiore children.)
ll, e fe l e l l ces Ezra Bla.
(:k, Fred ,lelinck, et a.1.
[(enne.th W. (3mr(:h. 1988.
Asso(:i~Lti(,n for (~omput~Ltimml l,inguistics, Morristowu, N.I.
I!red .felinck.
l,Mferty, aml Robert 1, Mercer.
I]~si(: niethods el prob~dfilistic context-fre(,.
(7omputcr ?pccch .rid Language, 6.
]ohu t,~Lfferty, I)~ufiel Sle~ttor, ~uid I)~vy [cmperley.
Igor A, Mel(:uk.
Yves S(:ha.bes.
If it can be shown that p(v  |c', r), for some hypernym c' of c, is a reasonable approximation of p(v  |c, r), then we have a way of estimating p(c  |v, r).
(We ignore the probability p(v  |c', r) and consider the probabilities p(v  |ci, r) only.)
(In fact, p(v  |c, r) is equal to p(v  |c, r) when c is a leaf node.)
Definition 1 A lexical conceptual structure (LCS) is a modified version of the representation proposed by Jackendoff (1983, 1990) that conforms to the following structural form: [T(X) X ([T(W) Wt], [T(Zq) Ztl] "" [T(Z,,) Ztn] [T(Q,) QI] - "  [T(Q,,,) Qm])] This corresponds to the tree-like representation shown in Figure 2, in which (1) X is the logical head; (2) W is the logical subject; (3) Z~... Z~ are the logical arguments; and (4) Q~ ... Q~m are the logical modifiers.
1 I JOHN TOLo c HAPPILY ?
601 Computational Linguistics Volume 20, Number 4 ( { ) x /  ... , l Position ) I (   [ x 7.
]; and (4) Q = HAPPILY ~ Q ~--~[ADV happily].
Q X (c) Demofional Divergence CLCS: Syntax: I Y-MAX X X-MAX I Z (d) Structural Divergence CLCS: ~ Y-M~0C Y-MAX O X-MAX IN X R I Z (e) Conflational Divergence CLCS: Syntax: / I X-MAX I x Q Figure 9 Translation mappings for cases in which G?~ default positions are overridden.
The syntactic structures and corresponding CLCS are shown here: (12) [C-MAX [I--MAX IN-MAX I] [V-MAX [V like] [N-MAX Mary]]]] [State BEIdent ([Thing II, [Position aTIdent ([Thing I], [Thing MARY])], [Manner LIKINGLY])] [C-MAX [I-MAX IN-MAX Maria] [V-MAX IV me gusta]l]] 1?
608 Bonnie J. Dorr Machine Translation Divergences The general solution to thematic divergence is diagrammed as follows: (14) RLCS 1: [T(X,) X ([T(W) W], [r(z,) Z] [T(Q) Q])] RLCS 2: [r(x,) X ([r(w) :INT W], [T(Z) :EXT Z] [T(Q) QI)] Trans la t ion :  [Y-MAX [Y-MAX W [X-MAX X Z]]  Q] [T(X) X  ([T(W) W  ] ,  [T(Z) Z   ]  [T(Q) Q])] [Y-MAX [Y-MAX Z [X-MAX X Wll QI This assumes that there is only one external argument and zero or more internal arguments.
The general solution to promotional divergence is diagrammed as follows: (18) RLCS 1: [T(Q) Q] RLCS 2: [T(Q) :PROMOTE Q] Translation: [Y-MAX [Y-MAX W [X--MAX X Z]] Q] [T(X) X ([T(W) W], [T(Z) Z] [T(Q) Q])] [Y-MAX [Y-MAX W [X-MAX Q [ ... X Z]]]] 4.3 Demotional Divergence Demotional divergence is characterized by the demotion (placement "lower down") of a logical head into an internal argument position (or vice versa), as shown in Figure 9c.
Z 4:> X Figure 9(c) shows the revised mapping.
The general solution to demotional divergence is diagrammed as follows: (22) RLCS 1: [v(x) X ([T(W) W], [T(Z) Z] [T(Q) Q])] RLCS 2: [T(X) X ([T(W) W], [T(Z) :DEMOTE Z] [T(Q) Q])] Translation: [Y-MAX [Y-MAX W IX-MAX X Z]] Q] [v(x) X ([T(W/ W], [T(Z) Z] [T(Q/ Q])l [Y--MAX [Y-MAX W IX-MAX Z] [ ... X QI]] 4.4 Structural Divergence Structural divergence differs from the last three divergence types in that it does not alter the positions used in the ~?T4 mapping, but it changes the nature of the relation between the different positions (i.e., the "4=~" correspondence).
The general solution to structural divergence is diagrammed as follows: (25) RLCS 1: [T(X,) X ([T(W) W], [T(R) R ([T(Z) * Z])] [r(Q,) Q])] RLCS 2: [r(x,) X ([r(w,) W], [r(R) * R ([r(z) Z])] [T(Q) Q])] Translation: [Y-MAX [Y-MAX [r(x,) X ([T(W) [Y-MAX [Y-MAX W [X-MAX X Z]] Q] W], [T(Z) Z] [T(Q) Q])] W [X-MAX X [ ... R Z]]] Q] Note that the logical argument R is associated with a * marker in the RLCS of the target language, but not in the RLCS of the source language.
The general solution to conflational divergence is diagrammed as follows: (28) RLCS 1: [~(x,) X ([T(W) W], [T(Z) * Z] [T(Q) Q])] RLCS 2: [T(X) X ([z(w) W], [T(Z) :CONFLATED Z] [T(Q) Q])] Trans lat ion:  [Y-MAX [Y-MAX W [X-MAX X Z]] Q] [T(X~ X ([T(W/ W], [T(Z} Z] [r;Qq Q])] [Y-MAX [Y-MAX W [X-MAX XI[ QI Note that the logical argument Z is associated with a :CONFLATED marker in the RLCS of the target language, but not in the RLCS of the source language.
Thus, categorial divergence is formally described as follows: a lexical-semantic type T(~) is related to a syntactic category CAT(~), where CAT(~) ~ CST4(T(~)).
(30) (i) (ii) Lexical entry for be: [State BEIdent ([Thing W], [Position aTIdent ([Thing W], [Property Y])])] Lexical entry for haben: [State BEIdent ([Thing W], [Position aTIdent ([Thing W], [Property :CAT(N) Y])])] Because the English entry does not contain a :CAT marker in the position correspond- ing to [Property Y], this constituent is realized in the syntactic structure as C$/~(Property) Adjective (i.e., hungry).
615 Computational Linguistics Volume 20, Number 4 The general solution to categorial divergence is diagrammed as follows: (31) RLCS 1: [T(X,) X ([T(W) W], IT(Z,) :Z] [T(Q) Q])] RLCS 2: It(x,) X ([7(w) W], IT(Z,) (:CAT 6) Z] [~(Q,) Q])] Translation: [Y-MAX [Y-MAX W IX-MAX X Z]] Q] [T(X) X ([T(W) W], [T(Z) Z] [T(Q) Q])I [Y-MAX [Y-MAX W [X-MAX X a]] QI where CAT(Z) = &MAX.
X 4=> X 1.1 Q ~ Z; Z 1 4=~ Z.
1.2 Z ~ Q; Q 4=~ Q.
1.3 Q <~ Z; Z ~ Q.
X 2.1 X ~ 4=~ Z; Z ??
2.2 X ~Z;Z  ~<=~Q.
617 Computational Linguistics Volume 20, Number 4 (a) Case 1.1 CLCS: X ~  Y-MAX] Y-MAX 71 X-MAX IN x Q (b) Case 1.2 CLCS: Syntax: Y-MAX I   Y-MAX Z / I X-MAX I x (e) Case 2.3 CLCS: Syntax: IN Y-MAX X / I Z~, X-MAX t ?
2.3 X ~ Q; Z 4~ Z.
Z ~X 3.1 X ~Z;Q ~Z.
3.2 x <=~ Z; Q ~=~ Q.
3.3 X ~ Q; Q <=~ Z.
3.4 X <=~ Q; Q <=~ Q.
For example, (3, 5, 4) is a permuted sequence while (2, 5) is not.
For instance, the permuted sequence (2, 3, 5, 4) is binarizable (with two possible binarization patterns) while (2, 4,1, 3) is not (see Figure 3).
The (unique) binarization tree bi(a) for a binarizable permuted sequence a is recursively defined as follows: For example, the binarization tree for (2, 3, 5, 4) is [[2, 3], (5, 4)], which corresponds to the binarization pattern in Figure 3(a).
Base case: n = 1, trivial.
Let c = c2).
So, (b, right of the rightmost binarizable split (b; c), which is a contradiction.
For instance, the rule e[—&quot;&quot; +ed ?
For example: [booked (ll VBD VBN)] Vo [book (NN VB)] e[—ed +&quot;&quot; ?
(NN VB) —(11 VBD VBN)] [advisable (JJ)] Vi [advise (NN VB)] el—able +&quot;e&quot; ?
We propose that the event Rij E T has some prior probability P(Rij E T), and P(Rij E T) + P(Rij E� T) = 1.
For any instance x E X from a bag X E X, let φ(x) be the (implicit) feature vector representation of x.
.
P(wja) = C(w, a)/C(a).
“has acquired”).
 .
 .
Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f |q) Pr(f) ].
The coverage of u by v is measured by: Cover(u, v) = ? f?F u ?F v w u (f) ? f?F u w u (f) The average can be arithmetic or harmonic: WeedsA(u, v) = 1 2 [Cover(u, v) + Cover(v, u)] WeedsH(u, v) = 2 ? Cover(u, v) ? Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al also used pmi for feature weights.
For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?.
  propose a directional measure for learning hyponymy between twowords, ?l? r?, by giving more weight to the cov erage of the features of l by r (with ? > 1 2 ): WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l) When ?=1, this measure degenerates into Cover(l, r), termed Precision(l, r).
With 850 Precision(l, r) we obtain a ?soft?
851 2.
3.
as the variable are: ?losing X?, ?X play?
and ?X play safe?.
X solve?, ?X find solution?
Xsolve?, ?solution to Y ? solve Y ? and ?find solu tion to Y ? solve Y ?.
X meet?
For example, ?slam X ? attack X?
7 = ~: $ X /   / /  / node / X  / n I I   - - -X - - - t 3" = S /   3 / ~~vithout IX   t - - /   - - / - -x - - / / +- - FiKure 2.1 The tree t dominnted by X in 3 is excised, ~ is inserted at the node n in "7 and the tree t is attached to the foot node (lab*lled X) of ~, i.e., ~ is inserted or adjoined to the node n in 3 pushing t downwards, Note that ~ljoinmg is not a suJmtitutioa operation.
We will now illustrate p,1~ (b) and (e).
Example 2.2: Let G J (I,A) where !
~t = ~t  : 5 I e $ T I   I n T t S I   I l b  I b S T Let us look s t  some der tv t t lons  tn G. "TO : ~ : Se I e 32 = S a/T / I / n S~= I I I b ?
L(G), the string language of G is L-- { , .eb .
/ Q>o } which is a context-free language.
F.~xample 2.3: Let G ,m (I,A) where $ I @ #t = #= = S T I   I m T a S II II I I   I I b S c b T c 8,3 The precise definition of L(G) is as follows: L(G) =- L t =.
be u TAG with I~  constraints where I: a I t S C~) / ~t s S (B2) I I a b 84 s (~t )  s (~=) I   I I  I a S (?~) (?~) S h In a t no anxiliary trees can be adjoined to the root node.
Thus, given "T: ~= S /  node n I k (C) I / I / I I The resultant tree "7 is k (C  ) / / / / / (C) q, I S / / / / k CC) / / - - - /   - - - / / A (C) / / - - .
Example 2.5: Let G == (I,A) be a TAG with local constraints where : Of -- A:  8= S (~) / I / I a S /1 /1 h I ?
= S (~) I I I I a S I I I I b I c S (?)
I S Adjoining ~ to the ceuter S node (the only node at which adjunction can be made) we have " I   :am S (~) I I I I ,~ ..~j" (~,) ,  / I  " t a S ~ ~ / I t ; /1 / b I ?
85 "~t = Ot  = S / ~ .
VP /   l DET ~1 V IP I I I I I I I I ~hn g i r l  I DET I tm I I n sea ler the gXrl ~n t sen/or ~1 = mid / MP $ / / ~P VP I / ?
Y Mp I I ant, l I BL11 $ / / ~Mp~ VP / ~ I ~\  ~ I MP    V ~P /   , S ~ I / DET 11 ; / ~ts  VET !
I i t lm S  I I the g i r l  I lVp/     a sen/or VP I I /   x I I not I x ~" p t I I The g l r l  who net BLl l  t,* n sealer 2.
I I   , I I V MP, ~  (@) J J o in  I I .
7 i I g~w v~ I persuaded I .
~ I / X I~ I TOVP i~PRO /   % .. Bill~ V l(P .
Z should belong to A(i,j,k,n[, see Fig 3.1a.
node Z has only one child X, and if X E A[i,j,k,l], then obviously Z E A{i,j,k,ll.
( , )  X" I I I I I Z / / I / I I   ?
/ V Y / /   / / /  / I I  I I !
I I I I t j k 1 ?
(b) x I I I I I Z / / / / / / / V Y / /   I / /  / I /  I .
I I I I I J i J 1 an  p (c) Y /% / / / / / / / .
/ I /  I n /  ?
/ / I I I I i J k I Pi l l ?
Step 3 For i :O to n- I  stop t do Step 4 fo r  J : l  to  n- I  s top 1 do Step 8 put foot  nodes of  a l l  aux i l i a ry  t rees  in Xt t .
l e f t  s ib l ing  in A  [ t .
k .n  ]  and the r ight  s ib l ing  in A[n.p.p.1]  sat i s fy ing  appropr ia te res t r i c t ionn  then put the i r  parent in A[i,j,k.i].
If the left sibling is in A[i,m.m.pl and the .ght  sibling is in A(p,j,k.I I, i -- m < p and p ~ j, then we put their parent in A[i,j,k,l I.
, j , k .1  ] .
If the left sibling E A[i,j,j,ml and the right sibling E A(m,p,p01[ then we can pat the parent in A[i,j,j,lJ if it is the c~, .
that ( i<  j _< mor i~ j < m) and(m < p ~ lo t  m _< p < | ) ,Th i s  may be written ae fo~ s : J t,o l - t  st,up I do for  p : J to 1 ?~*p t do f ?
r  .11 left ,  sLblLnKg in A  [ i .
J , J ,n  ]  and right, s ib l ings  i ?
A(n,p,p,1]  ?at1?fy1.nlg t, he appropr iate rant,rXcCio??
pot the i r  pgwuat, Xa A( / .
(e) Came 5 correspo?ds to adjoining.
If X is n node in A[m,j,k,pJ and Y is the root of a a?xiliary tree with same symbol as that of X, such that Y is in A[i,m,p,I] ((i <_ m _< p < ior i  < m_< p <_ l Jand(m < j < k ~ por to  ~j  ~_k < p)J.
co J 8t*p t do for p = u ~o I stop t do t f  t node X E A [a .
In k  [ t ,a .p , l  ]  t, heu put, X Xn A( i .
J , k , l  ] Case 4 corresponds to the case where s ?ode Y has only one child X If X E A~i,j,k,ll then put Y in A[i,j,k,l[.
4) times, because o( the four loop statements i?
steps 6 through 9.
Case h We need to know the co?tents of A[i,j,k.m[, A[m,p,p,I] where m < I, i < m. when we are trying to compute the co?tents or Aii.j,k,l [.
hypothesis) that for all m < I and for all p,q,r, the coate?ts of A[p,q,r,mJ are already computed.
Hence, the contents of A[i,j,k,mJ are known.
Similarly, for all m > i, and for all p,q, and r <_.
l, A[m,p,q,rJ would have been computed.
Thus, A[m,p,p,i I would also have bee?
Case 2: By s similar ream?lag, the co?tents of A(i,m,m,pJ and A[p,j,k,l I are known since p < I and p > i.
,Note j > i or j < I. tlence, we know that the co?tents of A[i,j.i,pj and A(p,q,q,l] would have bee?
Came 5: The co?tents of A[i,m,p,iJ and A(m,j,k,pJ must be k?own i ?
order to compote A(i,j,k,l[, where ( i _< m ~ p < I or i < m < p_<l )aad(m_<j_< k < por to  < j_< k_<p) .
Since either m > i or p < I, contents of Alm,j,k,pl will be know?.
Similarly, since either m < j or k < p, the co?re?re of A(i,m,p,l I would have been comp?tcd.
A[ij,k,m i and A(m,p,p.q respectively, then we add pointers from this node Z i?
A[i,j,k,l] to the nodes X, Y i?
A{i,j,k,mj and A[m,p,p,l[.
A paner based o?
eonstrnct ~ TAG G snch that L(G)mL t U L-a- Le* G I =- { 11, At, NI, S ), and G 2 = ( I~, A=, N~., S ) Without Io~ of senerality, we may assume that the N I N N:e =" h. Let G --  ( I l U 12 , At LJ A=, N t U N=, S ).
We claim that L(G) :~ L l Let x EL t  U L-z.
Then x EL I  or x E I~.
Similarly if x E [q , we can show that x E L(G).
Hence L t U L~ C L(G).
If x E L(G), then x is derived using either only Ij, A t or only l~,A:tsince N I I"1 N,j =,, ~.
Hence, x EL t  or X E t~  Thus, L(G} -- Lt U I~  Therefore, L(G) =- Lt U L~ 88 4.2.
C lmure  under  Concatena~on Let G t - - ( l t ,At ,N~,St) ,  G ,  ,,, ([~.~=,N~,S~) be two TAGs generating Lt, I~ respectively, such that N I I1 N= =- ~.
TAG G =- (I, A, N, S) such that L(G)=,, L!
We let N - -  N t IJ N ,  U {S}, A ,m A t U An.
For all t t E !1, t~ E I,, we add tl:~ to I, as shown in Fig 4.2.1.
it is easy to show that L(G) ,m L I .
s~ s~ s t= I  t~= I I  I I  I f"t2 : S / / / / s, s~ I X I X / *,t  / ~s Fib, urn 4 2. t 4.3.
Let S be a symbol not in N t, and let N m N I U {S}.
The set o( auxiliary tree, A is defined u A = {t~A / t t ?
To see why L(G) ,m Lt*, consider x ~ L(G).
Therefore, LI* E L(G).
Thus L(G) =~ Us*.
(*) % = S I n (b) ~IA : $ IX / S St / /  , r  t,t / (c) / / S IX /X /~* ~  t $ I St S I I I  .
TAG generating L T and M = (Q , ~ , 6 , q0 , QV) be a finite state automaton recognizing Lit.
(qz, w~) =- q~, and ~(q~, w=) =, qs.
Then we shall smucinte (p,q,q,r), (r,qs,qa,s) with Y and Z.
Then qs ~ q8 I q.
We will ammeiate with Y and 7. the quadruples (p,r,r,q) and (q,u,t) reap.
6*(qt,wl) --  q, 5*(q,w,) = q,.
Let 6*(q,wl) a p, ~(p,ws) I r. Then X will have associated with it the quadruple (q,p,p,r).
/ / / / / x / / / /   x /  y / /   / / /   / / /   /    / / /  /   /   / / /  /    /  / .
/  I v~ T v= w* t n= /  / / x/ lr!
~ W 2 e?1 e* 2 ~* (q t .v   t)=q=~* (p,v?
t)--pt &*(qa.w~)---p ~*(Pt.e=)=q, Fl~furn 4.4.1 / / / / / / / / / / / / / / / / .
Hence, L(Gt) - -  L T ~l Lit.
X -> LL~(A.B) ts * ruln ?n ~he grammar ~hnu X derlves vhugvx 4.
3) i f  X -> LCt(A.B) Ls a rulo In the grammar then X dertvnu vhxugv 4) i f  X -> LC~(A.B) in a rule [n the granm~r then X durlvee vhxtt~r 4 b Nov consider hoe u dertv .
a= S / / / X / / - / / - -  -  ~_~_7 ugv  $ / / !
/ x ,hT-~-x u~ ~= X / / / / X v h ?
,h~,  LC~)  - -  ~, LC~,~)  - -  ~ ,  -C,~L;) - -  ~, ~C,(;,X) - -  ~, =.~ Lc,(~,;) = ~.
| ->LC t (At, .
Aj  ) S]m->LCt (At  .
here I I  .A t  ,~  .
.A |   are ,,mr ?ou-tenLtna~ synbole,A !
then add the production gyx ->~, el ?e {it  1?
S..~2 I I a S I t I I / I b s(~) c ObviouS, L(Cl - -  {so~c- / ?
B -> I[ c -> LL~(S,D)/O 0 -> I.Ct(Z.F.G) -> ~ F -> "~ - )  -~- vhtch can be re~r~tten  u s -> s/~ S-> LCt(a,X) ~ ->  LL~(S,~c) or ~ ->u~(s,~c) I t  can be vur i f te~ Chat Chin grumsr  generates exact ly L(G).
See Figure 1(c) for a concrete example.
Figure 2).
(Passonneau, 1993a).
3.
The second heuristic merges [June 5] , [1995] into [June 5, 1995]; and [June] , [1995] into [June, 1995].
Section 3.2) ranging from α = 0 to α = 5.
1.
2.
3.
4.
5.
1.
2.
3.
4.
5.
6.
7.
8.
c ? 2008.
and ?from/to?.
(writer: around = 0.49, system: by = 0.51).
2.
3.
4.
6.
7.
sion (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)).
Rater?s Prep.
4.
5.
8.
In our current implementation, n -= 2, a = 59.
This also held for Google counts (t(87) = 3.16, p < .01; t(87) = 4.02, p < .01; t(87) = 4.03, p < .01).
These differences were significant for the BNC counts for AltaVista (t(87) = 8.38, p < .01; t(87) = 5.00, p < .01; t(87) = 5.03, p < .01) and Google (t(87) = 8.35, p < .01; t(87) = 5.00, p < .01; t(87) = 5.03, p < .01).
They were also significant for the NANTC counts for AltaVista (t(87) = 4.12, p < .01; t(87) = 3.72, p < .01; t(87) = 6.58, p < .01) and Google (t(87) = 4.08, p < .01; t(87) = 3.06, p < .01; t(87) = 6.47, p < .01).
Given the set of (v, n, v') triples (a total of 1,337), the task was to decide whether (v, n) or (v',n) was the correct (i.e., seen) pair by comparing the probabilities P(nlv) and P(nlv').
Given a set of (v, n, v') triples, the task was to decide whether (v, n) or (v', n) was the correct pair.
Here, the probability estimates f (v, n) and f (v, n') were used for the joint probability model, and f (v, n)/f (n) and f (v, n')/f (n') for the conditional probability model.
&quot; save.
If there is a genuine association between x and y, then the joint probability P(x,y) will be much larger than chance P(x) P(y), and consequently 1(x,y) >> 0.
If there is no interesting relationship between x and y, then P(x,y) P(x) P(y), and thus, 1(x,y)2-- 0.
If x and y are in complementary distribution, then P(x,y) will be much less than P(x) P(y), forcing 1(x,y) << 0.
First, joint probabilities are supposed to be symmetric: P(x,y)= P(y,x), and thus, mutual information is also symmetric: 1(x,y) = 1(y ,x).
This adjustment has the additional benefit of assuring that f(x,y) = f(x) = f(y) = N. When 1 ], as illustrated in the table below.
In contrast, when 1(x,y)=-' 0, the pairs less interesting.
Alternatively, one could make estimates of the variance and then make statements about confidence levels, e.g., with 95% confidence, P(x,y) > P(x) P(y).)
Then, P(x) P(y) 10-5 and chance is P(x)P(x) = 10i0.
 ]; in = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.)
&quot; save.
&quot;
A corpus, for example, can easily contain NPs A, B, C, and D for which Referent(A) U Referent(B) = Referent(C) U Referent(D).
Example 9).
.
Figure 9).
McKinley. b.
Figure 3).
4.
$, million, record).
A complex dag D subsumes a complex dag Ds if and only if D(1) C DT) for all I E dom(D) and Ds(P) = f/(q) for all paths p and q such that D(p) = D(q).
Using our previous example, consider a restrictor 00 generated from the set of paths ((a b), (d e 1), (d ij f)}.
D,(p) = D,(q).
Similarly, unifications of the form p = a where a is atomic would require that D,(p) = a.
Scanner step: If i 0 and tv; = a, then for all items (h, i — 1, X0 a.a13, DI add the item [h, i, X0 oia.fl, Di.
(See Appendix B.)
If two words x, y have p(x, y) > 0, an edge exists between them with weight logp(x,y).
An O .
.
The algorithm computes 60,T,o,v(S) using the following recurrences.
A left rotation changes a (A(BC)) structure to a ((AB)C) structure, and vice versa for a right rotation.
For example, after rebalancing, sentence (7) is bracketed as follows: (8) [R[The/e Authority/t] [will/14f, ([be/€ accountable/RN] [to the/e [e/lit] [Financial/k& Secretary/151 111)11] ./. ]
(<> = unrecognized input token.)
= argmax e wT ? h( f , e) (1) where h( f , e) is a large-dimension feature vector.
?, ?b?,?
Hierarchical phrase translation probabilities in both directions, h(?|?b?) and h(?b?|?), estimated by relative counts, count(?, ?b?).
Word-based lexically weighted models ofhlex(?|?b?) and hlex(?b?|?)
We may use any binary features, such as h( f , e) = ? ?
For instance, we define ((ei?1, f j?1), (ei, f j+1)),((ei, f j+1), (ei+2, f j+2)) and ((ei+2, f j+2), (ei+3, f j)) in dicated by the arrows in Figure 1.
f j?1X 2 f j+3 ? , X 2 ? ?
f j f j+1X 3 ? and X 3 ? ?
f j+2 ? .
Hierarchical features capture the dependency of the source words in a parent phrase to the source words in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in dicated by the arrows in Figure 2.
For instance, the word Algorithm 1 Online Training Algorithm Training data: T = {( f t, et)}Tt=1 m-best oracles: O = {}Tt=1 i = 0 1: for n = 1, ..., N do 2: for t = 1, ..., T do 3: Ct ? bestk( f t; wi) 4: Ot ? oraclem(Ot ? Ct; et) 5: wi+1 = update wi using Ct w.r.t. Ot 6: i = i + 1 7: end for 8: end for 9: return ?NT i=1 w i NT ?violate?
Line 5 of the weight vector update procedure in Algorithm 1 is replaced by the solution of: w?i+1 = argmin wi+1 ||wi+1 ? wi|| + C ? e?,e? ?(e?, e?)
subject to si+1( f t, e?)
si+1( f t, e?)
+ ?(e?, e?)
L(e?, e?; et) ?(e?, e?)
0 ?e? ? Ot,?e? ? Ct (3) where si( f t, e) = { wi}T ? h( f t, e).
L(?)
and e?
3 is: max?(?)?0 ? 1 2 || ? e?,e? ?(e?, e?)
( h( f t, e?)
h( f t, e?)
) ||2 + ? e?,e? ?(e?, e?)L(e?, e?; et) ? ?
e?,e? ?(e?, e?)
( si( f t, e?)
si( f t, e?)
) subject to ? e?,e? ?(e?, e?)
C (4) with the weight vector update: wi+1 = wi + ? e?,e? ?(e?, e?)
( h( f t, e?)
h( f t, e?)
) (5)Equation 4 is solved using a QP-solver, such as a co ordinate ascent algorithm, by heuristically selecting (e?, e?)
and by updating ?(?)
iteratively: ?(e?, e?)
= max (0, ?(e?, e?)
+ ?(e?, e?)) (6) ?(e?, e?)
= L(e?, e?; et) ?
( si( f t, e?)
si( f t, e?)
) ||h( f t, e?)
0, min ? ?
C, L(e?, e?; et) ?
( si( f t, e?)
si( f t, e?)
) ||h( f t, e?)
h( f t, e?)||2 ? ?
Tillmann and Zhang   used a different update style based on a convex loss function: ? = ?L(e?, e?; et) ?max ( 0, 1 ?
( si( f t, e?)
si( f t, e?)
1 N N ? n=1 log pn(E, E) ? ?
BP(E, E) (7) where pn(?)
1.
1.
7.
ESC.(1.10).
(1.5) they.
&name.
 ).
 ).
Flaubert’s Madame Bovary.)
 .
1.
2.
3.
Linguistics, 19(2):263-311.
Other pairs are derivationally reasonable: (&quot;b&quot; , &quot;v&quot;), (&quot;x&quot; , &quot;s&quot;) and (&quot;s&quot; , &quot;c&quot;); while some may be noise: (&quot;g&quot;, &quot;n&quot;) and (&quot;g&quot;, &quot;v&quot;).
3.
Let G = (JV, T , S, R, θ) be a Probabilistic Context Free Grammar with nonterminal symbols JV, terminal symbols T, start symbol S E JV, set of productions R of the form N —* β, N E JV, β E (JV U T )*.
It factors Q(t, θ) = Q(t)Q( ¯θ) = Qn i=1 Qi(ti)Q¯N∈N¯Q( θ ¯N).
Using linear interpolation we model P(x|N(1) ... N(k)) as a weighted average of two distributions λ1P1(x|N(1), ... , N(k)) + λ2P2(x|N(1), ... , N(k−1)), where the distribution P2 makes an independence assumption by dropping the conditioning event N(k).
For example, in EVG to smooth P(A = DT | d = left, H = NN, v = 0) with P2(A = DT | d = left, v = 0) we define the backoff set to be {LH  |H ∈ Vτ}.
The new P(A|h, H, d, v) distributions are set initially to their corresponding P(A|H, d, v) values.
In the above verb phrase A = 0, V = joined, Ni = board, P = as, and N2 = director.
There is no such sequence in the PP-attachment problem, and because of this there are four possible triples when backing off from quadruples ((v, nl, p), (v,p, n2), (nl,p, n2) and (v, nl, n2)) and six possible pairs when backing off from triples ((v,p), (nl,p), (p, n2), (v, n1), (v, n2) and (nl, n2)).
The backed-off method based on just the f(v,p) and Anil, p) counts would be: If P(11v,n1,p)>= 0.5 then choose noun attachment, else choose verb attachment, where f (1, v , f(1,n1,p) fi(liv,n1,P)= f(v,p)+ f(nl,p) An experiment was implemented to investigate the difference in performance between these two methods.
2.
Where p(w,t), p(w), and p(t) are probabilities of w and t co-occurring, w appearing, and t appearing in a tweet respectively.
3.
2.
: ' 1 (jyon.buroo), 7)1&quot;1&quot; 7 (aruhonsu. damatto), and -Q.
4 ' 7 .s/ (maiku.dewain).
(or subcorpora).
[3,5]).
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left to my daughter-in-law].
Therefore, the final labeling becomes c?1:M = argmax c1:M?F(PM ) M?
2.
3.
4.
7.
8.
9.
Given a cost vector p ? <d, a set of variables, z = (z1, . . .
k? i=1 zji?
The linear inequalities that represent this constraint are: ?m ? {1, . . .
,M} : M?
,M} : j?1?
4.
Let X = (X1, X2, ... , Xn) where each Xi = (Xi1, Xi2, .
.
.
B5: [That] ’s right.
B& [It] ’s healthy, .. .
”Uh-huh”, ”Yeah”, etc.).
The value of a pattern feature is estimated according the one of the following four scenarios3: ? ????????????????????????
0 ? ?
1 and 0 ? ?
characters in the sentence, (3) Number of ???
#ihatequotes?.
#fun?.
We compute the posterior probability of each n-gram w as: where Z(£) = EE'∈E exp(αH(E0, F)) (denominator in Equation 3) and Z(£w) = EE'∈Ew exp(αH(E0, F)).
The factors depend on a set of n-gram matches and counts (cn; n E 10, 1, 2, 3, 4}).
“nineteen seventynine”.
c P (c|w1)2 ? c P (c|w2)2 Jens.-Shan.
distjs(w2, w1) = 12 ( D ( p||p+q2 ) +D ( q||p+q2 )) where p = P (c|w1) and q = P (c|w2) ?-skew dist?(w2, w1) = D (p||(?.q + (1?
?).p)) where p = P (c|w1) and q = P (c|w2) conf.
simcp(w2|w1) = ? c P (w1|c).P (w2|c).P (c) P (w1) Jaccard?s simja(w2, w1) = |F (w1)?F (w2)| |F (w1)?F (w2)| where F (w) = {c : P (c|v) > 0} Jacc.+MI simja+mi(w2,W1) = |F (w1)?F (w2)| |F (w1)?F (w2)| where F (w) = {c : I(c, w) > 0} Lin?s simlin(w2, w1) = ? F (w1)?F (w2) (I(c,w1)+I(c,w2)) ? F (w1) I(c,w1)+ ? F (w2) I(c,w2) where F (w) = {c : I(c, w) > 0} precision simP(w2, w1) = ? F (w1)?F (w2) I(c,w2) ? F (w2) I(c,w2) where F (w) = {c : I(c, w) > 0} recall simR(w2, w1) = ? F (w1)?F (w2) I(c,w1) ? F (w1) I(c,w1) where F (w) = {c : I(c, w) > 0} harm.
Given a word w, each word w?
If NS(w,m) is the vector of such scores for word w and measure m, then theoverlap, C(NS(w,m1),NS(w,m2)), of two neigh bour sets is the cosine between the two vectors: C(NS(w,m1),NS(w,m2)) = ? w? rm1(w ?, w)?
We introduce the following notation for expected counts: N(i, j, k) indicates the expected count of term i in region j and topic k, and N(j, k) = Ei N(i, j, k).
The first two terms represent the difference in expected counts for term i under the variational distributions q(z, r) and q(z, r,β): this difference goes to zero when β(i) jk perfectly matches N(i, j, k)/N(j, k).
 .
SMF-20041076).
1.
The initial state is (x, 0, 0) where 0 is a “null” or empty analysis.
Each “allowable triple” is a tuple (X, Y, Z) where X, Y, Z E V .
For illustration, assume that the set B is {(S,NP,VP), (NP,NN,NN), (NP,NN,¯S), (S,NP,VP)}.
Initialize the Cache to include, for j = 1... N, (gj, 0, T2).
2.
.
.
.
Figure 2).
.
.
.
8.
NULLed.s.'
3.
4.
5.
6.
, /} ) : Pr(f~lel) = ~Pr(f i  ,a~le{) .1 ?
Putting everything together, we have the following basic HMM-based modeh .1 *(flJl~I) = ~ I I  [~,(-jla~.-,, z).
p(fj l%)] (1) at j= l with the alignment I)robability p(ili,I ) and the translation probability p(fle).
In the E-step the lexical and alignment 1086 counts for one sentenee-i)air (f, e) are calculated: c(flc; f, e) = E P"(a l f   e) ~ 5(f, f~)5(e, c~) a i,j ,.
:(ill, z; f, e) = E /  ,  (a i r ,  e) aj) a j In the M-step the lexicon and translation probabili- ties are: p(f le) o< ~-~c(fle;f(~),e (~)) 8 P( i l i  , I )  o (Ec ( i l i  , I ; fO) ,e (~) ) 8 To avoid the smlunation ov  use the maximum apllroxima- tion where only the Viterbi alignlnent )ath is used to collect counts.
Using a set of non-negative t)arameters {c(i - i  )} ,  we can write the alignment probabilities ill the fl)rm: ~(i - i) (2) p(i l i  ,  I)  = c(,,:" - i  ) This form ensures that for eadl word posilion it, i = 1, ..., I , the aligmnent probat)ilities atis(y th(, normalization constraint.
We enforce the following constraints for the tran- sitions in the HMM network (i _< I, i _< I): p(i  + I l i  , I )  = pff .
5( i , i  ) V(i + I l l   + I, I )  = J J .
5( i , i  ) p(i l i   + I, 1) = p(iIi  ,1) The parameter pff is the 1)robability of a transition to the emt)ty word.
Smooth ing For a t)etter estimation of infrequent events we in- troduce the following smoothing of alignment )rob- abilities: 1 F(a j I~ j - , ,~)  = ~" ~- + (1 - , , ) .p (a j la j _ l  , I ) in our exlleriments we use (t = 0.4.
Assmning a mfiform align- ment prol)ability p(i l j ,  I )  = 1/1, we obtain Model 1.
[---l  [ - "~ " .
C l l -1  " e .
), exlmriments.
Inequalities, 3:1 8.
Computational Linguistics, 19(2):263-311.
2.
Thus the expression a:b E:C describes the two-relation containing the single pair (a,bc), and a:b:c* q:r:s describes the three-relation {(anq,bnr,cns) I n > 0}.
We extend the function 6 to sets of states, so that for any P C Q and a E E', 6(P, a) = Up 6(p, a).
We also define the usual extension of 6 to a transition function 6* on E* as follows: for all r in Q, 6*(r,E) = 6(r, e) and for all u c E* and a E E6, 6*(r , ua) = 6(6* (r , u), a).
A finite-state transducer T defines the regular relation R(T), the set of pairs (x, y) such that 6* (q , x, y) contains a final state.
Clearly, for all languages L, L = Dom (Id(L)) = Range(Id(L)).
If L contains the single-character strings a and b, then Id(L) only contains the pairs (a, a) and (b, b) while L x L also contains (a, b) and (b, a).
Suppose that R1 is the relation {(an,bnc*) 1 n > 0} and R2 is the relation { (an ,b*cn) I n > 0}.
The intersection R1 n R2 is { (an, bn cn ) I n > 0}.
If the characters a and b are in E and S is {$}, for example, then Intro(S) contains an infinite set of string pairs including (a, a), (a, $a), (a, a$$$), (ab,$$a$b$$), and so on.
>V<B>V<B>V< >V<B>V<B>V< >V<B>V<B>V< >V<b>V<b>V< >V<b>V<B>V< >V<B>V<b>V< To take a somewhat more realistic example, when the rule at the beginning of the paper is applied to the string iNprobable, the preprocessed input string would contain the sequence <i<N<>p<r<o<>b<a<>b<1<e< The left context of the rule is empty, so there is a left-context marker < after every character from the original string.
.
. a into simply b.
We defined Prologue above as Intro(m U {O}).
We also redefine the generic cover symbols <, >, and m to stand for the respective collections of all brackets: < = Uk<k, > = Uk>k, m = <U>.
{ a, e, }).
.
.
(3)).
Figure 1).
(5)).
VI, Pl,P2 E V\[; P, a probability distribution which assigns a probability, 0 < P(X\[..z\] ~ A) < 1, to a rule, X\[..x\] -* A ~.
The language of a SLIG is defined as follows: L = {w E VT~ \[ S\[$\]~w}.
If 7?
It is defined as follows: w~/'~f { a i+t" .uj , i f i>_ j ' i f /< j Given a string w = at . . .
(P os, O, i, - , - , t) - P(B"/ C V~ s.t. t\[$\]=~ Wio pos\[$Ttl\] w~) (ii) If the node ~/does ubsume the foot node ~/!
of a then: 0 '~ (pos, O, i, j, k, l) aeJ- /'(37 ~ V~* s.t. t \ [$\]~ Wlo pos\[$Trl\] w~ and b\[$7~ll\]~w\]) Once the inside probabilities computed, the outside probabilities can be computed top-down by consider- ing smaller spans of the input string starting with O"( t ,$ ,O , - , - ,N ) = 1 (by definition).
5.1 In ferr ing the Language {a"b"\]n > 0}.
t\[$,Tg\] s:~4 t\[S,lg,78\] t\[$og\] o_~ t\[$,lg,lg\] t\[.-t/~\] z_~,o b\[,.~7~\] t \ [~\ ] ,..~o b\[,~\] t\[..~\] ~,?
1.8 1 .6 1,4 1.2 1 0 .8 0 .6 0.4 I I I I I I I I SLTAG - - SCFG . . .
NANTES, 23-28 AO~' 1992 4 3 0 PROC.
J ACM, 15:647-671.
Prat t , Fletcher.
If b\[$r/\] ~ a, I(b,7, i , - , - , I ) = dl if / = i+ 1 and if.
a = w~ +1, 0 otherwise.
2.
\] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and if k = l, 0 otherwise.
If we factor p(x, y) into p(x, y) = p(y|x)p(x), we can see that pt(x, y) can deviate from ps(x, y) in two different ways, corresponding to two different kinds of domain adaptation: Case 1 (Labeling Adaptation): pt(y|x) deviates from ps(y|x) to a certain extent.
For example, we can set pt(y|x, B) = p(y|x; B).
3.
 .
Writing these as f(i, j), f(i) and f(i, w) respectively, the transition probability from tag i to tag j is estimated as f (i, j)/ f (i) and the lexical probability as f(i, w)/ f (i).
DO Un-degraded lexical probabilities, calculated from f (i, w) / f (i).
TO Un-degraded transition probabilities, calculated from f (i, j)/ f (i).
A function f maps (x, y) pairs to feature-vectors f(x, y) E Rd.
The function e maps a sentence x paired with a spine (i, ,q) to a feature vector.
3).
 .
 ).
IB with K = 42 is 9% better).
A2).
Taking all these propositions together, we can infer first that (4)(n SuBJ) = 4)(n2) and then that (4)(M(n2)) SuBJ)=4)(n2).
Obviously, (c) is a more serious error than (b).
2.
Step B.
That is, sim(A, B) = f (I (common(A, B)), I (describe(A, B))) The domain of f (x, y) is {(x , y)ix > 0, y > 0, y > x}.
According to Information Theory  , /(s) = —log bP(s), where P(s) is the probability of s and b is the unit.
In other words: f (xi + x2, y) = f(xi,y)+ f(x2,y).
A corollary of Assumption 5 is that Vy, f(0, y) = f (x + 0, y) — f (x,y) = 0, which means that when there is no commonality between A and B, their similarity is 0, no matter how different they are.
When A and B are identical. knowning their commonalities means knowing what they are, i.e., I (comman(A, B)) = I (describe(A.
B)).
Therefore, the function f must have the following property: Vx, f (x, x) = 1.
Assumption 7: The function f (x. y) is continuous.
(due to Assumption 5).
Thus. f (x, y) = ;-; f (nx, y).
}* Step C.1: Construct a similarity matrix (8).
The matrix is divided into (k 1) x (k + 1) blocks.
For all 1, j, m, such that 1 E [1,ni,..] and 1 0 Imax and j imax and m E [1, nil' Let's consider again the word &quot;facility&quot; in (3).
Many of the selectors in Tables 1 and Table 2 have artifact senses, such as &quot;post&quot;, &quot;product&quot;, &quot;system&quot;, &quot;unit&quot;, &quot;memory device&quot;, &quot;machine&quot;, &quot;plant&quot;, &quot;model&quot;, &quot;program&quot;, etc.
;] as a token.
.
The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y.
For example, f(k)=αk – β when k >= 0, and α, β > 0.
Kehler  ).
Figure 1).
If x E OLD and y E NEW, then x y.
If x E MED and y E NEW, then x y.
(2) If x, y E OLD, or x, y E MED, or x, y E NEW, then if utt. utty, then x y, if utt.
Table 4).
Example feature: if Section-View(tokeno(h)) .9(h, n = 1 : = &quot;Preamble&quot; and f = person_unique 0 : else Activation example: CLINTON WARNS HUSSEIN ABOUT IRAQI DEFIANCE.
answer(RIVER), [1..9]) (RIVER?
TRAVERSE(STATE), [1..9]) (TRAVERSE?traverse, [1..4]) which1 rivers2 run3 through4 (STATE?
NEXT TO(STATE), [5..9]) (NEXT TO?
STATEID, [8..9]) (STATEID?
and ?(STATE ? STATEID, [8..9])?.
answer(RIVER), [1..9]) (RIVER?
STATEID, [8..9]) (STATEID?
(1) a. [Lee]Seller sold a textbook [to Abby]Buyer.
b. [Kim]Seller sold [the sweater]Goods.
Asemantic structure SemStruc = ?p,Set(SRA)?
49.11??
54.67??
59.65??
35.53??
42.36??
41.76??
 .
.
.
141, 150).
.
.
.
.
.
.))
.
.
.
.
.
146-147).
.
.
.
.
.
.
.
.
141, 150).
.
.
.
.
.
.
.
.
.
.
146-147).
.
.
.
.
.
.
.
.
Consider a span of length n = 5, with the words in the path denoted by v, w, x, y, z.
. w. .
. xi under this assumption, namely, as MAX (p(v,wir p(wi,x I)) for all wi.
.
.&quot;  , p. 7).
This list, in our case, is -i, e, u, o, r, tu, ou, c, iu, ie.
4.
5.
6.
yields the patterns ?the X cut * Y with?, ?* X * the Y *?, and so on.
For example, ?* X cut * Y *?
1)  .
For example, in Figure 4, BS(IP0, 6) = {e1, e2}.
Assume that we have a uniform prior on B, with p(B) = 1 for all B ∈ [0, 1].
We ran tests with d = 1, 2, 3, 5, 10, and oc (i.e., knowledge-free syntactic clustering).
It is defined in terms of entropy H and mutual information I: V I(C, C′) = H(C) + H(C′) − 2I(C, C′).
[4], and discussed in Section 5.
~fe l)oint out some useful l;e,%s (;hal; (lo nol; make this assuml)- lion, including computationally--intcnsive ran- d()mizat,ion 1;cs|;s. 1 I n t rodu( - t ion In Clnl)irical natural  ]al~gUag(~ l)rocessing, on(, is ottcal |:(~st;ing whether some new technique 1)ro(lu(es im])rove(l lesull;s (as mcasur(xl ])y one ()1 111017(` - IIICI;I]CS) Oit son Ic  i;esl; (lai;~L set; V]l(`-ll (;Olll])aIe(l l i ( )sol l le (; l lrrel lt  ( l )ascl i lm) l;c(:]lnique.
3) " 12 = ~( I ,  J lk -- Y l ) (~2k  - -  ~2) / ( g l t~2(  71 - -  l . )
set 1)etween l;he d i f fcrent variations.
How much error is introdu(:e(t t)y assuming independence?
?~ J}hen equation 2 reduces to s , , -  sV /2 ( l -  r12), where s = sl = ,s2.
A 2 x 2 ta- ble has 4 cells.
950 large, whel;he, r l;lm largeness is (:aused l)y t;he new t;eehni(lue t)l"o(lucing a, much l)(fl;l;er result; titan l;he current, l;e(:hlfique or vice-versa.
So 1,o l)e fair, we eolnl)ared l;he X 2 resull;s with a l;wo-sided version of l;hc rmldon~iz~fl;ion t;esl,: es- l;inm|;e, l;he likelihood glu~l; l;he obseaved magni- l;u(le of t, he resull; (lifleren(:e would 1)c matched or exceeded (regardless of which l;echnique pro- duced l;he betl;er resull;) raider the mill hyl)oth- esis.
A one-sided version of the test;, which is colnt)aral)le t;o what we use in l;he rest of the t)a - per, esl;inml;es l;he likelihood of a (tifferenl; oul;- come under t;he null hyt)oChesis: that of m~l:cll- ing or exceeding t;he (lit[erence of how lllllch l)?,l;ter i;he new (possibly 1)ett, er) l;e(:lmi(lues oh- s(aved result is than l;he currenl; l;e(hniques o|)- serve,(1 lesull;, ht t;he ahoy(; scenario, a one-sided t(;sl; t)rodu(:es ~ 3(~, tigure insl;ead of s~ d:% figure.
Th(, miswer is no, lml; t;]l()se lesl:s l;hal; (to nol; lltC}~Slll;e, how ll l l lch {;we l;e(;]lni(llles illl;(,ra(:l; (to need  i;o lmtke, some assmnpl ; ioH  al)oul; t;]m|; ill- I;(;r~t(l;ion mid l;yl)it:a.ll E l;]ml; assuml)l;ioll is in(te- t)(~]ldell(;e. Fhose I;esl;s I;ll~H; Hol;i(;c in S()lll(~ \r;~Br how much l;wo tc(:hniqucs hm;ra(:l; (;~1,11 lib(; ~h()se ol)servations insl;ead of relying on assumt)l:ions.
One w,~y t;o measure how 1;we l;e(:lmi(lucs in- i;erac(; is 1;o comtm.re ]tow similarly (;he, t;wo t;ecl> ni(tues tea.el; 1;o various l)arl;s ()f 1;he l;(;s[; seA;.
"his is done in the mal;t:hed-lm.ir 1, I;esl; (Hm- nctl;, 1982, Se(:.
Un- like, t;]le nl~tl;ched-tmir t: t;esI;~ neither of t, hese l;wo I;CSI;5 slSSllllte t;ln~l; I hc sum of l;he (litlcrences has a normal (Gaussian) (listribul;ion.
The i;wo tests are, so-calh~d nonl)a.rmut%ri(: l;esl;s, which (lo not; make assuml)l, ions a.1)out; how l, he rcsull;s axe dis- lnil)ut, ed (thrnel,l,, 1982, Ch.
It uses a 1)inomial dist,rilm|;ion to examine the munber of l;esl; smni)les where t;e(:hlfi(lUe ] 1)crforms ])el;- l;er t;ha.n l;e(:hnique 2 ve, rsus l;he munl)er where 1;he Ol)posite occurs.
?e use t;he sign l;esl; b(,causc iI; 11Htkcs fcwel" assumi)i;ions 1;hart i;he nml;chcd-l)air 1: I;est and is simplier l;han the Wihoxon I;esi;.
The 1)~dmmed F-score = 2ab/(a + b), where a is recall and b is precision.
2) (Cohen, 1995, Sec.
For us, when n < 20 (2" .<_.
N:om (Noreen, 1989, Sec.
aA.a), the significance level (probability under the null hypothesis) is at most (.,e + 1)/(,~t + 1), where ,,.
However, the (lmnees of getting an llllllSllal resl l l t  (;a,lt c]la.ll~re.
Coll~,pltt(ttiollgl Li ~,- gui,stic.s, 1!)(3).
Cornpu, tational Lin- guistic.s, 1!)(1.
Prentice-lI~dl~ N,J, USA.
Prentice-Hall, N J, USA, 2nd edition.
.
.)
1).
1.
2(c).
3).
Denote a is a continuous subsequence of indices a, a + 1, · · · a + l (a) for Ri’s children where l(a) is its length, as is the s-th element in a.
2004).
Then the A distance between two probability distributions is dA(D, D0) = 2 sup |PrD [A] − PrD, [A] |.
 .
Second panel: same, with 0 = 10 and d = 0 (bottom), .5 (middle) and .9 (top).
If u = 0, return w E W with probability G0(w).
Let S = {Sv : all contexts v}.
Stimulus, Degree, Experiencer, etc.).
(e.g.
“[holder I] was happy”).
(i.e.
First, .
.
” ” “ “ NPˆS DT NPˆVP ” .
!
As an example, consider the case of v = 1, h = 1.
2.
*>’), ChunkRule(’<DT><VB.><NN.
*>’), ChunkRule(’<.
*>’), UnChunkRule(’<IN|VB.*|CC|MD|RB.
*>’), UnChunkRule(&quot;<,|\\.|“|’’>&quot;), MergeRule(’<NN.*|DT|JJ.
*|CD>’, ’<NN.*|DT|JJ.
*|CD>’), SplitRule(’<NN.
*>’, ’<DT|JJ>’) ] The next example illustrates a brute-force statistical approach.
ANA: its \[@off/\]33\] CND: Apple \[@of 1/131\] 432 Apple \[/aol f/10\] \] 352 its \[@off/\].03\] 352 App\]e's \[@offf/\] I 5\] 1352 prilne real estat(!
\[@off/\]08\] 165 show f loor \ [ (aof f /1 \ ]2 l \]55 year \[@o~f/137 I 310/3 The candidate set illustrates several important points.
 ).
(or ?shallow?)
Figure 1).
VP ? V NP ?=?
?OBJ=?
DT[?SPEC=?]
NN[?=?]
a source said []1.
?SUBJ=?
SUBJ, ? COMP?
V ? signs ?PRED=sign??
SUBJ, ? OBJ?
Given a lemma l and an argument list s, the probability of s given l is estimated as: P(s|l) := count(l, s)?n i=1 count(l, si) Table 2 summarises the results.
: GFn : GF, traverse f along GF1 : . . .
2.
 .
.
MDF estimation substitutes p(x, y) with discriminant functions g(x, y).
Suppose J=1, the discriminant function of JESSCM is g(x, y) = A(x, y)p1(x1, y; 01)λI+1 where A(x, y) = exp(A · & fc(yc, x)).
For the normal-form model, a parse is simply a (head-lexicalized) derivation.2 We define a conditional log-linear model of a parse ω ∈ Q, given a sentence S, as follows: where λ · f (ω) = Ei λifi(ω).
In the example, the current category is (S[dcl]\NP1)/(S[to]\NP)2, which is associated with wants; this combines with (S[to]\NP)/(S[b]\NP), associated with to; and the argument of (S[to]\NP)/(S[b]\NP) is wean.
In the example, the current category is (S[dcl]\NP1)/(S[to]\NP)2, which is associated with wants; this combines with (S[to]\NP)/(S[b]\NP), associated with to; and the argument of (S[to]\NP)/(S[b]\NP) is wean.
.
., where z1 = 1 and each zn+1 < m + 1 where m = max(z1, ... , zn).
Suppose z = (z1, ... , zn) have already been generated and m = max(z).
The PYP predictive distribution is: where a E [0, 1] and b > 0 are adjustable parameters.
).
.
<form> == <&quot;<gen>&quot; &quot;<num>&quot; &quot;<case>&quot;>.
1 (Eq.
2).
The probability of the entire tree is given by p(x, y θ) = P(y(0) 1$, θ).
The θ are the multinomial distributions 0s(· , ·,·) and 0c(· 1 ·, ·).
For a natural number N, we denote by 1:N the set {1, ..., N}.
Definition 1.
Let Jk for k E 1:K be a collection of (disjoint) subsets of {In,j I n E 1:N, j E 1:`n, IIn,j = Nk}, such that all sets in Jk are of the same size, Nk.
Let ηk =|Jk |F-In,j∈Jk ηn,In,j, and θk i = exp(˜ηk,i) /Ei, exp(˜ηk,i,).
“Best approximation” is defined as the KL divergence between q(θ, y) and p(θ, y I x, µ, E, S).
Our variational inference algorithm uses a meanfield assumption: q(θ, y) = q(θ)q(y).
3 (Eq.
4.
(s (NP-SBJ I) (VP consider (S (I~P-SBJ Kris) (NP-PRD a fool)))) (SQ Was (NP-SBJ he) (ADVP-TMP ever) (ADJP-PRD successfu l ) ?)
NP, ADVP, PP, etc.)
(SBARQ (WHNP-1 Who) (Sq was (NP-SBJ-2 *T* - I ) (VPbel ieved 116 (S (NP-SBJ-3 *-2) (VP to (VP have (VP been (VP shot (NP *-3)))))))) ?)
(S (S (NP-SBJ-1 Mary) (VP likes (NP-2 Bach) ) ) and (S (NP-SBJffil Susan) (NPffi2 Beethoven))) 118 Predicate Argument Structure: like (Mary, Bach) and like (Susan,Beethoven) (S (S (NP-SBJ John) (VP gave (NP-I Mary) (~P-2 a book))) and (S (NP=I Bill) (NP=2 a pencil))) (s (s (NP-SBJ Z) (VP eat (NP-I breakfast) (PP-TMP-2 in (NP the morning)))) and (S (NP=i lunch) (PP-TMPffi2 in (NP the afternoon)))) We do not attempt to recover structure which is outside a single sentence.
(S (MP-SB3 Mary Ann) (VP thinks (SBAR 0 (FRAG (NP chocolate))))) 2.
!, !!
!, lol, thanks, and haha.
 .
The figure of merit for a span is VF (i, j) = maxl,m VF (i, j,l, m).
Likewise for the left edge, S[l, M] · Qma=l+1 I(a) · is a bitext cell within threshold.
The non-compositional phrase pairs satisfy e(i, j) = f(l, m) G 1.
 ).
The second issue is finding the assignment of z with the highest score(z) = & w · f(x, zi, i) that leads to an incorrect prediction y = C(x, z).
1.
Input ?information age?;.
2.
information ->
 0 5.
create a frequency vector )),(,),(),(( 21 mefefef L ),,1(, miEei L=?
using contexts containing e~ ; transforming the vector into )),(,),(),(( 21 nEEE cfcfcf L ),,1(, niCci L=?
using contexts containing c~ ; normalize the frequency vector , yielding ),,1(,)),~|(,),~|(),~|(( 21 niCcccPccPccP in LL =?
; calculate the posterior probability )|~( DcP with EM-NBC (generally EM-NBC-Ensemble), where ),,1(,)),(,),(),(( 21 niCccfcfcf inEEE LL =?=D 3.
We further define }),(|{ Rceec ?=?
Initially, we set for all Cc ? || 1)( C cP = , ??
Ee Ee Cc ecPef ecPef ceP ecPefcP cePcP cePcP ecP )|()( )|()()|( )|()()(StepM )|()( )|()()|(StepE Figure 3.
)~|(log)()~(logmaxarg )|~(maxarg ~ ~ ~ ~ ccPcfcP cP Cc E Cc Cc D (3) Equation (3) is based on Bayes?
)~|(log)()~(logminarg ~ ~ ccPcfcP Cc E Cc ?
(4) where 1??
create a frequency vector )),(,),(),(( 21 mefefef L ),,1(, miEei L=?
using contexts containing e~ ; transforming the vector into 21 )),c(f,),c(f),c(f( nEEE L ),,1(, niCci L=?
, using a translation dictionary and the EM algorithm; create a TF-IDF vector 11 )),cidf())c(f,),cidf()c(f( nnEE L=A ),,1(, niCci L=?
2.
for each ( Cc ~~ ? ){ create a frequency vector )),(,),(),(( 21 ncfcfcf L ),,1(, niCci L=?
using contexts containing c~ ; create a TF-IDF vector 11 ))cidf())c(f,),cidf()c(f( nnL=B ),,1(, niCc i L=?
; calculate ),cos()c~tfidf( BA= ; } 3.
Table 1.
We used contexts with window sizes of ?1, ?3, ?5, ?7, ?9, ?11.
A B C D
 Figure 5.
Input ?information asymmetry?;.
2.
      !#$ %()*#+ information asymmetry , 3.
frequencies: #+ 5 #-.
5 Figure 6.
 Figure 7.
?Prior?
Table 2.
Table 3.
Table 5.
or ?ftw?.
http://oohja.com/x9UbC?.
and ?beat?)
cover 95% of their positive examples and only 6 negative words (?hate?,?suck?,?wtf?,?piss?,?stupid?
and ?fail?)
Thus, polpos(w) = count(w, pos)/count(w) and polneg(w) = 1?
polpos(w).
 .
1.
1) are highlighted.
Similar emoticons are clustered together (G1–G4), including separate clusters of happy [[ :) =) ^_^ ]], sad/disappointed [[ :/ :( -_- </3 ]], love [[ ❑xoxo ❑.❑ ]] and winking [[ ;) (^_-) ]] emoticons.
2, G2–G3).
2.
 .
This maximises the likelihood of the parallel training sentences, D = {(e, f)}, penalised using a prior, i.e., AMAP = arg maxA pA(D)p(A).
This is due to the many rules that enforce monotone ordering around sur la, (X) —* (X 1 sur, X 1 in) (X) —* (X 1 sur la X 2, X 1 in the X 2) etc.
6.
7.
8.
We define the index set for dependency parsing to be Z = {(i, j) : i E {0 ... n}, j E {1... n}, i =� j}.
A dependency parse is a vector y = {y(i, j) : (i, j) E Z}, where y(i, j) = 1 if a dependency with head word i and modifier j is in the parse, 0 otherwise.
Given a function f : Y H R that assigns scores to parse trees, the optimal parse is A particularly simple definition of f(y) is f(y) = E(i,j)E-T y(i, j)θ(i, j) where θ(i, j) is the score for dependency (i, j).
For example, a structure z E Z could have z(i, j) = z(j, i) = 1 for some (i, j); it could contain longer cycles; or it could contain words that do not modify exactly one head.
Define l1 ... lp to be the sequence of left modifiers to word i under y|i, and r1 ... rq to be the set of right modifiers (e.g., consider the case where n = 5, i = 3, and we have y(3,1) = y(3, 5) = 0, and y(3, 2) = y(3, 4) = 1: in this case p = 1, l1 = 2, and q = 1, r1 = 4).
Under this model calculating argmaxy|i∈Zi (fi (y|i) − Ej u(i,j)y(i, j)) takes O(n2) time using dynamic programming, hence the model satisfies Assumption 1.
5 ensure that z = y for some y E Y, and hence that z E Y.
In the next section we will show that these u(i, j) variables are actually Lagrange multipliers for the z(i, j) = y(i, j) constraints.
At each iteration k, the algorithm finds y(k) E Y using an MST algorithm, and z(k) E i through separate decoding of the (n + 1) sibling models.
The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.
1 when γ(i, j) = 0 for all (i, j).
4 was the z = y constraints.
We first introduce Lagrange multipliers u = {u(i, j) : (i, j) E Z}, and define the Lagrangian This follows because if y = z, the right term in Eq.
7).
A subgradient of a convex function L(u) at u is a vector du such that for all v E R|I|, L(v) > L(u) + du · (v − u).
By standard results, argmaxy∈Y h(y) + Ei,j u(k)(i, j)y(i, j).
That said, for some functions h(y) and f(z) strong duality does hold, as stated in the following: L(u(k), z(k), y(k)) = L(u(k)), where the last equality is because y(k), z(k) are defined as the respective argmax’s.
9 is tight, and (y(k), z(k)) and u(k) are primal and dual optimal.
As before, we define i = {y : y|i E ii for i = 0 ... n}.
Second, we may have z∗↑(i, j) =� z∗(i, j) for some values of (i, j).
As in Theorem 1, if at any point in the algorithm z(k) = y(k), then (z(k), y(k)) is an exact solution to the problem in Eq.
The (I)LP experiments were carried out using Gurobi, a high-performance commercial-grade solver. for i = 0 ... n. However, if for some i, u(k)(i, j) = u(k−1)(i, j) for all j, then z(k) |�= z(k−1) |� .
In lazy decoding we immediately set z(k) |� if u(k)(i, j) = u(k−1)(i, j) for all j; this check takes O(n) time, and saves us from decoding with the i’th automaton.
As before, if we do not find y(k) = z(k) for some value of k G K, we choose the y(k) with optimal value for f(y(k)) as the final solution.
However only 89.29% of these solutions are produced with a certificate of optimality (y(k) = z(k)).
First, define S = f(z(1)) − f(y(1)), where (z(1), y(1)) is the output of the algorithm on the first iteration (note that we always have S > 0 since f(z(1)) = L(u(1))).
In our models, f(z) can be written as f'(z) + Eij α(i, j)z(i, j) where f'(z) includes only terms depending on higherorder (non arc-factored features), and α(i, j) are weights that consider the dependency between i and j alone.
For any value of 0 < Q < 1, the problem argmaxzEZ,yEY f2(z) + h2(y) is equivalent to the original problem, if f2(z) = f'(z) + (1 − Q) &,j α(i,j)z(i,j) and h2(y) = Q Ei,j α(i, j)y(i, j).
(a man-made object).
and ?stove.?
and ?lion.?
and ?center.?
links to ?dog#n? and ?dog#v?
(meaning ?to chase?).
(canine) and ?dog#n#5?
(to vomit).
(log(ri)?
= ? k Ek M = (?
?M? ? ?
The stationary distribution v is com puted via an iterative update algorithm: v(t) = ?v(0) + (1?
Our convergence criteria was ? ?v(t?1) ? v(t) ? ?
?sorcerer#n#1,?
and ?charming#a#1.?
and ?ex pert#n#1?
and ?occultist#n#1?
and ?Cagliostro#n#1?
The node?breeze through#v#1?
zlingly#r#1,?
The skew divergence2 accounts for zeros in q by mixing in a small amount of p. s?(p, q) = D(p ? ?q + (1?
?)p) = ? i pi log pi ?qi+(1??)piLee found that as ? ?
pi log pi (1??)pi = pi log 11??
= pi log 2 ? = pi ?.
(computed as 1?
2?).
I See  , ch.
3.
3.
3.
3.
(4.2)
1993).
 ).
For example, s(r3) = “held x2 with x1”.
2, “t” and “e” in “telephone”.)
1.
 .
).
Though there are differences in syntactic phenomena, this is simshikashi (‘however’), demo (`but'), sorenanoni (‘even though’), tadashi (`on condition that’), dakedo (`but'), gyakuni (`on the contrary’), tohaie (‘although’), keredomo (’however’), ippou (`on the other hand’) used in this paper.
], your (yours) [fem. dual], your (yours) [fem. pl.
], him (his), her (hers), their (theirs) [masc. dual], their (theirs) [fem. dual], their (theirs) [masc. pl], their (theirs) [fem. pl.].
 .
Stage 1 Stage 2 Stage 3 I L XIc?N I [ L.XICON ?
[6] [7] [81 [91 [10] Karttunen, L., K. Koskenniemi, and R. M. Kaplan.
(s, t) and Pr(s,t).
2.
3.
[Cutting et al., 1992]).
Bo- (:ause of this (:ircularity, we l)ropose to a(:quire.
942 ])uring ~ single iter~tion, we conqmt(; the score, Seere(p), for each cm~(lidate 1)attern p, using (;he fornmla~: S, :o  ,  ,@)  = IH n l~l IHI - 1,,~ IHn  ~.1 (:t) where 12.
The (to(:un~enl;s which t;he winning t)~(;t;ern hits are added (;o t;111(; relevant set.
The  t)al;l;(;rn s(;areh is then r(;sl;m:l;(;d. 3.5 Document  Re- rank ing Th(: above is a simt)lifi(;~l;ion of (;he a,(:tual pro- cedlll(}~ in severa] r(,st)e((;s. Only generalized t)ntl;erns are (:onsidered fi)r (:audi(t~my, with one or mot(, slol;s fill(:(1 wi(;h wihl-cmds.
In comput ing the score of th(, ge, n- (;raliz(:d ]);tttern, w(: do not take into (onsi(h:r- ;i,1;i()11 all possible va,hw, s of the, wil(1-(m:d role.
?e instea.d (:()llS(;raJll (;he wild-(:ar(l to thos(~ wd- u(:s wlli(:h l;ht,llls(;lv(;s ill (;llrH ]l;tV(: high scores.
Th(:se v~du(:s l;lw, n |)e(:on~e lllClll])(;lS of }/.
II(:W (:lass, whi(:h is l)rOdu(:ed in (;:tlldClll with the wimfing 1)att(:rn.
])o(umel~tS reh:wm(e is s(-ored (m ~ s(;ah: l)e- (;ween 0 and 1.
(:cet)ted ~,s trut]~; the do(mlw, nts (;hey mat(:]1 hnve rcle- vmme 1.
On i(;er;~tion i + 1, e~mh t)a(;tern p is assigned a precision measure, t)ase(l on the rel- (:Vall(;e of |;11(; (locllnlelfl;s i|; 111a, l;(;ll(,,q: ~ ".d~(d) (~) f f , , :d  +~ (v)  - -  IH(v) l ,~.
(,,) where l~,eli(d) is the re, levmlce of 1;11(: doeunmn(; fiom t;t1  ~W(: used ,:-- 0.1 and fl = 2. of t)~d;terns p C K m~l;(:h, mid the "cunmlative" precision of K as 1 ~ 1~4~(a,) (3) P~.~d +~(1() = IH U()I <.
These  patterns (tel;ermilm the new reh;wmce score of the document  as J~, "~l,~ " ( ,0  : 111~x (:tc,,.1,*(,O,v,.,;, .~" (K , ) )  (~:) This ensures tha.
~ "~ I I  (1 - ~,.~,.c~(p))"", (5) ~c K(d) where t;11(, weights ,wp arc (tetint;d using the tel- ewm(:(: of the (loeuments, a,s the total  SUl)l)or(; which the pa, I;I;ern p receives: % = log ~ l;.d,(d) dE 11 (p) and ;,7 is (;11( largest weight.
T ~ ~ : : ~  T ;!
g~t : -  il %i [!]
7 [!J Legend: Management/Test ?
.-{~ ...... ManagemenVl-raie - :*: -- MUC-6 ?
n a ( : ( lu i s i t ion .
(MUC-6), Columbia, MD, November.
[., McLean, VA, June.
l?asi ])~panainen a d Time .J/h:vinen.
The expression top(N,next(WN,i)) finds the top N sequences in next(WN,i).
The processing within generate_rhs is a simple iteration. generate_rhs(0). generate_rhs([First 1 Rest]) :generate (First) generate_rhs(Rest).
For example, here is the rule for sentences: s(Form, GO-G, Store)/quant(Q,X,R,S) ---> s(Form, GO-G, [qterm(Q,X,R)IStore])/S.
The term quant (Q,X,R,S) represents a quantified formula with quantifier Q, bound variable X, restriction R and scope S, and qterm(Q,X,R) is the corresponding store element.
As predicted, the correlation is negative (F., = -1.00, n = 6,p <.001, two-tailed).
Here, too, there is a negative correlation: rs = –.90, n = 5, p < .05, two-tailed.
7
Switchboard,  ).
), mode (ROOT, FINITE, etc.
Consider, for example, the ill-formed phrase &quot;each boats.&quot; Suppose the grammar has the three rules, ([np] [da] [noun]), ([noun] [root-noun]), and ([noun] -4 [root-noun] [pl]).
[1] Baglivo, J., Olivier, D., and Pagano, M.  .
[2] Bishop, Y. M.; Fienberg, S.; and Holland, P  .
[3] Black, Ezra  .
5.
( ( ( The cat) meowed ) . )
.
.
.
.
1993).
 .
.
.
. support).
In fact, I(X, Y) is a completely symmetric measure.
In fact, in cases such as the examples of Table 1, where p(X = 1 I Y = 1) = p(Y = 1 I X = 1), the Dice coefficient becomes equal to these conditional probabilities.
Clearly, Si =-- P1 Q, P2 = (?
), and Si = ri • P, for i > 2.
1]</sn><tag>NN</tag> <wd>to</wd><tag>TO</tag> <wd>Eugene</wd><pn>person</pn><sn> [noun .Tops.0] </sn><tag>NP</tag> <wd></wd><tag>POS</tag> <wd>s</wd><tag>NN</tag> <wd>bathroom</wd><sn> [noun.artifact.0]</sn> <tag>NN</tag> <wd>,</wd><tag>,</tag> <wd>to</wd><tag>TO</tag> <wd>turn_on</wd><sn> [verb.contact.0]</sn> <tag>VB</tag> <wd>the</wd><tag>DT</tag> <wd>hot-water-heater</wd><cm t WORD_MIS ING </cmt><tag>NN</tag> <wd>,</wd><tag>,</tag> <wd>and</wd><tag>CC</tag> <wd>on</wd><tag>IN</tag> <wd>the</w d><tag>DT</tag> <wd>side</wd><sn> [noun.location.0] </sn><tag>NN</tag> <wd>of</wd><tag>IN</tag> <wd>the</wd><tag>DT<[tag> <wd>tub</wd><sn> [noun.artifact.
1 ] </sn><tag>NN</tag> <wd>he</wd><tag>PP</tag> <wd>saw</wd><mwd>sce</mwd><msn> [verb.perception.0] </msnxtag>VBD</tag> <wd>a</wd><tag>DT</tag> <wd>pair</wd><sn> [noun .quantity .0] </sn> <tag>NN</tag> <wd>of</wd><tag>IN</tag> <wd>blue</wd><sn> [adj .all.0.col.3] </sn><tag>JJ</tag> <wd>wool</wd><sn> [noun.artifact.0]</sn><tag>NN</tag> <wd>swimming_.trunks</wd><sn> [noun.artifact.0]</sn> <tag>NN</tag> <wd>.</wd><tag>.</tag> </s> Note that the tokenizcrs mistaken linking of "went_down" has now been corrected by the tagger.
2.
3.
However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.
Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.
2.
3.
4.
6.
2.
3.
4.
FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e−14 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data.
However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.
Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.
 ).
 .
 .
(If multiple where the function clip[x,y](z) gives the closest number to z in the interval [x, y].
It is reasonable to assume that the antecedents in W are independent of each other; in other words, P(Wa-4-11W0i h, t, I, a) = P (wp+ilh, t, I , a).
In building a statistical parser for the Penn Tree-bank various statistics have been collected P (with, t, 1, a) = P(wolh. t, I) if i = a Then we have, P(W. ih, t, I, a) = P(wilt) P(walh, t, I) .
.P , two of which are P(walh, t, I) and P(wolt, 1).
 ).
?p).
Method 1.
Method 3.
Method 2.
Method 3.
Here we have used three languages for triangulation (it U {de, en, es, fr}\{s, t}).
?Y, S2??Z, S3?
?X, S1?
Let q = ?q1, ..., qn?
= argmax a?A p(a | q) (1) At a high level, we make three adjustments.
rule, p(a | q) ? p(q | a) ? p(a).
A dependency tree on a sequence w =?w1, ..., wk?
symbol at the left edge of the sentence; it has a single child (|{i : ?(i) = 0}| = 1).
Let x : {1, ..., n} ? {1, ...,m} be a mapping from indices of words in qto indices of words in a.
iq | qi, ?q(i), x(i), ?a) = (3) p#kids(|{j : ?q(j) = i, j < i}| | qi, left) ?p#kids(|{j : ?q(j) = i, j > i}| | qi, right) ? ?
j:?q(j)=i m?
x(j)=0 pkid (qj , ?q(j).lab | qi, ?q(i), x(i), x(j), ?a) ?p(?
Given q, the score for an answer is simply p(q, ?q | a, ?a).
The base case, for leaves of ?q, is: S(j, `) = (4) p#kids(0 | qj , left) ? p#kids(0 | qj , right) ? m? k=0 pkid (qj , ?q(j).lab | q?q(j) , `, k, ?a) Note that k ranges over indices of answer-words to be aligned to qj . The recursive case is S(i, `) = (5) p#kids(|{j : ?q(j) = i, j < i}| | qj , left) ?p#kids(|{j : ?q(j) = i, j > i}| | qj , right) ? m? k=0 pkid (qi, ?q(i).lab | q?q(i), `, k, ?a) ? ?
pbasekid (qi, ?q(i).lab | q?q(i), `, k, ?a) = p(qi.pos | ak.pos) ? p(qi.ne | ak.ne) ?p(?q(i).lab | config(?q, ?a, i)) (6) where qi.pos is question-word i?s POS label and qi.ne is its named-entity label.
| ?) = ?p base kid (?
| ?)+(1??)p ls kid (?
7 Stan-.
q,?q,a,?a p?(q, ?q,a, ?a) log p?(q, ?q | a, ?a) ? ??
P x p?(q,?q,x|a,?a) (8) Note the hidden variable x being summed out; that makes the optimization problem non-convex.
a Jeopardy!
The last one (',) l 0, otherwise plates.
 .
Mazankowski: *ro 1.
We also assume that Pt (e) = Pr (f), Pr (eff ) = Pr (eef ), and Pr ([e) = Pr(f f).
m | [pe~ntlZlt mdtvtdt ta l  mmeoae I I maclune a } S Step 2: Interpretauon 1 tMr,numer} lpenm} {marina% machine s I" Step 2: Interpretation 2 (pez -~ ind iv idua l  me,  ) (~ehme a m~h,ne  s ) Step 2" InterpretaUon 3 iMr.n~tef} [permn} {n,aclane, I Step 2: Interpret=non 4 FFtgure 2 Step 2 Interpretations I to 4 .
ut e 1 ~HA/,N ?
AUX S do NP VP n v $ I I you think s ( B3} tiP_ V.P~ Mary  ~z N I t hought olin} NP VP Mary  v S I t bought S ~ "  ~.
e::= NIL TOP a I :e l el A e2 e~.
: li,m re- ispectivel~y.
717 For exampl% the reentrant feature structure used in Section 1.2, is represented by the set of equations z = eat : S h l : y A 2 : (eat : VP  h age : z A subject : y) y = cat : N P A agr  : z We represent a set of equations, xi = ei for 1 <: i < n as rec ( Zh .
,Xn  >~( e l , .
bsoo,)  A N A .
( t ,oo ,  A.. .
t ,  A f#(bs)  ^ .
t ,  A b, A ~..) which can be obtained by representing T by 718 (...t,~ A Z(b,~) A...) where I is the identity function.
( / )  V I ( I ) )  = ~f .
V l~( I )  v f ) and represent 7 by (.. .t,  A F(b,) A. .
= Ax.e,~ e l , .
, f l ,  as discussed above.
, / , ) (eo,e~,.. .
11 e"  l ,e ra ,e l , "  ,  k,g .
Subjaceney in a Tree A,Ijoh g C,ramnmr.
I(~mnds, W. C. and Kaspcr, 1{.
CompsL I)ing.address a,d page.s82--93.
A ,q_Judy of Tcc Atl~oinin9 (;tammar.,~.
2.
9.
In another experiment, we selected a random sample of 97 words; 67 of them were unambiguous and therefore had a baseline performance of 100%.10 The remaining thirty words are listed along with the number of senses and baseline performance: virus (2, 98%), device (3, 97%), direction (2, 96%), reader (2, 96%), core (3, 94%), hull (2, 94%), right (5, 94%), proposition (2, 89%), deposit (2, 88%), hour (4, 87%), path (2, 86%), view (3, 86%), pyramid (3, 82%), antenna (2, 81%), trough (3, 77%), tyranny (2, 75%), figure (6, 73%), institution (4, 71%), crown (4, 64%), drum (2, 63%), pipe (4, 60%), processing (2, 59%), coverage (2, 58%), execution (2, 57%), min (2, 57%), interior (4, 56%), campaign (2, 51%), output (2, 51%), gin (3, 50%), drive (3, 49%).
For these cases, npr--,- np(1 — p).
1995).
2.
3.
Recall that the semantic interpreter is required to compute P(M s ,T) P(W IT) .
For example, a classifier may label <X, Y> as BEFORE.
A total of 99 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German, and Spanish news sites:2 Czech: Blesk (1), CTK (1), E15 (1), den´ık (4), iDNES.cz (3), iHNed.cz (3), Ukacko (2), Zheny (1) French: Canoe (3), Croix (3), Le Devoir (3), Les Echos (3), Equipe (2), Le Figaro (3), Liberation (3) Spanish: ABC.es (4), Milenio (4), Noroeste (4), Nacion (3), El Pais (3), El Periodico (3), Prensa Libre (3), El Universal (4) English: CNN (3), Fox News (2), Los Angeles Times (3), New York Times (3), Newsweek (1), Time (3), Washington Post (3) German: Berliner Kurier (1), FAZ (3), Giessener Allgemeine (2), Morgenpost (3), Spiegel (3), Welt (3) The translations were created by the professional translation agency CEET.3 All of the translations were done directly, and not via an intermediate language.
In our case, this happens randomly, say, 1:A, 2:B, 3:C, 4:D (for simplicity’s sake).
.
(17) a.
(18) a.
.
.
2.
3.
4.
Text 0 1 Y. J.
Case 1 - LINK (.)
2.
2.
3.
3.
4.
4.
That is, 6 = (12 - lic)/ Vlis2.
That is, D(i,j) is defined by the following recurrence with the initial condition D(i, j) = 0.
(in press).
Sankoff, D., and Kruskal, J.
 .
2.
= (h1, . . .
= (s1, . . .
, sn).
UofL??
UofL??
1.
 .
},,,{ 21 Isssst.
(Rayner & Frazier 831.)
Consider (9) above.
8.
2.
The label ?S? means ?sentence?, ?NP?
(time) and ??MNR?
(GTB-beta).
Figure 1.
(GTB beta).
(a) Filter by contraindices.
Decoder = argmax Pr (S I T) = argmax Pr (S,T) A Decoder performs the actual translation.
.
.
. sn) = Pr (si) Pr (s2I si) .
.
.
Pr (sn I sis2 s,_ i).
.
. s3_.
It should also be noted that RA, B) = P(B, A).
P(root of tree6) = 1).
: LDC2007T07).
 .
2003).
Given two trees T and T' with root nodes v and v', respectively, the similarity Sim(T,T') between the trees is defined to be the maximum of the three expressions NodeCompare(T,T'), maxs∈c(T) Sim(Ts,T'), and maxs,∈c(T') Sim(T,T,,).
*/ return (node-sim + res.score, (tree1.top, tree2.top) U res.map) ;
{car, auto, automobile, machine, motorcar } .
(6) all x1 (mosaic nn(x1) internet nn(x1) & browser nn(x1)).
337 [hyper,336,123,299,335] nn nnc($c111,$c111,$c111,$c111).
347 [hyper,300,34] make vb($c126,$c96,$c111).
356 [hyper,347,44] create vb($c126,$c96,$c111).
372 [hyper,332,302,303,304,305,306,307] organization at($c96).
373 [hyper,372,74] company nn($c96).
374 [hyper,373,294,372,356,336,335,299,337] $F.
1a shows a probabilistic FST with input alphabet E = {a, b}, output alphabet A = {x, z}, and all states final.
It represents a joint distribution (since Ex,y f(x, y) = 1).
1c defines a conditional one (bx Ey f(x, y) = 1).
1a, thanks to complex parameter tying: arcs ® b:p −) @, ® b:q −) ® in Fig.
1b get respective probabilities (1 − A)µν and (1 − µ)ν, which covary with ν and vary oppositely with µ.
1  .
P(v, z) def = Ew,x,y P(v|w)P(w, x)P(y|x)P(z|y), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic  .
.
1a and suppose (xi, yi) = (a(a + b)*, xxz).
2.
2.
This nontrivially works out to (4, 1, 0,1,1,1,1, 2).
We will enforce an invariant: the weight of any pathset H must be (&EΠ P(π), &EΠ P(π) val(π)) E R>0 x V , from which (1) is trivial to compute.
® lets us concatenate (e.g.) simple paths π1, π2 to get a longer path π with P(π) = P(π1)P(π2) and val(π) = val(π1) + val(π2).
The definition of ® guarantees that path π’s weight will be (P(π), P(π) · val(π)).
2).
For instance, recall that traversing Q a:x −) Q should have the same effect as traversing both the underlying arcs ® a:p −) ® and © p:x −) ©.
distribution P(?)
rule: P(?|D) ? P(D|?)P(?).
0 and that for all nonterminals A ? N , ?A???R ?A??
= 1.
A PCFG (G, ?) defines a probability distribution over trees t as follows: PG(t|?)
= ? r?R ?fr(t)r where t is generated by G and fr(t) is the number of times the production r = A ? ?
= 0.
= ? t:y(t)=w PG(t|?).
Given a corpus of strings w = (w1, . . .
rule (Equation 1) to obtain: P(?|w) ? PG(w|?)P(?), where PG(w|?)
= n ? i=1 PG(wi|?).
The joint posterior distribution on t and ? is given by: P(t, ?|w) ? P(w|t)P(t|?)P(?)
= ( n ? i=1 P(wi|ti)P(ti|?)
) P(?)
with P(wi|ti) = 1 if y(ti) = wi, and 0 otherwise.
We take P(?)
has a corresponding Dirichlet param eter ?A??.
The Dirichlet prior PD(?|?)
is: PD(?|?)
= ? A?N PD(?A|?A), where PD(?A|?A) = 1 C(?A) ? r?RA ??r?1r and C(?A) = ? r?RA ?(?r) ?(?r?RA ?r) (2) where ? is the generalized factorial function andC(?)
rule, PG(?|t, ?) ?
PG(t|?)
PD(?|?)
( ? r?R ?fr(t)r )( ? r?R ??r?1r ) = ? r?R ?fr(t)+?r?1r which is a Dirichlet distribution with parameters f(t) + ?, where f(t) is the vector of production counts in t indexed by r ? R. We can thus write: PG(?|t, ?) = PD(?|f(t) + ?)which makes it clear that the production counts com bine directly with the parameters of the prior.
For example, in our case, given samples (ti, ?i) for i = 1, . . .
1 ? ?
In our case, this means alternating between sampling from two distributions: P(t|?,w, ?) = n ? i=1 P(ti|wi, ?), and P(?|t,w, ?) = PD(?|f(t) + ?) = ? A?N PD(?A|fA(t) + ?A).
.?A1 . . .
?A|N| ?A1 . . ..
?Aj ?A|N| . . .
PG(ti|wi, ?) = PG(ti|?)
PG(wi|?)
The production probabilities ?A for each nonterminal A ? N are sampled from a Dirchlet distibution with parameters ??A = fA(t) + ?A. There are several methods forsampling ? = (?1, . . .
, ?m) from a Dirichlet distri bution with parameters ? = (?1, . . .
, ?m), with thesimplest being sampling xj from a Gamma(?j) dis tribution for j = 1, . . .
,m and then setting ?j = xj/ ?m k=1 xk  .
, wn) where each wi ? T ,and define wi,k = (wi+1, . . .
pA,k?1,k = ?A?wk pA,i,k = ? A?B C?R ? i<j<k ?A?B C pB,i,j pC,j,k for all A,B,C ? N and 0 ? i < j < k ? n. At the end of the Inside algorithm, PG(w|?)
function SAMPLE(A, i, k) : if k ? i = 1 then return TREE(A,wk) (j,B,C) = MULTI(A, i, k) return TREE(A, SAMPLE(B, i, j), SAMPLE(C, j, k))In this pseudo-code, TREE is a function that con structs unary or binary tree nodes respectively, and 142 MULTI is a function that produces samples from a multinomial distribution over the possible ?split?
positions j and nonterminal children B and C , where: P(j,B,C) = ?A?BC PGB (wi,j|?)
PGC (wj,k|?)PGA(wi,k|?)
, ti?1, ti+1, . . .
, tn) is the current set of parses for w?i = (w1, . . .
, wi?1, wi+1, . . .
, wn).
Let PD(?|?)
PG(t|?)PD(?|?)d? = ? A?N C(?A + fA(t)) C(?A) (3) where C was defined in Equation 2.
We can use Equation 3 to compute the conditional probability P(ti|t?i, ?) as follows: P(ti|t?i, ?) = P(t|?)
P(t?i|?)
= ? A?N C(?A + fA(t)) C(?A + fA(t?i)) Now, if we could sample from P(ti|wi, t?i, ?) = P(wi|ti)P(ti|t?i, ?) P(wi|t?i, ?) we could construct a Gibbs sampler whose states were the parse trees t. Unfortunately, we don?t evenknow if there is an efficient algorithm for calculating P(wi|t?i, ?), let alne an efficient sampling al gorithm for this distribution.Fortunately, this difficulty is not fatal.
= min { 1, ?(s ?)Q(s|s?) ?(s)Q(s?|s) }and with probability 1 ?A(s, s?)
Then we sample t?i fromP(ti|wi, ??)
Finally, we accept the proposal t?i given the old parse ti for wi with probability: A(ti, t?i) = min { 1, P(t ? i|wi, t?i, ?)P(ti|wi, ??)
P(ti|wi, t?i, ?)P(t?i|wi, ??)
} = min { 1, P(t ? i|t?i, ?)P(ti|wi, ??)
P(ti|t?i, ?)P(t?i|wi, ??)
of the proposal distribution P(t?i|wi, ??).
to the expected value E[?|t?i, ?] of ? given t?i and ? as follows: ??r = fr(t?i) + ?r ? r??RA fr?(t?i) + ?r?
We know that PCFGs ? = (0.1, 1.0) ? = (0.5, 1.0) ? = (1.0, 1.0) Binomial parameter ?1 P(?1|?)
As ?1 ? 0, P(?1|?)
Annealing, i.e., using P(t|w)1/?
?Jittering?
V = 1, and every string w ? w is analysed as a single morpheme V.
are both 0).
1).
Let T(W) be the set of pairs (r, w') such that log x 1,r,is positive.
A policy of candidate space (A, I, J, f, e, x) is a function that maps each member i E I to a member of J(i).
As can be seen in Figure 1, using w = [−2, 1], Hw(p1) = 9 and Hw(p2) = −8.
(2008b; 2009).
Specifically, for candidate translation pair e(i, j) and e(i, j'), we want: g(i,j) > g(i,j') ⇔ hw(i,j) > hw(i,j').
For example, given the candidate space of Figure 1, since g(1,1) > g(1, 3), we would add ([−4, 3], +) and ([4, −3], −) to our training set.
For each source sentence i, the sampler generates F candidate translation pairs hj, j'i, and accepts each pair with probability αi(|g(i,j) − g(i, j')|).
(2008b; 2009).
In p(f|f1, f2,.
The path A − D − G − H is a reverse-time backoff model.
In CallHome-Arabic, words are accompanied with deterministically derived factors: morphological class (M), stems (S), roots (R), and patterns (P).
We seek a function f : x H R that gives a continuous rating f(x) to a document x.
2.
Tuning ranges are c ∈ {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and α ∈ {0.01, 0.1, 0.5,1.0,1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}.
Tuning ranges are k' E 12, 3, 5,10, 20} and Q E 10.001, 0.01, 0.1,1.0,10.01.
The optimal parameters are k' = 5 and Q = 1.0.
2.
3.
A).?
2.
1.
2.
3.
4.
5.
6.
7.
9.
The Gildea and Palmer (G&P) System.
The first ?C&R Sys tem I?
The second ?C&R System II?
Task P R F1 A (%) (%) (%)ALL Id. 75.8 71.4 73.5 ARGs Classification - - - 83.8Id.
7.
8.
2, 1869”, “2nd October 1869”, “October 2 1869”, and so on).
).
P(S _k+1.••tn); which we denote by a(Nick).
Our final thresholding measure is P(X)x)3(Nfk).
(explicit) ?I like Ike?
Model 2 is the geometric mean: Model 2: cwcpif wcpscP ij n i i cn = ?= ? = ? )|(argmax ,)|(10)|( j 1 1)( 2.2.4 Examples The following are two example outputs.
Three systems were phrase-based (A, C and E), two hierarchical (B and D) and one syntax-based (F).
Rule 2'). a.
- - CONT. cheap - cheap exp.
- exp.
- exp. exp.
- exp.
10c'.
2.
3.
4.
 .
So a feature extractor maps each y to a vector of feature values f(y) = (f1(y), ..., fm(y)).
This section describes how each parse y is mapped to a feature vector f(y) = (f1(y), ... , fm(y)).
(s1), ... , y?(sn).
 .
= components, cr.pr.
 .
128 24% 161 29% 47 9% 148 27% 32 6% 17 3% 11 2% 6.
 , and Thompson  ).
2.
The tagset consists of S(ubjective), O(bjective) and U(ncertain).
The basic QLF analysis might be (ignoring tense): _s :meet (term(+b, <type=q,lex=every> ,boy , _q, _x) , term(+g,<type=q,lex=a>, Y'and(girl(Y),tall(Y)),_r,_y)).
.
For John slept we might have: _s:sleep(term(+e,<type=event>, E-form(<type=trel,tense=past>, 11- and( event (E) ,R(E)) , _t) , _q1,_e), term(+j,<type=name>, J-name(J,'John'),_q2,_j)).
Constructs in the 'standard' predicate logic subset of QLF receive their semantics with the usual evaluation rules, for example: The evaluation rule for a formula F with a scoping variable instantiated to [1,3, .] and containing a term T=term(I,C,R,Q,A) is as follows: • [[[1)3,...] :F]]=1 iff [[Q(R' ,F')]]=1, and 0 otherwise, where Ft' is X-(and(R(X),X=A))[X/I], and F' is X&quot;( [3, ..
.
Q6 W(F,v) if W(F[Q/_q, where: F is a formula containing the term T=term(I,C,R,_q,_r), and Q is a quantifier such that S(C,Q).
Machine translation takes a source sequence, S = [s1 s2 . . .
sK ] and generates a target sequence, T = [t1 t2 . . .
We seek to maximize the alignment probability by finding the optimum link configuration Lopt, p(Lopt|S, T ) = argmax L?L p(L|S, T ) = p(lMi |tM1 , sK1 ) = M ? i=0 p(li|tM1 , sK1 , li?11 ).We factor this into a transition model and an obser vation model, p(L|S, T ) = 1Z M ? i=0 p(li|li?1)?p(li|tM1 , sK1 , li?11 )1??.
f = ?(li) h = [ ti?11 , sK1 ] p(f |h) = 1Z(h) exp ? i ?i?i(h, f), where Z(h) is the normalizing constant, Z(h) = ? f exp ? i ?i?i(h, f).
and ?i(h, f) are binary valued feature functions.
and that ?for?, ?in?and ?to?
align to ?b#?
1 Anno.
1?
1?
probability with an IBM Model 1 estimate, p(li|tM1 , sK1 ) = 1 Z pME(li|t M 1 , sK1 )?pM1(s|ti)1??
This all relies on a score factorization over adjacent words in the compression, s(x, I(yj−1), I(yj)) = w · f(x, I(yj−1), I(yj)).
[NP Det N n(2,3).
1983).
8.
2 Yess!
 .
Generate the word we,i from Mult(o,;e,i).
 .
This auxiliary quantity can then be calculated recursively from DLev(i − 1, l), DLev(i, l − 1), and DLev(i − 1, l − 1).
The calculation of D(i, l) requires all values of D(i', l) to be known, even for i' > i.
Therefore, this algorithm calculates dCD in time O(I · L) and space O(I).
2.
In short, cSUB ≤ 2.
For instance, if p is a preposition, f (p) = Ew f (w, p).
Where f (N , p) = En f (n,p), f (V , p) = E, f (v, p), f (N) = En f (n) and 4 The nonintegral count for send is a consequence of the data-splitting step Ambiguous Attach 2, and the definition of unigram frequencies as a sum of bigram frequencies.
1.
2).
2.
3 (c).
, a} are the positive model parameters and { , fk} are known as &quot;features&quot;, where fj (h, t) E {OM.
• • • w,}, a tag sequence candidate {ti .
 .
The components, a(1), a(2), and a(4) of a above a(5) drift up the path in which runs from the substitution node.
The components, a(1), a(2), and a(4) of a above a(5) drift up the path in which runs from the substitution node.
Consider a DTG G = (VN, VT S D).
The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion.
Thus, there are ai = (bk + 1)(ci + 1) possible subtrees headed by A@j.
-1- = -.
First, compute the inside probabilities, e(s,t, X) = P(X Ws.--wt).
2001).
2001).
, , and .
.
.
2.
2.
 .
7.
.
.
.
.
.
By symmetry, we have Pr[hr(u) =� hr(v)] = 2Pr[u.r > 0, v.r < 0].
Thus, we have Pr[u.r > 0, v.r < 0] = 0(u, v)/27r.
Proceeding we have Pr[hr(u) =� hr(v)] = 0(u, v)/7r and Pr[hr(u) = hr(v)] = 1 − 0(u, v)/7r.
3.
We can represent the signature of vector u as: u� = {hr1(u), hr2(u), , hrd(u)}.
This step takes O(n), since we can assume B, q, d to be a constant.
(i.e., n < k).
For instance, like(e, x, y) is a well-formed SEP.
We write a SSEMENT as: [i1][i2][SEPs]{EQs}.
Zeevat, 1989).
.
Fernando  ).
.}
.}
In gral)hi{:al re i ) rese l l ta l ; i{} l l  ,  ] l o{tes  &I; which substitul;ion can take 1)lac{~ are ]uarked with dow]>arrows.
St)ccifically, a node q in the intml; si;ru(:- ture is assigned ~t sui)e, rt;~g s so th;tt the 1)rol):t - |)ilil;y of fin(ling the treelet (;()m])ose(t of ~1 with superta X ,~ ;rod :dl of its (l;mght(;rs (as foun(t in I;he ini)ut sl;rucl;ure) is m;rximiz(;d, and such l;ha, t .~ is (:Oml)a, tit)le with q~s mother ~tll(l her sut)e, rtag .sin.
No(;c that while a (le, riw~tion tree in TAG fully Sl)(:(:- iiies a derivation and thus :t smTth,(:e, s(mte.n(:e, the oul;lmt fl:om the ~l~-ee Chooser (loes not;.
Gen(~rationAcc,,racy = (1 - 54 + I I + 1) -t- ,q ) (2) The siml)lc, a(:cura(y, g(merntion a((:ur;my a,n(l tim av(n:ag(~ time, ti)r goamration of (;a,(:h l;cst; s(~,u - t(m(c for tim tino,(, (}Xl)crinmnts ;~r(~ tabul~m,xl in %d)le 1.
1)igr~ml 1no(M).
Ih)w(,ver, (;t1(; first t)hases nr(, quit(, ditf(;r(mt.
FEII.GUS (:ould (,asily |)(; augm(;nt(;d with a t)r(;t)ro(:cssor l;h~d; maps a so, m;mti(: rc, t)ro, s(mtal;ion t;o ore: syn- ta(:ti(: inl)ut; this is not the focus of our r(~sc~uch.
[Iowev(,r, ther(~ are two more imt)orl,mfl; differ- (m(es.
Computational Linguistics, 25(2).
For example, (&quot;s&quot;, NULL) constitutes such a pair from Figure 2.
Algorithm one had proposed that Xa=fa,b,c,d,e,f,g,il when in reality, = Ya={ a,b,c,d}.
Since IX,, n Ya I = 4 and IYak4, then CA=4/ 4.
F-Measure (F1).
Thus for COARSEARGM ALL we count {0} as a true positive span, {1,2} , {3,4}, and {7,8,9} as false positive, and {1, 2, 3, 4} and {7, 8, 9} as false negative.
.
.
2005).
Similarly, any index x E/ [i, k] is external to T[Z,k].
1.
.
(2).
2.
A1 = {(aj , j)lj = 1 .
.
.
The alignment (i, j) has the neighbouring links (i — 1,j), (i, j — 1), (i + 1,j), and (i, j + 1).
3.
4.
5.
(8) is very large.
2.
(9) P(zlik) instead of p(zI4).
 .
 .)
(r(p,) = 0.51 and r(2)) = 0.48: for tree accuracy and generation tree accuracy, for both p < 0.05).
This model was significant: F(3,19) = 6.62, p < 0.005.
The tagged dependency tree defined for x by c = (E, B, A, 7r, 6) is the tree (Vx, A) with labeling functions 7r and 6, which we write TREE(x, c).
2.
T = {(xt, yt)}t_1 denotes the training data.
In equation (1), s(x, y) is calculated with respect to the weight vector after optimization, w(Z+1).
If j k, we rename Ck(k) as Ck_i (i) and for 1 i,j, we set Ck-i (1) to Ck(/).
Obviously, Ik-i = Ik(i,j).
preposed adjuncts = 0/218 (0%) conjoined S = 1/218 ( .5%) conjoined VP = 7/218 (3%)
preposed adjuncts = 2/73 (2.7%) conjoined S = 3/73 (4%) conjoined VP = 20/73 (27%)
6).
In [3] H(c) approximately consists of the label, head, and head-part-of-speech for the parent of c: m(c), i(c), and u(c) respectively.
One exception is the distribution p(e(c) j l(c), t(c), h(c), H(c)), where H only includes m and u.1 Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c).
We also empirically checked that our individual distributions (p(t 1 l, m, u, i), and p(h 1 t,l, m, u, i) from Equation 5 and p(L 1 l, t, h, m, u), p(M 1 l, t, h, m, u), and p(R 1 l, t, h, m, u) from Equation 5) sum to one for a large, random, selection of conditioning events2 As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established threshold) have their probabilities recomputed according to the complete probability model of Equation 5.
2.
3.
First, we add C to M(i, i).
3.
 .)
A: No.
(ovaPSP ovaTO(cos) [3c]; regPSPregTO(cos) [4c]) Q: Could you explain that notation, please?
Q: Thanks.
A: No.
As an example, let T = ‘blau-adjective’, t1 =‘masculine’ and t2 =‘feminine’.
.
.
The two entries (‘beabsichtigt’|‘intends’) and (‘beabsichtigt’|‘intended’), for example, produce three new entries: (‘beabsichtigt-verb-ind.-pres.-sg.3rd’|‘intends’), (‘beabsichtigt-verb-past-part.’|‘intended’), and (‘beabsichtigtadjective-pos.’|‘intended’).
2000).
For example, it is likely that a(i1j, m, 1) will be very close to a(i + 11j+ 1, m, /) and a(i1j, m + 1,1+1).
 ).
and ?murderer?)
to ?liquidator?).
H: Clinton?s book is a big seller.
entities in drs(T).
T implies H (shows entailment) 2.
An example is ?e?x?y(event(e)?agent(e,x)?in(e,y)?in(x,y)) which states that if an event is located in y, then so is the agent of that event.
A hyponymy relation between two 630 synsets A and B is converted into ?x(A(x)?B(x)).
Two synset sisters A and B are translated into ?x(A(x)?
?B(x)).
Examples 78 and 1952 would be supported by knowledge similar to: ?x(clinton(x)?person(x)) ?x(book(x)?artifact(x)) ?x(artifact(x)?
An example: ?x?y(paris(x)?france(y)?in(x,y)) 3.4 Model Building.
For instance, the model returned for fol(drs(T)) in Example 78 is one where the domain consists of three entities (domain size = 3): D = {d1,d2,d3} F(loc) = {} F(book) = {d1,d2} F(seller) = {} F(clinton) = {d3} F(be) = {} F(of) = {(d1,d3)} F(agent) = {} F(big) = {} F(patient) = {}Model builders like Paradox generate finite mod els by iteration.
 ).
For instance, the following model D = {d1,d2,d3} F(cat) = {d1,d2} F(john) = {d3} F(of) = {(d1,d3)} F(like) = {(d3,d1),(d3,d2)} has a domain size of 3 and 3 instantiated two-place relations, yielding a model size of 3 ? 3 = 9.
It is not a trivial problem and has been approached by many researchers [GCY92], [YA93], [B&W94], [RE95], [YA95], [KAE96], fL196}, etc.
2.
1.
2.
)[p(PP ADvl A = A.
)log2p(PIDADvl A = + p(PPADA A = A.,)log 2 p(PPADA A = A.
3.
