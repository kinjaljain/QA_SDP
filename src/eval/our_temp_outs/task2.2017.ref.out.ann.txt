Citance Number: 1 | Reference Article: W09-0621 | Citing Article: D12-1016 | Citation Marker Offset: '83' | Citation Marker: 2009.0 | Citation Offset: '83','84' | Citation Text: <S sid ="83" ssid = "12">GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009) </S> | Reference Offset: ['3'] | Reference Text: <S sid ="3" ssid = "3">We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: W09-0621 | Citing Article: D13-1155 | Citation Marker Offset: '102' | Citation Marker: Wubben et al., 2009 | Citation Offset: '102' | Citation Text: <S sid ="102" ssid = "5">Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan &amp; Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.</S> | Reference Offset: ['13'] | Reference Text: <S sid ="13" ssid = "13">News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '43' | Citation Marker: Wubben et al. | Citation Offset: '43' | Citation Text: <S sid ="43" ssid = "15">Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].</S> | Reference Offset: ['3'] | Reference Text: <S sid ="3" ssid = "3">We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '253' | Citation Marker: 9.0 | Citation Offset: '253' | Citation Text: <S sid ="253" ssid = "9">The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].</S> | Reference Offset: ['10'] | Reference Text: <S sid ="10" ssid = "10">Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '288' | Citation Marker: Wubben et al. | Citation Offset: '288' | Citation Text: <S sid ="288" ssid = "44">Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.</S> | Reference Offset: ['40'] | Reference Text: <S sid ="40" ssid = "24">Our first approach is to use a clustering algorithm to cluster similar headlines.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '289' | Citation Marker: Wubben et al. | Citation Offset: '289' | Citation Text: <S sid ="289" ssid = "45">Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.</S> | Reference Offset: ['43'] | Reference Text: <S sid ="43" ssid = "27">The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k &lt; n) in a vector space.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '349' | Citation Marker: Wubben et al. | Citation Offset: '349' | Citation Text: <S sid ="349" ssid = "105">However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.</S> | Reference Offset: ['60'] | Reference Text: <S sid ="60" ssid = "44">For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '362' | Citation Marker: Wubben et al. | Citation Offset: '362' | Citation Text: <S sid ="362" ssid = "118">The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.</S> | Reference Offset: ['81'] | Reference Text: <S sid ="81" ssid = "5">It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: W09-0621 | Citing Article: PS15684-W09 | Citation Marker Offset: '370' | Citation Marker: 9.0 | Citation Offset: '368','369','370' | Citation Text: <S sid ="368" ssid = "124">Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.</S> | Reference Offset: ['68'] | Reference Text: <S sid ="68" ssid = "52">With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos() = V 1  V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: W09-0621 | Citing Article: W10-4223 | Citation Marker Offset: '33' | Citation Marker: 2009.0 | Citation Offset: '33','34' | Citation Text: <S sid ="33" ssid = "10">This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.</S> | Reference Offset: ['57'] | Reference Text: <S sid ="57" ssid = "41">We use an F -score with a  of 0.25 as we favour precision over recall.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: W09-0621 | Citing Article: w11-1604 | Citation Marker Offset: '34' | Citation Marker: Wubben et al., 2009 | Citation Offset: '34' | Citation Text: <S sid ="34" ssid = "7">So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.</S> | Reference Offset: ['1'] | Reference Text: <S sid ="1" ssid = "1">For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: W09-0621 | Citing Article: w11-1604 | Citation Marker Offset: '73' | Citation Marker: Wubben et al., 2009 | Citation Offset: '73' | Citation Text: <S sid ="73" ssid = "16">To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.</S> | Reference Offset: ['46'] | Reference Text: <S sid ="46" ssid = "30">We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '17' | Citation Marker: Rosti et al.,2007b | Citation Offset: '16','17' | Citation Text: <S sid ="16" ssid = "16">In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.</S><S sid ="17" ssid = "17">This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '32' | Citation Marker: Rosti et al.,2007b | Citation Offset: '32' | Citation Text: <S sid ="32" ssid = "7">Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '92' | Citation Marker: Rosti et al.,2007b | Citation Offset: '88','89','90','91','92' | Citation Text: <S sid ="88" ssid = "35">Bangalore et al.</S><S sid ="89" ssid = "36">(2001), Sim et al.</S><S sid ="90" ssid = "37">(2007), Rosti et al.</S><S sid ="91" ssid = "38">(2007a), and Rosti et al.</S><S sid ="92" ssid = "39">(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: P07-1040 | Citing Article: C08-1014 | Citation Marker Offset: '97' | Citation Marker: Rosti et al.,2007b | Citation Offset: '93','94','95','96','97' | Citation Text: <S sid ="93" ssid = "40">Bangalore et al.</S><S sid ="94" ssid = "41">(2001) used a WER based alignment and Sim et al.</S> <S sid ="95" ssid = "42">(2007), Rosti et al.</S><S sid ="96" ssid = "43">(2007a), and Rosti et al.</S><S sid ="97" ssid = "44">(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.</S> | Reference Offset: ['89'] | Reference Text: <S sid ="89" ssid = "10">The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: P07-1040 | Citing Article: D09-1115 | Citation Marker Offset: '7' | Citation Marker: Rosti et al.,2007a | Citation Offset: '7' | Citation Text: <S sid ="7" ssid = "7">In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: P07-1040 | Citing Article: D09-1115 | Citation Marker Offset: '133', '134' | Citation Marker: Rosti et al.,2007a | Citation Offset: '133','134' | Citation Text: <S sid ="133" ssid = "3">While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.</S><S sid ="134" ssid = "4">(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.</S> | Reference Offset: ['124', '125', '126'] | Reference Text: <S sid ="124" ssid = "3">All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.</S> <S sid ="125" ssid = "4">All confusion network are connected to a common end node with NULL arcs.</S><S sid ="126" ssid = "5">The final arcs have a probability of one.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: P07-1040 | Citing Article: D09-1115 | Citation Marker Offset: '148' | Citation Marker: Rosti et al.,2007a | Citation Offset: '147','148' | Citation Text: <S sid ="147" ssid = "17">Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).</S><S sid ="148" ssid = "18">ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).</S> | Reference Offset: ['80', '81', '82'] | Reference Text: <S sid ="80" ssid = "1">Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.</S><S sid ="81" ssid = "2">The other hypotheses are aligned against the skeleton.</S><S sid ="82" ssid = "3">Either votes or some form of confidences are assigned to each word in the network.</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: P07-1040 | Citing Article: N09-2003 | Citation Marker Offset: '24', '25' | Citation Marker: 2007.0 | Citation Offset: '24','25' | Citation Text: <S sid ="24" ssid = "24">Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.</S><S sid ="25" ssid = "25">(2007), each component system produces a set of translations, which are then grafted to form a confusion network.</S> | Reference Offset: ['119', '120', '121'] | Reference Text: <S sid ="119" ssid = "15">On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.</S><S sid ="120" ssid = "16">This -best list is then re-scored with the higher order -gram.</S><S sid ="121" ssid = "17">The second set of weights is used to find the final -best from the re-scored -best list.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '2', '3' | Citation Marker: 2007.0 | Citation Offset: '2','3' | Citation Text: <S sid ="2" ssid = "2">We build our confusion networks using the method of Rosti et al.</S><S sid ="3" ssid = "3">(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '23', '24' | Citation Marker: 2007.0 | Citation Offset: '23','24' | Citation Text: <S sid ="23" ssid = "23">The procedure described by Rosti et al.</S> <S sid ="24" ssid = "24">(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.</S> | Reference Offset: ['196','197'] | Reference Text: <S sid ="196" ssid = "4">Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.</S><S sid ="197" ssid = "5">This guarantees that the best path will not be found from a network generated for a system with zero weight.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '29', '30' | Citation Marker: 2007.0 | Citation Offset: '28','29','30' | Citation Text: <S sid ="28" ssid = "28">In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.</S><S sid ="29" ssid = "29">For this, Rosti et al.</S>S sid ="30" ssid = "30">(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.</S> | Reference Offset: ['64','70'] | Reference Text: <S sid ="64" ssid = "15">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S><S sid ="70" ssid = "21">ric since it is based on the rate of edits required to transform the hypothesis into the reference.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '69', '70' | Citation Marker: 2007.0 | Citation Offset: '69','70' | Citation Text: <S sid ="69" ssid = "1">ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.</S><S sid ="70" ssid = "2">(2007).</S> | Reference Offset: ['6','144','145','146'] | Reference Text: <S sid ="6" ssid = "6">A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.</S><S sid ="144" ssid = "6">The algorithm explores better weights iteratively starting from a set of initial weights.</S><S sid ="145" ssid = "7">First, each dimension is optimized using a grid-based line minimization algorithm.</S><S sid ="146" ssid = "8">Then, a new direction based on the changes in the objective function is estimated to speed up the search.</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: P07-1040 | Citing Article: P08-2021 | Citation Marker Offset: '78', '79' | Citation Marker: 2007.0 | Citation Offset: '78','79' | Citation Text: <S sid ="78" ssid = "10">Note that the algorithm of Rosti et al.</S><S sid ="79" ssid = "11">(2007) used N -best lists in the combination.</S> | Reference Offset: ['93'] | Reference Text: <S sid ="93" ssid = "14">When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.</S> | Discourse Facet: Method_Citation | 
Citance Number: 14 | Reference Article: P07-1040 | Citing Article: P11-1125 | Citation Marker Offset: '47', '48' | Citation Marker: 2007b | Citation Offset: '46','47','48' | Citation Text: <S sid ="46" ssid = "16">This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).</S><S sid ="47" ssid = "17">Rosti et al.</S><S sid ="48" ssid = "18">(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 15 | Reference Article: P07-1040 | Citing Article: P11-1125 | Citation Marker Offset: '127' | Citation Marker: 2007b | Citation Offset: '127' | Citation Text: <S sid ="127" ssid = "18">Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: P07-1040 | Citing Article: P39_p07 | Citation Marker Offset: '24', '25' | Citation Marker: 2007.0 | Citation Offset: '24','25' | Citation Text: <S sid ="24" ssid = "24">@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.</S><S sid ="25" ssid = "25">(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.</S> | Reference Offset: ['119','120','121'] | Reference Text: <S sid ="119" ssid = "15">On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.</S><S sid ="120" ssid = "16">This -best list is then re-scored with the higher order -gram.</S><S sid ="121" ssid = "17">The second set of weights is used to find the final -best from the re-scored -best list.</S> | Discourse Facet: Method_Citation | 
Citance Number: 17 | Reference Article: P07-1040 | Citing Article: P101121_p07 | Citation Marker Offset: '20', '21' | Citation Marker: 2007.0 | Citation Offset: '19' 20','21' | Citation Text: <S sid ="19" ssid = "19">If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.</S> <S sid ="20" ssid = "20">For example, Rosti et al.</S><S sid ="21" ssid = "21">(2007) report such an effect.</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: P07-1040 | Citing Article: Pling_p07 | Citation Marker Offset: '13' | Citation Marker: 2007.0 | Citation Offset: '12','13' | Citation Text: <S sid ="12" ssid = "12">In this paper, a system combination based on confusion network (CN) is described.</S><S sid ="13" ssid = "13">This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).</S> | Reference Offset: ['36'] | Reference Text: <S sid ="36" ssid = "36">In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 19 | Reference Article: P07-1040 | Citing Article: Pling_p07 | Citation Marker Offset: '41' | Citation Marker: 2007.0 | Citation Offset: '41' | Citation Text: <S sid ="41" ssid = "13">This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.</S> | Reference Offset: ['199','200','201','202','203'] | Reference Text: <S sid ="199" ssid = "7">The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks.</S><S sid ="200" ssid = "8">Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.</S> <S sid ="201" ssid = "9">The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR.</S><S sid ="202" ssid = "10">The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.</S><S sid ="203" ssid = "11">It also seems like METEOR should not be used in tuning due to high insertion rate and low precision.</S> | Discourse Facet: Method_Citation | 
Citance Number: 20 | Reference Article: P07-1040 | Citing Article: Psem_p07 | Citation Marker Offset: '13' | Citation Marker: Rosti et al. 2007 | Citation Offset: '13' | Citation Text: <S sid ="13" ssid = "13">The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).</S> | Reference Offset: ['140'] | Reference Text: <S sid ="140" ssid = "2">A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.</S> | Discourse Facet: Method_Citation | 
Citance Number: 21 | Reference Article: P07-1040 | Citing Article: Psem_p07 | Citation Marker Offset: '16' | Citation Marker: Rosti et al. 2007 | Citation Offset: '116' | Citation Text: <S sid ="116" ssid = "8">In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.</S> | Reference Offset: ['64','70'] | Reference Text: <S sid ="64" ssid = "15">Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.</S><S sid ="70" ssid = "21">ric since it is based on the rate of edits required to transform the hypothesis into the reference.</S> | Discourse Facet: Method_Citation | 
Citance Number: 22 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '7' | Citation Marker: Rosti et al. 2007 | Citation Offset: '7' | Citation Text: <S sid ="7" ssid = "7">The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).</S> | Reference Offset: ['86'] | Reference Text: <S sid ="86" ssid = "7">The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.</S> | Discourse Facet: Method_Citation | 
Citance Number: 23 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '17' | Citation Marker: Rosti et al. 2007 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.</S> | Reference Offset: ['123','124','125'] | Reference Text: <S sid ="123" ssid = "2">To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.</S><S sid ="124" ssid = "3">All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.</S> <S sid ="125" ssid = "4">All confusion network are connected to a common end node with NULL arcs.</S> | Discourse Facet: Method_Citation | 
Citance Number: 24 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '39' | Citation Marker: Rosti et al. 2007 | Citation Offset: '39' | Citation Text: <S sid ="39" ssid = "17">Other scores for the word arc are set as in (Rosti et al., 2007).</S> | Reference Offset: ['40'] | Reference Text: <S sid ="40" ssid = "40">In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.</S> | Discourse Facet: Method_Citation | 
Citance Number: 25 | Reference Article: P07-1040 | Citing Article: W08-0329 | Citation Marker Offset: '88' | Citation Marker: Rosti et al. 2007 | Citation Offset: '87','88' | Citation Text: <S sid ="87" ssid = "21">The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.</S><S sid ="88" ssid = "22">to the pairwise TER alignment described in (Rosti et al., 2007).</S> | Reference Offset: ['95'] | Reference Text: <S sid ="95" ssid = "16">Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.</S><S sid ="96" ssid = "17">Similar approach to estimate word posteriors is adopted in this work.</S> | Discourse Facet: Method_Citation | 
Citance Number: 26 | Reference Article: P07-1040 | Citing Article: W09-0441 | Citation Marker Offset: '36' | Citation Marker: Rosti et al.,2007 | Citation Offset: '35','36' | Citation Text: <S sid ="35" ssid = "9">The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.</S><S sid ="36" ssid = "10">Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).</S> | Reference Offset: ['140'] | Reference Text: <S sid ="140" ssid = "2">A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: W06-3909 | Citing Article: D09-1025 | Citation Marker Offset: '93' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '93' | Citation Text: <S sid ="93" ssid = "16">We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.</S> | Reference Offset: ['8'] | Reference Text: <S sid ="8" ssid = "8">From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.</S> | Discourse Facet: Method citation | 
Citance Number: 2 | Reference Article: W06-3909 | Citing Article: D09-1025 | Citation Marker Offset: '120' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '119','120' | Citation Text: <S sid ="119" ssid = "42">Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.</S>   <S sid ="120" ssid = "43">This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).</S> | Reference Offset: ['121'] | Reference Text: <S sid ="121" ssid = "16">Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.</S> | Discourse Facet: Method citation | 
Citance Number: 3 | Reference Article: W06-3909 | Citing Article: P09-1045 | Citation Marker Offset: '127' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '127' | Citation Text: <S sid ="127" ssid = "7">Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).</S> | Reference Offset: ['196'] | Reference Text: <S sid ="196" ssid = "1">We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.</S> | Discourse Facet: Method citation | 
Citance Number: 4 | Reference Article: W06-3909 | Citing Article: P09-1113 | Citation Marker Offset: '17' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).</S> | Reference Offset: ['19'] | Reference Text: <S sid ="19" ssid = "5">Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.</S | Discourse Facet: Method citation | 
Citance Number: 5 | Reference Article: W06-3909 | Citing Article: P170300_w06 | Citation Marker Offset: '82' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '82' | Citation Text: <S sid ="82" ssid = "26">To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "13">We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((AdjPIPENoun)+PIPE((AdjPIPENoun)*(NounPrep)?)(AdjPIPENoun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point).</S> | Discourse Facet: Method citation | 
Citance Number: 6 | Reference Article: W06-3909 | Citing Article: P846406_w06 | Citation Marker Offset: '104' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '104' | Citation Text: <S sid ="104" ssid = "66">In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.</S> | Reference Offset: ['20','21'] | Reference Text: <S sid ="20" ssid = "6">Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].</S>   <S sid ="21" ssid = "7">Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.</S> | Discourse Facet: Method citation | 
Citance Number: 7 | Reference Article: W06-3909 | Citing Article: P846406_w06 | Citation Marker Offset: '105' | Citation Marker: Pantel and Pennacchiotti,2006 | Citation Offset: '105' | Citation Text: <S sid ="105" ssid = "67">In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S | Reference Offset: ['30'] | Reference Text: <S sid ="30" ssid = "16">The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.</S> | Discourse Facet: Method citation | 
Citance Number: 9 | Reference Article: W06-3909 | Citing Article: Pfram_w06 | Citation Marker Offset: '596' | Citation Marker: Pantel and Pennacchiotti | Citation Offset: '596' | Citation Text: <S sid ="596" ssid = "596">Pennacchiotti and Pantel [32] describes a system called Espresso.</S> | Reference Offset: ['196'] | Reference Text: <S sid ="196" ssid = "1">We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.</S> | Discourse Facet: Method citation | 
Citance Number: 10 | Reference Article: W06-3909 | Citing Article: Pfram_w06 | Citation Marker Offset: '601' | Citation Marker: Pantel and Pennacchiotti | Citation Offset: '601' | Citation Text: <S sid ="601" ssid = "601">Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.</S> | Reference Offset: ['68'] | Reference Text: <S sid ="68" ssid = "30">The recall of a pattern p can be approximated by the fraction of input instances in I&apos; that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).</S> | Discourse Facet: Method citation | 
Citance Number: 11 | Reference Article: W06-3909 | Citing Article: Ponto_w06 | Citation Marker Offset: '18' | Citation Marker: Pantel and Pennacchiotti | Citation Offset: '18' | Citation Text: <S sid ="18" ssid = "18">As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.</S> | Reference Offset: ['33'] | Reference Text: <S sid ="33" ssid = "19">Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.</S> | Discourse Facet: Method citation | 
Citance Number: 12 | Reference Article: W06-3909 | Citing Article: Ponto_w06 | Citation Marker Offset: '27' | Citation Marker: Pantel and Pennacchiotti | Citation Offset: '27' | Citation Text: <S sid ="27" ssid = "2">Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.</S> | Reference Offset: ['19'] | Reference Text: <S sid ="19" ssid = "5">Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.</S | Discourse Facet: Method citation | 
Citance Number: 13 | Reference Article: W06-3909 | Citing Article: PSS07_w06 | Citation Marker Offset: '49' | Citation Marker: Pantel and pennacchiotti | Citation Offset: '49' | Citation Text: <S sid ="49" ssid = "14">Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti &amp; Pantel 2006).</S> | Reference Offset: ['122','123','124','125','126','127','128',129','130'] | Reference Text: <S sid ="122" ssid = "17">For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations:  succession: This relation indicates that one proper noun succeeds another in a position or title.</S>   <S sid ="123" ssid = "18">For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II.</S>   <S sid ="124" ssid = "19">We evaluate this relation on the TREC9 corpus.</S>   <S sid ="125" ssid = "20"> reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction.</S>   <S sid ="126" ssid = "21">For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid.</S>   <S sid ="127" ssid = "22">We evaluate this relation on the CHEM corpus.</S>   <S sid ="128" ssid = "23"> production: This relation occurs when a process or element/object produces a result.</S>   <S sid ="129" ssid = "24">For example, ammonia produces nitric oxide.</S>   <S sid ="130" ssid = "25">We evaluate this relation on the CHEM corpus.</S> | Discourse Facet: Method citation | 
Citance Number: 1 | Reference Article: P00-1025 | Citing Article: J06-1004 | Citation Marker Offset: '104' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '104' | Citation Text: <S sid ="104" ssid = "39">Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "12">If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.</S> | Discourse Facet: Method Citation | 
Citance Number: 2 | Reference Article: P00-1025 | Citing Article: P55-62_p00 | Citation Marker Offset: '52' | Citation Marker: 7.0 | Citation Offset: '52' | Citation Text: <S sid ="52" ssid = "52">Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.</S> | Reference Offset: ['123'] | Reference Text: <S sid ="123" ssid = "50">This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.</S> | Discourse Facet: Method Citation | 
Citance Number: 3 | Reference Article: P00-1025 | Citing Article: P9852_p00 | Citation Marker Offset: '200','201' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '200', '201' | Citation Text: <S sid ="200" ssid = "43">Thus, we employ the com pile-replace feature in xfst (Beesley &amp; Karttunen, 2000).</S>   <S sid ="201" ssid = "44">This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos;&quot;&apos;[ &quot; and &quot;A 1 &quot; to mark the domain of re duplication.</S> | Reference Offset: ['203'] | Reference Text: <S sid ="203" ssid = "130">Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation.</S> | Discourse Facet: Method Citation | 
Citance Number: 5 | Reference Article: P00-1025 | Citing Article: PE2006_p00 | Citation Marker Offset: '104' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '104' | Citation Text: <S sid ="104" ssid = "39">Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.</S> | Reference Offset: ['62'] | Reference Text: <S sid ="62" ssid = "2">The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.</S> | Discourse Facet: Method Citation | 
Citance Number: 6 | Reference Article: P00-1025 | Citing Article: Plex_p00 | Citation Marker Offset: '31' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '31' | Citation Text: <S sid ="31" ssid = "31">Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).</S> | Reference Offset: ['119'] | Reference Text: <S sid ="119" ssid = "46">In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages.</S> | Discourse Facet: Result Citation | 
Citance Number: 7 | Reference Article: P00-1025 | Citing Article: Plex_p00 | Citation Marker Offset: '32' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '32' | Citation Text: <S sid ="32" ssid = "32">Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).</S> | Reference Offset: ['16'] | Reference Text: <S sid ="16" ssid = "16">This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below.</S> | Discourse Facet: Method Citation | 
Citance Number: 8 | Reference Article: P00-1025 | Citing Article: Pmorph_p00 | Citation Marker Offset: '67' | Citation Marker: Beesley and Karttunen | Citation Offset: '67' | Citation Text: <S sid ="67" ssid = "1">In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].</S> | Reference Offset: ['96'] | Reference Text: <S sid ="96" ssid = "23">In the regular expression calculus there are several operators that involve concatenation.</S> | Discourse Facet: Method Citation | 
Citance Number: 10 | Reference Article: P00-1025 | Citing Article: Pstat_p00 | Citation Marker Offset: '86' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "59">The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.</S> | Reference Offset: ['131'] | Reference Text: <S sid ="131" ssid = "58">3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.</S> | Discourse Facet: Method Citation | 
Citance Number: 11 | Reference Article: P00-1025 | Citing Article: W02-0503-parscit130908 | Citation Marker Offset: '17' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.</S> | Reference Offset: ['202'] | Reference Text: <S sid ="202" ssid = "129">The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.</S> | Discourse Facet: Method Citation | 
Citance Number: 12 | Reference Article: P00-1025 | Citing Article: W07-0802-parscit130908 | Citation Marker Offset: '134' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '134' | Citation Text: <S sid ="134" ssid = "1">A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).</S> | Reference Offset: ['123'] | Reference Text: <S sid ="123" ssid = "50">This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.</S> | Discourse Facet: Method Citation | 
Citance Number: 13 | Reference Article: P00-1025 | Citing Article: W08-0703 | Citation Marker Offset: '15' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '15' | Citation Text: <S sid ="15" ssid = "15">In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.</S> | Reference Offset: ['50'] | Reference Text: <S sid ="50" ssid = "33">All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime.</S> | Discourse Facet: Method Citation | 
Citance Number: 14 | Reference Article: P00-1025 | Citing Article: W09-0802 | Citation Marker Offset: '207' | Citation Marker: Beesley and Karttunen,2000 | Citation Offset: '207' | Citation Text: <S sid ="207" ssid = "74">The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).</S> | Reference Offset: ['205'] | Reference Text: <S sid ="205" ssid = "132">The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific</S> | Discourse Facet: Method Citation | 
Citance Number: 1 | Reference Article: W11-0815 | Citing Article: D13-1060 | Citation Marker Offset: '7' | Citation Marker: Acosta et al, 2011 | Citation Offset: '7' | Citation Text: <S sid ="7" ssid = "7">The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).</S> | Reference Offset: ['217'] | Reference Text: <S sid ="217" ssid = "1">This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: W11-0815 | Citing Article: P09207_w11 | Citation Marker Offset: '625', '626' | Citation Marker: Acosta et al, 2011 | Citation Offset: '625', '626' | Citation Text: <S sid ="625" ssid = "267">Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.</S> | Reference Offset: ['18'] | Reference Text: <S sid ="18" ssid = "18">The selection of appropriate indexing terms is a key factor for the quality of IR systems.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: W11-0815 | Citing Article: Pproc9_w11 | Citation Marker Offset: '15', '16' | Citation Marker: Acosta et al, 2011 | Citation Offset: '15','16' | Citation Text: <S sid ="15" ssid = "15">Our research in compositionality is motivated by the hypothesis that a special treatment of se mantically non-compositional expressions can im prove results in various Natural Language Process ing (NPL) tasks, as shown for example by Acosta et al.</S> | Reference Offset: ['1'] | Reference Text: <S sid ="1" ssid = "1">The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.</S> | Discourse Facet: Method_CItation | 
Citance Number: 4 | Reference Article: W11-0815 | Citing Article: Pproc14_w11 | Citation Marker Offset: '16','17','18','19' | Citation Marker: Acosta et al, 2011 | Citation Offset: '16','17','18','19' | Citation Text: <S sid ="16" ssid = "16">As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti cal machine translation.</S> | Reference Offset: ['47'] | Reference Text: <S sid ="47" ssid = "10">This task aimed to explore the contribution of the disambiguation of words to bilingual or monolingual IR.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: W11-0815 | Citing Article: S13-1039 | Citation Marker Offset: '5' | Citation Marker: Acosta et al, 2011 | Citation Offset: '5' | Citation Text: <S sid ="5" ssid = "5">Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).</S> | Reference Offset: ['6'] | Reference Text: <S sid ="6" ssid = "6">One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: W11-0815 | Citing Article: W12-3311 | Citation Marker Offset: '31' | Citation Marker: Acosta et al, 2011 | Citation Offset: '31' | Citation Text: <S sid ="31" ssid = "31"> Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).</S> | Reference Offset: ['21'] | Reference Text: <S sid ="21" ssid = "21">For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101109, Portland, Oregon, USA, 23 June 2011.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: W11-0815 | Citing Article: W13-1006 | Citation Marker Offset: '15','16' | Citation Marker: Acosta et al, 2011 | Citation Offset: '15','16' | Citation Text: <S sid ="15" ssid = "15">Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.</S> | Reference Offset: ['15'] | Reference Text: <S sid ="15" ssid = "15">In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: W11-0815 | Citing Article: W13-3208 | Citation Marker Offset: '16' | Citation Marker: Acosta et al, 2011 | Citation Offset: '16' | Citation Text: <S sid ="16" ssid = "16">Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.</S> | Discourse Facet: Result_Citation | 
Citance Number: 9 | Reference Article: W11-0815 | Citing Article: W15-0909 | Citation Marker Offset: '4','5','6' | Citation Marker: Acosta et al, 2011 | Citation Offset: '4','5','6' | Citation Text: <S sid ="4" ssid = "4">(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (IR: Acosta et al.</S> | Reference Offset: ['30'] | Reference Text: <S sid ="30" ssid = "4">The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: W11-0815 | Citing Article: W15-0909 | Citation Marker Offset: '7','8' | Citation Marker: Acosta et al, 2011 | Citation Offset: '7','8' | Citation Text: <S sid ="7" ssid = "7">For instance, Acosta et al.</S> | Reference Offset: ['62','64'] | Reference Text: <S sid ="62" ssid = "25">We used Zettair to generate the ranked list of documents retrieved in response to each query.</S><S sid ="64" ssid = "27">We used the cosine metric to calculate the scores and rank the documents.</S> | Discourse Facet: Result_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: C10-2052 | Citation Marker Offset: '22' | Citation Marker: Chiang et al., 2009 | Citation Offset: '22' | Citation Text: <S sid ="22" ssid = "22">We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '6' | Citation Marker: Chiang et al., 2009 | Citation Offset: '6' | Citation Text: <S sid ="6" ssid = "6">Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '9' | Citation Marker: Chiang et al., 2009 | Citation Offset: '9' | Citation Text: <S sid ="9" ssid = "9">Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '25' | Citation Marker: Chiang et al., 2009 | Citation Offset: '25' | Citation Text: <S sid ="25" ssid = "25">(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '159' | Citation Marker: Chiang et al., 2009 | Citation Offset: '159' | Citation Text: <S sid ="159" ssid = "11">Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.</S> | Reference Offset: ['32' '33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '239' | Citation Marker: Chiang et al., 2009 | Citation Offset: '239' | Citation Text: <S sid ="239" ssid = "91">Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '13' | Citation Marker: Chiang et al., 2009 | Citation Offset: '13' | Citation Text: <S sid ="13" ssid = "13">The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '78', '79' | Citation Marker: Chiang et al., 2009 | Citation Offset: '77', '78', '79' | Citation Text: <S sid ="77" ssid = "2">The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.</S>   <S sid ="78" ssid = "3">(2007) and Chiang et al.</S>   <S sid ="79" ssid = "4">(2008b; 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '208' | Citation Marker: Chiang et al., 2009 | Citation Offset: '207', '208', '209' | Citation Text: <S sid ="207" ssid = "55">We used the following feature classes in SBMT and PBMT extended scenarios:  Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1)  Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '210' | Citation Marker: Chiang et al., 2009 | Citation Offset: '210', '211' | Citation Text: <S sid ="210" ssid = "58">Chiang et al. (2009), Section 4.1):10  Rule overlap features  Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '227' | Citation Marker: Chiang et al., 2009 | Citation Offset: '227', '228', '229' | Citation Text: <S sid ="227" ssid = "16">5.4.1 MERT We used David Chiangs CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009). | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: N12-1006 | Citation Marker Offset: '102' | Citation Marker: Chiang et al., 2009 | Citation Offset: '102', '103' | Citation Text: <S sid ="102" ssid = "7">Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009) </S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: P12-1001 | Citation Marker Offset: '147' | Citation Marker: Chiang et al., 2009 | Citation Offset: '147' | Citation Text: <S sid ="147" ssid = "64">Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '36' | Citation Marker: Chiang et al., 2009 | Citation Offset: '36' | Citation Text: <S sid ="36" ssid = "16">Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '102' | Citation Marker: Chiang et al., 2009 | Citation Offset: '102' | Citation Text: <S sid ="102" ssid = "56">task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '115' | Citation Marker: Chiang et al., 2009 | Citation Offset: '115' | Citation Text: <S sid ="115" ssid = "9">The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "38">Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '120' | Citation Marker: Chiang et al., 2009 | Citation Offset: '120' | Citation Text: <S sid ="120" ssid = "14">For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: N09-1025 | Citing Article: P28_n09 | Citation Marker Offset: '51' | Citation Marker: Chiang et al., 2009 | Citation Offset: '51' | Citation Text: <S sid ="51" ssid = "23">Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: N09-1025 | Citing Article: P134_n09 | Citation Marker Offset: '27' | Citation Marker: Chiang et al., 2009 | Citation Offset: '27', '28' | Citation Text: <S sid ="27" ssid = "27">First, we used features proposed by Chiang et al. (2009):  phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+)  target word insertion features  source word deletion features  word translation features  phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5). | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: '15' | Citation Marker: Chiang et al., 2009 | Citation Offset: '15' | Citation Text: <S sid ="15" ssid = "15">These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: '16' | Citation Marker: Chiang et al., 2009 | Citation Offset: '16' | Citation Text: <S sid ="16" ssid = "16">In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '218' | Citation Marker: Chiang et al., 2009 | Citation Offset: '218' | Citation Text: <S sid ="218" ssid = "109">This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '245' | Citation Marker: Chiang et al., 2009 | Citation Offset: '245' | Citation Text: <S sid ="245" ssid = "136">The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "38">Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '252' | Citation Marker: Chiang et al., 2009 | Citation Offset: '252' | Citation Text: <S sid ="252" ssid = "6">We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: N09-1025 | Citing Article: PSMPT_n09 | Citation Marker Offset: '73' | Citation Marker: Chiang et al., 2009 | Citation Offset: '73', '74' | Citation Text: <S sid ="73" ssid = "6">Chiang et w(X  (, ,  )) = ii (2) i al. 2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: '79' | Citation Marker: Chiang et al., 2009 | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "79">For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 14 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: '86' | Citation Marker: Chiang et al., 2009 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "86">Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 15 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: '48' | Citation Marker: Chiang et al., 2009 | Citation Offset: '48' | Citation Text: <S sid ="48" ssid = "25">When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: '209' | Citation Marker: Chiang et al., 2009 | Citation Offset: '209' | Citation Text: <S sid ="209" ssid = "25">Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.</S> | Reference Offset: ['32', '33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). | Discourse Facet: Method_Citation | 
Citance Number: 17 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '53' | Citation Marker: Chiang et al., 2009 | Citation Offset: '53', '54' | Citation Text: <S sid ="53" ssid = "30">Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '92' | Citation Marker: Chiang et al., 2009 | Citation Offset: '92' | Citation Text: <S sid ="92" ssid = "25">A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 19 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '97' | Citation Marker: Chiang et al., 2009 | Citation Offset: '97' | Citation Text: <S sid ="97" ssid = "30">The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.</S> | Reference Offset: ['32','33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1001 | Citing Article: N09_csl2013 | Citation Marker Offset: '147' | Citation Marker: 2009.0 | Citation Offset: '146', '147' | Citation Text: <S sid ="146" ssid = "86">The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.</S>			<S sid ="147" ssid = "87">(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.</S> | Reference Offset: ['126', '127', '133', '207'] | Reference Text: <S sid ="126" ssid = "39">We conduct the experiments on two different gold standard datasets.</S>			<S sid ="127" ssid = "40">One is the MicroWNOp corpus, ntu.edu.tw/cjlin/libsvm/.</S><S sid ="133" ssid = "5">It includes 298 words with 703 objective and 358 subjective WordNet senses.</S><S sid ="207" ssid = "49">Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).</S> | Discourse Facet: Results_Citation | 
Citance Number: 2 | Reference Article: N09-1001 | Citing Article: N09_qwn | Citation Marker Offset: '22' | Citation Marker: Su and Markert, 2009 | Citation Offset: '22' | Citation Text: <S sid ="22" ssid = "22">Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.</S> | Reference Offset: ['58', '59' 60'] | Reference Text: <S sid ="58" ssid = "11">Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.</S>			<S sid ="59" ssid = "12">Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components.</S>			<S sid ="60" ssid = "13">More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories).</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: N09-1001 | Citing Article: N09_qwn | Citation Marker Offset: '22' | Citation Marker: Su and Markert, 2009 | Citation Offset: '22' | Citation Text: <S sid ="22" ssid = "22">Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.</S> | Reference Offset: ['20'] | Reference Text: <S sid ="20" ssid = "20">Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: N09-1001 | Citing Article: N09prod | Citation Marker Offset: '35' | Citation Marker: 5.0 | Citation Offset: '35' | Citation Text: <S sid ="35" ssid = "5">Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.</S> | Reference Offset: ['4'] | Reference Text: <S sid ="4" ssid = "4">We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: N09-1001 | Citing Article: N09prod | Citation Marker Offset: '121' | Citation Marker: 5.0 | Citation Offset: '121' | Citation Text: <S sid ="121" ssid = "25">Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.</S> | Reference Offset: ['4'] | Reference Text: <S sid ="4" ssid = "4">We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.</S> | Discourse Facet: Aim_citation | 
Citance Number: 6 | Reference Article: N09-1001 | Citing Article: N15-1071 | Citation Marker Offset: '147' | Citation Marker: 2009.0 | Citation Offset: '147' | Citation Text: <S sid ="147" ssid = "36">The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).</S> | Reference Offset: ['1'] | Reference Text: <S sid ="1" ssid = "1">We supplement WordNet entries with information on the subjectivity of its word senses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: N09-1001 | Citing Article: P101167 | Citation Marker Offset: '146' | Citation Marker: 2009.0 | Citation Offset: '146' | Citation Text: <S sid ="146" ssid = "96">Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.</S> | Reference Offset: ['104', '105' ,'106'] | Reference Text: <S sid ="104" ssid = "17">Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge.</S>			<S sid ="105" ssid = "18">Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.</S>			<S sid ="106" ssid = "19">However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie..</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: N09-1001 | Citing Article: P1018 | Citation Marker Offset: '10 | Citation Marker: 2009.0 | Citation Offset: '10' | Citation Text: <S sid ="10" ssid = "10">In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).</S> | Reference Offset: ['72', '73', '74', '75'] | Reference Text: <S sid ="72" ssid = "25">We define two vertices s (source) and t (sink),.</S>			<S sid ="73" ssid = "26">which correspond to the subjective and objective category, respectively.</S>			<S sid ="74" ssid = "27">Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices.</S>			<S sid ="75" ssid = "28">Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: N09-1001 | Citing Article: P1018 | Citation Marker Offset: '180' | Citation Marker: 2009.0 | Citation Offset: '180' | Citation Text: <S sid ="180" ssid = "2">For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.</S> | Reference Offset: ['4', '207'] | Reference Text: <S sid ="4" ssid = "4">We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.</S><S sid ="207" ssid = "49">Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).</S> | Discourse Facet: ['Method_Citation', 'Results_Citation'] | 
Citance Number: 10 | Reference Article: N09-1001 | Citing Article: PEAAI_n09 | Citation Marker Offset: '70' | Citation Marker: 2009.0 | Citation Offset: '70' | Citation Text: <S sid ="70" ssid = "30">Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "18">We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: N09-1001 | Citing Article: Pproc2014_n09 | Citation Marker Offset: '33' | Citation Marker: 2009.0 | Citation Offset: '33' | Citation Text: <S sid ="33" ssid = "4">The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.</S> | Reference Offset: ['20'] | Reference Text: <S sid ="20" ssid = "20">Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: N09-1001 | Citing Article: Pproc2014_n09 | Citation Marker Offset: '70' | Citation Marker: 2009.0 | Citation Offset: '70' | Citation Text: <S sid ="70" ssid = "16">Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.</S> | Reference Offset: ['20'] | Reference Text: <S sid ="20" ssid = "20">Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: N09-1001 | Citing Article: W11-0311 | Citation Marker Offset: '231' | Citation Marker: 2009.0 | Citation Offset: '231' | Citation Text: <S sid ="231" ssid = "1">One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).</S> | Reference Offset: ['1'] | Reference Text: <S sid ="1" ssid = "1">We supplement WordNet entries with information on the subjectivity of its word senses.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: D09-1023 | Citing Article: D09-1086 | Citation Marker Offset: '78' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '78' | Citation Text: <S sid ="78" ssid = "14">In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '1' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '1' | Citation Text: <S sid ="1" ssid = "1">We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '16' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '16' | Citation Text: <S sid ="16" ssid = "16">Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.</S> | Discourse Facet: Result_Citation | 
Citance Number: 5 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '23' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '23' | Citation Text: <S sid ="23" ssid = "1">We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '41' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '41' | Citation Text: <S sid ="41" ssid = "17">We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).</S> | Reference Offset: ['17'] | Reference Text: <S sid ="17" ssid = "17">We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: D09-1023 | Citing Article: D11-1044 | Citation Marker Offset: '146' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '146' | Citation Text: <S sid ="146" ssid = "6">For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.</S> | Reference Offset: ['260'] | Reference Text: <S sid ="260" ssid = "2">Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle non-local features using generic techniques that also support efficient parameter estimation.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: D09-1023 | Citing Article: Pjournal | Citation Marker Offset: '79' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "18">(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.</S> | Reference Offset: ['204'] | Reference Text: <S sid ="204" ssid = "14">We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura Phrase Syntactic Features: features: +f att  f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: D09-1023 | Citing Article: Pjournal | Citation Marker Offset: '139' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '139' | Citation Text: <S sid ="139" ssid = "78">Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).</S> | Reference Offset: ['1'] | Reference Text: <S sid ="1" ssid = "1">We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: D09-1023 | Citing Article: Pproc_d09 | Citation Marker Offset: '205' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '205' | Citation Text: <S sid ="205" ssid = "24">Gimpel &amp; Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.</S> | Reference Offset: ['123'] | Reference Text: <S sid ="123" ssid = "26">Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: D09-1023 | Citing Article: W10-1730-parscit | Citation Marker Offset: '17' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).</S> | Reference Offset: ['11'] | Reference Text: <S sid ="11" ssid = "11">Here we take first steps toward such a universal decoder, making the following contributions:Arbitrary feature model (2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: D09-1023 | Citing Article: W11-2139 | Citation Marker Offset: '37' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '37' | Citation Text: <S sid ="37" ssid = "3">On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).</S> | Reference Offset: ['254'] | Reference Text: <S sid ="254" ssid = "64">Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.</S> | Discourse Facet: Result_Citation | 
Citance Number: 20 | Reference Article: D09-1023 | Citing Article: N10-1040 | Citation Marker Offset: '6' | Citation Marker: Gimpel and Smith, 2009 | Citation Offset: '6' | Citation Text: <S sid ="6" ssid = "6">Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.</S> | Reference Offset: ['191'] | Reference Text: <S sid ="191" ssid = "1">Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: C98-1097 | Citing Article: C02-1033 | Citation Marker Offset: '15' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '15' | Citation Text: <S sid ="15" ssid = "15">Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.</S> | Reference Offset: ['45'] | Reference Text: <S sid ="45" ssid = "1">To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: C98-1097 | Citing Article: C02-1033 | Citation Marker Offset: '136' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '136' | Citation Text: <S sid ="136" ssid = "91">Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.</S> | Reference Offset: ['115'] | Reference Text: <S sid ="115" ssid = "23">Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: C98-1097 | Citing Article: ICDAR99 | Citation Marker Offset: '31' | Citation Marker: 11.0 | Citation Offset: '31','32' | Citation Text: <S sid ="31" ssid = "1">In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.</S><S sid ="32" ssid = "2">To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.</S> | Reference Offset: ['45'] | Reference Text: <S sid ="45" ssid = "1">To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '17' | Citation Marker: Job- bins and Evett, 1998 | Citation Offset: '17' | Citation Text: <S sid ="17" ssid = "17">Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.</S> | Reference Offset: ['45'] | Reference Text: <S sid ="45" ssid = "1">To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '30' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '30','31','32' | Citation Text: <S sid ="30" ssid = "9">When no external knowledge is used, this similarity is only based on the strict reiteration of words.</S><S sid ="31" ssid = "10">But it can be enhanced by taking into account semantic relations between words.</S><S sid ="32" ssid = "11">This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.</S> | Reference Offset: ['20','45'] | Reference Text: <S sid ="20" ssid = "20">Another approach extracted semantic information from Roget&apos;s Thesaurus (RT).</S><S sid ="45" ssid = "1">To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '90' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '89','90' | Citation Text: <S sid ="89" ssid = "5">The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.</S><S sid ="90" ssid = "6">This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).</S> | Reference Offset: ['41','47','50'] | Reference Text: <S sid ="41" ssid = "41">Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing).</S><S sid ="47" ssid = "3">Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.</S><S sid ="50" ssid = "6">Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '98' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '97','98' | Citation Text: <S sid ="97" ssid = "13">This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.</S><S sid ="98" ssid = "14">As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).</S> | Reference Offset: ['123','124','125'] | Reference Text: <S sid ="123" ssid = "31">The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69.</S><S sid ="124" ssid = "32">When used in isolation, the performance of each feature was inferior to a combined approach.</S><S sid ="125" ssid = "33">This fact provides evidence that different lexical relations are detected by each linguistic feature considered.</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '203' | Citation Marker: Jobbins and Evett, 1998 | Citation Offset: '203','204' | Citation Text: <S sid ="203" ssid = "21">In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.</S><S sid ="204" ssid = "22">In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.</S> | Reference Offset: ['45'] | Reference Text: <S sid ="62" ssid = "1">The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: C98-1097 | Citing Article: P07-1061 | Citation Marker Offset: '216' | Citation Marker: Job- bins and Evett, 1998 | Citation Offset: '216' | Citation Text: <S sid ="216" ssid = "8">This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).</S> | Reference Offset: ['62'] | Reference Text: <S sid ="62" ssid = "1">The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: C98-1097 | Citing Article: P04470 | Citation Marker Offset: '27' | Citation Marker: 5.0 | Citation Offset: '27' | Citation Text: <S sid ="27" ssid = "4">In other words, meaning of UW can be found generally through co- occurrence words [5].</S> | Reference Offset: ['45'] | Reference Text: <S sid ="45" ssid = "1">To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: C98-1097 | Citing Article: P06128 | Citation Marker Offset: '10' | Citation Marker: 1.0 | Citation Offset: '10' | Citation Text: <S sid ="10" ssid = "10">In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].</S> | Reference Offset: ['7','8'] | Reference Text: <S sid ="7" ssid = "7">The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.</S><S sid ="8" ssid = "8">Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user&apos;s query can be retrieved.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: C98-1097 | Citing Article: S0885 | Citation Marker Offset: '110' | Citation Marker: 1998.0 | Citation Offset: '109','110' | Citation Text: <S sid ="109" ssid = "27">Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.</S><S sid ="110" ssid = "28">These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).</S> | Reference Offset: ['36','37'] | Reference Text: <S sid ="36" ssid = "36">Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.</S><S sid ="37" ssid = "37">Identifying semantic relations in a text can be a useful indicator of its conceptual structure.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: D10-1058 | Citing Article: C16-1060 | Citation Marker Offset: '48' | Citation Marker: 2010.0 | Citation Offset: '48' | Citation Text: <S sid ="48" ssid = "27">Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.</S> | Reference Offset: ['3','4','6'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid ="4" ssid = "4">This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.</S><S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '24' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '24' | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.</S> | Reference Offset: ['18','26','33','34'] | Reference Text: <S sid ="18" ssid = "18">Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.</S><S sid ="26" ssid = "26">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S><S sid ="33" ssid = "33">Our model is much faster than IBM Model 4.</S><S sid ="34" ssid = "34">In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '33' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '33' | Citation Text: <S sid ="33" ssid = "4">, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: ['6','46'] | Reference Text: <S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '43' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '43' | Citation Text: <S sid ="43" ssid = "14">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.</S> | Reference Offset: ['86'] | Reference Text: <S sid ="86" ssid = "1">Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '82' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '82' | Citation Text: <S sid ="82" ssid = "3">Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: ['88','90'] | Reference Text: <S sid ="88" ssid = "3">Our model has only one parameter for each target word, which can be learned more reliably.</S><S sid ="90" ssid = "5">i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '99' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '99' | Citation Text: <S sid ="99" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: ['110'] | Reference Text: <S sid ="110" ssid = "4">1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: D10-1058 | Citing Article: P59105ca | Citation Marker Offset: '45' | Citation Marker: 15.0 | Citation Offset: '45' | Citation Text: <S sid ="45" ssid = "45">Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.</S> | Reference Offset: ['113','114','115'] | Reference Text: <S sid ="113" ssid = "7">This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm.</S><S sid ="114" ssid = "8">However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.</S><S sid ="115" ssid = "9">Instead, we do batch learning: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: D10-1058 | Citing Article: P87-94 | Citation Marker Offset: '79' | Citation Marker: Zhao | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "2">Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.</S> | Reference Offset: ['14','26','27'] | Reference Text: <S sid ="14" ssid = "14">Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.</S><S sid ="26" ssid = "26">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S><S sid ="27" ssid = "27">It is easier to understand than IBM Model 4 (see Section 3).</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: '104' | Citation Marker: 2010.0 | Citation Offset: '103','104' | Citation Text: <S sid ="103" ssid = "24">For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).</S><S sid ="104" ssid = "25">134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: ['29','30'] | Reference Text: <S sid ="29" ssid = "29">We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.</S><S sid ="30" ssid = "30">Qc 2010 Association for Computational Linguistics estimation.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: '131' | Citation Marker: 2010.0 | Citation Offset: '131' | Citation Text: <S sid ="131" ssid = "13">Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.</S> | Reference Offset: ['29','30','107','108'] | Reference Text: <S sid ="29" ssid = "29">We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.</S><S sid ="30" ssid = "30">Qc 2010 Association for Computational Linguistics estimation.</S><S sid ="107" ssid = "1">Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).</S><S sid ="108" ssid = "2">The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: D10-1058 | Citing Article: Pcoling_D10 | Citation Marker Offset: '232' | Citation Marker: 2010.0 | Citation Offset: '232' | Citation Text: <S sid ="232" ssid = "9">Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.</S> | Reference Offset: ['3'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '24' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '24' | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.</S> | Reference Offset: ['3','46'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '38' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '38' | Citation Text: <S sid ="38" ssid = "9">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.</S> | Reference Offset: ['86'] | Reference Text: <S sid ="86" ssid = "1">Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.</S> | Discourse Facet: Method_Citation | 
Citance Number: 14 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '39' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '39' | Citation Text: <S sid ="39" ssid = "10">I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).</S> | Reference Offset: ['6','46'] | Reference Text: <S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
Citance Number: 15 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '86' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "3">Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: ['88','90'] | Reference Text: <S sid ="88" ssid = "3">Our model has only one parameter for each target word, which can be learned more reliably.</S><S sid ="90" ssid = "5">i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '104' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '104' | Citation Text: <S sid ="104" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: ['110'] | Reference Text: <S sid ="110" ssid = "4">1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.</S> | Discourse Facet: Method_Citation | 
Citance Number: 17 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: '60' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '60' | Citation Text: <S sid ="60" ssid = "3">The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).</S> | Reference Offset: ['3','4','5','6'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid ="4" ssid = "4">This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.</S><S sid ="5" ssid = "5">It is similar in some ways to IBM Model 4, but is much easier to understand.</S><S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: '130' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '130' | Citation Text: <S sid ="130" ssid = "22">Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.</S> | Reference Offset: ['46'] | Reference Text: <S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
