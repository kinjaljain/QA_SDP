1,D11-1056,D10-1083,0.7712609970674487,0.7379032258064516,False,False,0.005847953216374269,0,0.008670520231213872,0.011947991097575262,0.03367579908675799,0,0,0,0.01,0.05555555555555555,0,0,0,0,1,"['We report results for the best and median hyperparameter settings obtained in this way.', 'Specifically, for both settings we report results on the median run for each setting.']"
3,D11-1059,D10-1083,0.04716981132075472,0.08870967741935484,True,False,0.07407407407407407,0.05714285714285714,0.10404624277456641,0.11702003045566364,0.19748858447488582,0.23684210526315788,0,0.21875,0.22,0.19444444444444445,0,0,0,0,1,"['Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.', 'Table 6: Type-level Results: Each cell report the type-level accuracy computed against the most frequent tag of each word type.', 'We have presented a method for unsupervised part-of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.', 'Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.', 'First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type-level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).']"
4,D11-1059,D10-1083,0.07547169811320754,0.9516129032258065,False,False,0.03898635477582846,0.04285714285714286,0.041425818882466284,0.04978329623989691,0.06906392694063926,0,0,0,0.015,0.05555555555555555,0,0,0,1,0,"['We have presented a method for unsupervised part-of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.', 'In our model, we associate these features at the type-level in the lexicon.', 'This assumption, however, is not inherent to type-based tagging models.']"
6,D11-1059,D10-1083,0.1509433962264151,0.08064516129032258,True,False,0.04775828460038986,0.07142857142857142,0.09055876685934484,0.11924563664050615,0.17180365296803649,0.23684210526315788,0,0.21875,0.22,0.19444444444444445,0,0,0,1,0,"['The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.', 'We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-.', 'Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.']"
7,D11-1059,D10-1083,0.42452830188679247,0.08064516129032258,False,False,0.03898635477582846,0.09999999999999999,0.081888246628131,0.10940611456014994,0.14098173515981732,0.23684210526315788,0,0.21875,0.22499999999999998,0.19444444444444445,0,0,0,1,0,"['The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.', 'Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.', 'encodes the one tag per word constraint and is uni form over type-level tag assignments.', 'In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.']"
8,D11-1059,D10-1083,0.6084905660377359,0.6612903225806451,False,False,0.021442495126705652,0,0.014450867052023121,0.01499355745578072,0.0273972602739726,0,0,0,0.0225,0.08333333333333333,0,0,0,0,1,"['Following the setup of Johnson  , we use the whole of the Penn Treebank corpus for training and evaluation on English.', 'On each language we investigate the contribution of each component of our model.']"
9,D12-1086,D10-1083,0.30962343096234307,0.967741935483871,False,False,0.031189083820662766,0.05714285714285714,0.07707129094412327,0.10917184022490341,0.13527397260273977,0.23684210526315788,0,0.21875,0.22499999999999998,0.19444444444444445,0,0,0,0,1,"['In this paper, we make a simplifying assumption of one-tag-per-word.', 'The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.', 'encodes the one tag per word constraint and is uni form over type-level tag assignments.', 'Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.', 'First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type-level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).']"
10,D12-1125,D10-1083,0.7526501766784452,0.7056451612903226,False,False,0.003898635477582846,0,0.007707129094412331,0.01651634063488345,0.016552511415525113,0,0,0,0.01,0.05555555555555555,0,0,0,1,0,"['As is standard, we report the greedy one-to-one   and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags.']"
11,D12-1127,D10-1083,0.06220095693779904,0.8104838709677419,False,False,0.012670565302144249,0,0.009633911368015413,0.006442544219280778,0.005136986301369863,0,0,0,0.01,0.05555555555555555,0,0,0,1,0,"['While Berg-Kirkpatrick et al.', 'The system of Berg-Kirkpatrick et al.']"
12,D13-1004,D10-1083,0.11297071129707113,0.10887096774193548,True,False,0.02046783625730994,0.02857142857142857,0.03275529865125241,0.049197610401780466,0.0593607305936073,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type-level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).', 'Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naveBayes approach to incorporate features.']"
13,N12-1045,D10-1083,0.05,0.1975806451612903,False,False,0.050682261208576995,0.02857142857142857,0.023121387283236993,0.03162703525828745,0.02682648401826484,0.02631578947368421,0,0.0625,0.04,0.125,1,0,0,0,0,"['Another thread of relevant research has explored the use of features in unsupervised POS induction  .', 'While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al.', 'While Berg-Kirkpatrick et al.', 'The system of Berg-Kirkpatrick et al.', 'We consider two variants of Berg-Kirkpatrick et al.']"
14,P11-1087,D10-1083,0.21739130434782608,0.3387096774193548,False,False,0.05360623781676413,0.05714285714285714,0.09826589595375722,0.14501581351762918,0.16038812785388123,0.23684210526315788,0,0.21875,0.22499999999999998,0.19444444444444445,0,0,0,1,0,"['Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint.', 'The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.', 'This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler  .', 'encodes the one tag per word constraint and is uni form over type-level tag assignments.', 'Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.']"
15,P11-1087,D10-1083,0.8260869565217391,0.7096774193548387,True,False,0.037037037037037035,0.04285714285714286,0.06743737957610789,0.09382687126625278,0.13013698630136983,0,0,0,0.01,0.05555555555555555,0,0,0,0,1,"['We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-.', 'For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR).']"
16,P13-1150,D10-1083,0.2631578947368421,0.9516129032258065,False,False,0.04191033138401559,0.04285714285714286,0.03564547206165703,0.038889539650931226,0.05821917808219178,0,0,0,0.01,0.05555555555555555,0,0,0,1,0,"['We have presented a method for unsupervised part-of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.', 'This departure from the traditional token-based tagging approach allows us to explicitly capture type-level distributional properties of valid POS tag as signments as part of the model.']"
17,W11-0301,D10-1083,0.4722222222222222,0.22580645161290322,True,False,0.05360623781676413,0,0.09537572254335258,0.12287688883682799,0.15353881278538808,0,0,0,0.005,0,0,0,0,1,0,"['Conditioned on T , features of word types W are drawn.', 'The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters .', 'P St = n.  T VARIABLES  Y W : Word types (W1 ,.', 'Once HMM parameters (, ) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from .', 'Uniform Tag Prior (1TW) Our initial lexicon component will be uniform over possible tag assignments as well as word types.']"
18,W12-1914,D10-1083,0.08045977011494253,0.13306451612903225,False,False,0.06530214424951267,0.02857142857142857,0.04913294797687862,0.050368982078013345,0.07534246575342463,0.2631578947368421,0,0.28125,0.245,0.2638888888888889,1,0,0,1,0,"['Recent work has made significant progress on unsupervised POS tagging  .', 'We have presented a method for unsupervised part-of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.', 'Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.']"
1,D12-1016,W09-0621,0.2694805194805195,0.3838383838383838,True,False,0.029239766081871343,0,0.033718689788053945,0.061145601499355724,0.037100456621004564,0.05263157894736842,0.3333333333333333,0.125,0.185,0.06944444444444445,0,0,0,1,0,"['The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity.', '2.2 Pairwise similarity.', 'We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.', 'We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76.', 'The headlines are stemmed using the porter stemmer for Dutch  .']"
2,D13-1155,W09-0621,0.44541484716157204,0.12121212121212122,False,False,0.007797270955165692,0,0.014450867052023121,0.01745343797586974,0.010844748858447488,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.']"
3,PS15684-W09,W09-0621,0.09976798143851508,0.030303030303030304,False,False,0.028265107212475632,0,0.02986512524084778,0.044043575026355856,0.032534246575342464,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.', 'News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web.', 'In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.']"
4,PS15684-W09,W09-0621,0.5870069605568445,0.2222222222222222,False,False,0.010721247563352826,0,0.012524084778420038,0.014173597282417707,0.01141552511415525,0.23684210526315788,0,0.21875,0.22,0.1388888888888889,0,0,0,1,0,"['For the development of our system we use data which was obtained in the DAESO-project.', 'Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.']"
5,PS15684-W09,W09-0621,0.6682134570765661,0.40404040404040403,False,False,0.011695906432748537,0.014285714285714285,0.01348747591522158,0.01862480965210261,0.013127853881278538,0,0,0,0.0225,0,0,0,0,1,0,"['Our first approach is to use a clustering algorithm to cluster similar headlines.', 'It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.']"
6,PS15684-W09,W09-0621,0.6705336426914154,0.8181818181818182,True,False,0.007797270955165692,0,0.016377649325626204,0.028230057397212127,0.016552511415525113,0,0,0,0.0225,0,0,0,0,1,0,"['It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.', 'For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.', 'The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.', 'We use the k-means implementation in the CLUTO1 software package.']"
7,PS15684-W09,W09-0621,0.8097447795823666,0.8181818181818182,False,False,0.0009746588693957114,0,0.00674373795761079,0.005856858381164343,0.00228310502283105,0,0,0,0.0225,0,0,0,0,1,0,"['It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.', 'Sub-clustering is no trivial task, however.']"
8,PS15684-W09,W09-0621,0.839907192575406,0.40404040404040403,False,False,0.010721247563352826,0.014285714285714285,0.007707129094412331,0.013353637109054703,0.01141552511415525,0,0,0,0.01,0,0,0,0,1,0,['Our first approach is to use a clustering algorithm to cluster similar headlines.']
9,PS15684-W09,W09-0621,0.8538283062645011,0.8585858585858586,True,False,0.023391812865497075,0,0.023121387283236993,0.04146655733864355,0.02910958904109589,0.05263157894736842,0.3333333333333333,0.125,0.175,0.06944444444444445,0,0,0,1,0,"['We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76.', 'We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.']"
10,W10-4223,W09-0621,0.3173076923076923,0.030303030303030304,False,False,0.021442495126705652,0,0.023121387283236993,0.032212721096403885,0.02454337899543379,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.', 'In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.']"
11,W11-1604,W09-0621,0.3238095238095238,0.010101010101010102,False,False,0.038011695906432746,0,0.03949903660886319,0.058451446644020126,0.045662100456621,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.', 'In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.', 'News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web.', 'We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.']"
12,W11-1604,W09-0621,0.6952380952380952,0.030303030303030304,False,False,0.01364522417153996,0,0.008670520231213872,0.014759283120534145,0.0136986301369863,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.']"
1,C08-1014,P07-1040,0.08121827411167512,0.16822429906542055,False,False,0.07115009746588694,0.04285714285714286,0.10404624277456642,0.12217406583108828,0.13013698630136986,0.23684210526315788,0,0.21875,0.22749999999999998,0.1527777777777778,0,0,0,1,0,"['In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.', 'System combination has been shown to improve classification performance in various tasks.', 'Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights.', 'The average TER score was computed between each systems -best hypothesis and all other hypotheses.', 'Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.']"
2,C08-1014,P07-1040,0.16243654822335024,0.0794392523364486,True,False,0.030214424951267055,0.04285714285714286,0.07032755298651251,0.10589199953145136,0.10616438356164382,0.2894736842105263,0.3333333333333333,0.28125,0.3625,0.20833333333333334,0,0,0,1,0,"['Recently, confusion network decoding for MT system combination has been proposed  .', 'Section 3 describes confusion network decoding for MT system combination.', 'Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.', 'Recently, confusion network decoding has been applied in machine translation system combination.', 'The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.']"
3,C08-1014,P07-1040,0.4467005076142132,0.14953271028037382,True,False,0.037037037037037035,0,0.04142581888246628,0.09101557924329393,0.08105022831050226,0.23684210526315788,0,0.21875,0.2375,0.1527777777777778,0,0,0,1,0,"['The average TER score was computed between each systems -best hypothesis and all other hypotheses.', 'The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.', 'Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.', 'In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.', 'Confusion networks are generated by choosing one hypothesis as the skeleton, and other hypotheses are aligned against it.']"
4,C08-1014,P07-1040,0.4720812182741117,0.40186915887850466,False,False,0.03313840155945419,0.014285714285714285,0.09633911368015413,0.12404826051306082,0.11529680365296799,0.2894736842105263,0.3333333333333333,0.28125,0.38,0.22222222222222224,0,0,0,1,0,"['The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.', 'In  , Levenshtein alignment was used to generate the network.', 'Hypothesis alignment is also very important in confusion network generation.', 'The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.', 'Recently, confusion network decoding has been applied in machine translation system combination.']"
5,D09-1115,P07-1040,0.03211009174311927,0.03271028037383177,True,False,0.06335282651072124,0.02857142857142857,0.0953757225433526,0.13025653039709503,0.1238584474885844,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.', 'Recently, confusion network decoding for MT system combination has been proposed  .', 'Section 3 describes confusion network decoding for MT system combination.', 'Recently, confusion network decoding has been applied in machine translation system combination.', 'In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.']"
6,D09-1115,P07-1040,0.6100917431192661,0.5186915887850467,True,False,0.1608187134502924,0.02857142857142857,0.2533718689788054,0.3291554410214364,0.3287671232876712,0,0,0,0.015,0,0,0,0,1,0,"['First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.', 'A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.', 'Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.', 'Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding.', 'If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty.']"
7,D09-1115,P07-1040,0.6743119266055045,0.4953271028037383,True,False,0.05458089668615984,0,0.09826589595375722,0.11702003045566362,0.12728310502283105,0,0,0,0.005,0,0,0,0,1,0,"['Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.']"
8,N09-2003,P07-1040,0.24242424242424243,0.11682242990654206,True,False,0.038011695906432746,0.02857142857142857,0.0655105973025048,0.09007848190230763,0.08447488584474883,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Qc 2007 Association for Computational Linguistics potheses in  .', 'Recently, confusion network decoding for MT system combination has been proposed  .', 'Section 3 describes confusion network decoding for MT system combination.', 'Recently, confusion network decoding has been applied in machine translation system combination.', 'In speech recognition, confusion network decoding   has become widely used in system combination.']"
9,P08-2021,P07-1040,0.020833333333333332,0.9158878504672897,False,False,0.025341130604288498,0,0.033718689788053945,0.06524540236617077,0.05821917808219178,0.23684210526315788,0,0.21875,0.22749999999999998,0.1527777777777778,0,0,0,1,0,"['Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.', 'In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.']"
10,P08-2021,P07-1040,0.23958333333333334,0.40186915887850466,False,False,0.022417153996101363,0,0.07514450867052022,0.07168794658545156,0.0776255707762557,0,0,0,0.015,0,0,0,0,1,0,"['The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.', 'The only difference to word error rate is that the TER allows shifts.']"
11,P08-2021,P07-1040,0.2916666666666667,0.3130841121495327,True,False,0.056530214424951264,0,0.09922928709055875,0.12931943305610877,0.1404109589041096,0,0,0,0.01,0,0,0,0,1,0,"['Confusion network decoding usually requires finding the path with the highest confidence in the network.', 'Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.']"
12,P08-2021,P07-1040,0.71875,0.03271028037383177,True,False,0.04873294346978557,0,0.060693641618497114,0.08867283589082818,0.1004566210045662,0.2894736842105263,0.3333333333333333,0.28125,0.37,0.22222222222222224,0,0,0,1,0,"['The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.', 'All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.', 'Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.']"
13,P08-2021,P07-1040,0.8125,0.8738317757009346,False,False,0.04191033138401559,0.08571428571428572,0.08381502890173409,0.12439967201593069,0.12499999999999994,0,0,0,0.0175,0,0,0,0,1,0,"['Again, the best scores on each metric are obtained by the combination tuned for that metric.', 'As expected, the scores on the metric used in tuning are the best on that metric.', 'The best results on a given metric are again obtained by the combination optimized for the corresponding metric.', 'A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.', 'The second set of weights is used to find the final -best from the re-scored -best list.']"
14,P11-1125,P07-1040,0.27218934911242604,0.46261682242990654,True,False,0.03216374269005848,0.08571428571428572,0.035645472061657024,0.054117371441958516,0.05707762557077625,0,0,0,0.0125,0,0,0,0,1,0,"['In  , the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.', 'All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.']"
15,P11-1125,P07-1040,0.7514792899408284,0.6308411214953271,True,False,0.0935672514619883,0.014285714285714285,0.14547206165703278,0.2195150521260397,0.24200913242009134,0.23684210526315788,0,0.21875,0.2275,0.1388888888888889,0,0,0,1,0,"['This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in  .', 'In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.', 'The number of paths through a confusion network grows exponentially with the number of nodes.', 'Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.', 'Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.']"
16,P39-P07,P07-1040,0.23529411764705882,0.0794392523364486,True,False,0.038011695906432746,0.02857142857142857,0.0655105973025048,0.09007848190230762,0.08447488584474883,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Recently, confusion network decoding for MT system combination has been proposed  .', 'Qc 2007 Association for Computational Linguistics potheses in  .', 'Section 3 describes confusion network decoding for MT system combination.', 'Recently, confusion network decoding has been applied in machine translation system combination.', 'In speech recognition, confusion network decoding   has become widely used in system combination.']"
17,P101121-P07,P07-1040,0.05828220858895705,0.20093457943925233,False,False,0.03313840155945419,0.014285714285714285,0.04431599229287091,0.04462926086447228,0.04908675799086758,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Powells method   is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set.', 'The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while  .']"
18,PLING-P07,P07-1040,0.08108108108108109,0.0794392523364486,True,False,0.05165692007797271,0.04285714285714286,0.07803468208092484,0.1260395923626567,0.12100456621004563,0.2894736842105263,0.3333333333333333,0.28125,0.37,0.22222222222222224,0,0,0,1,0,"['Recently, confusion network decoding for MT system combination has been proposed  .', 'Section 3 describes confusion network decoding for MT system combination.', 'Recently, confusion network decoding has been applied in machine translation system combination.', 'The improved confusion network decoding approach allows arbitrary features to be used in the combination.', 'In speech recognition, confusion network decoding   has become widely used in system combination.']"
19,PLING-P07,P07-1040,0.27702702702702703,0.14953271028037382,False,False,0.0029239766081871343,0,0.004816955684007707,0.006559681386904064,0.00684931506849315,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,['The average TER score was computed between each systems -best hypothesis and all other hypotheses.']
20,PSEM-P07,P07-1040,0.07027027027027027,0.014018691588785047,False,False,0.0341130604288499,0,0.030828516377649322,0.054703057280074956,0.06164383561643835,0.05263157894736842,0.3333333333333333,0.0625,0.15999999999999998,0.08333333333333334,0,0,0,1,0,"['This paper describes an improved confusion network based method to combine outputs from multiple MT systems.', 'Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.']"
21,PSEM-P07,P07-1040,0.6270270270270271,0.42990654205607476,False,False,0.05458089668615984,0.1285714285714286,0.08766859344894025,0.09898090664167747,0.09360730593607305,0.2894736842105263,0.3333333333333333,0.28125,0.375,0.20833333333333334,0,0,0,1,0,"['For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  .', 'The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.', 'Also, a more heuristic alignment method has been proposed in a different system combination approach  .', 'Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs.', 'Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning.']"
22,W08-0329,P07-1040,0.06306306306306306,0.11214953271028037,True,False,0.004873294346978557,0,0.019267822736030827,0.02213892468080121,0.0091324200913242,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate   was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312319, Prague, Czech Republic, June 2007.']"
23,W08-0329,P07-1040,0.15315315315315314,0.7383177570093458,False,False,0.01949317738791423,0,0.020231213872832367,0.053648822771465376,0.06050228310502283,0.23684210526315788,0,0.21875,0.2225,0.1388888888888889,0,0,0,1,0,"['The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration.', 'All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.', 'To generate confusion networks, hypotheses have to be aligned against each other.']"
24,W08-0329,P07-1040,0.35135135135135137,0.3037383177570093,False,False,0.018518518518518517,0.05714285714285714,0.05780346820809248,0.06512826519854752,0.07819634703196346,0,0,0,0.0225,0,0,0,0,1,0,"['Each arc represents an alternative word at that.', 'Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.', 'However, the other scores are worse in common with the tuning set results.', 'Full test set scores are obtained by accumulating statistics over all test sentences.', 'Full test set scores are obtained by accumulating the edits and the average reference lengths.']"
25,W08-0329,P07-1040,0.7837837837837838,0.8177570093457944,True,False,0.05458089668615984,0.014285714285714285,0.057803468208092484,0.056694389129670834,0.07990867579908675,0,0,0,0.0125,0,0,0,0,1,0,"['Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04.', 'Table 2: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT05.', 'Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.', 'Table 1: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT03+MT04.', 'The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3.']"
26,W09-0441,P07-1040,0.09749303621169916,0.9158878504672897,False,False,0.049707602339181284,0,0.05202312138728323,0.09335832259575966,0.09646118721461186,0.2894736842105263,0.3333333333333333,0.28125,0.3825,0.22222222222222224,0,0,0,1,0,"['Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.', 'All four reference translations available for the tuning and test sets were used.', 'This paper describes an improved confusion network based method to combine outputs from multiple MT systems.', 'In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.']"
2,D11-1084,P06-2124,0.1991701244813278,0.09716599190283401,True,False,0.01949317738791423,0,0.03564547206165703,0.03209558392878059,0.02682648401826484,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['In a typical SMT setting, each document-pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.', 'Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence-pairs: E = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.']"
3,D11-1084,P06-2124,0.2157676348547718,0.008097165991902834,True,False,0.018518518518518517,0.02857142857142857,0.09537572254335258,0.05997422982312286,0.041666666666666664,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,1,0,1,0,"['Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', 'The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair.', 'Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.', '(a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.', 'Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair.']"
5,P07-1066,P06-2124,0.13145539906103287,0.08502024291497975,True,False,0.03996101364522417,0.02857142857142857,0.16377649325626198,0.13400491976104031,0.0998858447488584,0.3157894736842105,0,0.25,0.2575,0.19444444444444445,0,0,0,1,0,"['We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.', 'In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.', '(a) Sample sentence-length Jn from Poisson(); (b) Sample a topic zdn from a Multinomial(d ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far.', 'BiTAM: Bilingual Topic AdMixture Models forWord Alignment', 'The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).']"
7,P10-2025,P06-2124,0.5526315789473685,0.631578947368421,True,False,0.009746588693957114,0,0.011560693641618497,0.008433876068876654,0.0228310502283105,0,0,0,0.0125,0.027777777777777776,0,0,0,0,1,"['Table 3: Three most distinctive topics are displayed.', 'With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3.']"
8,P10-2025,P06-2124,0.5657894736842105,0.8178137651821862,False,False,0.0009746588693957114,0,0.0009633911368015414,0.0012885088438561558,0.0017123287671232876,0,0,0,0.0125,0.027777777777777776,0,0,0,1,1,"['Union of two directions gives high-recall; Refined grows the intersection with the neighboring word-pairs seen in the union, and yields high-precision and high-recall alignments.']"
9,P11-2032,P06-2124,0.08661417322834646,0.020242914979757085,False,False,0,0,0,0.0022256061848424506,0.00228310502283105,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,1,1,0,['Efficient variational approximation algorithms are designed for inference and parameter estimation.']
10,P12-1048,P06-2124,0.09941520467836257,0.06477732793522267,False,False,0.008771929824561403,0,0.030828516377649322,0.0187419468197259,0.0136986301369863,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,1,0,1,0,"['With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous.', 'Beyond the sentence-level, corpus-level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.']"
11,P12-1048,P06-2124,0.8713450292397661,0.008097165991902834,False,False,0.04191033138401559,0.014285714285714285,0.1560693641618497,0.09710671195970484,0.09075342465753422,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,1,0,0,"['Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', 'Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).', 'We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', 'Marginally, a sentence-pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.', 'We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.']"
12,P12-1048,P06-2124,0.935672514619883,0.008097165991902834,False,False,0.001949317738791423,0,0.0279383429672447,0.009488110577486237,0.0017123287671232876,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.']"
13,P12-1079,P06-2124,0.03018867924528302,0.02834008097165992,True,False,0.03313840155945419,0.02857142857142857,0.13391136801541426,0.07918472531334192,0.07990867579908675,0.3157894736842105,0.3333333333333333,0.28125,0.3775,0.2361111111111111,0,0,1,0,0,"['Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.', 'Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models.', 'The proposed models significantly improve the alignment accuracy and lead to better translation qualities.', 'The word level translation lexicon probabilr ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }.', '5.3 Topic-Specific Translation.']"
14,P12-1079,P06-2124,0.1509433962264151,0.6680161943319838,False,False,0.00682261208576998,0,0.007707129094412331,0.01499355745578072,0.0136986301369863,0,0,0,0.0125,0.027777777777777776,1,1,0,0,0,"['Choosing the number of topics is a model selection problem.', 'BiTAMs discriminate the two by considering the topics of the context.']"
15,P12-2023,P06-2124,0.2857142857142857,0.08502024291497975,False,False,0.0341130604288499,0.014285714285714285,0.15414258188824662,0.08773573854984194,0.07534246575342463,0.3684210526315789,0.3333333333333333,0.3125,0.4,0.2638888888888889,1,1,0,0,0,"['We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.', 'BiTAM: Bilingual Topic AdMixture Models forWord Alignment', 'We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', 'In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.', 'Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.']"
16,P13-2122,P06-2124,0.21774193548387097,0.08502024291497975,True,False,0.021442495126705652,0,0.0674373795761079,0.050954667916129764,0.04794520547945205,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,1,1,0,0,0,"['We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.', 'Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence-pairs: E = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.']"
17,W07-0722,P06-2124,0.20967741935483872,0.02834008097165992,True,False,0.04678362573099415,0.02857142857142857,0.18208092485549132,0.11748857912615675,0.11015981735159816,0.07894736842105263,0.3333333333333333,0.0625,0.155,0.06944444444444445,0,0,0,1,1,"['Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.', 'Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', 'Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence-pairs: E = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.', 'We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', 'In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.']"
18,W07-0722,P06-2124,1.0,0.4817813765182186,False,False,0.00682261208576998,0,0.0077071290944123304,0.016633477802506735,0.010844748858447488,0,0,0,0.0025,0,0,0,0,1,0,"['Second: interpolation smoothing.', 'In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.']"
1,P09-1083,H05-1115,0.2830188679245283,0.2789473684210526,True,False,0.12183235867446393,0.02857142857142857,0.02119460500963391,0.02823005739721213,0.014840182648401826,0.39473684210526316,0,0.21875,0.225,0.1388888888888889,1,0,0,0,0,"['Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.', 'Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.', 'Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.', '3.4 Experiments with topic-sensitive LexRank.', 'We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.']"
2,P09-1083,H05-1115,0.4009433962264151,0.17894736842105263,False,False,0.043859649122807015,0.014285714285714285,0.06069364161849711,0.05587442895630782,0.06735159817351596,0.2631578947368421,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap).', 'Therefore,we examined properties of the questions within eachcluster in order to see what effect they might have onsystem performance.We hypothesized that the baseline system, which compares the similarity of each sentence to the question using IDF-weighted word overlap, should perform well on questions that provide many contentwords.']"
3,P09-1083,H05-1115,0.2028301886792453,0.8105263157894737,False,False,0.042884990253411304,0.02857142857142857,0.01348747591522158,0.01042520791847253,0.009703196347031963,0.02631578947368421,0,0,0,0,1,0,0,0,0,"['Since we are interested in a passage retrieval mechanism that findssentences relevant to a given question, providing input to the question answering component of our system, the improvement in average TRDR score isvery promising.']"
4,N06-1027,H05-1115,0.3014705882352941,0.08421052631578947,True,False,0.13255360623781676,0.09999999999999999,0.04238921001926781,0.03666393346608878,0.03995433789954338,0.3157894736842105,0.3333333333333333,0.28125,0.365,0.20833333333333334,1,0,0,0,0,"['Currently, we address the question-focused sentence retrieval task.', 'In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary.', 'We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time.', 'Therefore, there is often morethan one correct answer to a question.We aim to develop a method for sentence retrieval that goes beyond finding sentences that aresimilar to a single query.']"
5,C08-1062,H05-1115,0.6747967479674797,0.4421052631578947,True,False,0.0341130604288499,0.02857142857142857,0.01348747591522158,0.007965327398383508,0.005136986301369863,0.07894736842105263,0,0,0.0125,0,1,0,0,0,0,"['3.4 Experiments with topic-sensitive LexRank.', 'We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.']"
6,C10-2049,H05-1115,0.518348623853211,0.0,False,False,0.06335282651072124,0,0.014450867052023121,0.02377884502752723,0.0136986301369863,0.05263157894736842,0,0.03125,0.037500000000000006,0.05555555555555555,0,0,0,1,0,"['Using Random Walks for Question-focused Sentence Retrieval', 'Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.']"
7,D08-1032,H05-1115,0.11594202898550725,0.24210526315789474,True,False,0.04678362573099415,0,0.02697495183044316,0.023661707859903944,0.019406392694063926,0.10526315789473684,0,0,0.0125,0,1,0,0,0,0,"['To apply LexRank, a similarity graph is producedfor the sentences in an input document set.', 'Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences.']"
8,D08-1032,H05-1115,0.45410628019323673,0.18947368421052632,True,False,0.145224171539961,0.02857142857142857,0.03949903660886319,0.04498067236734215,0.02910958904109589,0.15789473684210525,0,0,0.015000000000000001,0,0,0,0,1,0,"['Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.', 'Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.', '3.4 Experiments with topic-sensitive LexRank.', 'We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.', 'As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval  .']"
9,P08-2003,H05-1115,0.14473684210526316,0.4421052631578947,True,False,0.12183235867446393,0.02857142857142857,0.02119460500963391,0.028230057397212134,0.014840182648401826,0.39473684210526316,0,0.21875,0.225,0.1388888888888889,1,0,0,0,0,"['3.4 Experiments with topic-sensitive LexRank.', 'We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.', 'Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.', 'Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.', 'Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.']"
10,P08-2003,H05-1115,0.2894736842105263,0.2578947368421053,True,False,0.1608187134502924,0.05714285714285714,0.05009633911368015,0.05657725196204754,0.04052511415525114,0.10526315789473684,0,0,0.015000000000000001,0,1,0,0,0,0,"['The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.', 'Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.', 'We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.', 'Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.', 'As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval  .']"
11,P10-2055,H05-1115,0.8455284552845529,0.9105263157894737,False,False,0.02631578947368421,0.02857142857142857,0.011560693641618497,0.006442544219280779,0.003995433789954338,0.02631578947368421,0,0,0.0125,0,1,0,0,0,0,['We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.']
1,D09-1025,W06-3909,0.36470588235294116,0.4927536231884058,True,False,0.008771929824561403,0,0.013487475915221578,0.020616141501698486,0.0091324200913242,0,0,0,0.0125,0,0,0,0,1,0,"['Intuitively, a reliable instance is one that is highly associated with as many reliable patterns as possible (i.e., we have more confidence in an instance when multiple reliable patterns instantiate it.)', 'Hearst [12] pioneered using patterns to extract hyponym (is-a) relations.']"
2,D09-1025,W06-3909,0.4666666666666667,0.00966183574879227,False,False,0.07309941520467836,0.11428571428571428,0.10019267822736029,0.1229940260044512,0.08105022831050226,0.05263157894736842,0.3333333333333333,0.0625,0.185,0.08333333333333334,0,0,0,1,0,"['Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.', 'Then, the Web counts can also be used to filter out incorrect instances from the generic patterns instantiations.', 'Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.', 'Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances.', 'Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.']"
3,P09-1045,W06-3909,0.49034749034749037,0.4492753623188406,False,False,0.015594541910331383,0.04285714285714286,0.0279383429672447,0.03221272109640389,0.02454337899543379,0,0,0,0.0125,0,0,0,0,1,0,"[""Specifically, for each instance i I, the system creates a set of queries, using each pattern in P' with its y term instantiated with is y term."", 'A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences.']"
4,P09-1113,W06-3909,0.07555555555555556,0.00966183574879227,False,False,0.050682261208576995,0.05714285714285714,0.07129094412331405,0.06946234040060913,0.043949771689497714,0.2894736842105263,0.3333333333333333,0.28125,0.37,0.22222222222222224,0,0,0,1,0,"['Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.', 'Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances.', 'In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.)']"
5,P170300-W06,W06-3909,0.49696969696969695,0.17391304347826086,True,False,0.02729044834307992,0.04285714285714286,0.03660886319845857,0.05236031392760921,0.0365296803652968,0,0,0,0.0125,0,0,0,0,1,0,"['Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters.', 'We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point).']"
6,P846406-W06,W06-3909,0.6887417218543046,0.391304347826087,True,False,0.01364522417153996,0,0.019267822736030827,0.02705868572097927,0.017123287671232876,0,0,0,0.02,0,0,0,0,1,0,"['In future work, this parameter can be learned using a development corpus.', 'Datasets We perform our experiments using the following two datasets:  TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection.']"
7,P846406-W06,W06-3909,0.695364238410596,0.9710144927536232,False,False,0.0029239766081871343,0,0.0019267822736030828,0.004334075202061615,0.008561643835616438,0,0,0,0.0175,0.013888888888888888,0,0,0,1,0,['Early results appear very promising.']
9,PFRAM-W06,W06-3909,0.5447897623400365,0.8115942028985508,False,False,0.00682261208576998,0.02857142857142857,0.024084778420038533,0.011128030924212254,0.01141552511415525,0,0,0,0.0125,0,0,0,0,1,0,['The relative recall is always given in relation to the Espresso system.']
10,PFRAM-W06,W06-3909,0.5493601462522852,0.8695652173913043,True,False,0.021442495126705652,0.07142857142857142,0.033718689788053945,0.04556635820545857,0.02454337899543379,0,0,0,0.025,0,0,0,0,1,0,"['However, generic patterns, while having low precision, yield a high recall, as also reported by [11].', 'Precision and Recall.', 'The challenge, then, is to harness the expressive power of the generic patterns whilst maintaining the precision of Espresso.', 'This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances.', ""The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).""]"
11,PONTO-W06,W06-3909,0.07964601769911504,0.07246376811594203,True,False,0.024366471734892786,0.014285714285714285,0.04624277456647398,0.031861309593534025,0.0273972602739726,0,0,0,0.005,0,0,0,0,1,0,"['To date, most research on lexical relation harvesting has focused on is-a and part-of relations.', 'Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].']"
12,PONTO-W06,W06-3909,0.11946902654867257,0.966183574879227,True,False,0.025341130604288498,0.04285714285714286,0.0394990366088632,0.03268126976689703,0.02511415525114155,0,0,0,0.0175,0.013888888888888888,0,0,0,1,0,"['As mentioned above in Section 4.3, we are working on improving system recall with a web-based method to identify generic patterns and filter their instances.', 'We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.']"
13,PSS07-W06,W06-3909,0.17314487632508835,0.7536231884057971,True,False,0.022417153996101363,0.02857142857142857,0.08381502890173409,0.05013470774276675,0.06335616438356165,0.23684210526315788,0,0.21875,0.2225,0.1388888888888889,0,0,0,1,0,"['System performance on the part-of relation on the TREC9 dataset.', 'System performance on the succession relation on the TREC9 dataset.', 'From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.']"
1,D12-1046,C10-1045,0.1517509727626459,0.8411214953271028,True,False,0.04873294346978557,0,0.05394990366088631,0.03806957947756823,0.09474885844748857,0.3947368421052631,0,0.25,0.255,0.29166666666666663,0,0,0,1,0,"['6 Joint Segmentation and Parsing.', 'Better Arabic Parsing: Baselines, Evaluations, and Analysis', 'To our knowledge, ours is the first analysis of this kind for Arabic parsing.', 'We propose a limit of 70 words for Arabic parsing evaluations.', 'Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.']"
2,J13-1007,C10-1045,0.09461235216819974,0.8411214953271028,True,False,0.003898635477582846,0,0.005780346820809248,0.0038655265315684665,0.007990867579908675,0.10526315789473684,0,0,0.01,0.06944444444444445,0,0,0,1,0,['6 Joint Segmentation and Parsing.']
3,J13-1007,C10-1045,0.19448094612352168,0.7476635514018691,True,False,0.08187134502923976,0.02857142857142857,0.05587668593448939,0.05903713248213657,0.08961187214611868,0.10526315789473684,0,0,0.01,0.06944444444444445,0,0,0,0,1,"['The Stanford parser includes both the manually annotated grammar () and an Arabic unknown word model with the following lexical features: 1.', 'Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models  .13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation  .']"
4,J13-1007,C10-1045,0.6294349540078844,0.8660436137071651,False,False,0.025341130604288498,0.02857142857142857,0.024084778420038533,0.028230057397212134,0.037100456621004564,0.10526315789473684,0,0,0.01,0.06944444444444445,0,0,0,1,0,"['Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic.', 'Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.']"
5,J13-1007,C10-1045,0.6701708278580815,0.8411214953271028,True,False,0.003898635477582846,0,0.005780346820809248,0.0038655265315684665,0.007990867579908675,0.10526315789473684,0,0,0.01,0.06944444444444445,0,0,0,0,1,['6 Joint Segmentation and Parsing.']
6,J13-1007,C10-1045,0.8804204993429697,0.018691588785046728,True,False,0.009746588693957114,0.014285714285714285,0.02119460500963391,0.012065128265198549,0.037100456621004564,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 25% F1.']"
7,J13-1007,C10-1045,0.8830486202365309,0.7009345794392523,True,False,0.1101364522417154,0.02857142857142857,0.0770712909441233,0.08094178282769121,0.13413242009132417,0.10526315789473684,0,0,0.01,0.06944444444444445,0,0,0,0,1,"['(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length  40.', 'But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.', 'Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models  .13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation  .', 'The Stanford parser includes both the manually annotated grammar () and an Arabic unknown word model with the following lexical features: 1.']"
8,J13-1008,C10-1045,0.21428571428571427,0.03426791277258567,False,False,0.02729044834307992,0,0.009633911368015413,0.00855101323649994,0.021689497716894976,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,1,0,0,0,0,"['To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results.']"
9,J13-1008,C10-1045,0.2153846153846154,0.06853582554517133,False,False,0.0341130604288499,0,0.009633911368015413,0.022607473351294365,0.033105022831050226,0.23684210526315788,0,0.21875,0.2125,0.1527777777777778,0,0,0,1,1,"['We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic ().', 'In our grammar, features are realized as annotations to basic category labels.']"
10,J13-1008,C10-1045,0.7307692307692307,0.07165109034267912,False,False,0.015594541910331383,0,0.02215799614643545,0.015462106126273866,0.030251141552511414,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed ().']"
11,J13-1009,C10-1045,0.25745257452574527,0.3644859813084112,True,False,0.06140350877192982,0,0.036608863198458574,0.06114560149935574,0.07990867579908675,0.3421052631578947,0,0.21875,0.2225,0.22222222222222224,0,0,0,1,0,"['We start with noun features since written Arabic contains a very high proportion of NPs.', 'The Stanford parser includes both the manually annotated grammar () and an Arabic unknown word model with the following lexical features: 1.', 'But Arabic contains a variety of linguistic phenomena unseen in English.']"
12,J13-1009,C10-1045,0.26151761517615174,0.3987538940809969,True,False,0.023391812865497075,0,0.0038535645472061657,0.0073796415602670725,0.02054794520547945,0.23684210526315788,0,0.21875,0.2125,0.1527777777777778,0,0,0,0,1,"['8 We use head-finding rules specified by a native speaker.', 'of Arabic.', 'We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic ().']"
13,J13-1009,C10-1045,0.4281842818428184,0.0,True,False,0.05263157894736842,0.014285714285714285,0.05202312138728323,0.039240951153801086,0.10502283105022829,0.3157894736842105,0,0.25,0.2575,0.22222222222222224,0,0,0,0,1,"['Better Arabic Parsing: Baselines, Evaluations, and Analysis', 'To our knowledge, ours is the first analysis of this kind for Arabic parsing.', 'We propose a limit of 70 words for Arabic parsing evaluations.', 'By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.']"
14,J13-1009,C10-1045,0.537940379403794,0.9376947040498442,True,False,0.04483430799220273,0.014285714285714285,0.08092485549132945,0.04966615907227362,0.1409817351598173,0.15789473684210525,0.3333333333333333,0.0625,0.1525,0.16666666666666669,0,0,0,0,1,"['Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.', 'Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.', 'Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 25% F1.', 'We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).', '6 Joint Segmentation and Parsing.']"
15,J13-1009,C10-1045,0.5501355013550135,0.7694704049844237,False,False,0.03313840155945419,0,0.010597302504816955,0.016047791964390302,0.026255707762557076,0.10526315789473684,0,0,0.01,0.06944444444444445,1,0,0,0,0,"['Modifying the Berkeley parser for Arabic is straightforward.', 'Because the Bikel parser has been parameterized for Arabic by the LDC, we do not change the default model settings.']"
17,P11-1159,C10-1045,0.436,0.06853582554517133,False,False,0.0341130604288499,0,0.009633911368015413,0.022607473351294365,0.033105022831050226,0.23684210526315788,0,0.21875,0.2125,0.1527777777777778,1,0,0,0,0,"['We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic ().', 'In our grammar, features are realized as annotations to basic category labels.']"
18,P11-2037,C10-1045,0.265625,0.8660436137071651,True,False,0.0594541910331384,0.02857142857142857,0.04624277456647398,0.04064659716528054,0.0656392694063927,0.10526315789473684,0,0,0.01,0.06944444444444445,1,0,0,0,0,"['Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic.', 'Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models  .13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation  .']"
19,P11-2122,C10-1045,0.056451612903225805,0.22429906542056074,True,False,0.021442495126705652,0,0.02119460500963391,0.015110694623404006,0.04509132420091324,0.10526315789473684,0,0,0.01,0.09722222222222222,0,0,0,0,1,"['We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).', 'The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.']"
20,P11-2122,C10-1045,0.5887096774193549,0.470404984423676,True,False,0.03996101364522417,0,0.018304431599229284,0.018390535316856034,0.04965753424657534,0.2894736842105263,0.3333333333333333,0.28125,0.36,0.22222222222222224,0,0,0,0,1,"['All experiments use ATB parts 13 divided according to the canonical split suggested by Chiang et al.', 'Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic.', 'We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic ().']"
21,P11-2122,C10-1045,0.8790322580645161,0.838006230529595,False,False,0.017543859649122806,0,0.030828516377649325,0.01710202647299988,0.0502283105022831,0.10526315789473684,0,0,0.01,0.06944444444444445,0,0,0,0,1,"['The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.', 'Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.']"
22,P11-2124,C10-1045,0.13043478260869565,0.8660436137071651,True,False,0.05555555555555555,0,0.0443159922928709,0.04123228300339698,0.07876712328767121,0.39473684210526316,0,0.25,0.2575,0.3055555555555556,0,0,0,1,0,"['Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic.', 'Better Arabic Parsing: Baselines, Evaluations, and Analysis', 'To our knowledge, ours is the first analysis of this kind for Arabic parsing.', 'We propose a limit of 70 words for Arabic parsing evaluations.', 'of Arabic.']"
23,P12-1016,C10-1045,0.7075812274368231,0.7694704049844237,True,False,0.08284600389863547,0,0.048169556840077066,0.059974229823122865,0.0930365296803653,0.10526315789473684,0,0,0.0175,0.08333333333333334,1,0,0,0,0,"['Modifying the Berkeley parser for Arabic is straightforward.', 'We are unaware of prior results for the Stanford parser.', 'We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.', 'The Stanford parser includes both the manually annotated grammar () and an Arabic unknown word model with the following lexical features: 1.', 'able at http://nlp.stanford.edu/projects/arabic.shtml.']"
24,P12-2002,C10-1045,0.08823529411764706,0.07476635514018691,False,False,0.007797270955165692,0,0.011560693641618497,0.008668150404123227,0.019406392694063926,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing ().']"
25,P12-2002,C10-1045,0.25,0.956386292834891,True,False,0.01949317738791423,0.014285714285714285,0.002890173410404624,0.004568349537308189,0.006278538812785388,0.10526315789473684,0,0,0.01,0.06944444444444445,1,0,0,0,0,"['Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty   found that unweighted lattices were more effective for Hebrew.']"
26,P12-2002,C10-1045,0.27941176470588236,0.07476635514018691,False,False,0.01949317738791423,0.02857142857142857,0.026974951830443156,0.019561906993088903,0.04509132420091324,0.3421052631578947,0,0.21875,0.22,0.20833333333333334,0,0,0,1,0,"['Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing ().', 'Lattice parsing   is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.']"
27,W13-4904,C10-1045,0.42435424354243545,0.4392523364485981,True,False,0.015594541910331383,0.014285714285714285,0.011560693641618497,0.013939322947171137,0.02511415525114155,0,0,0,0.0075,0.041666666666666664,0,0,0,1,0,"['markContainsVerb is especially effective for distinguishing root S nodes of equational sentences.', 'English parsing evaluations usually report results on sentences up to length 40.']"
28,W13-4904,C10-1045,0.8634686346863468,0.22429906542056074,True,False,0.030214424951267055,0,0.026974951830443156,0.02659013705048612,0.050799086757990865,0.10526315789473684,0,0,0.01,0.09722222222222222,0,0,0,0,1,"['We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).', 'Recently, lattices have been used successfully in the parsing of Hebrew  , a Semitic language with similar properties to Arabic.']"
29,W13-4904,C10-1045,0.9003690036900369,0.07165109034267912,False,False,0.015594541910331383,0,0.02215799614643545,0.015462106126273866,0.030251141552511414,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"['To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed ().']"
31,W13-4917,C10-1045,0.3205918618988903,0.4766355140186916,True,False,0.028265107212475632,0.04285714285714286,0.04335260115606935,0.030572800749677866,0.06906392694063926,0.02631578947368421,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.', 'We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.']"
32,W13-4917,C10-1045,0.32305795314426633,0.4797507788161994,True,False,0.00682261208576998,0,0.008670520231213872,0.00855101323649994,0.014840182648401826,0,0,0,0.0075,0.013888888888888888,0,0,0,1,0,"['At the phrasal level, we remove all function tags and traces.', '7 Unlike Dickinson  , we strip traces and only con-.', 'Finally, we add DT to the tags for definite nouns and adjectives  .']"
33,W13-4917,C10-1045,0.2688039457459926,0.009345794392523364,False,False,0.014619883040935672,0,0.011560693641618497,0.009019561906993087,0.0228310502283105,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic.']"
1,C10-2104,C02-1025,0.5958549222797928,0.6097560975609756,True,False,0.024366471734892786,0.014285714285714285,0.018304431599229287,0.05107180508375306,0.0656392694063927,0,0,0,0.0325,0.013888888888888888,0,0,0,1,0,"['4.2 Global Features.', 'Global features are extracted from other occurrences of the same token in the whole document.', 'The features we used can be divided into 2 classes: local and global.', 'Such a classification can be seen as a not-always-correct summary of global features.', 'The effect of a second reference resolution classifier is not entirely the same as that of global features.']"
2,C10-2167,C02-1025,0.20833333333333334,0.2097560975609756,True,False,0.018518518518518517,0.02857142857142857,0.024084778420038533,0.021436101675061495,0.03424657534246575,0.02631578947368421,0,0.09375,0.0275,0.1111111111111111,0,0,0,0,1,"['It uses a maximum entropy framework and classifies each word given its features.', 'On the MUC6 data, Bikel et al.']"
3,I05-3013,C02-1025,0.3534136546184739,0.05365853658536585,False,False,0.007797270955165692,0,0.019267822736030827,0.015462106126273868,0.02054794520547945,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,['We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).']
4,I05-3030,C02-1025,0.4583333333333333,0.28780487804878047,False,False,0.02046783625730994,0.014285714285714285,0.016377649325626204,0.016282066299636872,0.02226027397260274,0,0,0.03125,0.0025,0.041666666666666664,0,0,0,0,1,['The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.']
6,P02-1061,C02-1025,0.27717391304347827,0.2097560975609756,False,False,0.025341130604288498,0.04285714285714286,0.04238921001926782,0.04041232283003396,0.057648401826484015,0.02631578947368421,0,0.03125,0.015000000000000001,0.041666666666666664,0,0,0,1,0,"['It uses a maximum entropy framework and classifies each word given its features.', 'This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models  .']"
7,P03-1028,C02-1025,0.7419354838709677,0.2634146341463415,True,False,0.0029239766081871343,0,0.012524084778420038,0.01054234508609582,0.012557077625570776,0,0,0.03125,0.0025,0.041666666666666664,0,0,1,0,0,['We have used the Java-based opennlp maximum entropy package1.']
10,P03-1028,C02-1025,0.23963133640552994,0.00975609756097561,False,False,0.011695906432748537,0,0.012524084778420038,0.015110694623404006,0.012557077625570776,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.']"
12,P05-1045,C02-1025,0.8743718592964824,0.3073170731707317,False,False,0.025341130604288498,0,0.03564547206165703,0.07426496427316386,0.08105022831050226,0,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['Global features are extracted from other occurrences of the same token in the whole document.', 'Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document.', 'Multiple features can be used for the same token.']"
13,P05-1051,C02-1025,0.16666666666666666,0.37073170731707317,True,False,0.030214424951267055,0,0.0279383429672447,0.05599156612393112,0.0502283105022831,0,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['Table 1: Features based on the token string that are based on the probability of each name class during training.', 'Multiple features can be used for the same token.']"
15,W03-0423,C02-1025,0.0967741935483871,0.6097560975609756,True,False,0.024366471734892786,0.014285714285714285,0.018304431599229287,0.05107180508375306,0.0656392694063927,0,0,0,0.0325,0.013888888888888888,0,0,0,0,1,"['4.2 Global Features.', 'Global features are extracted from other occurrences of the same token in the whole document.', 'The features we used can be divided into 2 classes: local and global.', 'Such a classification can be seen as a not-always-correct summary of global features.', 'The effect of a second reference resolution classifier is not entirely the same as that of global features.']"
16,W03-0423,C02-1025,0.25806451612903225,0.5853658536585366,True,False,0.029239766081871343,0,0.026011560693641616,0.04427784936160244,0.041666666666666664,0,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix-List.', 'Table 1: Features based on the token string that are based on the probability of each name class during training.']"
17,W03-0423,C02-1025,0.3548387096774194,0.2975609756097561,True,False,0.028265107212475632,0,0.04431599229287091,0.08785287571746514,0.08447488584474885,0,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['The features we used can be divided into 2 classes: local and global.', ""The local features used are similar to those used in BBN' s IdentiFinder   or MENE  ."", '4.2 Global Features.', 'Multiple features can be used for the same token.', '4.1 Local Features.']"
18,W03-0423,C02-1025,0.5,0.4195121951219512,True,False,0.08089668615984405,0.04285714285714286,0.08092485549132947,0.15567529577134834,0.13869863013698625,0,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.', 'This group contains a large number of features (one for each token string present in the training data).', 'Table 1: Features based on the token string that are based on the probability of each name class during training.', 'Local features are features that are based on neighboring tokens, as well as the token itself.', 'Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.']"
19,W03-0423,C02-1025,0.3709677419354839,0.3121951219512195,False,False,0.05360623781676413,0.014285714285714285,0.0789980732177264,0.13060794189996489,0.11643835616438353,0,0,0.03125,0.0225,0.05555555555555555,0,0,0,1,0,"[""The local features used are similar to those used in BBN' s IdentiFinder   or MENE  ."", 'Multiple features can be used for the same token.', 'We group the features used into feature groups.', 'This might be because our features are more comprehensive than those used by Borthwick.', 'The system described in this paper is similar to the MENE system of  .']"
20,W03-0432,C02-1025,0.4036697247706422,0.13170731707317074,True,False,0.0594541910331384,0.014285714285714285,0.08285163776493255,0.09675530045683496,0.09189497716894976,0.3157894736842105,0.3333333333333333,0.34375,0.41,0.2916666666666667,0,0,0,1,0,"['  did make use of information from the whole document.', 'It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.', 'As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.', 'The secondary classifier in   uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document.', 'Same for . In the case where the next token is a hyphen, then is also used as a feature: (initCaps, ) is set to 1.']"
21,W04-0705,C02-1025,0.049079754601226995,0.01951219512195122,True,False,0.023391812865497075,0.014285714285714285,0.05876685934489401,0.0462691812111983,0.06792237442922373,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.', 'This paper presents a maximum entropy-based named entity recognizer (NER).', 'It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.']"
22,W04-0705,C02-1025,0.901840490797546,0.33658536585365856,True,False,0.01949317738791423,0.02857142857142857,0.03468208092485549,0.05599156612393111,0.046232876712328765,0,0,0,0.02,0.013888888888888888,0,0,0,1,0,"['Multiple features can be used for the same token.', 'Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.']"
23,W06-0119,C02-1025,0.125,0.2097560975609756,True,False,0.04093567251461988,0.02857142857142857,0.05298651252408477,0.0409980086681504,0.059931506849315065,0,0,0.03125,0.015000000000000001,0.041666666666666664,0,0,0,0,1,"['It uses a maximum entropy framework and classifies each word given its features.', 'The baseline system in Table 3 refers to the maximum entropy system that uses only local features.']"
1,J06-1004,P00-1025,0.17105263157894737,0.013215859030837005,False,False,0.014619883040935672,0,0.016377649325626204,0.0514232165866229,0.021118721461187213,0.2894736842105263,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"['We describe a new technique for constructing finite-state transducers that involves reapplying the regular-expression compiler to its own output.', 'The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.', 'The compile-replace algorithm then reapplies the regular-expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.', 'The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.', 'Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite-state network.']"
2,P55-62,P00-1025,0.13648293963254593,0.0,True,False,0.015594541910331383,0,0.012524084778420038,0.0160477919643903,0.014840182648401826,0.10526315789473684,0.3333333333333333,0.09375,0.1925,0.125,0,0,0,1,0,"['Finite-State Non-Concatenative Morphotactics', 'Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems.', '4.2 Arabic Morphological.', 'modeled in finite state terms as concatenation.']"
3,P9852-P00,P00-1025,0.796812749003984,0.8898678414096917,True,False,0.004873294346978557,0,0.010597302504816955,0.025301628206629963,0.00684931506849315,0,0,0,0.0125,0,0,0,0,1,0,"['The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.', 'Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular-expression substring.Figure 9: After the Application of Compile Replace lower) of the network.']"
5,PE2006-P00,P00-1025,0.17105263157894737,0.013215859030837005,False,False,0.014619883040935672,0,0.016377649325626204,0.0514232165866229,0.021118721461187213,0.2894736842105263,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"['We describe a new technique for constructing finite-state transducers that involves reapplying the regular-expression compiler to its own output.', 'The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.', 'The compile-replace algorithm then reapplies the regular-expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.', 'The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.', 'Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite-state network.']"
6,PLEX-P00,P00-1025,0.09090909090909091,0.00881057268722467,False,False,0.010721247563352826,0.014285714285714285,0.007707129094412331,0.014056460114794424,0.02682648401826484,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,0,1,"['However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena.', 'Implemented in an algorithm called compile-replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.']"
7,PLEX-P00,P00-1025,0.093841642228739,0.3964757709251101,True,False,0.004873294346978557,0.02857142857142857,0.008670520231213872,0.016164929132013586,0.008561643835616438,0,0,0,0.0125,0,0,0,0,1,0,"['Productive reduplication cannot be described by finite-state or even context-free formalisms.', '3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.']"
8,PMORPH-P00,P00-1025,0.3059360730593607,0.42290748898678415,False,False,0.001949317738791423,0,0.002890173410404624,0.007965327398383506,0.0005707762557077625,0,0,0,0.0125,0,0,0,0,1,0,"['In the regular expression calculus there are several operators that involve concatenation.', 'To introduce the merge operation into the Xerox regular expression calculus we need to choose an operator symbol.']"
10,PSTAT-P00,P00-1025,0.6935483870967742,0.3303964757709251,True,False,0.02631578947368421,0.02857142857142857,0.05394990366088632,0.09616961461871859,0.04509132420091324,0,0,0,0.015000000000000001,0,0,0,0,1,0,"['Figure 8: A Network with a Regular-Expression Substring on the Lower Side The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation.', 'Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14.', 'We use the networks in Figure 13 to illustrate the effect of the merge algorithm.', 'The compile-replace algorithm, applied to the lower-side of this network, recognizes each individual delimited regular-expression substring like ^[{bagi}^2^], compiles it, and replaces it with the result of the compilation, here bagibagi.', 'If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.']"
11,W02-0503,P00-1025,0.07423580786026202,0.013215859030837005,True,False,0.02046783625730994,0.02857142857142857,0.055876685934489405,0.06922806606536253,0.04337899543378995,0.2894736842105263,0.3333333333333333,0.28125,0.3675,0.20833333333333334,0,0,0,1,0,"['We describe a new technique for constructing finite-state transducers that involves reapplying the regular-expression compiler to its own output.', 'The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.', 'Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite-state network.', 'The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.', 'If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.']"
12,W07-0802,P00-1025,0.8481012658227848,0.004405286343612335,False,False,0.003898635477582846,0,0.0077071290944123304,0.00784819023076022,0.003995433789954338,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems.']"
13,W08-0703,P00-1025,0.09433962264150944,0.8898678414096917,True,False,0.017543859649122806,0,0.012524084778420038,0.023310296357034084,0.016552511415525113,0,0,0,0.0125,0,0,0,0,1,0,"['The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.', '3.2.1 Review of Earlier Work Much of the work in non-concatenative finite-state morphotactics has been dedicated to handling Semitic stem interdigitation.']"
14,W09-0802,P00-1025,0.7931034482758621,0.2643171806167401,False,False,0.036062378167641324,0.014285714285714285,0.03371868978805394,0.06196556167271874,0.053082191780821915,0.23684210526315788,0,0.21875,0.225,0.1388888888888889,0,0,0,1,0,"['We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.', 'But this task can be accomplished, in fact quite efficiently, by using the compile-replace technique.', 'The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific', 'To understand the general solution to full-stem reduplication using the compile-replace algorithm requires a bit of background.', 'This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full-stem reduplication and Arabic stem interdigitation, which will be described below.']"
1,D13-1060,W11-0815,0.03167420814479638,0.022026431718061675,False,False,0.07212475633528265,0.02857142857142857,0.03468208092485549,0.06231697317558859,0.07876712328767121,0.05263157894736842,0.3333333333333333,0.0625,0.145,0.06944444444444445,0,0,0,1,0,"['Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.', 'Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms.', 'This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.']"
2,P09207-W11,W11-0815,0.8223684210526315,0.29955947136563876,False,False,0.007797270955165692,0,0.007707129094412331,0.00972238491273281,0.0091324200913242,0,0,0,0.0025,0,0,0,0,1,0,"['In this work, we focused on MWEs composed of exactly two words (i.e. bigrams).']"
3,PPROC9-W11,W11-0815,0.0847457627118644,0.013215859030837005,False,False,0.029239766081871343,0,0.02890173410404624,0.03361836710788333,0.04052511415525114,0.05263157894736842,0.3333333333333333,0.0625,0.145,0.06944444444444445,0,0,0,1,0,"['Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment.']"
4,PPROC14-W11,W11-0815,0.07960199004975124,0.022026431718061675,False,False,0.037037037037037035,0.02857142857142857,0.023121387283236993,0.031392760923040886,0.04337899543378995,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.', 'Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.']"
5,S13-1039,W11-0815,0.025,0.06607929515418502,False,False,0.03313840155945419,0,0.026011560693641612,0.030338526414431286,0.037100456621004564,0.23684210526315788,0,0.21875,0.2125,0.1388888888888889,0,0,0,1,0,"['In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.', 'This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.']"
6,W12-3311,W11-0815,0.21678321678321677,0.013215859030837005,True,False,0.0769980506822612,0.02857142857142857,0.06358381502890173,0.07168794658545154,0.08904109589041095,0.3421052631578947,0.3333333333333333,0.3125,0.38749999999999996,0.2638888888888889,0,0,0,1,0,"['Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World  , pages 101109, Portland, Oregon, USA, 23 June 2011.', 'Identification and Treatment of Multiword Expressions applied to Information Retrieval', 'Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.', 'In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.']"
7,W13-1006,W11-0815,0.08670520231213873,0.013215859030837005,False,False,0.06725146198830409,0,0.06262042389210018,0.06911092889773926,0.08276255707762559,0.34210526315789475,0.3333333333333333,0.3125,0.39,0.2638888888888889,0,0,0,1,0,"['Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment.', 'Identification and Treatment of Multiword Expressions applied to Information Retrieval', 'In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.', 'One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.']"
8,W13-3208,W11-0815,0.08205128205128205,0.013215859030837005,False,False,0.03996101364522417,0,0.027938342967244695,0.03830385381281479,0.04965753424657534,0.05263157894736842,0.3333333333333333,0.0625,0.145,0.06944444444444445,0,0,0,0,1,"['Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.']"
9,W15-0909,W11-0815,0.04040404040404041,0.9559471365638766,False,False,0.08284600389863547,0.02857142857142857,0.04624277456647398,0.07496778727890363,0.09646118721461186,0.2894736842105263,0.3333333333333333,0.28125,0.355,0.20833333333333334,0,0,0,1,0,"['This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.', 'Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.', 'Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.', 'In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms.']"
10,W15-0909,W11-0815,0.0707070707070707,0.013215859030837005,False,False,0.02046783625730994,0,0.017341040462427744,0.020030455663582053,0.025684931506849314,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,0,1,"['Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.']"
1,W13-4047,J00-3003,0.11304347826086956,0.008532423208191127,False,False,0.050682261208576995,0.02857142857142857,0.018304431599229284,0.02834719456483542,0.028538812785388126,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.']"
2,W12-1634,J00-3003,0.04819277108433735,0.89419795221843,False,False,0.19785575048732942,0.07142857142857142,0.035645472061657024,0.056108703291554395,0.04794520547945205,0.07894736842105263,0.3333333333333333,0.0625,0.15999999999999998,0.08333333333333334,1,0,0,0,0,"['An extensive comparison of the prosodic DA modeling literature with our work can be found in Shriberg et al.  .', 'We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.', 'We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus.', 'The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.', 'We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY.']"
3,W12-1634,J00-3003,0.07228915662650602,0.22013651877133106,True,False,0.09064327485380116,0,0.028901734104046235,0.04697200421693803,0.030821917808219176,0.15789473684210525,0.3333333333333333,0.09375,0.185,0.125,0,0,0,1,0,"['2.3 Major Dialogue Act Types.', 'Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.', 'Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech', '5.2 Dialogue Act Classification Using Prosody.', '4 5.1 Dialogue Act Classification Using Words.']"
4,W10-1012,J00-3003,0.8767772511848341,0.0,False,False,0.1608187134502924,0.02857142857142857,0.026011560693641612,0.04158369450626684,0.0410958904109589,0.07894736842105263,0,0.03125,0.052500000000000005,0.06944444444444445,1,0,0,0,0,"['Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech', 'We believe that dialogue-related tasks have much to benefit from corpus-driven, automatic learning techniques.', 'We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus.', 'DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition.', 'We also investigated non-n-gram discourse models, based on various language modeling techniques known from speech recognition.']"
5,W13-4011,J00-3003,0.012345679012345678,0.15187713310580206,True,False,0.018518518518518517,0.02857142857142857,0.028901734104046235,0.023895982195150517,0.032534246575342464,0.05263157894736842,0,0,0.0075,0,0,0,0,1,0,"['The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance).', 'One frequent example in our corpus was the distinction between BACKCHANNELS and AGREEMENTS (see Table 2), which share terms such as right and yeah.']"
6,W01-1627,J00-3003,0.17307692307692307,0.17064846416382254,False,False,0.014619883040935672,0,0.02312138728323699,0.0178048494787396,0.016552511415525113,0.02631578947368421,0,0,0.0075,0,0,0,0,1,0,"['We began with the DAMSL markup system, but modified it in several ways to make it more relevant to our corpus and task.', 'The DA labeling system had special provisions for rare cases where utterances seemed to combine aspects of several DA types.']"
7,N13-1099,J00-3003,0.125,0.12457337883959044,True,False,0.03898635477582846,0,0.04238921001926781,0.04966615907227364,0.048515981735159815,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14).', 'Section 6 shows how DA models can be used to benefit speech recognition.']"
8,H01-1001,J00-3003,0.536723163841808,0.6877133105802048,False,False,0.017543859649122806,0,0.007707129094412331,0.003045566358205459,0.003424657534246575,0.02631578947368421,0,0,0,0,1,0,0,0,0,"['The choice of tasks was motivated by an analysis of confusions committed by a purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, and BACKCHANNELS for AGREEMENTS (and vice versa).']"
9,N06-2021,J00-3003,0.460431654676259,0.16382252559726962,True,False,0.042884990253411304,0.014285714285714285,0.026011560693641612,0.01850767248447932,0.032534246575342464,0.05263157894736842,0,0,0.0075,0,1,0,0,0,0,"['Based on these considerations, we decided not to confound the DA classification task with the additional problems introduced by automatic segmentation and assumed the utterance-level segmentations as given.', 'Table 9 Combined utterance classification accuracies (chance = 35%).']"
10,W12-1616,J00-3003,0.029556650246305417,0.05631399317406143,False,False,0.003898635477582846,0,0.0038535645472061657,0.003982663699191754,0.005136986301369863,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.']"
1,J01-2004,J98-2005,0.15417558886509636,0.1619047619047619,True,False,0.0682261208576998,0.07142857142857142,0.0279383429672447,0.025301628206629956,0.03139269406392694,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,1,0,0,0,0,"['We will show that in both cases the estimated probability is tight.', 'Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the ""relative frequency"" estimator y\'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production ""."" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions.']"
2,N03-1027,J98-2005,0.2641509433962264,0.10476190476190476,True,False,0.08089668615984405,0.02857142857142857,0.04527938342967245,0.06559681386904063,0.04794520547945205,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['What if the production probabilities are estimated from data?', ""If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?"", 'Therefore, when the corpus consists of yields only, we shall assume a priori a model free of null and unit productions, and study tightness for probabilities estimated under such a model.', 'We show here that estimated production probabilities always yield proper distributions.', 'For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG.']"
3,P01-1017,J98-2005,0.42934782608695654,0.1619047619047619,False,False,0.008771929824561403,0,0.002890173410404624,0.004802623872554762,0.003424657534246575,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,1,0,0,0,0,['We will show that in both cases the estimated probability is tight.']
6,P13-1102,J98-2005,0.10043668122270742,0.7523809523809524,True,False,0.14912280701754385,0.14285714285714285,0.042389210019267806,0.05353168560384209,0.0593607305936073,0.2631578947368421,0,0.21875,0.21,0.1388888888888889,1,0,0,0,0,"['More generally, ~ will refer to the probability distribution on (possibly infinite) parse trees induced by the maximum-likelihood estimator.', ""If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?"", 'We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1.', 'The maximum-likelihood probability is tight.', 'Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the ""relative frequency"" estimator y\'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production ""."" in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions.']"
2,P13-1138,W08-2222,0.2749003984063745,0.0,False,False,0.022417153996101363,0.02857142857142857,0.08285163776493255,0.04240365467962983,0.03367579908675799,0.3157894736842105,0,0.25,0.2625,0.19444444444444445,0,0,0,1,0,"['Wide-Coverage Semantic Analysis with Boxer', 'Boxer is a wide-coverage system for semantic interpretation.', 'Boxer is an open-domain tool for computing and reasoning with semantic representations.', 'In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs.', 'Boxer was able to produce semantic representation for all text without any further modifications to the software.']"
3,Q13-1015,W08-2222,0.14285714285714285,0.5636363636363636,True,False,0.021442495126705652,0.014285714285714285,0.038535645472061654,0.02776150872671899,0.021689497716894976,0.02631578947368421,0,0,0.0175,0,0,0,0,1,0,"['The output of Boxer for this text is shown in Figure 3.', 'Boxer is a wide-coverage system for semantic interpretation.', 'Here we discuss the output of Boxer on the Shared Task Texts  .']"
4,S12-1040,W08-2222,0.05240174672489083,0.3878787878787879,True,False,0.038011695906432746,0.014285714285714285,0.05202312138728324,0.05224317675998591,0.0410958904109589,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing  .', 'Here we discuss the output of Boxer on the Shared Task Texts  .', 'Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.', 'Based on Discourse Representation Theory  , Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called boxes because of the way they are graphically displayed) for English sentences and texts.']"
5,S13-1002,W08-2222,0.16342412451361868,0.006060606060606061,False,False,0.043859649122807015,0,0.10211946050096339,0.06243411034321188,0.04680365296803653,0.13157894736842105,0.3333333333333333,0.09375,0.19,0.125,0,0,0,1,0,"['Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).', 'Wide-Coverage Semantic Analysis with Boxer', 'The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.', 'Boxer is a wide-coverage system for semantic interpretation.', 'Boxer was able to produce semantic representation for all text without any further modifications to the software.']"
6,W10-1750,W08-2222,0.3888888888888889,0.012121212121212121,True,False,0.021442495126705652,0,0.04335260115606937,0.031509898090664165,0.021689497716894976,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.', 'Boxer is distributed with the C&C tools and freely available for research purposes.', 'Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).']"
7,W11-2408,W08-2222,0.9763779527559056,0.9757575757575757,False,False,0.029239766081871343,0.02857142857142857,0.020231213872832363,0.03197844676115731,0.028538812785388126,0.02631578947368421,0,0,0.0125,0,0,0,1,1,0,"['Manually inspecting the output gives us the following impression:  computed predicate argument structure is generally of good quality, including hard constructions involving control or coordination;  discourse structure triggered by conditionals, negation or discourse adverbs is overall correctly computed;  some measure and time expressions are correctly analysed, others arent;  several shallow analyses are given for lexical phrases that require deep analysis;  bridging references and pronouns are not resolved in most cases; but when they are, they are mostly correctly resolved (high precision at the cost of recall).', 'The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics  , where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations.']"
8,W13-2101,W08-2222,0.037037037037037035,0.06060606060606061,False,False,0.029239766081871343,0.014285714285714285,0.09826589595375719,0.037483893639451794,0.03139269406392694,0.3157894736842105,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,1,0,0,"['Boxer is an open-domain tool for computing and reasoning with semantic representations.', 'Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).', 'The existence of CCGbank   and robust parsers trained on it   make Boxer a state-of-the-art open-domain tool for deep semantic analysis.', 'It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.']"
9,W13-3209,W08-2222,0.2,0.01818181818181818,False,False,0.02631578947368421,0,0.047206165703275536,0.03350122994026004,0.0228310502283105,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.', 'Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).']"
1,C10-2023,C08-1098,0.5727699530516432,0.5619469026548672,True,False,0.06237816764132553,0,0.05973025048169558,0.05224317675998593,0.0884703196347032,0.2894736842105263,0.3333333333333333,0.34375,0.37749999999999995,0.33333333333333337,0,0,1,0,1,"['It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).']"
2,D12-1133,C08-1098,0.5488126649076517,0.9867256637168141,True,False,0.0682261208576998,0.02857142857142857,0.059730250481695564,0.07203935808832143,0.10958904109589035,0.23684210526315788,0,0.34375,0.2675,0.2638888888888889,0,0,0,1,0,"['The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'Our tagger was first evaluated on data from the German Tiger treebank.', 'In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).', 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tN = t1, ..., tN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.']"
3,D12-1133,C08-1098,0.6015831134564644,0.4424778761061947,True,False,0.06725146198830409,0,0.07225433526011557,0.0660653625395338,0.11415525114155253,0.23684210526315788,0,0.28125,0.235,0.2777777777777778,0,0,0,0,1,"['The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.', 'Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.', 'Note that only the words, but not the POS tags from the test and development data were used, here.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).']"
4,D13-1032,C08-1098,0.8688524590163934,0.008849557522123894,True,False,0.0594541910331384,0.08571428571428572,0.05394990366088632,0.06489399086330092,0.07648401826484015,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.', 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tN = t1, ..., tN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.']"
5,D13-1033,C08-1098,0.5207100591715976,0.084070796460177,True,False,0.017543859649122806,0,0.012524084778420038,0.019796181328335483,0.03824200913242009,0.23684210526315788,0,0.28125,0.2425,0.1388888888888889,0,0,0,1,0,"['Qc 2008.', 'The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.']"
6,E09-1079,C08-1098,0.5707547169811321,0.29646017699115046,False,False,0.02046783625730994,0.05714285714285714,0.029865125240847782,0.046152044043575015,0.037100456621004564,0.23684210526315788,0,0.21875,0.2175,0.1388888888888889,0,0,0,1,0,"['All context attributes other than the base POS are always used in combination with the base POS.', 'Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.']"
7,P10-1020,C08-1098,0.6235294117647059,0.8008849557522124,True,False,0.005847953216374269,0.014285714285714285,0.016377649325626204,0.012767951270938268,0.021118721461187213,0,0,0.125,0.0575,0.125,0,0,0,1,0,"['Relative to the TnT tagger, however, the accuracy is quite similar for test and development data.', 'Morces tagging accuracy was 95.12%, 0.3% better than the n-gram tagger.']"
8,P10-1068,C08-1098,0.23728813559322035,0.9867256637168141,True,False,0.038011695906432746,0.02857142857142857,0.036608863198458574,0.048026238725547614,0.07363013698630136,0.23684210526315788,0,0.28125,0.2425,0.1388888888888889,0,0,0,1,0,"['The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tN = t1, ..., tN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.']"
9,P10-1068,C08-1098,0.4180790960451977,0.3495575221238938,True,False,0.005847953216374269,0.02857142857142857,0.002890173410404624,0.0040998008668150406,0.0028538812785388126,0,0,0.0625,0.0325,0.125,0,0,0,1,0,"['Given a threshold of 6, the node is therefore not pruned.', '4 5 84.11 89.14 8 5.']"
10,P10-1068,C08-1098,0.4576271186440678,0.6371681415929203,True,False,0.025341130604288498,0,0.025048169556840073,0.029167154738198427,0.034817351598173514,0,0,0.0625,0.025,0.125,0,0,0,1,0,"['7 It was planned to include also the Stanford tagger.', 'In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).']"
11,W10-1704,C08-1098,0.25806451612903225,0.5619469026548672,True,False,0.09161793372319688,0.02857142857142857,0.08381502890173409,0.08574440670024602,0.15011415525114152,0.2894736842105263,0.3333333333333333,0.40625,0.41000000000000003,0.33333333333333337,0,0,0,1,0,"['It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.', 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.']"
12,W10-1727,C08-1098,0.2887323943661972,0.5619469026548672,False,False,0.021442495126705652,0,0.05105973025048169,0.03701534496895864,0.05422374429223744,0.23684210526315788,0,0.28125,0.235,0.2638888888888889,0,0,0,0,1,"['It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'Tagsets of this size contain little or no information about number, gender, case and similar morphosyntactic features.']"
13,W11-2135,C08-1098,0.30246913580246915,0.9867256637168141,True,False,0.07602339181286549,0.04285714285714286,0.052023121387283246,0.062434110343211884,0.10787671232876711,0.2894736842105263,0,0.3125,0.27749999999999997,0.19444444444444445,0,0,0,1,0,"['The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.', 'Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.']"
14,W11-2145,C08-1098,0.5976331360946746,0.5619469026548672,True,False,0.0847953216374269,0,0.0838150289017341,0.08223029167154738,0.1386986301369863,0.2894736842105263,0.3333333333333333,0.34375,0.37749999999999995,0.33333333333333337,0,0,0,0,1,"['It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'We also evaluated our tagger on the Czech Academic corpus   which contains 652.131 tokens and about 1200 different POS tags.', 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', 'Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.']"
15,W11-2147,C08-1098,0.1897810218978102,0.5619469026548672,True,False,0.07017543859649122,0.014285714285714285,0.08670520231213869,0.06161415016984887,0.09703196347031964,0.2894736842105263,0.3333333333333333,0.34375,0.37749999999999995,0.33333333333333337,0,0,1,0,0,"['It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'Tagsets of this size contain little or no information about number, gender, case and similar morphosyntactic features.', 'Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.', 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.']"
16,W12-3141,C08-1098,0.5493827160493827,0.9867256637168141,False,False,0.029239766081871343,0.02857142857142857,0.024084778420038533,0.033501229940260044,0.06164383561643835,0,0,0.0625,0.0325,0,0,0,0,1,0,"['The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.']"
17,W12-3144,C08-1098,0.6614583333333334,0.5619469026548672,True,False,0.09161793372319688,0.02857142857142857,0.08381502890173409,0.08574440670024602,0.15011415525114152,0.2894736842105263,0.3333333333333333,0.40625,0.41000000000000003,0.33333333333333337,0,0,0,0,1,"['It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.', 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.']"
18,W12-3402,C08-1098,0.38613861386138615,0.9336283185840708,False,False,0.038011695906432746,0.02857142857142857,0.03082851637764932,0.03186130959353402,0.046232876712328765,0,0,0.0625,0.0325,0,0,0,0,1,0,"['Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.']"
19,W13-2204,C08-1098,0.22151898734177214,0.9867256637168141,False,False,0.029239766081871343,0.02857142857142857,0.024084778420038533,0.033501229940260044,0.06164383561643835,0,0,0.0625,0.0325,0,0,0,0,1,0,"['The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.']"
20,W13-2210,C08-1098,0.26618705035971224,0.008849557522123894,True,False,0.04093567251461988,0.02857142857142857,0.033718689788053945,0.038772402483307954,0.04223744292237443,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.', 'Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.']"
21,W13-2210,C08-1098,0.6115107913669064,0.09734513274336283,True,False,0.04483430799220273,0.014285714285714285,0.049132947976878616,0.04322361485299286,0.08276255707762559,0.23684210526315788,0,0.28125,0.235,0.2638888888888889,0,0,0,1,0,"['Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.', 'Note that only the words, but not the POS tags from the test and development data were used, here.', 'Results for 2 and for 10 preceding POS tags as context are reported for our tagger.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.']"
22,W13-2211,C08-1098,0.7,0.004424778761061947,True,False,0.04678362573099415,0,0.027938342967244695,0.026004451212369682,0.0502283105022831,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,1,0,0,0,0,"['We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', 'The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).']"
23,W13-2228,C08-1098,0.6979166666666666,0.9557522123893806,False,False,0.08576998050682261,0.1,0.04431599229287091,0.06747100855101325,0.10159817351598169,0.2894736842105263,0,0.3125,0.27749999999999997,0.19444444444444445,0,0,0,1,0,"['Czech POS tagging has been extensively studied in the past  .', 'Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging', 'The German tagging results are, to the best of our knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.', 'These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.', 'In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).']"
24,W13-2230,C08-1098,0.19886363636363635,0.25663716814159293,True,False,0.06530214424951267,0.07142857142857142,0.051059730250481696,0.05564015462106124,0.0776255707762557,0.2894736842105263,0.3333333333333333,0.28125,0.36,0.20833333333333334,0,0,0,1,0,"['In a second step, the decision tree may be pruned in order to avoid overfit-ting to the training data.', 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tN = t1, ..., tN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.', 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.']"
25,W13-2230,C08-1098,0.32954545454545453,0.05309734513274336,True,False,0.025341130604288498,0.02857142857142857,0.02215799614643545,0.024364530865643663,0.048515981735159815,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['The German Tiger treebank   is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.']"
26,W13-2230,C08-1098,0.4715909090909091,0.6548672566371682,True,False,0.031189083820662766,0,0.020231213872832367,0.03549256178985592,0.0365296803652968,0,0,0.0625,0.025,0.125,0,0,0,1,0,"['Table 2: Tagging accuracies on development data in percent.', 'Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.']"
28,W13-2302,C08-1098,0.41232227488151657,0.7566371681415929,True,False,0.029239766081871343,0,0.04238921001926782,0.06360548201944476,0.08105022831050229,0,0,0.0625,0.025,0.125,0,0,0,1,1,"['The best results are obtained with a context size of 10.', 'Our tagger was used with a context size of 10.', 'This strategy returned the best results on the development data.', 'Results for 2 and for 10 preceding POS tags as context are reported for our tagger.', 'Figure 3: Accuracy on development data depend ing on context size The best accuracy of our tagger on the development set was 88.9% obtained with a context of 4 preceding POS tags.']"
29,W13-2708,C08-1098,0.502262443438914,0.004424778761061947,False,False,0.05458089668615984,0,0.03468208092485548,0.029987114911561436,0.04737442922374429,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', 'In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).']"
1,A00-2032,J96-3004,0.9038461538461539,0.19238900634249473,False,False,0.029239766081871343,0,0.03757225433526011,0.035141150286986056,0.032534246575342464,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,"['Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih   is the only published instance of such an approach.', 'Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism.']"
3,C00-2095,J96-3004,0.49079754601226994,0.4820295983086681,False,False,0.0029239766081871343,0,0.024084778420038533,0.007496778727890359,0.007990867579908675,0,0,0.0625,0.03,0.013888888888888888,0,0,1,0,0,['Full Chinese personal names are in one respect simple: they are always of the form family+given.']
4,C02-1049,J96-3004,0.27102803738317754,0.24312896405919662,False,False,0.05263157894736842,0.014285714285714285,0.05973025048169557,0.05411737144195852,0.05878995433789954,0.02631578947368421,0,0.1875,0.0625,0.027777777777777776,0,0,0,1,0,"['Others depend upon various lexical heuristics: for example Chen and Liu   attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.', 'This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.', 'An initial step of any text-analysis task is the tokenization of the input into words.']"
5,C02-1049,J96-3004,0.5794392523364486,0.5983086680761099,True,False,0.01949317738791423,0,0.036608863198458574,0.037132482136581935,0.02968036529680365,0,0,0.09375,0.03,0.027777777777777776,0,0,0,1,0,"[""Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chi-nese personal name, retains a foreign flavor because of liM."", ""com ambridge, UK Email: nc201@eng.cam.ac.uk  1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [ lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.""]"
6,C02-1080,J96-3004,0.09009009009009009,0.28964059196617337,False,False,0.03508771929824561,0,0.06358381502890173,0.055757291788684535,0.07534246575342465,0.05263157894736842,0,0.09375,0.0675,0.06944444444444445,0,0,0,1,0,"['Chinese word segmentation can be viewed as a stochastic transduction problem.', 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.']"
7,C02-1143,J96-3004,0.816793893129771,0.6448202959830867,False,False,0.017543859649122806,0,0.01348747591522158,0.022021787513177928,0.021118721461187213,0.02631578947368421,0,0.15625,0.0575,0.125,0,0,0,1,0,"['A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.', 'The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.']"
8,E09-1063,J96-3004,0.4841628959276018,0.19661733615221988,False,False,0.028265107212475632,0.07142857142857142,0.031791907514450865,0.03736675647182851,0.04052511415525114,0.02631578947368421,0,0.15625,0.0625,0.013888888888888888,0,0,1,0,1,"['Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.', 'First, the model assumes independence between the first and second hanzi of a double given name.']"
9,I05-3031,J96-3004,0.05454545454545454,0.18604651162790697,False,False,0.015594541910331383,0,0.02119460500963391,0.018741946819725897,0.0228310502283105,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,"['There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo   and Wu and Tseng  .']"
10,J00-3004,J96-3004,0.10268948655256724,0.7357293868921776,False,False,0.014619883040935672,0.07142857142857142,0.06551059730250482,0.02190465034555464,0.04908675799086758,0.02631578947368421,0,0.15625,0.0575,0.125,0,0,0,0,1,"['However, this result is consistent with the results of experiments discussed in Wu and Fung  .', 'This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.']"
11,J00-3004,J96-3004,0.23471882640586797,0.4799154334038055,True,False,0.00682261208576998,0,0.0163776493256262,0.0187419468197259,0.01141552511415525,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"['4.4 Chinese Personal Names.', 'Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.']"
12,J04-1004,J96-3004,0.12589073634204276,0.0,False,False,0.02046783625730994,0,0.04046242774566474,0.030572800749677873,0.0410958904109589,0.05263157894736842,0,0.09375,0.065,0.06944444444444445,0,0,0,0,1,"['A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'Chinese word segmentation can be viewed as a stochastic transduction problem.']"
13,J04-1004,J96-3004,0.2684085510688836,0.23678646934460887,False,False,0.036062378167641324,0,0.029865125240847782,0.03232985826402717,0.0365296803652968,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,"['The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.', 'Several systems propose statistical methods for handling unknown words  .']"
14,J04-1004,J96-3004,0.501187648456057,0.8710359408033826,False,False,0.021442495126705652,0.04285714285714286,0.07225433526011558,0.046034906875951735,0.0747716894977169,0,0,0.125,0.0625,0.013888888888888888,0,0,1,0,0,"['Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.', 'each word in the lexicon whether or not each string is actually an instance of the word in question.']"
15,J04-1004,J96-3004,0.7624703087885986,0.2748414376321353,True,False,0.08187134502923976,0.2285714285714286,0.2379576107899807,0.09113271641091725,0.13299086757990863,0.02631578947368421,0,0.1875,0.0575,0.1388888888888889,0,0,1,0,0,"['Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.', 'Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.', 'This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.', 'The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.', 'An initial step of any text-analysis task is the tokenization of the input into words.']"
16,J05-4005,J96-3004,0.09513513513513513,0.8414376321353065,False,False,0.0341130604288499,0,0.07610789980732177,0.0677052828862598,0.06050228310502283,0.02631578947368421,0,0.21875,0.095,0.013888888888888888,0,0,0,1,0,"['In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'More formally, we start by representing the dictionary D as a Weighted Finite State Transducer  .', 'Previous Work.', 'The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.']"
17,J05-4005,J96-3004,0.13513513513513514,0.7399577167019028,False,False,0.014619883040935672,0.02857142857142857,0.03468208092485549,0.019327632657842334,0.04223744292237443,0,0,0.0625,0.025,0.125,0,0,0,0,1,"['Under this scheme, n human judges are asked independently to segment a text.', 'For a given ""word"" in the automatic segmentation, if at least k of the human judges agree that this is a word, then that word is considered to be correct.']"
18,J05-4005,J96-3004,0.14162162162162162,0.7399577167019028,False,False,0,0,0.008670520231213872,0.0007028230057397212,0.007990867579908675,0,0,0.0625,0.025,0.125,0,0,0,0,1,"['Under this scheme, n human judges are asked independently to segment a text.']"
19,J05-4005,J96-3004,0.5286486486486487,0.9682875264270613,False,False,0.021442495126705652,0,0.027938342967244702,0.029401429073445007,0.019406392694063926,0,0,0,0.0175,0.013888888888888888,0,0,0,1,0,"['As described in Sproat  , the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.']"
20,J11-1005,J96-3004,0.1105121293800539,0.7441860465116279,False,False,0.014619883040935672,0.02857142857142857,0.02601156069364162,0.018624809652102614,0.03424657534246575,0,0,0.0625,0.025,0.125,0,0,0,0,1,"['For a given ""word"" in the automatic segmentation, if at least k of the human judges agree that this is a word, then that word is considered to be correct.']"
21,J11-3001,J96-3004,0.5451505016722408,0.7357293868921776,False,False,0.005847953216374269,0.014285714285714285,0.011560693641618497,0.007262504392643787,0.015410958904109588,0,0,0.0625,0.025,0.125,0,0,1,0,0,"['However, this result is consistent with the results of experiments discussed in Wu and Fung  .']"
23,J97-4004,J96-3004,0.013846153846153847,0.18604651162790697,True,False,0.030214424951267055,0,0.050096339113680145,0.049666159072273626,0.0547945205479452,0.02631578947368421,0,0.15625,0.0575,0.125,0,0,0,1,0,"['There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo   and Wu and Tseng  .', ""For example, the Wang, Li, and Chang system fails on the sequence 1:f:p:]nian2 nei4 sa3 in (k) since 1F nian2 is a possible, but rare, family name, which also happens to be written the same as the very common word meaning 'year.'""]"
25,J97-4004,J96-3004,0.9415384615384615,0.2917547568710359,False,False,0.05458089668615984,0,0.105009633911368,0.09968372964741716,0.08961187214611871,0.02631578947368421,0,0.21875,0.095,0.013888888888888888,0,0,0,1,0,"['More formally, we start by representing the dictionary D as a Weighted Finite State Transducer  .', 'In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.', 'Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.']"
26,J97-4004,J96-3004,0.9553846153846154,0.9640591966173362,False,False,0.018518518518518517,0,0.02890173410404624,0.04416071219397914,0.03139269406392694,0,0,0.0625,0.05,0.013888888888888888,0,0,0,1,0,"['The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.', 'This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.']"
27,N10-1068,J96-3004,0.02631578947368421,0.9661733615221987,False,False,0.017543859649122806,0,0.022157996146435453,0.016633477802506732,0.010273972602739725,0,0,0,0.0175,0.013888888888888888,0,0,0,1,0,"['The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.']"
28,P03-1035,J96-3004,0.15648854961832062,0.8414376321353065,False,False,0.057504873294346975,0,0.11753371868978803,0.1163172074499239,0.1113013698630137,0.07894736842105263,0,0.25,0.13,0.06944444444444445,0,0,0,1,0,"['In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'More formally, we start by representing the dictionary D as a Weighted Finite State Transducer  .', 'The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.', 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)']"
29,P03-1035,J96-3004,0.46564885496183206,0.48625792811839325,True,False,0.03996101364522417,0.05714285714285714,0.03564547206165703,0.041349420171020265,0.04680365296803653,0,0,0.125,0.055,0.1388888888888889,0,0,0,1,0,"['Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.', 'The breakdown of the different types of words found by ST in the test corpus is given in Table 3.']"
30,P03-1035,J96-3004,0.5877862595419847,0.5940803382663847,False,False,0.004873294346978557,0,0.007707129094412331,0.01534496895865058,0.008561643835616438,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,['Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.']
31,P06-1010,J96-3004,0.2131979695431472,0.4799154334038055,True,False,0.001949317738791423,0,0.008670520231213872,0.0040998008668150406,0.0028538812785388126,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"['4.4 Chinese Personal Names.', '4.5 Transliterations of Foreign Words.']"
32,P06-1126,J96-3004,0.026923076923076925,0.18604651162790697,False,False,0.023391812865497075,0,0.031791907514450865,0.028815743235328567,0.0319634703196347,0.07894736842105263,0,0.125,0.0675,0.05555555555555555,1,0,0,0,0,"['There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo   and Wu and Tseng  .', 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese']"
33,P07-1015,J96-3004,0.551219512195122,0.5940803382663847,True,False,0.00682261208576998,0,0.016377649325626204,0.0187419468197259,0.01141552511415525,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"['Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.', '4.4 Chinese Personal Names.']"
34,P07-1016,J96-3004,0.2641509433962264,0.6765327695560254,True,False,0.03216374269005848,0.02857142857142857,0.056840077071290934,0.05564015462106124,0.0639269406392694,0,0,0.125,0.055,0.1388888888888889,0,0,0,1,0,"['from the subset of the United Informatics corpus not used in the training of the models.', 'This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.']"
35,P12-1110,J96-3004,0.4449152542372881,0.8414376321353065,False,False,0.038011695906432746,0.014285714285714285,0.07899807321772638,0.06301979618132833,0.0821917808219178,0,0,0.0625,0.05,0.013888888888888888,0,0,0,1,0,"['In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.']"
36,P12-1111,J96-3004,0.3654618473895582,0.18816067653276955,False,False,0.03508771929824561,0,0.03275529865125241,0.03935808832142437,0.02968036529680365,0.02631578947368421,0,0.15625,0.065,0,0,0,0,1,0,"['Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexical rule-based approaches, and approaches that combine lexical information with statistical information.', 'We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.']"
37,P97-1041,J96-3004,0.06557377049180328,0.8498942917547568,True,False,0.06237816764132553,0.02857142857142857,0.09441233140655103,0.07461637577603372,0.08789954337899544,0.07894736842105263,0,0.25,0.1325,0.06944444444444445,0,0,0,1,0,"['(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)', 'There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo   and Wu and Tseng  .', 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'Chinese word segmentation can be viewed as a stochastic transduction problem.', ""The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's   discussion of the role of segmentation in information retrieval.""]"
38,P98-1076,J96-3004,0.8421052631578947,0.2917547568710359,False,False,0.0029239766081871343,0,0.010597302504816955,0.004685486704931475,0.0017123287671232876,0,0,0.0625,0.03,0.013888888888888888,0,0,1,0,0,"['More formally, we start by representing the dictionary D as a Weighted Finite State Transducer  .']"
39,P98-1076,J96-3004,0.8713450292397661,0.09936575052854123,True,False,0.008771929824561403,0,0.017341040462427744,0.022607473351294365,0.02054794520547945,0,0,0.0625,0.0325,0.013888888888888888,0,0,0,1,0,"['TIS systems in general need to do more than simply compute the.', 'Figure 5 shows how this model is implemented as part of the dictionary WFST.']"
40,P99-1036,J96-3004,0.02252252252252252,0.9619450317124736,False,False,0.022417153996101363,0.014285714285714285,0.03757225433526011,0.02998711491156144,0.0410958904109589,0,0,0,0.0175,0.013888888888888888,0,0,0,0,1,"['Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.']"
41,P99-1036,J96-3004,0.036036036036036036,0.2832980972515856,True,False,0.043859649122807015,0.08571428571428572,0.07129094412331405,0.041115145835773685,0.039383561643835614,0.02631578947368421,0,0.15625,0.065,0,0,0,1,0,0,"['The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.', 'The major problem for our segmenter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).']"
42,P99-1036,J96-3004,0.04504504504504504,0.3594080338266385,True,False,0.02046783625730994,0.02857142857142857,0.043352601156069356,0.038538128148061374,0.043949771689497714,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"['This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.']"
43,P99-1036,J96-3004,0.8018018018018018,0.6257928118393234,True,False,0.028265107212475632,0.12857142857142856,0.07610789980732173,0.05774862363828039,0.07990867579908673,0.02631578947368421,0,0.21875,0.09,0.125,0,0,0,1,0,"['Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.', 'First of all, most previous articles report performance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.', 'Following Sproat and Shih  , performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.']"
44,W00-0803,J96-3004,0.14427860696517414,0.8329809725158562,True,False,0.007797270955165692,0,0.0279383429672447,0.018976221154972474,0.0273972602739726,0,0,0.09375,0.025,0.1388888888888889,0,0,0,1,0,"['Evaluation of Morphological Analysis.', ""2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..""]"
45,W00-1207,J96-3004,0.06535947712418301,0.2642706131078224,True,False,0.022417153996101363,0,0.05394990366088632,0.033852641443129904,0.043949771689497714,0.02631578947368421,0,0.15625,0.0625,0.013888888888888888,0,0,1,0,0,"['Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not actually tag the words as belonging to one or another class of expression.', 'Chinese word segmentation can be viewed as a stochastic transduction problem.']"
46,W01-0513,J96-3004,0.18018018018018017,0.042283298097251586,True,False,0.029239766081871343,0.014285714285714285,0.14450867052023114,0.056577251962047534,0.0656392694063927,0,0,0.09375,0.025,0.1388888888888889,0,0,1,0,0,"['Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writ-ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.', 'Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.']"
47,W02-1117,J96-3004,0.20634920634920634,0.8076109936575053,True,False,0.016569200779727095,0,0.011560693641618497,0.009488110577486235,0.01141552511415525,0.02631578947368421,0,0.15625,0.0575,0.125,0,0,0,1,0,"[""19 We note that it is not always clear in Wang, Li, and Chang's examples which segmented words."", 'Papers that use this method or minor variants thereof include Liang  , Li et al.']"
48,W02-1808,J96-3004,0.19230769230769232,0.5116279069767442,True,False,0.09064327485380116,0.1,0.07225433526011558,0.12123696849010193,0.07363013698630136,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"[""The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on."", 'G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of:  the probability that a word chosen randomly from a text will be a name-p(rule 1), and  the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and  the probability that the family name is the particular hanzi F1-p(rule 6), and  the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.']"
49,W03-1025,J96-3004,0.0825242718446602,0.6405919661733616,True,False,0.014619883040935672,0,0.033718689788053945,0.02167037601030807,0.04680365296803653,0.02631578947368421,0,0.15625,0.0575,0.125,0,0,0,0,1,"['(See also Wu and Fung [1994].)', 'Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu   have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.']"
50,W03-1025,J96-3004,0.8737864077669902,0.28964059196617337,False,False,0.07309941520467836,0.04285714285714286,0.14547206165703275,0.11221740658310883,0.15753424657534246,0.05263157894736842,0,0.21875,0.14,0.20833333333333334,0,0,0,0,1,"['Chinese word segmentation can be viewed as a stochastic transduction problem.', 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'For a given ""word"" in the automatic segmentation, if at least k of the human judges agree that this is a word, then that word is considered to be correct.', 'In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.']"
51,W03-1025,J96-3004,0.9029126213592233,0.0,False,False,0.05847953216374269,0.014285714285714285,0.1194605009633911,0.0935925969310062,0.1232876712328767,0.05263157894736842,0,0.15625,0.115,0.08333333333333334,0,0,0,1,0,"['A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.', 'Chinese word segmentation can be viewed as a stochastic transduction problem.']"
52,W03-1728,J96-3004,0.03488372093023256,0.8773784355179705,True,False,0.0341130604288499,0.08571428571428572,0.040462427745664734,0.01979618132833548,0.01769406392694064,0.02631578947368421,0,0.15625,0.065,0,0,0,0,1,0,"['The major problem for our segmenter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).', 'More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.']"
53,W05-0709,J96-3004,0.3229571984435798,0.2832980972515856,False,False,0.018518518518518517,0.02857142857142857,0.0394990366088632,0.026707274218109405,0.02511415525114155,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,['The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.']
54,W06-1630,J96-3004,0.6666666666666666,0.3594080338266385,True,False,0.023391812865497075,0.02857142857142857,0.06743737957610789,0.04603490687595173,0.05194063926940639,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"['This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.', 'Full Chinese personal names are in one respect simple: they are always of the form family+given.']"
55,W10-3212,J96-3004,0.11428571428571428,0.226215644820296,False,False,0.031189083820662766,0,0.040462427745664734,0.04650345554644487,0.043949771689497714,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,"['The second concerns the methods used (if any) to ex-tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.', 'Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism.']"
56,W10-3708,J96-3004,0.07373271889400922,0.7441860465116279,False,False,0.014619883040935672,0.02857142857142857,0.02601156069364162,0.018624809652102614,0.03424657534246575,0,0,0.0625,0.025,0.125,0,0,0,0,1,"['For a given ""word"" in the automatic segmentation, if at least k of the human judges agree that this is a word, then that word is considered to be correct.']"
57,W11-0823,J96-3004,0.7532467532467533,0.19661733615221988,False,False,0.017543859649122806,0.04285714285714286,0.02119460500963391,0.017687712311116318,0.02511415525114155,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,['Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.']
58,W12-1011,J96-3004,0.1729957805907173,0.0507399577167019,True,False,0.014619883040935672,0,0.0327552986512524,0.02682441138573269,0.03424657534246575,0.05263157894736842,0,0.0625,0.035,0.06944444444444445,0,0,0,0,1,"[""2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji.."", 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese']"
59,W12-1011,J96-3004,0.8607594936708861,0.0,False,False,0.02046783625730994,0,0.04046242774566474,0.030572800749677873,0.0410958904109589,0.05263157894736842,0,0.09375,0.065,0.06944444444444445,0,0,0,0,1,"['A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', 'Chinese word segmentation can be viewed as a stochastic transduction problem.']"
60,W12-2303,J96-3004,0.060109289617486336,0.2558139534883721,False,False,0.01949317738791423,0,0.009633911368015413,0.010542345086095818,0.01141552511415525,0.02631578947368421,0,0.09375,0.0325,0,0,0,0,1,0,"['Note that Chang, Chen, and Chen  , in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.']"
61,W12-2303,J96-3004,0.8469945355191257,0.8414376321353065,False,False,0.038011695906432746,0.014285714285714285,0.07899807321772638,0.06301979618132833,0.0821917808219178,0,0,0.0625,0.05,0.013888888888888888,0,0,1,0,0,"['In this paper we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.', 'Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.']"
62,W97-0120,J96-3004,0.09027777777777778,0.2832980972515856,False,False,0.029239766081871343,0.02857142857142857,0.055876685934489405,0.045097809534965436,0.03881278538812785,0.02631578947368421,0,0.15625,0.0625,0.013888888888888888,0,0,0,1,0,"['The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.', 'One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix ir, menD.']"
63,W97-0120,J96-3004,0.23958333333333334,0.6490486257928119,False,False,0,0,0,0.0023427433524657376,0.0005707762557077625,0,0,0.0625,0.025,0.125,0,0,0,1,0,"['An anti-greedy algorithm, AG: instead of the longest match, take the.']"
64,W97-0120,J96-3004,0.2534722222222222,0.3594080338266385,True,False,0.08187134502923976,0.17142857142857143,0.08381502890173406,0.08387021201827341,0.08904109589041093,0,0,0.0625,0.03,0.013888888888888888,0,0,0,1,0,"['This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.', 'Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec  ; one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.']"
65,W97-0120,J96-3004,0.2986111111111111,0.18604651162790697,False,False,0.028265107212475632,0,0.051059730250481696,0.0392409511538011,0.0547945205479452,0.02631578947368421,0,0.15625,0.0625,0.013888888888888888,0,0,0,1,0,"['There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo   and Wu and Tseng  .', 'Chinese word segmentation can be viewed as a stochastic transduction problem.']"
66,W97-0120,J96-3004,0.4201388888888889,0.6257928118393234,False,False,0.008771929824561403,0.07142857142857142,0.027938342967244702,0.01675061497013002,0.02454337899543379,0,0,0.0625,0.025,0.125,0,0,0,1,1,"['Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.']"
67,W97-0316,J96-3004,0.062111801242236024,0.26004228329809725,False,False,0.050682261208576995,0.02857142857142857,0.04238921001926782,0.04533208387021201,0.03995433789954338,0.02631578947368421,0,0.15625,0.065,0,0,0,0,1,0,"['Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular sequence of hanzi might be a name, but that it is likely to be a name with some probability.', 'The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.']"
2,C08-1088,P05-1053,0.07653061224489796,0.0273972602739726,True,False,0.023391812865497075,0.02857142857142857,0.08092485549132947,0.054703057280074956,0.08618721461187216,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,0,1,['Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.']
3,C08-1088,P05-1053,0.9183673469387755,0.5068493150684932,True,False,0.004873294346978557,0.014285714285714285,0.015414258188824663,0.009605247745109522,0.014269406392694063,0,0.3333333333333333,0.125,0.0375,0.013888888888888888,0,0,1,0,0,"['5.1 Experimental Setting.', 'Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.']"
5,C10-1018,P05-1053,0.1935483870967742,0.0867579908675799,True,False,0.08089668615984405,0.02857142857142857,0.2379576107899807,0.16832610987466326,0.22431506849315064,0.2894736842105263,0.3333333333333333,0.34375,0.3575,0.20833333333333334,0,0,0,1,0,"['We also demonstrate how semantic information such as WordNet   and Name List can be used in the feature-based framework.', 'This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.', 'Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.', 'Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.']"
6,C10-1018,P05-1053,0.2073732718894009,0.1278538812785388,True,False,0.02631578947368421,0.04285714285714286,0.02986512524084778,0.046737729881691455,0.043949771689497714,0.23684210526315788,0.3333333333333333,0.28125,0.2425,0.1527777777777778,0,0,0,1,0,"['Section 3 and Section 4 describe our approach and various features employed respectively.', ' Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.']"
8,D07-1076,P05-1053,0.07179487179487179,0.8767123287671232,False,False,0.08771929824561403,0.05714285714285714,0.31502890173410414,0.1573152161180743,0.22602739726027393,0.3157894736842105,0.6666666666666666,0.40625,0.41,0.2916666666666667,0,0,0,1,0,"['In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).', 'Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.']"
9,D07-1076,P05-1053,0.07179487179487179,0.8767123287671232,False,False,0.08771929824561403,0.05714285714285714,0.31502890173410414,0.1573152161180743,0.22602739726027393,0.3157894736842105,0.6666666666666666,0.40625,0.41,0.2916666666666667,0,0,1,0,0,"['In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).', 'Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.']"
10,D07-1076,P05-1053,0.8256410256410256,0.1506849315068493,True,False,0.024366471734892786,0.14285714285714288,0.09441233140655103,0.0472062785521846,0.07134703196347031,0.02631578947368421,0,0.0625,0.025,0.06944444444444445,0,0,1,0,0,"['Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.', 'Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering.']"
11,D07-1076,P05-1053,0.8512820512820513,0.1552511415525114,True,False,0.0935672514619883,0.18571428571428572,0.17726396917148363,0.19163640623169742,0.2488584474885844,0.02631578947368421,0.3333333333333333,0.125,0.09,0.125,0,0,0,1,0,"['Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', 'This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.', 'Two features are defined to include this information:  ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype.', 'This category of features concerns about the information inherent only in the full parse tree.', 'While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.']"
12,D07-1076,P05-1053,0.9025641025641026,0.2009132420091324,True,False,0.018518518518518517,0.1142857142857143,0.061657032755298644,0.04322361485299284,0.06506849315068493,0.02631578947368421,0,0.0625,0.025,0.06944444444444445,0,0,0,0,1,"['It also shows that our system outperforms tree kernel-based systems   by over 20 F-measure on extracting 5 ACE relation types.', 'Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering.']"
14,D09-1149,P05-1053,0.27319587628865977,0.273972602739726,True,False,0.07407407407407407,0.04285714285714286,0.2321772639691715,0.1289680215532389,0.18778538812785384,0.07894736842105263,0.6666666666666666,0.1875,0.23249999999999998,0.19444444444444445,0,0,0,1,0,"['For each pair of mentions3, we compute various lexical, syntactic and semantic features.', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.', 'Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.']"
15,D09-1149,P05-1053,0.4175257731958763,0.5342465753424658,True,False,0.07407407407407407,0.08571428571428572,0.1888246628131021,0.1028464331732459,0.1432648401826484,0.23684210526315788,0.3333333333333333,0.34375,0.2475,0.1527777777777778,0,0,1,0,0,"['Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.', 'Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.', 'Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).']"
16,D12-1074,P05-1053,0.4424778761061947,0.4246575342465753,False,False,0.04678362573099415,0.05714285714285714,0.08092485549132947,0.06489399086330092,0.09417808219178078,0.02631578947368421,0,0.0625,0.0575,0.1111111111111111,0,0,0,0,1,"['This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.', 'Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.']"
17,E06-2012,P05-1053,0.5784313725490197,0.1506849315068493,True,False,0.03216374269005848,0.05714285714285714,0.1358381502890173,0.05868572097926671,0.09018264840182649,0.07894736842105263,0.3333333333333333,0.125,0.16749999999999998,0.1388888888888889,0,0,0,1,0,"['Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.']"
18,E12-1020,P05-1053,0.1875,0.091324200913242,False,False,0.01364522417153996,0,0.03468208092485549,0.02799578306196556,0.03881278538812785,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,1,['Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.']
19,E12-1020,P05-1053,0.3784722222222222,0.2374429223744292,False,False,0.01364522417153996,0.08571428571428572,0.028901734104046232,0.02705868572097926,0.028538812785388126,0.02631578947368421,0,0.0625,0.0275,0.06944444444444445,0,0,0,1,0,"['The reason why we choose SVMs for this purpose is that SVMs represent the state-ofthe-art in the machine learning research community, and there are good implementations of the algorithm available.', 'Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering.']"
20,E12-1020,P05-1053,0.7777777777777778,0.5981735159817352,True,False,0.03313840155945419,0.014285714285714285,0.08188824662813103,0.05681152629729412,0.07534246575342465,0.05263157894736842,0.6666666666666666,0.125,0.175,0.08333333333333334,0,0,0,1,0,"['Table 2 also measures the contributions of different features by gradually increasing the feature set.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.']"
21,I08-1004,P05-1053,0.07936507936507936,0.0228310502283105,True,False,0.08576998050682261,0.08571428571428572,0.3526011560693643,0.16469485767834152,0.2420091324200913,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).', 'Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.', 'Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.']"
22,N06-1037,P05-1053,0.045454545454545456,0.1872146118721461,False,False,0.05555555555555555,0.1285714285714286,0.23892100192678228,0.10706337120768424,0.17694063926940634,0.3157894736842105,0.3333333333333333,0.34375,0.37749999999999995,0.2777777777777778,0,0,1,0,0,"['Compared with Kambhatla  , we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.', 'Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.', 'Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.', 'Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.']"
23,N06-1037,P05-1053,0.15656565656565657,0.0,False,False,0.0594541910331384,0.02857142857142857,0.21965317919075142,0.11889422513763621,0.16837899543378992,0.13157894736842105,0.6666666666666666,0.21875,0.235,0.20833333333333334,0,0,0,1,0,"['Exploring Various Knowledge in Relation Extraction', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This suggests that relation detection is critical for relation extraction.', 'Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.', 'Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.']"
24,N06-1037,P05-1053,0.17676767676767677,0.1552511415525114,False,False,0.02046783625730994,0.02857142857142857,0.060693641618497114,0.05001757057514349,0.0702054794520548,0.02631578947368421,0.3333333333333333,0.125,0.0575,0.08333333333333334,0,0,0,0,1,"['Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', ' Chunking features are very useful.']"
25,N06-1037,P05-1053,0.1919191919191919,0.4429223744292237,True,False,0.06432748538011696,0.14285714285714288,0.08766859344894024,0.13283354808480732,0.172945205479452,0,0.3333333333333333,0.0625,0.065,0.05555555555555555,0,0,0,1,0,"['This category of features concerns about the information inherent only in the full parse tree.', ' Chunking features are very useful.', 'While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.', ' To our surprise, incorporating the dependency tree and parse tree features only improve the F-measure by 0.6 and 0.4 respectively.', 'Dependency tree th parse trees.']"
26,N06-1037,P05-1053,0.6818181818181818,0.5981735159817352,True,False,0.016569200779727095,0.014285714285714285,0.015414258188824663,0.028464331732458707,0.03367579908675799,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,0,1,0,['Table 2 also measures the contributions of different features by gradually increasing the feature set.']
27,N06-1037,P05-1053,0.6919191919191919,0.502283105022831,False,False,0.003898635477582846,0.05714285714285714,0.021194605009633907,0.027175822888602544,0.014840182648401826,0.23684210526315788,0,0.28125,0.215,0.1388888888888889,0,0,0,1,0,"['In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.', 'Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.']"
28,N07-1015,P05-1053,0.04938271604938271,0.4246575342465753,False,False,0.0341130604288499,0.04285714285714286,0.06743737957610789,0.06184842450509546,0.0776255707762557,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,"['This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.', 'In this paper, we separate the features of base phrase chunking from those of full parsing.']"
29,N07-1015,P05-1053,0.13580246913580246,0.0182648401826484,False,False,0.04093567251461988,0.05714285714285714,0.1695568400770713,0.09417828276912267,0.1307077625570776,0.13157894736842105,0.6666666666666666,0.21875,0.235,0.20833333333333334,0,0,0,1,0,"['This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.', ' Chunking features are very useful.', 'Exploring Various Knowledge in Relation Extraction', 'This suggests that relation detection is critical for relation extraction.', 'Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.']"
31,N07-1015,P05-1053,0.5020576131687243,0.4063926940639269,False,False,0.012670565302144249,0,0.014450867052023121,0.023661707859903944,0.023401826484018264,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,['Most of the chunking features concern about the head words of the phrases between the two mentions.']
32,N07-1015,P05-1053,0.5349794238683128,0.6529680365296804,False,False,0.017543859649122806,0.05714285714285714,0.017341040462427744,0.03279840693452032,0.045662100456621,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,0,1,0,"['While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.']"
33,N09-3012,P05-1053,0.05555555555555555,0.8767123287671232,False,False,0.08771929824561403,0.04285714285714286,0.2822736030828516,0.13353637109054706,0.2100456621004566,0.2894736842105263,0.6666666666666666,0.34375,0.385,0.22222222222222224,0,0,0,0,1,"['In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.', 'In this paper, we only measure the performance of relation extraction on true mentions with true chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).']"
34,N13-1093,P05-1053,0.6407185628742516,0.6027397260273972,True,False,0.043859649122807015,0.04285714285714286,0.1262042389210019,0.10237788450275273,0.1415525114155251,0.23684210526315788,0.3333333333333333,0.28125,0.2425,0.1527777777777778,0,0,0,1,0,"['Table 2: Contribution of different features over 43 relation subtypes in the test data  Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F-measure.', 'Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.']"
35,N13-1093,P05-1053,0.6706586826347305,0.2009132420091324,True,False,0.021442495126705652,0.05714285714285714,0.07803468208092484,0.05271172543047908,0.08390410958904111,0.02631578947368421,0.3333333333333333,0.125,0.0575,0.08333333333333334,0,0,0,1,0,"['It also shows that our system outperforms tree kernel-based systems   by over 20 F-measure on extracting 5 ACE relation types.', 'The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.']"
36,P06-1016,P05-1053,0.065,0.0,False,False,0.021442495126705652,0.02857142857142857,0.10886319845857416,0.03795244230994495,0.050799086757990865,0.07894736842105263,0.3333333333333333,0.15625,0.0925,0.1388888888888889,0,0,1,0,0,"['Exploring Various Knowledge in Relation Extraction', 'This suggests that relation detection is critical for relation extraction.', 'Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.']"
37,P06-1016,P05-1053,0.09,0.0776255707762557,False,False,0.022417153996101363,0.014285714285714285,0.07418111753371867,0.031158486587794305,0.04965753424657534,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).']"
39,P06-1016,P05-1053,0.225,0.0776255707762557,True,False,0.08966861598440545,0.1142857142857143,0.2996146435452794,0.169848893053766,0.27054794520547926,0.3157894736842105,0.6666666666666666,0.40625,0.41,0.2916666666666667,0,0,0,0,1,"['This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).', 'Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.', 'Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.', 'Zhang   approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.', 'It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.']"
40,P06-1016,P05-1053,0.64,0.5616438356164384,True,False,0.031189083820662766,0.04285714285714286,0.09537572254335258,0.06594822537191049,0.07648401826484018,0,0.3333333333333333,0.0625,0.065,0.05555555555555555,0,0,0,1,0,"['Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.', 'In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent-Of-M2 vs. M2-Parent-Of-M1.', 'This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.', 'Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.', 'The semantic relation is determined between two mentions.']"
43,P06-1017,P05-1053,0.9671361502347418,0.9817351598173516,True,False,0.07894736842105263,0.04285714285714286,0.26493256262042386,0.13646480028112917,0.192351598173516,0.05263157894736842,0.6666666666666666,0.1875,0.18,0.08333333333333334,0,0,0,1,0,"['The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.', 'This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.']"
45,P06-1104,P05-1053,0.17647058823529413,0.0776255707762557,True,False,0.07894736842105263,0.04285714285714286,0.2562620423892099,0.1105774862363828,0.16952054794520555,0.3157894736842105,0.6666666666666666,0.40625,0.4425,0.33333333333333337,0,0,0,1,0,"['This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.', 'For each pair of mentions3, we compute various lexical, syntactic and semantic features.']"
47,P08-2023,P05-1053,0.232,0.7077625570776256,True,False,0.057504873294346975,0.1,0.2447013487475915,0.18062551247510844,0.21518264840182644,0.2894736842105263,0.6666666666666666,0.34375,0.4175,0.2638888888888889,0,0,1,0,1,"['This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.', 'Two features are defined to include this information:  ET1Country: the entity type of M1 when M2 is a country name  CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other-Personal.', ' Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype ROLE.Citizen-Of from ROLE.', 'Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.']"
48,P09-1113,P05-1053,0.17333333333333334,0.0776255707762557,False,False,0.022417153996101363,0.014285714285714285,0.07418111753371867,0.031158486587794305,0.04965753424657534,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"['This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).']"
49,P09-1113,P05-1053,0.2222222222222222,0.1415525114155251,True,False,0.03313840155945419,0.08571428571428572,0.105009633911368,0.05212603959236264,0.08447488584474884,0.02631578947368421,0,0.0625,0.025,0.06944444444444445,0,0,0,1,0,"['Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.', 'Culotta et al   extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.']"
50,P09-1114,P05-1053,0.07744107744107744,0.7442922374429224,True,False,0.07602339181286549,0.05714285714285714,0.18015414258188822,0.11280309242122524,0.15753424657534254,0,0.3333333333333333,0.125,0.0375,0.013888888888888888,0,0,0,1,0,"['This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.', 'It also shows that feature-based methods dramatically outperform kernel methods.', 'The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.', 'This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.']"
51,P09-1114,P05-1053,0.02356902356902357,0.9817351598173516,True,False,0.011695906432748537,0.02857142857142857,0.04142581888246628,0.026355862715239545,0.041666666666666664,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,0,1,1,['The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.']
52,P11-1053,P05-1053,0.027472527472527472,0.1415525114155251,False,False,0.09064327485380116,0.07142857142857142,0.2736030828516377,0.12299402600445125,0.19121004566210048,0.3157894736842105,0.6666666666666666,0.40625,0.41,0.2916666666666667,0,0,0,1,0,"['Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).']"
53,P11-1053,P05-1053,0.21978021978021978,0.4931506849315068,False,False,0.08187134502923976,0.014285714285714285,0.28034682080924855,0.14770996837296474,0.192351598173516,0.05263157894736842,0.6666666666666666,0.1875,0.18,0.08333333333333334,0,0,0,1,0,"['This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.']"
54,P11-1053,P05-1053,0.5302197802197802,0.771689497716895,True,False,0.0009746588693957114,0,0.0009633911368015414,0.000937097340986295,0.0005707762557077625,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,0,1,0,"['1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.', '8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.']"
55,P11-1056,P05-1053,0.040145985401459854,0.1141552511415525,True,False,0.025341130604288498,0.05714285714285714,0.105009633911368,0.05903713248213657,0.0901826484018265,0.23684210526315788,0.3333333333333333,0.28125,0.2425,0.1527777777777778,0,0,0,1,0,"['Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.', 'The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.']"
56,P11-1056,P05-1053,0.2116788321167883,0.9771689497716894,True,False,0.06725146198830409,0.08571428571428572,0.13487475915221578,0.09206981375190347,0.12899543378995437,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,0,1,0,"['Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.', 'In this paper, we only measure the performance of relation extraction on true mentions with true chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.', 'ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype Founder under the type ROLE.', 'Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.']"
57,P11-3012,P05-1053,0.2463768115942029,0.593607305936073,True,False,0.016569200779727095,0.014285714285714285,0.06358381502890172,0.032095583928780605,0.06164383561643835,0,0.3333333333333333,0.0625,0.065,0.05555555555555555,0,0,0,1,0,"['It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.', 'For each pair of mentions3, we compute various lexical, syntactic and semantic features.']"
58,P11-3012,P05-1053,0.34057971014492755,0.45662100456621,False,False,0.04093567251461988,0.02857142857142857,0.10019267822736033,0.07567061028464331,0.08390410958904108,0.05263157894736842,0.3333333333333333,0.0625,0.175,0.1111111111111111,0,0,0,0,1,"['Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.']"
59,P13-1147,P05-1053,0.5859649122807018,0.6027397260273972,True,False,0.0594541910331384,0.04285714285714286,0.10789980732177262,0.11280309242122528,0.15125570776255703,0,0.3333333333333333,0.0625,0.065,0.05555555555555555,0,0,0,1,0,"['Table 2: Contribution of different features over 43 relation subtypes in the test data  Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F-measure.', 'This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.']"
60,W06-1634,P05-1053,0.038135593220338986,0.1689497716894977,False,False,0.011695906432748537,0.20000000000000004,0.038535645472061654,0.02448166803326695,0.02968036529680365,0.02631578947368421,0,0.0625,0.025,0.06944444444444445,0,0,0,1,0,"['Tree kernel-based approaches proposed by Zelenko et al   and Culotta et al   are able to explore the implicit feature space without much feature engineering.', 'Zelenko et al   proposed extracting relations by computing kernel functions between parse trees.']"
61,W06-1634,P05-1053,0.07203389830508475,0.6575342465753424,False,False,0.04191033138401559,0.014285714285714285,0.12042389210019265,0.053063136933348944,0.07648401826484016,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,1,0,0,"['However, full parsing is always prone to long distance errors although the Collins parser used in our system represents the state-of-the-art in full parsing.', 'Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins parser used in our system achieves the state-of-the-art performance.']"
63,W06-1667,P05-1053,0.9157894736842105,0.4246575342465753,False,False,0.1023391812865497,0.15714285714285717,0.1926782273603083,0.18554527351528652,0.25742009132420074,0.02631578947368421,0.3333333333333333,0.125,0.09,0.125,0,0,0,0,1,"['This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.', 'Kambhatla   employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', 'Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.', 'While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.', 'This category of features concerns about the information inherent only in the full parse tree.']"
64,W08-0602,P05-1053,0.16049382716049382,0.1187214611872146,False,False,0.015594541910331383,0.014285714285714285,0.009633911368015413,0.01792198664636289,0.018835616438356163,0.23684210526315788,0,0.21875,0.2425,0.18055555555555555,1,0,0,0,0,"['The rest of this paper is organized as follows.', 'Normally, the above overlap features are too general to be effective alone.']"
65,W08-0602,P05-1053,0.5596707818930041,0.7488584474885844,False,False,0.005847953216374269,0.07142857142857142,0.00674373795761079,0.004919761040178048,0.009703196347031963,0,0.3333333333333333,0.0625,0.0325,0.013888888888888888,0,0,0,1,0,['The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task.']
66,W11-1101,P05-1053,0.8982300884955752,0.1415525114155251,False,False,0.02046783625730994,0.14285714285714288,0.049132947976878616,0.029401429073444993,0.04280821917808219,0.02631578947368421,0,0.0625,0.025,0.06944444444444445,0,0,0,1,0,"['Miller et al   augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.', 'Zelenko et al   proposed extracting relations by computing kernel functions between parse trees.']"
68,W11-1815,P05-1053,0.03460207612456748,0.7488584474885844,False,False,0.008771929824561403,0.18571428571428575,0.020231213872832363,0.012533676935691695,0.016552511415525113,0.02631578947368421,0.3333333333333333,0.125,0.0575,0.08333333333333334,0,1,0,0,0,"['The tree kernels developed in Culotta et al   are yet to be effective on the ACE RDC task.', 'Zelenko et al   proposed extracting relations by computing kernel functions between parse trees.']"
1,P99-1061,C90-2039,0.2436548223350254,0.10900473933649289,False,False,0.0029239766081871343,0,0.010597302504816955,0.001991331849595877,0.0028538812785388126,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,1,0,0,['Copying sharable parts is called redundant copying.']
2,C90-3046,C90-2039,0.4788732394366197,0.0,False,False,0.24853801169590642,0.014285714285714285,0.11078998073217727,0.07531919878177344,0.0702054794520548,0.3684210526315789,0.3333333333333333,0.3125,0.38749999999999996,0.2638888888888889,1,0,0,0,0,"['Strategic Lazy Incremental Copy Graph Unification', 'This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).', 'To achieve this, I, he LING unification method, which uses copy dependency information, was developed.', 'The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.', ""The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.""]"
3,P91-1031,C90-2039,0.1952191235059761,0.6587677725118484,True,False,0.057504873294346975,0.014285714285714285,0.0443159922928709,0.023661707859903944,0.023972602739726026,0.23684210526315788,0,0.25,0.2125,0.1388888888888889,0,0,1,0,0,"['In Section 5, a method which uses this generalized strategy is proposed.', 'Ile proposed an incremental copy graph unification method to avoid over copying and early copying.']"
4,P91-1031,C90-2039,0.24701195219123506,0.6540284360189573,False,False,0.029239766081871343,0.014285714285714285,0.025048169556840076,0.03736675647182851,0.026255707762557076,0,0,0.03125,0.0025,0,0,0,0,1,0,"['This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.', 'By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.']"
5,E93-1008,C90-2039,0.2692307692307692,0.1895734597156398,True,False,0.05555555555555555,0,0.05491329479768785,0.029987114911561436,0.034817351598173514,0.2631578947368421,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"[""This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method."", '5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.', 'Copying sharable parts is called redundant copying.']"
6,C92-2068,C90-2039,0.09012875536480687,0.9620853080568721,False,False,0.0847953216374269,0,0.05973025048169555,0.04673772988169145,0.041666666666666664,0.3157894736842105,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"[""The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method."", ""This paper proposes an FS unification method that allows structure sharing with constant m'der node access time."", 'One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.']"
7,P91-1041,C90-2039,0.06694560669456066,0.06635071090047394,True,False,0.023391812865497075,0,0.038535645472061654,0.020733278669321776,0.018835616438356163,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,1,0,0,"['Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.', 'In this paper, some of the efficiency of the procedure-based system is introduced into an FS unification-based system.']"
8,P91-1041,C90-2039,0.9623430962343096,0.1895734597156398,False,False,0.037037037037037035,0,0.026011560693641612,0.0160477919643903,0.017123287671232876,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"[""This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.""]"
9,C94-2143,C90-2039,0.26046511627906976,0.0947867298578199,False,False,0.05555555555555555,0.014285714285714285,0.0443159922928709,0.024950216703760096,0.0228310502283105,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Ile proposed an incremental copy graph unification method to avoid over copying and early copying.', ""Therefore, Pereira's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification.""]"
10,C94-2143,C90-2039,0.3209302325581395,0.1895734597156398,True,False,0.17446393762183235,0.02857142857142857,0.1001926782273603,0.06067705282886257,0.05251141552511415,0.3421052631578947,0.3333333333333333,0.34375,0.38999999999999996,0.2638888888888889,0,0,0,1,0,"[""This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method."", 'Ile proposed an incremental copy graph unification method to avoid over copying and early copying.', 'The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.', 'Strategic Lazy Incremental Copy Graph Unification', 'Incremental Copy Graph Unification PROCEDURE Unify(node1, node2) node1 = Dereference(nodel).']"
11,P91-1042,C90-2039,0.2642857142857143,0.11374407582938388,False,False,0.12962962962962962,0.014285714285714285,0.1117533718689788,0.05224317675998593,0.06506849315068493,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,1,0,0,"['A better method would nfinimize the copying of sharable varts.', 'Copying sharable parts is called redundant copying.', 'Ile proposed an incremental copy graph unification method to avoid over copying and early copying.', 'This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).', ""Pereira's structure sharing FS unification method can avoid this problem.""]"
12,P91-1042,C90-2039,0.8285714285714286,0.6872037914691943,True,False,0.05360623781676413,0,0.01348747591522158,0.011245168091835537,0.00684931506849315,0.02631578947368421,0,0,0,0,1,0,0,0,0,"['The LING unification procedure uses a revised CopyNode procedure which does not copy structures immediately.', 'Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information.']"
13,W97-1503,C90-2039,0.6601941747572816,0.1895734597156398,True,False,0.08576998050682261,0.02857142857142857,0.05684007707129094,0.04170083167389012,0.0365296803652968,0.23684210526315788,0,0.25,0.2125,0.1388888888888889,0,0,0,1,0,"[""This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method."", ""Figure 6: Incremental copy graph unification procedure The problem with Wroblewski's method is that tile whole result DG is created by using only newly created structures.""]"
1,C10-2052,N09-1025,0.07051282051282051,0.48598130841121495,False,False,0.023391812865497075,0,0.048169556840077066,0.061848424505095465,0.06050228310502283,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,"[' , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.']"
1,D11-1081,N09-1025,0.023346303501945526,0.8504672897196262,True,False,0.049707602339181284,0,0.07803468208092484,0.12627386669790328,0.10388127853881274,0.07894736842105263,0.3333333333333333,0.125,0.22749999999999998,0.18055555555555555,0,0,0,1,0,"['When training over 10,000 features on a modest amount of data, we, like Watanabe et al.', 'These features are somewhat similar to features used by Watanabe et al.', 'The work of Och et al   is perhaps the best-known study of new features and their impact on translation quality.', 'We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system.']"
1,D11-1081,N09-1025,0.03501945525291829,0.8504672897196262,True,False,0.04483430799220273,0,0.04238921001926782,0.10097223849127328,0.09874429223744291,0,0,0,0.06,0.041666666666666664,0,0,0,1,0,"['When training over 10,000 features on a modest amount of data, we, like Watanabe et al.', 'These features are somewhat similar to features used by Watanabe et al.', 'To remedy this problem, Chiang et al.', 'This seems in line with the finding of Watanabe et al.', 'Discount features Both of our systems calculate several features based on observed counts of rules in the training data.']"
1,D11-1081,N09-1025,0.09727626459143969,0.2803738317757009,True,False,0.010721247563352826,0.02857142857142857,0.02023121387283237,0.028112920229588844,0.0273972602739726,0,0,0,0.0475,0,0,0,0,1,0,"['For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position.', 'Table 1: Adding new features with MIRA significantly improves translation accuracy.']"
1,D11-1081,N09-1025,0.6186770428015564,0.7710280373831776,True,False,0.031189083820662766,0.02857142857142857,0.0491329479768786,0.06770528288625982,0.08504566210045664,0,0,0,0.06,0.041666666666666664,0,0,0,1,0,"['In Table 6 are shown feature weights learned for the word-context features.', '6.2 Word context features.', 'Table 3 shows word-insertion feature weights.', 'We now turn to features that make use of source-side context.', '(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)']"
1,D11-1081,N09-1025,0.9299610894941635,0.48130841121495327,False,False,0.028265107212475632,0.014285714285714285,0.033718689788053945,0.056342977626800975,0.05422374429223744,0,0,0,0.06,0.041666666666666664,0,0,0,1,0,"['These features are somewhat similar to features used by Watanabe et al.', 'For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets.']"
1,D11-1125,N09-1025,0.04868913857677903,0.06542056074766354,False,False,0.001949317738791423,0.014285714285714285,0.008670520231213872,0.008785287571746516,0.010273972602739725,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Our results add to a growing body of evidence   that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.']"
1,D11-1125,N09-1025,0.2883895131086142,0.48130841121495327,False,False,0.01364522417153996,0,0.016377649325626204,0.03994377415954082,0.037100456621004564,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,['These features are somewhat similar to features used by Watanabe et al.']
1,D11-1125,N09-1025,0.7752808988764045,0.3925233644859813,False,False,0.021442495126705652,0,0.038535645472061654,0.045683495373081875,0.04965753424657534,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,"['Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.']"
1,D11-1125,N09-1025,0.7865168539325843,0.3925233644859813,True,False,0.02729044834307992,0,0.04527938342967244,0.052594588262855804,0.062214611872146115,0,0,0,0.06,0.041666666666666664,0,0,0,1,0,"['Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.', 'Table 4 shows weights for rule-overlap features.']"
1,D11-1125,N09-1025,0.850187265917603,0.308411214953271,True,False,0.02046783625730994,0,0.018304431599229287,0.034438327281246336,0.021118721461187213,0.23684210526315788,0,0.21875,0.22999999999999998,0.1388888888888889,0,0,0,1,0,"[' , we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.', 'The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  .']"
1,N12-1006,N09-1025,0.36823104693140796,0.5794392523364486,True,False,0.056530214424951264,0.014285714285714285,0.06840077071290944,0.12357971184256766,0.10331050228310498,0,0,0,0.08,0.041666666666666664,0,0,0,1,0,"['We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.', 'Adding the source-side and discount features to Hiero yields a +1.5 B improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B improvement.', 'For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets.', 'Our syntax-based system transforms source Chinese strings into target English syntax trees.', '4.2 Source-side features.']"
1,P12-1001,N09-1025,0.5787401574803149,0.06074766355140187,True,False,0.017543859649122806,0,0.024084778420038533,0.044043575026355856,0.02968036529680365,0.23684210526315788,0,0.21875,0.2375,0.1388888888888889,0,0,0,1,0,"['The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  .', 'Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.']"
1,P13-1110,N09-1025,0.1836734693877551,0.004672897196261682,False,False,0.016569200779727095,0,0.038535645472061654,0.045683495373081875,0.02968036529680365,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,['We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system.']
2,P13-1110,N09-1025,0.5204081632653061,0.8457943925233645,False,False,0.014619883040935672,0.014285714285714285,0.016377649325626204,0.03174417242591074,0.027968036529680364,0.23684210526315788,0,0.21875,0.2375,0.1388888888888889,0,0,0,1,0,"['Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training.', 'The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  .']"
3,P13-1110,N09-1025,0.5867346938775511,0.308411214953271,True,False,0.007797270955165692,0,0.0038535645472061657,0.009839522080356097,0.0028538812785388126,0,0,0,0.02,0,0,0,0,1,0,"[' , we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.']"
4,P13-1110,N09-1025,0.6122448979591837,0.34579439252336447,True,False,0.011695906432748537,0.014285714285714285,0.009633911368015413,0.03408691577837648,0.030821917808219176,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,"['A rule like: IN(at)  zai will have feature rule-root-IN set to 1 and all other rule-root features set to 0.', 'Our rule root features range over the original (non-split) nonterminal set; we have 105 in total.']"
5,P28-N09,N09-1025,0.5,0.32710280373831774,False,False,0.037037037037037035,0,0.0394990366088632,0.06934520323298582,0.06963470319634704,0,0,0,0.052500000000000005,0.041666666666666664,0,0,0,1,0,"['Discount features Both of our systems calculate several features based on observed counts of rules in the training data.', 'For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words.']"
6,P134-N09,N09-1025,0.12162162162162163,0.6495327102803738,True,False,0.00682261208576998,0.014285714285714285,0.008670520231213872,0.008316738901253368,0.014840182648401826,0,0,0,0.0275,0,0,0,0,1,0,['Table 3 shows word-insertion feature weights.']
7,PMERT-N09,N09-1025,0.05263157894736842,0.205607476635514,False,False,0.010721247563352826,0,0.0163776493256262,0.03232985826402717,0.02054794520547945,0,0,0,0.0475,0,0,0,0,1,0,"['All features are linearly combined and their weights are optimized using MERT.', 'It turns out that in translation hypotheses that move X said or X asked away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear.']"
8,PMERT-N09,N09-1025,0.056140350877192984,0.205607476635514,True,False,0.050682261208576995,0.02857142857142857,0.07418111753371869,0.13552770294014294,0.11757990867579907,0,0,0,0.0475,0,0,0,0,1,0,"['All features are linearly combined and their weights are optimized using MERT.', 'The baseline model includes 12 features whose weights are optimized using MERT.', 'Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.', 'First, we have shown that these new features can improve the performance even of top-scoring MT systems.', 'How did the various new features improve the translation quality of our two systems?']"
9,PMTS-N09,N09-1025,0.8482490272373541,0.8177570093457944,True,False,0.015594541910331383,0,0.018304431599229287,0.042755066182499704,0.030251141552511414,0,0,0,0.0475,0,0,0,0,1,0,"['Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.', 'All features are linearly combined and their weights are optimized using MERT.']"
10,PMTS-N09,N09-1025,0.953307392996109,0.205607476635514,False,False,0.010721247563352826,0,0.008670520231213872,0.023310296357034088,0.018835616438356163,0,0,0,0.02,0,0,0,0,1,0,['All features are linearly combined and their weights are optimized using MERT.']
11,PMTS-N09,N09-1025,0.980544747081712,0.5841121495327103,True,False,0.009746588693957114,0.014285714285714285,0.019267822736030827,0.03748389363945179,0.021118721461187213,0,0,0,0.0275,0,0,0,0,1,0,"['We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.', 'Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.']"
12,PSMPT-N09,N09-1025,0.30416666666666664,0.004672897196261682,False,False,0.07017543859649122,0,0.13969171483622353,0.18109406114560156,0.1501141552511415,0.3157894736842105,0.3333333333333333,0.34375,0.4099999999999999,0.3194444444444445,0,0,0,1,0,"['We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system.', ' , but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng   and incorporated as a submodel of a translation system by Chan et al.', 'The work of Och et al   is perhaps the best-known study of new features and their impact on translation quality.', ' ; here, we are incorporating some of its features directly into the translation model.', 'Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.']"
13,PTASL-N09,N09-1025,0.21351351351351353,0.2616822429906542,False,False,0.056530214424951264,0,0.08381502890173409,0.14665573386435515,0.11872146118721458,0.23684210526315788,0,0.21875,0.2575,0.1388888888888889,0,0,0,1,0,"['features String-to-tree MT offers some unique levers to pull, in terms of target-side features.', 'We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.', 'What linguistic features can improve statistical machine translation (MT)?', 'Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.', 'How did the various new features improve the translation quality of our two systems?']"
14,PTASL-N09,N09-1025,0.23243243243243245,0.6495327102803738,True,False,0.010721247563352826,0.02857142857142857,0.025048169556840073,0.027878645894342274,0.02910958904109589,0,0,0,0.0475,0,0,0,0,1,0,"['Table 3 shows word-insertion feature weights.', 'We represent the translation model as a tree transducer  .']"
15,W10-1757,N09-1025,0.20253164556962025,0.48130841121495327,True,False,0.049707602339181284,0,0.06840077071290943,0.11151458357736917,0.09303652968036527,0.2894736842105263,0.3333333333333333,0.28125,0.4125,0.25,0,0,0,1,0,"['These features are somewhat similar to features used by Watanabe et al.', 'When training over 10,000 features on a modest amount of data, we, like Watanabe et al.', 'We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system.', 'In this paper, we address these questions by experimenting with a large number of new features.', 'This seems in line with the finding of Watanabe et al.']"
16,W10-1757,N09-1025,0.8818565400843882,0.7476635514018691,False,False,0.04775828460038986,0.014285714285714285,0.0789980732177264,0.11830853929951977,0.08618721461187212,0.3157894736842105,0.3333333333333333,0.34375,0.40499999999999997,0.2777777777777778,0,0,0,1,0,"['Therefore, the new features work to discourage these hypotheses.', 'By contrast, we incorporate features directly into hierarchical and syntax-based decoders.', 'We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system.', 'Further:  Do syntax-based translation systems have unique and effective levers to pull when designing new features?', 'We have described a variety of features for statistical machine translation and applied them to syntax-based and hierarchical systems.']"
17,W10-1761,N09-1025,0.2548076923076923,0.8271028037383178,False,False,0.05263157894736842,0.02857142857142857,0.06743737957610789,0.12182265432821834,0.09931506849315068,0.2631578947368421,0,0.28125,0.2625,0.20833333333333334,0,0,0,1,0,"['We have described a variety of features for statistical machine translation and applied them to syntax-based and hierarchical systems.', 'By contrast, we incorporate features directly into hierarchical and syntax-based decoders.', 'For the source-side syntax features, we used the Berkeley parser   to parse the Chinese side of both sets.', 'Further:  Do syntax-based translation systems have unique and effective levers to pull when designing new features?', 'We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.']"
18,W10-1761,N09-1025,0.4423076923076923,0.14018691588785046,True,False,0.02631578947368421,0,0.025048169556840073,0.04252079184725313,0.03995433789954338,0,0,0,0.02,0,0,0,0,1,0,"['Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X  X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar  .', 'The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25.']"
19,W10-1761,N09-1025,0.46634615384615385,0.43457943925233644,True,False,0.025341130604288498,0,0.02697495183044316,0.03408691577837647,0.04052511415525114,0,0,0,0.0325,0.041666666666666664,0,0,0,1,0,"['Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik  .', 'Our rule root features range over the original (non-split) nonterminal set; we have 105 in total.']"
1,C04-1061,W04-0213,0.10483870967741936,0.0,True,False,0.0594541910331384,0,0.05298651252408477,0.06793955722150637,0.0867579908675799,0.34210526315789475,0.3333333333333333,0.3125,0.39749999999999996,0.2638888888888889,0,0,0,1,0,"['The Potsdam Commentary Corpus', 'At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.', 'For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.']"
2,C04-1061,W04-0213,0.25806451612903225,0.018633540372670808,False,False,0.042884990253411304,0,0.04238921001926781,0.05048611924563663,0.06335616438356165,0.3421052631578947,0.3333333333333333,0.3125,0.38749999999999996,0.2638888888888889,0,0,0,1,0,"['A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', 'The Potsdam Commentary Corpus']"
3,C04-1061,W04-0213,0.3790322580645161,0.37888198757763975,False,False,0.017543859649122806,0,0.017341040462427744,0.01827339814923275,0.016552511415525113,0,0,0,0.0025,0,0,0,0,1,0,"['annotation guidelines that tell annotators what to do in case of doubt.', 'It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives.']"
4,C08-2009,W04-0213,0.031746031746031744,0.7763975155279503,False,False,0.025341130604288498,0,0.038535645472061654,0.04111514583577368,0.03424657534246575,0,0,0,0.0125,0,0,0,0,1,0,"['That is, we can use the discourse parser on PCC texts, emulating for instance a co-reference oracle that adds the information from our co-reference annotations.', 'The annotator can then click away those words that are here not used as connectives (such as the conjunction und (and) used in lists, or many adverbials that are ambiguous between connective and discourse particle).']"
5,P06-3008,W04-0213,0.15942028985507245,0.018633540372670808,True,False,0.02729044834307992,0,0.02215799614643545,0.030572800749677866,0.04052511415525114,0.2894736842105263,0,0.25,0.245,0.19444444444444445,0,0,0,1,0,"['A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'The Potsdam Commentary Corpus', 'At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.']"
6,P08-2062,W04-0213,0.028037383177570093,0.7763975155279503,False,False,0.015594541910331383,0,0.015414258188824663,0.014407871617664282,0.01598173515981735,0,0,0,0.0125,0,0,0,0,1,0,"['That is, we can use the discourse parser on PCC texts, emulating for instance a co-reference oracle that adds the information from our co-reference annotations.', 'Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information.']"
7,P08-2062,W04-0213,0.14018691588785046,0.8571428571428571,False,False,0.005847953216374269,0,0.01348747591522158,0.017921986646362895,0.012557077625570776,0,0,0,0.01,0,0,0,0,1,0,['One key issue here is to seek a discourse-based model of information structure.']
8,W06-2709,W04-0213,0.8674698795180723,0.006211180124223602,True,False,0.06140350877192982,0.014285714285714285,0.05780346820809249,0.07801335363710914,0.0776255707762557,0.2894736842105263,0.3333333333333333,0.28125,0.355,0.20833333333333334,0,0,0,1,0,"['A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', 'All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory  .', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.', 'The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.']"
9,W07-1525,W04-0213,0.7623318385650224,0.018633540372670808,True,False,0.04580896686159844,0,0.043352601156069356,0.054117371441958516,0.0656392694063927,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.', 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.']"
10,W07-1530,W04-0213,0.24369747899159663,0.037267080745341616,True,False,0.00682261208576998,0,0.004816955684007707,0.008668150404123226,0.010273972602739725,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.']"
11,W07-1530,W04-0213,0.33613445378151263,0.0,True,False,0.02729044834307992,0,0.022157996146435446,0.030572800749677866,0.04052511415525114,0.2894736842105263,0,0.25,0.245,0.19444444444444445,0,0,0,1,0,"['The Potsdam Commentary Corpus', 'At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.']"
12,W11-0401,W04-0213,0.16666666666666666,0.006211180124223602,False,False,0.03898635477582846,0,0.038535645472061654,0.04544922103783529,0.055365296803652965,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,1,1,0,"['A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.']"
13,W12-3205,W04-0213,0.5212765957446809,0.7267080745341615,True,False,0.04873294346978557,0,0.0443159922928709,0.0542345086095818,0.0684931506849315,0.2894736842105263,0.3333333333333333,0.28125,0.3625,0.20833333333333334,0,0,0,1,0,"['For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.']"
14,W13-2708,W04-0213,0.7285067873303167,0.006211180124223602,False,False,0.022417153996101363,0,0.025048169556840073,0.028581468900081987,0.033105022831050226,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.']"
15,W13-3306,W04-0213,0.19135802469135801,0.006211180124223602,False,False,0.05458089668615984,0.02857142857142857,0.057803468208092484,0.07309359259693099,0.07420091324200914,0.2894736842105263,0.3333333333333333,0.28125,0.37,0.22222222222222224,0,0,0,1,0,"['A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', 'Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.']"
1,W11-1104,E03-1020,0.11764705882352941,0.030303030303030304,False,False,0.050682261208576995,0.04285714285714286,0.07514450867052022,0.10378353051423218,0.11529680365296802,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', 'The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  .', ""This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words.""]"
2,S13-2038,E03-1020,0.2222222222222222,0.8080808080808081,True,False,0.08187134502923976,0.02857142857142857,0.1011560693641618,0.11737144195853352,0.09931506849315067,0.2894736842105263,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"['We used the simple graph model based on co-occurrences of nouns in lists (cf.', 'The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.', 'The algorithm is based on a graph model representing words and relationships between them.', 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', 'Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering  .']"
5,P04-1080,E03-1020,0.9521531100478469,0.20202020202020202,True,False,0.1111111111111111,0.09999999999999999,0.1271676300578034,0.13927609230408816,0.14098173515981732,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.', 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', 'The algorithm is based on a graph model representing words and relationships between them.', 'Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering  .', 'The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  .']"
6,D10-1073,E03-1020,0.14344262295081966,0.8080808080808081,True,False,0.05458089668615984,0.02857142857142857,0.06647398843930635,0.0804732341571981,0.0702054794520548,0.2894736842105263,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"['We used the simple graph model based on co-occurrences of nouns in lists (cf.', 'In section 2, we present the graph model from which we discover word senses.', 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.']"
7,C04-1194,E03-1020,0.12209302325581395,0.0,True,False,0.06432748538011696,0.04285714285714286,0.10693641618497107,0.13095935340283474,0.15867579908675797,0.34210526315789475,0.3333333333333333,0.3125,0.39249999999999996,0.2638888888888889,0,0,0,1,0,"['Discovering Corpus-Specific Word Senses', 'Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.', 'The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  .', 'In section 2, we present the graph model from which we discover word senses.', 'This paper presents an unsupervised algorithm which automatically discovers word senses from text.']"
8,C04-1194,E03-1020,0.1569767441860465,0.5050505050505051,True,False,0.04775828460038986,0.04285714285714286,0.07610789980732177,0.09136699074616376,0.11015981735159816,0.23684210526315788,0,0.21875,0.215,0.1388888888888889,0,0,0,1,0,"['Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.', 'The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  .', 'In section 2, we present the graph model from which we discover word senses.']"
9,C04-1194,E03-1020,0.9127906976744186,0.37373737373737376,True,False,0.02631578947368421,0,0.03853564547206165,0.03877240248330795,0.04509132420091324,0.23684210526315788,0,0.21875,0.2125,0.1527777777777778,0,0,0,0,1,"['To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs  .', 'Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering  .']"
10,C04-1194,E03-1020,0.9302325581395349,0.29292929292929293,True,False,0.029239766081871343,0.014285714285714285,0.035645472061657024,0.03092421225254773,0.027968036529680364,0.05263157894736842,0.3333333333333333,0.0625,0.145,0.08333333333333334,0,0,0,1,0,"['However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.', 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.']"
11,C04-1194,E03-1020,0.9593023255813954,0.9191919191919192,True,False,0.03898635477582846,0.05714285714285714,0.0327552986512524,0.045097809534965436,0.03367579908675799,0,0,0,0.0075,0,0,0,0,1,0,"['The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning.', 'Following the method in  , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.']"
12,N07-3010,E03-1020,0.8390804597701149,0.2727272727272727,True,False,0.04678362573099415,0,0.06647398843930635,0.06922806606536255,0.07420091324200914,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['Figure 1: Local graph of the word mouse', 'In section 2, we present the graph model from which we discover word senses.', 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.']"
13,W11-2214,E03-1020,0.045662100456621,0.5050505050505051,False,False,0.029239766081871343,0.04285714285714286,0.05009633911368015,0.06044277849361603,0.08105022831050226,0.23684210526315788,0,0.21875,0.215,0.1388888888888889,0,0,0,1,0,"['Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.', 'The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context  .']"
14,W11-2214,E03-1020,0.365296803652968,0.12121212121212122,True,False,0.038011695906432746,0,0.05491329479768785,0.04697200421693803,0.05365296803652968,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,1,0,0,"['Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering  .', 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.']"
15,W06-3812,E03-1020,0.88,0.030303030303030304,True,False,0.056530214424951264,0,0.07321772639691713,0.09980086681504041,0.0993150684931507,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', ""If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus."", ""This is achieved, in a manner similar to Pantel and Lin's   sense clustering approach, by removing c's features from the set of features used for finding similar words."", 'In section 2, we present the graph model from which we discover word senses.']"
1,W02-0812,N01-1011,0.8733333333333333,0.517948717948718,True,False,0.07504873294346978,0.11428571428571428,0.07707129094412331,0.10483776502284176,0.14497716894977167,0.05263157894736842,0.3333333333333333,0.0625,0.145,0.06944444444444445,0,0,0,1,0,"['Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient.', 'This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.', 'This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.']"
2,W04-0813,N01-1011,0.33076923076923076,0.11794871794871795,False,False,0.010721247563352826,0.014285714285714285,0.005780346820809248,0.010542345086095818,0.012557077625570776,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,['This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.']
3,W02-1011,N01-1011,0.7431693989071039,0.8666666666666667,False,False,0.10136452241715399,0.2142857142857143,0.13872832369942195,0.18683378235914266,0.28481735159817345,0.15789473684210525,0.3333333333333333,0.15625,0.215,0.19444444444444445,0,0,0,0,1,"['Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram  ,  ,  ).', 'This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.', 'There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word.', 'This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.', 'A Decision Tree of Bigrams is an Accurate Predictor of Word Sense']"
4,N03-3004,N01-1011,0.28823529411764703,0.8666666666666667,False,False,0.11500974658869395,0.21428571428571433,0.1608863198458574,0.2144781539182382,0.31792237442922366,0.15789473684210525,0.3333333333333333,0.15625,0.215,0.19444444444444445,0,0,0,0,1,"['Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram  ,  ,  ).', 'A Decision Tree of Bigrams is an Accurate Predictor of Word Sense', 'This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.', 'Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies  ,  ).', 'This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.']"
7,J02-2003,N01-1011,0.4169014084507042,0.20512820512820512,False,False,0.003898635477582846,0.014285714285714285,0.017341040462427744,0.004451212369684901,0.005136986301369863,0,0,0.03125,0,0,0,0,1,0,0,"[""However,   suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.""]"
8,W08-0611,N01-1011,0.553763440860215,0.0,False,False,0.0341130604288499,0.11428571428571428,0.04624277456647398,0.0685252430596228,0.10958904109589042,0.07894736842105263,0,0.03125,0.0475,0.05555555555555555,0,0,0,1,0,"['A Decision Tree of Bigrams is an Accurate Predictor of Word Sense', 'This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.']"
1,A00-2019,W95-0104,0.115,0.15680473372781065,False,False,0.11306042884990253,0.18571428571428575,0.03371868978805394,0.05903713248213657,0.06278538812785388,0.2894736842105263,0.3333333333333333,0.21875,0.23500000000000001,0.19444444444444445,1,0,0,1,0,"['decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Ve try two ways of combining these components: decision lists, and Bayesian classifiers.', 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations.""]"
2,A97-1025,W95-0104,0.13744075829383887,0.04142011834319527,True,False,0.08966861598440545,0.2,0.033718689788053945,0.07332786693217758,0.07134703196347031,0.3421052631578947,0.6666666666666666,0.28125,0.3775,0.2638888888888889,0,0,0,1,0,"['Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', 'Ve try two ways of combining these components: decision lists, and Bayesian classifiers.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations.""]"
3,A97-1025,W95-0104,0.5924170616113744,0.23372781065088757,True,False,0.07309941520467836,0.02857142857142857,0.06840077071290944,0.07321072976455428,0.18664383561643827,0.07894736842105263,0.3333333333333333,0,0.0175,0.06944444444444445,0,0,0,0,1,"['Table 1: Performance of the baseline method for 18 confusion sets.', 'Table 3 shows the results of varying for the usual confusion sets.', 'Table 1 shows the performance of the baseline method for 18 confusion sets.', ""We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7."", 'The ambiguity among words is modelled by confusion sets.']"
4,C04-1131,W95-0104,0.21395348837209302,0.04142011834319527,True,False,0.08869395711500974,0.2142857142857143,0.048169556840077066,0.0822302916715474,0.07248858447488585,0.3421052631578947,0.6666666666666666,0.28125,0.3775,0.2638888888888889,0,1,0,1,0,"['Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations."", 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', 'Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)', 'The previous section confirmed that decision lists are effective at combining two complementary methods-context words and collocations.']"
5,D07-1012,W95-0104,0.2610294117647059,0.7928994082840237,False,False,0.07115009746588694,0.05714285714285714,0.044315992292870914,0.08621295537073914,0.07990867579908675,0.05263157894736842,0.3333333333333333,0,0.025,0.05555555555555555,0,0,0,1,0,"['If both features are context words, we say the features never conflict (as in the method of context words).', 'The idea is to make one big list of all features - in this case, context words and collocations.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations.""]"
6,D11-1119,W95-0104,0.145985401459854,0.9881656804733728,True,False,0.15594541910331383,0.24285714285714288,0.051059730250481696,0.07508492444652687,0.07077625570776257,0.3157894736842105,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"['It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', ""This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction."", ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations."", 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', 'Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.']"
7,E06-1030,W95-0104,0.6919831223628692,0.17159763313609466,True,False,0.08674463937621832,0.07142857142857142,0.10115606936416183,0.09944945531217053,0.2243150684931506,0.07894736842105263,0.3333333333333333,0,0.015000000000000001,0.05555555555555555,0,0,0,0,1,"['The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.', 'Table 1: Performance of the baseline method for 18 confusion sets.', 'Table 1 shows the performance of the baseline method for 18 confusion sets.', '1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set.', 'The ""Most frequent word"" column gives the word in the confusion set that occurred most frequently in the training corpus.']"
8,E99-1024,W95-0104,0.1658291457286432,0.9881656804733728,True,False,0.21539961013645223,0.17142857142857143,0.0626204238921002,0.06337120768419818,0.0684931506849315,0.3684210526315789,0.3333333333333333,0.28125,0.37,0.22222222222222224,1,0,0,1,0,"['It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', 'Table 7: Performance of six methods for context-sensitive spelling correction.', 'Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.', ""This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction."", 'The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.']"
9,H01-1052,W95-0104,0.15625,0.07396449704142012,False,False,0.030214424951267055,0.08571428571428572,0.008670520231213872,0.026355862715239542,0.027968036529680364,0.2631578947368421,0.3333333333333333,0.21875,0.2225,0.19444444444444445,0,0,0,1,0,"['Ve try two ways of combining these components: decision lists, and Bayesian classifiers.', 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.']"
10,J98-1006,W95-0104,0.27,0.257396449704142,False,False,0.014619883040935672,0,0.024084778420038533,0.027292960056225838,0.033105022831050226,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,0,0,0,1,0,"[', cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.']"
11,N03-2035,W95-0104,0.24731182795698925,0.9792899408284024,False,False,0.14912280701754385,0.6142857142857143,0.057803468208092484,0.06337120768419818,0.05593607305936073,0.3157894736842105,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,1,"['Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.', 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.', 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.', ""This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction."", 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.']"
12,N03-2035,W95-0104,0.6021505376344086,0.5739644970414202,True,False,0.08089668615984405,0.11428571428571428,0.029865125240847782,0.05025184491039005,0.0639269406392694,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,0,0,0,1,0,"['3.4 Hybrid method 1: Decision lists.', '3.5 Hybrid method 2: Bayesian classifiers.', 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.']"
13,N03-2035,W95-0104,0.956989247311828,0.7633136094674556,True,False,0.08284600389863547,0.014285714285714285,0.059730250481695564,0.0685252430596228,0.10730593607305935,0.05263157894736842,0.3333333333333333,0,0.025,0.05555555555555555,0,0,0,1,0,"['3.5 Hybrid method 2: Bayesian classifiers.', ""A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.""]"
14,N04-1016,W95-0104,0.43171806167400884,0.9763313609467456,True,False,0.13352826510721247,0.17142857142857143,0.060693641618497114,0.09804380930069112,0.09589041095890409,0.3421052631578947,0.6666666666666666,0.28125,0.3775,0.2638888888888889,0,0,0,1,0,"[""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations."", 'The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take ""features"" in that figure to include both context words and collocations.', 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Ve then apply each of the two component methods mentioned above-context words and collocations.', 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.']"
15,N04-1016,W95-0104,0.44933920704845814,0.16568047337278108,True,False,0.06237816764132553,0.02857142857142857,0.0809248554913295,0.08715005271172543,0.2003424657534246,0.07894736842105263,0.3333333333333333,0,0.015000000000000001,0.05555555555555555,0,0,0,0,1,"['The methods handle multiple confusion sets by applying the same technique to each confusion set independently.', 'We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the ""confusable"" relation is no longer transitive.', 'Table 1: Performance of the baseline method for 18 confusion sets.', 'This collection of confusion sets will be used for evaluating the methods throughout the paper.', 'Table 1 shows the performance of the baseline method for 18 confusion sets.']"
16,N04-1016,W95-0104,0.45374449339207046,0.07988165680473373,False,False,0.05360623781676413,0.02857142857142857,0.01348747591522158,0.012299402600445122,0.01769406392694064,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,1,0,0,0,1,"['The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.']"
17,N04-1016,W95-0104,0.5022026431718062,0.19230769230769232,True,False,0.06530214424951267,0,0.07321772639691715,0.09347545976338292,0.1957762557077625,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,0,0,0,0,1,"['Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.', 'Table 3 shows the results of varying for the usual confusion sets.']"
18,N04-1016,W95-0104,0.5110132158590308,0.06804733727810651,True,False,0.029239766081871343,0.014285714285714285,0.018304431599229284,0.025067353871383386,0.05365296803652968,0.2631578947368421,0.3333333333333333,0.21875,0.2225,0.19444444444444445,0,0,0,1,0,"['We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods.', 'Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.']"
19,N10-1019,W95-0104,0.05583756345177665,0.09467455621301775,True,False,0.16764132553606237,0.1,0.057803468208092484,0.055171605950568095,0.08162100456621005,0.3157894736842105,0,0.21875,0.2275,0.1527777777777778,0,0,0,1,0,"['Ve treat context-sensitive spelling correction as a task of word disambiguation.', 'Table 7: Performance of six methods for context-sensitive spelling correction.', 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', 'The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.']"
20,P01-1005,W95-0104,0.14393939393939395,0.9911242603550295,False,False,0.06530214424951267,0.014285714285714285,0.05298651252408477,0.06079418999648588,0.09417808219178081,0.02631578947368421,0,0,0.0125,0,0,0,0,1,0,"[""A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.""]"
21,P96-1010,W95-0104,0.12562814070351758,0.01775147928994083,False,False,0.22709551656920077,0.17142857142857143,0.07032755298651253,0.06911092889773923,0.07876712328767123,0.18421052631578946,0.3333333333333333,0.09375,0.1925,0.125,0,0,0,1,1,"[""This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction."", 'A Bayesian hybrid method for context-sensitive spelling correction', 'Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.', 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', 'Ve treat context-sensitive spelling correction as a task of word disambiguation.']"
22,P96-1010,W95-0104,0.19095477386934673,0.9704142011834319,True,False,0.14327485380116958,0.02857142857142857,0.10597302504816956,0.12709382687126627,0.23173515981735154,0.02631578947368421,0,0,0.015000000000000001,0.013888888888888888,0,0,0,0,1,"['This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.', 'In such cases, the Bayesian hybrid method is clearly better.', ""On the other hand, when the words in the confusion set have different parts of speech-as in, for example, {there, their, they're}- trigrams are often better than the Bayesian method."", ""A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise."", 'Trigrams are at their worst when the words in the confusion set have the same part of speech.']"
23,P96-1010,W95-0104,0.39195979899497485,0.985207100591716,False,False,0.09161793372319688,0.12857142857142856,0.05202312138728323,0.06536253953379406,0.0747716894977169,0.2894736842105263,0.3333333333333333,0.21875,0.23500000000000001,0.19444444444444445,0,0,0,1,0,"['A method for doing this, based on Bayesian classifiers, was presented.', 'A method is presented for doing this, based on Bayesian classifiers.', 'This section presents a method of doing this based on Bayesian classifiers.', 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Ya.rowsky proposed decision lists as a way to get the best of both methods.']"
24,P96-1010,W95-0104,0.3969849246231156,0.893491124260355,False,False,0.09551656920077972,0.014285714285714285,0.07321772639691715,0.08527585802975285,0.13641552511415522,0.02631578947368421,0,0,0.015000000000000001,0.013888888888888888,0,0,0,1,0,"['The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.', ""A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.""]"
25,P98-2138,W95-0104,0.5277777777777778,0.09467455621301775,True,False,0.21052631578947367,0.12857142857142856,0.06358381502890173,0.06817383155675297,0.09075342465753425,0.3157894736842105,0,0.21875,0.2275,0.1527777777777778,1,0,0,0,0,"['Ve treat context-sensitive spelling correction as a task of word disambiguation.', 'Table 7: Performance of six methods for context-sensitive spelling correction.', 'Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.', 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', 'The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.']"
26,P98-2138,W95-0104,0.7944444444444444,0.07100591715976332,True,False,0.10623781676413255,0.07142857142857142,0.07321772639691715,0.10507203935808833,0.09189497716894973,0.2631578947368421,0.3333333333333333,0.21875,0.2225,0.19444444444444445,0,0,0,1,0,"['Ve then apply each of the two component methods mentioned above-context words and collocations.', 'The previous section confirmed that decision lists are effective at combining two complementary methods-context words and collocations.', 'There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps).', 'The idea is to make one big list of all features - in this case, context words and collocations.', 'This method is essentially the same as the one for collocations (see Figure 2), except that it uses context words as well as collocations for the features.']"
27,W00-0701,W95-0104,0.16083916083916083,0.09467455621301775,False,False,0.036062378167641324,0.014285714285714285,0.019267822736030827,0.017687712311116318,0.025684931506849314,0.05263157894736842,0,0,0.0025,0,1,0,0,1,0,['Ve treat context-sensitive spelling correction as a task of word disambiguation.']
28,W00-0701,W95-0104,0.7902097902097902,0.7692307692307693,False,False,0.03996101364522417,0.5000000000000001,0.03468208092485549,0.028230057397212138,0.032534246575342464,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,0,1,0,0,0,"['In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.', 'Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined.']"
29,W01-0502,W95-0104,0.04065040650406504,0.04142011834319527,True,False,0.06627680311890838,0.14285714285714285,0.025048169556840073,0.05318027410097223,0.05194063926940639,0.3157894736842105,0.3333333333333333,0.28125,0.365,0.20833333333333334,0,0,0,1,0,"['Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations."", 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', 'Ve try two ways of combining these components: decision lists, and Bayesian classifiers.']"
30,W02-1005,W95-0104,0.03896103896103896,0.2455621301775148,True,False,0.03898635477582846,0.02857142857142857,0.018304431599229287,0.026004451212369682,0.03424657534246575,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,1,0,0,1,0,"['Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.', 'Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.']"
32,W02-1005,W95-0104,0.7792207792207793,0.24260355029585798,True,False,0.03898635477582846,0.04285714285714286,0.0394990366088632,0.03221272109640389,0.057648401826484015,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,1,0,0,0,1,"['The ""Baseline"" column gives the prediction accuracy of the baseline system on the test corpus.', 'The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.']"
33,W04-3238,W95-0104,0.08843537414965986,0.08579881656804733,False,False,0.21539961013645223,0.12857142857142856,0.06358381502890173,0.06512826519854747,0.06906392694063926,0.3947368421052631,0.3333333333333333,0.3125,0.39,0.2638888888888889,1,0,0,0,0,"['Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.', ""This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction."", 'The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.', 'A Bayesian hybrid method for context-sensitive spelling correction', 'Ve treat context-sensitive spelling correction as a task of word disambiguation.']"
34,W06-1624,W95-0104,0.2968197879858657,0.6390532544378699,True,False,0.06530214424951267,0.12857142857142856,0.03082851637764932,0.056108703291554395,0.04737442922374429,0.2894736842105263,0.3333333333333333,0.21875,0.23500000000000001,0.19444444444444445,0,0,0,1,0,"['Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)', 'Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', 'Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations.""]"
35,W06-3604,W95-0104,0.9419354838709677,0.05621301775147929,False,False,0.07115009746588694,0.05714285714285714,0.018304431599229287,0.02869860606770528,0.02511415525114155,0.2631578947368421,0,0.21875,0.2225,0.1388888888888889,1,0,0,0,0,"['The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations.""]"
37,W12-0304,W95-0104,0.15,0.15680473372781065,True,False,0.08966861598440545,0.2,0.033718689788053945,0.07332786693217758,0.07134703196347032,0.3421052631578947,0.6666666666666666,0.28125,0.3775,0.2638888888888889,1,0,0,1,0,"['decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Ve try two ways of combining these components: decision lists, and Bayesian classifiers.', 'Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods-context words and collocations."", 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.']"
38,W96-0108,W95-0104,0.15555555555555556,0.0,True,False,0.17251461988304093,0.014285714285714285,0.09344894026974952,0.11022607473351295,0.17694063926940634,0.10526315789473684,0.3333333333333333,0.03125,0.0625,0.125,1,0,0,1,0,"['A Bayesian hybrid method for context-sensitive spelling correction', ""A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise."", 'The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.', 'In such cases, the Bayesian hybrid method is clearly better.', '3.5 Hybrid method 2: Bayesian classifiers.']"
39,W98-1234,W95-0104,0.18181818181818182,0.591715976331361,True,False,0.07992202729044834,0.04285714285714286,0.03371868978805395,0.0516574909218695,0.06621004566210045,0.02631578947368421,0.3333333333333333,0,0.0125,0.05555555555555555,1,0,0,0,0,"['The first feature that 46 Training phase (1) (2) (3) (3.5) (4) Propose all possible features as candidat e features.', '3.2 Component method 1: Context words.', 'of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2.', '3.4 Hybrid method 1: Decision lists.', '3.5 Hybrid method 2: Bayesian classifiers.']"
1,N09-CSL2013,N09-1001,0.6952380952380952,0.16371681415929204,False,False,0.05165692007797271,0,0.06936416184971098,0.08082464566006795,0.12442922374429222,0.2631578947368421,0,0.28125,0.235,0.20833333333333334,0,0,0,0,1,"['In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.', 'Information on subjectivity of senses can also improve other tasks such as word sense disambiguation  .', 'Both Wiebe and Mihalcea   and our prior work   present an annotation scheme for word sense subjectivity and algorithms for automatic classification.']"
2,N09-QWN,N09-1001,0.12571428571428572,0.008849557522123894,True,False,0.08966861598440545,0.04285714285714286,0.08188824662813099,0.11046034906875955,0.12499999999999997,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.', 'Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.', 'In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.', 'In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.']"
3,N09-QWN,N09-1001,0.12571428571428572,0.008849557522123894,True,False,0.08966861598440545,0.04285714285714286,0.08188824662813099,0.11046034906875955,0.12499999999999997,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.', 'Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.', 'In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.', 'In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.']"
4,N09PROD,N09-1001,0.2046783625730994,0.6017699115044248,True,False,0.038011695906432746,0.02857142857142857,0.03564547206165703,0.04017804849478739,0.0502283105022831,0.2631578947368421,0,0.21875,0.22,0.1388888888888889,0,0,0,1,0,"['In Section 4.5, we shortly discuss results on.', 'Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.', 'classification vertices in the Mincut approach.', 'The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut.']"
5,N09PROD,N09-1001,0.7076023391812866,0.8982300884955752,True,False,0.03313840155945419,0,0.025048169556840076,0.024833079536136816,0.019406392694063926,0.07894736842105263,0.3333333333333333,0.0625,0.15,0.06944444444444445,1,0,0,0,0,"['In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1).', 'We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.']"
6,N15-1071,N09-1001,0.930379746835443,0.05752212389380531,False,False,0.042884990253411304,0.04285714285714286,0.06358381502890172,0.07555347311702001,0.09474885844748857,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Second, different word senses of a single word can actually be of different subjectivity or polarity.', 'Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.']"
7,P101167,N09-1001,0.29435483870967744,0.9734513274336283,True,False,0.07797270955165692,0.02857142857142857,0.10789980732177262,0.1162000702823006,0.11187214611872143,0.3157894736842105,0.3333333333333333,0.28125,0.36,0.20833333333333334,0,0,0,1,0,"['We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.', 'We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.', 'We supplement WordNet entries with information on the subjectivity of its word senses.', 'Section 3 describes our proposed semi-supervised minimum cut framework in detail.', 'Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.']"
8,P1018,N09-1001,0.05102040816326531,0.16371681415929204,False,False,0.028265107212475632,0,0.054913294797687855,0.06266838467845848,0.10102739726027396,0.02631578947368421,0,0.0625,0.025,0.06944444444444445,0,0,0,1,0,"['In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.', 'Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification.']"
9,P1018,N09-1001,0.9183673469387755,0.4646017699115044,True,False,0.04483430799220273,0.04285714285714286,0.054913294797687855,0.05189176525711607,0.04737442922374429,0.07894736842105263,0.3333333333333333,0.0625,0.1525,0.06944444444444445,0,0,0,1,1,"['Not all WordNet relations we use are subjectivitypreserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.', 'Option1: Use a subset of the ten relations to generate the unlabeled data (and edges between example vertices).', 'We use Lesk as it is one of the few measures applicable across all parts-of-speech.', 'We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.', 'We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL).']"
10,PEAAI-N09,N09-1001,0.25,0.7477876106194691,True,False,0.011695906432748537,0,0.00674373795761079,0.016164929132013586,0.006278538812785388,0.02631578947368421,0,0,0.0075,0,0,0,0,1,0,"['4.4 Semi-supervised Graph Mincuts.', '3.3 Formulation of Semi-supervised Mincuts.', '3.2 Why might Semi-supervised Minimum.', 'As can be seen, the semi-supervised Mincuts is consistently better than SVM.', 'Second, Mincuts can be easily expanded into a semi-supervised framework  .']"
11,PPROC2014-N09,N09-1001,0.07349665924276169,0.6991150442477876,False,False,0.01949317738791423,0,0.033718689788053945,0.04755769005505447,0.050799086757990865,0.02631578947368421,0,0.0625,0.0275,0.06944444444444445,0,0,0,1,0,"['This dataset was first used with a different annotation scheme in Esuli and Sebastiani   and we also used it in Su and Markert  .', 'Esuli and Sebastiani   determine the polarity (positive/negative/objective) of word senses in WordNet.']"
12,PPROC2014-N09,N09-1001,0.155902004454343,0.9734513274336283,False,False,0.06335282651072124,0.02857142857142857,0.105009633911368,0.12182265432821836,0.1512557077625571,0.3157894736842105,0.3333333333333333,0.28125,0.36,0.20833333333333334,0,0,0,1,0,"['We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.', 'We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.', 'We supplement WordNet entries with information on the subjectivity of its word senses.', 'Second, different word senses of a single word can actually be of different subjectivity or polarity.', 'Information on subjectivity of senses can also improve other tasks such as word sense disambiguation  .']"
13,W11-0311,N09-1001,0.9166666666666666,0.17699115044247787,True,False,0.08382066276803118,0.05714285714285714,0.14450867052023117,0.16223497715825244,0.17808219178082188,0.2631578947368421,0,0.28125,0.235,0.20833333333333334,0,0,0,1,0,"['Esuli and Sebastiani   determine the polarity (positive/negative/objective) of word senses in WordNet.', 'Second, different word senses of a single word can actually be of different subjectivity or polarity.', 'Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.', 'However, in Su and Markert (2008a) as well as Wiebe and Mihalcea   we find that human can assign the binary distinction to word senses with a high level of reliability.', '  use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign.']"
1,W93-0111,H89-2014,0.839622641509434,0.34210526315789475,False,False,0.057504873294346975,0.05714285714285714,0.041425818882466284,0.04826051306079418,0.034817351598173514,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi  .', 'An alternative approach taken by Jelinek,   is to view the training problem in terms of a ""hidden"" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.']"
2,A92-1018,H89-2014,0.1910569105691057,0.506578947368421,True,False,0.05458089668615984,0,0.08477842003853561,0.14501581351762924,0.0839041095890411,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.', 'It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.', 'State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.', 'Extending the Basic Model The basic model was used as a benchmark for successive improvements.', 'To model such dependency across the phrase, the networks shown in Figure 2 can be used.']"
3,A92-1018,H89-2014,0.43902439024390244,0.5394736842105263,True,False,0.004873294346978557,0,0.008670520231213872,0.012533676935691697,0.007990867579908675,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,['For an n category model this requires n 3 transition probabilities.']
4,J93-2006,H89-2014,0.08849557522123894,0.006578947368421052,True,False,0.19298245614035087,0.08571428571428572,0.12524084778420033,0.10999180039826642,0.10958904109589038,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,1,0,0,0,0,"['The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.', 'An alternative approach taken by Jelinek,   is to view the training problem in terms of a ""hidden"" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.', 'The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed ""category"").', 'A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.', 'In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text.']"
5,H91-1046,H89-2014,0.1590909090909091,0.9736842105263158,True,False,0.0594541910331384,0,0.06840077071290943,0.10600913669907468,0.06278538812785388,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.', 'Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.', 'Mixed higher-order context can be modeled by introducing explicit state sequences.', 'To model such dependency across the phrase, the networks shown in Figure 2 can be used.', 'In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text.']"
6,H91-1046,H89-2014,0.45454545454545453,0.19078947368421054,True,False,0.038011695906432746,0,0.0674373795761079,0.08761860138221857,0.09132420091324203,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['In this regard, word equivalence classes were used  .', 'A word sequence is considered as being generated from an underlying sequence of categories.', 'Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.', 'In the 21 category model reported in Kupiec   only 129 equivalence classes were required to cover a 30,000 word dictionary.']"
7,C00-1081,H89-2014,0.4427083333333333,0.4605263157894737,True,False,0.06530214424951267,0,0.057803468208092484,0.06875951739486938,0.0867579908675799,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.', 'In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.', 'A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.']"
8,H92-1022,H89-2014,0.0972972972972973,0.7368421052631579,True,False,0.049707602339181284,0,0.07418111753371867,0.0677052828862598,0.05136986301369863,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,1,0,0,"['The basic model tagged these sentences correctly, except for- ""range"" and ""rises"" which were tagged as noun and plural-noun respectively 1.', 'In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text.', 'For example, 9 errors are from 3 instances of ""... as well as ..."" that arise in the text.', 'For an n category model this requires n 3 transition probabilities.', 'The model has the advantage that a pre-tagged training corpus is not required.']"
9,H92-1022,H89-2014,0.04864864864864865,0.8486842105263158,True,False,0.02729044834307992,0.014285714285714285,0.054913294797687855,0.05564015462106126,0.06050228310502283,0.2894736842105263,0,0.25,0.245,0.19444444444444445,0,0,0,1,0,"['The former represents 95.6% correct word tagging on the text as a whole (ignoring unknown words), and 89% on the ambiguous words.', 'Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging']"
10,C92-1060,H89-2014,0.9051094890510949,0.019736842105263157,True,False,0.06237816764132553,0,0.08959537572254335,0.10882042872203354,0.11586757990867577,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.', 'In the 21 category model reported in Kupiec   only 129 equivalence classes were required to cover a 30,000 word dictionary.', 'There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.', 'In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model.', 'In this regard, word equivalence classes were used  .']"
2,W01-0712,P98-1081,0.5990783410138248,0.3231707317073171,True,False,0.015594541910331383,0,0.01348747591522158,0.0133536371090547,0.0136986301369863,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory).']"
3,W01-0712,P98-1081,0.6175115207373272,0.23170731707317074,True,False,0.03508771929824561,0,0.04335260115606936,0.04451212369684901,0.05251141552511415,0.23684210526315788,0,0.25,0.2225,0.18055555555555555,0,0,0,1,0,"['Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model.', 'The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3.']"
5,W02-1004,P98-1081,0.5865921787709497,0.4573170731707317,True,False,0.028265107212475632,0.014285714285714285,0.01926782273603082,0.03830385381281479,0.05593607305936073,0,0,0.03125,0.0175,0.05555555555555555,0,0,0,0,1,"['The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.', 'When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.']"
7,E99-1025,P98-1081,0.5679611650485437,0.20121951219512196,True,False,0.022417153996101363,0,0.004816955684007707,0.008668150404123227,0.018835616438356163,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.']"
9,P06-2060,P98-1081,0.2777777777777778,0.20121951219512196,True,False,0.05360623781676413,0,0.02215799614643545,0.02998711491156144,0.0410958904109589,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,1,0,0,0,0,"['Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.', 'Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text.']"
10,A00-1024,P98-1081,0.17427385892116182,0.20121951219512196,True,False,0.022417153996101363,0,0.004816955684007707,0.008668150404123227,0.018835616438356163,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"['Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.']"
11,W05-1518,P98-1081,0.05263157894736842,0.4878048780487805,False,False,0.01949317738791423,0.014285714285714285,0.015414258188824663,0.021553238842684785,0.025684931506849314,0,0,0.03125,0.0125,0.041666666666666664,0,0,0,1,0,"['In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.']"
12,W05-1518,P98-1081,0.4263157894736842,0.5975609756097561,True,False,0.01949317738791423,0,0.009633911368015413,0.011479442427082112,0.02511415525114155,0,0,0.03125,0.0125,0.041666666666666664,0,0,0,0,1,"['Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.', '5 The most straightforward selection method is an n-way vote.']"
13,W05-1518,P98-1081,0.47368421052631576,0.8414634146341463,True,False,0.014619883040935672,0.02857142857142857,0.00674373795761079,0.016047791964390302,0.03139269406392694,0,0,0.03125,0.0125,0.041666666666666664,0,0,0,1,0,"['1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.']"
14,W05-1518,P98-1081,0.5842105263157895,0.7012195121951219,False,False,0.00682261208576998,0.014285714285714285,0.026011560693641616,0.021084690172191636,0.02682648401826484,0,0,0.03125,0.0125,0.041666666666666664,0,0,0,1,0,"['Pairwise Voting So far, we have only used information on the performance of individual taggers.']"
15,W00-0733,P98-1081,0.33766233766233766,0.054878048780487805,False,False,0.02729044834307992,0,0.03179190751445086,0.03982663699191753,0.048515981735159815,0.23684210526315788,0,0.25,0.2225,0.18055555555555555,0,0,0,1,0,"['Data driven methods appear to be the more popular.', 'In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).']"
16,W00-0733,P98-1081,0.23376623376623376,0.24390243902439024,False,False,0.021442495126705652,0.014285714285714285,0.033718689788053945,0.03572683612510249,0.04337899543378995,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['The Viterbi algorithm is used to determine the most probable tag sequence.', 'In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.']"
17,W00-0733,P98-1081,0.4155844155844156,0.45121951219512196,False,False,0.0029239766081871343,0,0.009633911368015413,0.011479442427082112,0.010273972602739725,0,0,0,0.005,0.013888888888888888,0,0,0,1,0,['This part is used to train the individual tag-gers.']
18,J01-2002,P98-1081,0.06420545746388442,0.03048780487804878,True,False,0.03313840155945419,0.09999999999999999,0.0722543352601156,0.04298934051774627,0.09075342465753419,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,0,1,"['All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.', ""Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words.""]"
19,J01-2002,P98-1081,0.29373996789727125,0.8963414634146342,True,False,0.012670565302144249,0.04285714285714286,0.07321772639691715,0.036546796298465495,0.04680365296803653,0.05263157894736842,0.3333333333333333,0.09375,0.155,0.1111111111111111,0,0,1,0,0,"['The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.', 'All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.']"
20,J01-2002,P98-1081,0.9149277688603531,0.4878048780487805,True,False,0.031189083820662766,0.02857142857142857,0.03179190751445086,0.04673772988169146,0.05821917808219178,0,0,0.03125,0.0175,0.05555555555555555,0,0,0,1,0,"['In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.', 'The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.']"
21,J01-2002,P98-1081,0.1637239165329053,0.7134146341463414,True,False,0.042884990253411304,0.04285714285714286,0.052023121387283225,0.048729061731287326,0.07762557077625569,0.05263157894736842,0.3333333333333333,0.09375,0.155,0.1111111111111111,0,0,0,1,0,"['We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.', 'When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.', 'In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.', 'All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.']"
22,J01-2002,P98-1081,0.7817014446227929,0.7621951219512195,True,False,0.02729044834307992,0,0.015414258188824663,0.018507672484479324,0.03538812785388128,0.23684210526315788,0,0.25,0.2225,0.18055555555555555,0,0,0,0,1,"['Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.', 'Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.']"
23,J01-2002,P98-1081,0.2825040128410915,0.38414634146341464,False,False,0.01949317738791423,0.014285714285714285,0.017341040462427744,0.01721916364062317,0.023972602739726026,0.23684210526315788,0,0.21875,0.215,0.1527777777777778,0,0,0,1,0,"['The data we use for our experiment consists of the tagged LOB corpus  .', 'First of all, tagging is a widely researched and well-understood task (cf.']"
24,J01-2002,P98-1081,0.4959871589085072,0.20121951219512196,True,False,0.022417153996101363,0,0.004816955684007707,0.008668150404123227,0.018835616438356163,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"['Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.']"
25,J01-2002,P98-1081,0.6388443017656501,0.8414634146341463,True,False,0.014619883040935672,0.02857142857142857,0.00674373795761079,0.016047791964390302,0.03139269406392694,0,0,0.03125,0.0125,0.041666666666666664,0,0,0,0,1,"['1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.']"
1,E95-1024,C94-2154,0.738562091503268,0.6186440677966102,False,False,0.005847953216374269,0.02857142857142857,0.008670520231213872,0.007262504392643786,0.006278538812785388,0,0,0,0.0075,0,0,0,0,1,0,"['The Troll system, which is based on this idea, effectively inqflements type resolution.']"
2,C94-2204,C94-2154,0.9596774193548387,0.5677966101694916,True,False,0.05360623781676413,0.15714285714285714,0.04142581888246626,0.06770528288625978,0.04452054794520548,0,0,0,0.0075,0,0,0,0,1,0,"['By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.', 'Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.']"
3,W97-1506,C94-2154,0.04242424242424243,0.00847457627118644,False,False,0.009746588693957114,0.3857142857142858,0.008670520231213872,0.00562258404591777,0.008561643835616438,0.05263157894736842,0.3333333333333333,0.0625,0.15,0.06944444444444445,0,1,0,0,0,"[""in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch."", 'type inferencing fails?']"
4,C96-1076,C94-2154,0.37423312883435583,0.5677966101694916,False,False,0.04093567251461988,0.1285714285714286,0.04046242774566473,0.05692866346491739,0.03824200913242009,0.05263157894736842,0,0.03125,0.0425,0.05555555555555555,0,0,0,1,0,"['By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.', 'THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES']"
1,N13-1140,E09-2008,0.8797814207650273,0.015873015873015872,False,False,0.008771929824561403,0,0.011560693641618497,0.023661707859903944,0.010844748858447488,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.']"
2,W11-2605,E09-2008,0.37244897959183676,0.047619047619047616,False,False,0.014619883040935672,0,0.023121387283236993,0.04427784936160241,0.02054794520547945,0.34210526315789475,0.3333333333333333,0.3125,0.38749999999999996,0.2638888888888889,0,0,0,1,0,"['Foma is largely compatible with the Xerox/PARC finite-state toolkit.', 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', 'Foma: a finite-state compiler and library', 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.', 'The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Ts fsm   and Lextools  , the Xerox/PARC finite-state toolkit   and the SFST toolkit  .']"
3,W12-1003,E09-2008,0.5058823529411764,0.07936507936507936,False,False,0.008771929824561403,0,0.011560693641618497,0.02366170785990394,0.010844748858447488,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.', 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.']"
4,W12-6202,E09-2008,0.6326530612244898,0.07936507936507936,False,False,0.008771929824561403,0,0.011560693641618497,0.02366170785990394,0.010844748858447488,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.', 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.']"
5,W12-6211,E09-2008,0.4260869565217391,0.015873015873015872,True,False,0.021442495126705652,0,0.02986512524084778,0.04673772988169143,0.0319634703196347,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', 'The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Ts fsm   and Lextools  , the Xerox/PARC finite-state toolkit   and the SFST toolkit  .', 'Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.', 'Foma is largely compatible with the Xerox/PARC finite-state toolkit.', 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.']"
6,W12-6212,E09-2008,0.2682926829268293,0.047619047619047616,False,False,0.01949317738791423,0.02857142857142857,0.02890173410404624,0.047089141384561294,0.0228310502283105,0.34210526315789475,0.3333333333333333,0.3125,0.38749999999999996,0.2638888888888889,0,0,0,1,0,"['Foma is largely compatible with the Xerox/PARC finite-state toolkit.', 'Foma: a finite-state compiler and library', 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.', 'The Foma API gives access to basic functions, such as constructing a finite-state machine from a regular expression provided as a string, performing a transduction, and exhaustively matching against a given string starting from every position.']"
7,W12-6212,E09-2008,0.975609756097561,0.047619047619047616,False,False,0.0009746588693957114,0,0.002890173410404624,0.0049197610401780474,0.00228310502283105,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['Foma is largely compatible with the Xerox/PARC finite-state toolkit.', 'One of Fomas design goals has been compatibility with the Xerox/PARC toolkit.']"
8,W12-6213,E09-2008,0.875,0.07936507936507936,False,False,0.00682261208576998,0,0.00674373795761079,0.015462106126273864,0.006278538812785388,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.']"
1,D09-1086,D09-1023,0.2653061224489796,0.3333333333333333,False,False,0.01364522417153996,0,0.018304431599229284,0.02659013705048611,0.03424657534246575,0,0,0,0.005,0,0,0,0,1,0,"['Smith and Eisner   grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment.', 'Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al.']"
3,D11-1044,D09-1023,0.003436426116838488,0.3068181818181818,False,False,0.01949317738791423,0,0.03757225433526011,0.04943188473702705,0.04452054794520548,0.10526315789473684,0.3333333333333333,0.09375,0.1825,0.125,0,0,0,1,0,"['Grammars A quasi-synchronous dependency grammar   specifies a conditional model p(t, t, a | s, s).', 'Feature-Rich Translation by Quasi-Synchronous Lattice Parsing', 'We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.']"
4,D11-1044,D09-1023,0.054982817869415807,0.0,True,False,0.05165692007797271,0.02857142857142857,0.0558766859344894,0.07801335363710907,0.11757990867579896,0.34210526315789475,0.3333333333333333,0.3125,0.38749999999999996,0.2638888888888889,0,0,0,0,1,"['Feature-Rich Translation by Quasi-Synchronous Lattice Parsing', 'The core of the approach is a novel decoder based on lattice parsing with quasi-synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic.', 'Decoding as QG parsing (4): We present anovel decoder based on lattice parsing with quasi synchronous grammar  .2 Further, we exploit generic approximate inference techniques to incorporate arbitrary non-local features in the dynamic programming algorithm  .Parameter estimation (): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation   with hidden variables to discriminatively and efficiently train our model.']"
5,D11-1044,D09-1023,0.07903780068728522,0.0,False,False,0.009746588693957114,0,0.02119460500963391,0.024130256530397094,0.02682648401826484,0.05263157894736842,0,0.03125,0.04,0.05555555555555555,0,0,0,1,0,"['Feature-Rich Translation by Quasi-Synchronous Lattice Parsing', 'Grammars A quasi-synchronous dependency grammar   specifies a conditional model p(t, t, a | s, s).']"
6,D11-1044,D09-1023,0.140893470790378,0.24242424242424243,False,False,0.029239766081871343,0,0.02601156069364162,0.05716293780016398,0.06107305936073059,0,0,0,0.0025,0,0,0,0,1,0,"['Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible.', 'For generality we permit these features to see all structures and denote them greor (s, s, a, t, t).']"
7,D11-1044,D09-1023,0.5017182130584192,0.4659090909090909,True,False,0.004873294346978557,0.014285714285714285,0.012524084778420038,0.017921986646362888,0.014840182648401826,0,0,0,0.005,0,0,0,0,1,0,"['Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.']"
9,PJOURNAL,D09-1023,0.08997722095671981,0.0946969696969697,True,False,0.12475633528265107,0.02857142857142857,0.18593448940269752,0.28241771113974473,0.27111872146118715,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"[', m}  2{1,...,n}  source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where s (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where t (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s;  denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tjN +1 ) language model features (.2): N -gram probabilities gsyn (t, t ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I  {1, . . .']"
10,PJOURNAL,D09-1023,0.15831435079726652,0.007575757575757576,True,False,0.08187134502923976,0.02857142857142857,0.1030828516377649,0.13541056577251973,0.18264840182648384,0.3421052631578947,0.3333333333333333,0.3125,0.39249999999999996,0.2638888888888889,0,0,0,1,0,"['The core of the approach is a novel decoder based on lattice parsing with quasi-synchronous grammar  , a syntactic formalism that does not require source and target trees to be isomorphic.', 'We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.', 'Decoding as QG parsing (4): We present anovel decoder based on lattice parsing with quasi synchronous grammar  .2 Further, we exploit generic approximate inference techniques to incorporate arbitrary non-local features in the dynamic programming algorithm  .Parameter estimation (): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation   with hidden variables to discriminatively and efficiently train our model.', 'Feature-Rich Translation by Quasi-Synchronous Lattice Parsing', 'Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in t (or a deliberate choice is made by the decoder to translate it to NULL).']"
12,PPROC-D09,D09-1023,0.9070796460176991,0.3939393939393939,True,False,0.004873294346978557,0.014285714285714285,0.019267822736030827,0.022373199016047788,0.015410958904109588,0,0,0,0.005,0,0,0,0,1,0,"['4.1 Translation as Monolingual Parsing.', 'Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.']"
16,W10-1730,D09-1023,0.1650485436893204,0.04924242424242424,False,False,0.042884990253411304,0.05714285714285714,0.05684007707129094,0.10237788450275273,0.079337899543379,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.', 'Here we take first steps toward such a universal decoder, making the following contributions:Arbitrary feature model (): We define a sin gle, direct log-linear translation model   that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.']"
18,W11-2139,D09-1023,0.29133858267716534,0.8939393939393939,False,False,0.018518518518518517,0,0.028901734104046242,0.038772402483307954,0.05365296803652968,0,0,0,0.015000000000000001,0.027777777777777776,0,0,0,0,1,"['In particular, we compare the effects of combining phrase features and syntactic features.', 'In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models  .']"
20,N10-1040,D09-1023,0.058823529411764705,0.022727272727272728,True,False,0.17446393762183235,0.04285714285714286,0.2475915221579962,0.41466557338643584,0.37043378995433784,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders  .', ', m}  2{1,...,n}  source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where s (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where t (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s;  denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tjN +1 ) language model features (.2): N -gram probabilities gsyn (t, t ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I  {1, . . .', 'Here we take first steps toward such a universal decoder, making the following contributions:Arbitrary feature model (): We define a sin gle, direct log-linear translation model   that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.']"
1,C02-1033,C98-1097,0.09259259259259259,0.030534351145038167,False,False,0.010721247563352826,0.02857142857142857,0.0394990366088632,0.03795244230994494,0.04052511415525114,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,"['These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.']"
2,C02-1033,C98-1097,0.8395061728395061,0.0,False,False,0.06237816764132553,0.02857142857142857,0.10115606936416184,0.11994845964624577,0.1324200913242009,0.2894736842105263,0,0.25,0.245,0.19444444444444445,0,0,0,1,0,"['Text Segmentation Using Reiteration and Collocation', 'Another approach to text segmentation is the detection of semantically related words.', 'These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation.', 'Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result.']"
3,ICDAR99,C98-1097,0.16847826086956522,0.3435114503816794,False,False,0.08089668615984405,0.1,0.13391136801541426,0.18753660536488245,0.17123287671232879,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.', 'Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows.', 'These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.', ""Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text."", 'This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.']"
4,P07-1061,C98-1097,0.0787037037037037,0.35877862595419846,True,False,0.0341130604288499,0.02857142857142857,0.04913294797687861,0.07578774745226662,0.07591324200913241,0,0,0,0.005,0,0,0,0,1,0,"['Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.', 'Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.']"
5,P07-1061,C98-1097,0.1388888888888889,0.022900763358778626,True,False,0.07797270955165692,0.05714285714285714,0.13680154142581888,0.13248213658193747,0.10216894977168949,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['The lexical cohesion relations of reiteration and collocation are used to identify related words.', 'Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.', 'All semantic relations not classified under the class of reiteration are attributed to the class of collocation.', 'Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT  .', 'Lexical cohesion relations   between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).']"
6,P07-1061,C98-1097,0.41203703703703703,0.1984732824427481,False,False,0.015594541910331383,0.02857142857142857,0.011560693641618497,0.023544570692280664,0.0182648401826484,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"[""Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.""]"
7,P07-1061,C98-1097,0.44907407407407407,0.022900763358778626,False,False,0.023391812865497075,0.02857142857142857,0.03564547206165703,0.045332083870212016,0.026255707762557076,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['The lexical cohesion relations of reiteration and collocation are used to identify related words.', 'Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.']"
8,P07-1061,C98-1097,0.9398148148148148,0.015267175572519083,False,False,0.0341130604288499,0.02857142857142857,0.0279383429672447,0.04205224317675997,0.030821917808219176,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity.', ""Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.""]"
9,P07-1061,C98-1097,1.0,0.06870229007633588,False,False,0.03508771929824561,0,0.0443159922928709,0.047440552887431174,0.043949771689497714,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Text segmentation could also be used as a pre-processing step in automatic summarisation.', 'Previous work on text segmentation has used term matching to identify clusters of related text.']"
10,P04470,C98-1097,0.23684210526315788,0.3282442748091603,True,False,0.015594541910331383,0.014285714285714285,0.04527938342967245,0.04123228300339697,0.046232876712328765,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel).', 'Relation weights are word repetition 8.5 3.62 (97.6%) calculated between pairwise words according to their location in RT.']"
11,P06128,C98-1097,0.05235602094240838,0.061068702290076333,False,False,0.03508771929824561,0,0.026974951830443156,0.03022138924680801,0.0319634703196347,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"[""Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved."", 'Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.']"
12,S0885,C98-1097,0.2678132678132678,0.16030534351145037,True,False,0.08284600389863547,0.07142857142857142,0.07803468208092484,0.1194799109757526,0.07990867579908675,0.2894736842105263,0.3333333333333333,0.28125,0.3575,0.20833333333333334,0,0,0,1,0,"['Lexical cohesion relations   between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).', 'The lexical cohesion relations of reiteration and collocation are used to identify related words.', 'Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.', 'The first two methods represent lexical cohesion relations.', ""Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.""]"
1,C16-1060,D10-1058,0.35294117647058826,0.8226600985221675,True,False,0.03996101364522417,0,0.09055876685934489,0.13529342860489635,0.10388127853881277,0.23684210526315788,0,0.21875,0.23249999999999998,0.16666666666666669,0,0,0,1,0,"['AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.', 'Gibbs sampling for the fertility IBM Model 1 is similar but simpler.', 'Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.', '1 j=1 1 1 1 In IBM Model 1, the word order does not mat ter.', 'Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.']"
2,P13-2002,D10-1058,0.22429906542056074,0.1724137931034483,True,False,0.0341130604288499,0,0.08092485549132947,0.13681621178399908,0.07648401826484018,0.2894736842105263,0,0.25,0.2675,0.2222222222222222,0,0,0,1,0,"['Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.', 'A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC', 'In this case, the fertility hidden Markov model is not faster than the HMM.', 'Initially, the fertility IBM Model 1 and fertility HMM did not perform well.', 'We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.']"
3,P13-2002,D10-1058,0.308411214953271,0.23645320197044334,True,False,0.10916179337231968,0,0.13872832369942195,0.18531099918003982,0.15867579908675802,0.3157894736842105,0,0.25,0.2675,0.19444444444444445,0,0,0,1,0,"['  applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.', 'The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.', '1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.', 'A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC', 'We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.']"
4,P13-2002,D10-1058,0.40186915887850466,0.7389162561576355,True,False,0.014619883040935672,0,0.02697495183044316,0.0759048846198899,0.028538812785388126,0,0,0,0.0225,0.027777777777777776,0,0,0,1,0,"['IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.', 'In this case, the fertility hidden Markov model is not faster than the HMM.', 'We choose t = 1, 5, and 30 for the fertility HMM.', 'We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.', 'Initially, the fertility IBM Model 1 and fertility HMM did not perform well.']"
5,P13-2002,D10-1058,0.7663551401869159,0.8866995073891626,False,False,0.04093567251461988,0.05714285714285714,0.04142581888246628,0.04462926086447228,0.03367579908675799,0,0,0,0.015000000000000001,0.027777777777777776,0,0,0,1,0,"['One may try to solve it by forcing all these words to share a same parameter (einfrequent).', 'We solve the problem in the following way: estimate the parameter (enon empty ) for all nonempty words, all infrequent words share this parameter.', 'Therefore, we assume that  follows a Poisson distribution with parameter I ().', 'Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.']"
6,P13-2002,D10-1058,0.9252336448598131,0.08374384236453201,False,False,0.011695906432748537,0,0.012524084778420038,0.020030455663582053,0.02054794520547945,0.23684210526315788,0,0.21875,0.2225,0.16666666666666669,0,0,0,1,0,"['Our work is different from others in essential ways.', 'AER results are computed using traditional HMM Viterbi decoding for both models.']"
7,P59105CA,D10-1058,0.13005780346820808,0.8275862068965517,True,False,0.01949317738791423,0,0.03757225433526011,0.09804380930069112,0.04337899543378995,0,0,0,0.0225,0.027777777777777776,0,0,0,1,0,"['We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.', 'Initially, the fertility IBM Model 1 and fertility HMM did not perform well.', 'Gibbs sampling for the fertility IBM Model 1 is similar but simpler.', 'We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.', '2.2 IBM Model 1 and HMM.']"
8,P87-94,D10-1058,0.797979797979798,0.1724137931034483,True,False,0.06335282651072124,0,0.1310211946050096,0.1498184373901839,0.158675799086758,0.23684210526315788,0,0.21875,0.23249999999999998,0.16666666666666669,0,0,0,1,0,"['Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.', 'For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . .', '  applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.', 'Results are shown in Table 2; we see that better word alignment results do not lead to better translations.', 'We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model.']"
9,PBULLETIN,D10-1058,0.38721804511278196,0.009852216748768473,False,False,0.022417153996101363,0,0.04913294797687861,0.06301979618132834,0.06506849315068491,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,['Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.']
10,PBULLETIN,D10-1058,0.4924812030075188,0.8719211822660099,True,False,0.04483430799220273,0,0.061657032755298644,0.12240834016633478,0.07191780821917809,0.05263157894736842,0.3333333333333333,0.0625,0.165,0.09722222222222222,0,0,0,1,0,"['Initially, the fertility IBM Model 1 and fertility HMM did not perform well.', 'Gibbs sampling for the fertility IBM Model 1 is similar but simpler.', 'Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.', 'Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1.', 'We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.']"
11,PCOLING-D10,D10-1058,0.9098039215686274,0.6157635467980296,True,False,0.02046783625730994,0.014285714285714285,0.036608863198458574,0.08363593768302682,0.04452054794520548,0,0,0,0.0225,0.027777777777777776,0,0,0,1,0,"['Surprisingly, we can achieve better results than the HMM by computing as few as 1 sample for each alignment, so the fertility hidden Markov model is much faster than the HMM.', 'IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.', 'In this case, the fertility hidden Markov model is not faster than the HMM.', 'We choose t = 1, 5, and 30 for the fertility HMM.', 'We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.']"
12,PPROC-D10,D10-1058,0.21428571428571427,0.0,True,False,0.0341130604288499,0,0.08092485549132947,0.13681621178399905,0.07648401826484019,0.2894736842105263,0,0.25,0.2675,0.22222222222222224,0,0,0,1,0,"['A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC', 'In this case, the fertility hidden Markov model is not faster than the HMM.', 'Initially, the fertility IBM Model 1 and fertility HMM did not perform well.', 'We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.', 'Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.']"
13,PPROC-D10,D10-1058,0.3392857142857143,0.7389162561576355,True,False,0.010721247563352826,0,0.01734104046242774,0.057397212135410554,0.021689497716894976,0,0,0,0.0225,0.027777777777777776,0,0,0,1,0,"['IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.', 'In this case, the fertility hidden Markov model is not faster than the HMM.', 'We choose t = 1, 5, and 30 for the fertility HMM.', 'Initially, the fertility IBM Model 1 and fertility HMM did not perform well.', 'The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.']"
14,PPROC-D10,D10-1058,0.3482142857142857,0.4236453201970443,True,False,0.056530214424951264,0.04285714285714286,0.06454720616570327,0.11233454375073217,0.06563926940639267,0,0,0,0.0025,0,0,0,0,1,0,"['Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.', 'However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints  : and these two models cannot assign consistent fertility to words.']"
15,PPROC-D10,D10-1058,0.7678571428571429,0.8866995073891626,False,False,0.038011695906432746,0.05714285714285714,0.04046242774566474,0.04158369450626683,0.033105022831050226,0,0,0,0.0125,0.027777777777777776,0,0,0,1,0,"['One may try to solve it by forcing all these words to share a same parameter (einfrequent).', 'We solve the problem in the following way: estimate the parameter (enon empty ) for all nonempty words, all infrequent words share this parameter.', 'Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.']"
16,PPROC-D10,D10-1058,0.9285714285714286,0.08374384236453201,False,False,0.011695906432748537,0,0.012524084778420038,0.020030455663582053,0.02054794520547945,0.23684210526315788,0,0.21875,0.2225,0.16666666666666669,0,0,0,1,0,"['Our work is different from others in essential ways.', 'AER results are computed using traditional HMM Viterbi decoding for both models.']"
17,Q13-1024,D10-1058,0.2564102564102564,0.04926108374384237,True,False,0.0341130604288499,0.014285714285714285,0.07321772639691715,0.08925852172894459,0.07534246575342465,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.', 'Most models have limited ability to model fertility.', 'Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.', 'Liang et al.', 'DeNero et al.']"
18,Q13-1024,D10-1058,0.5555555555555556,0.6551724137931034,True,False,0.024366471734892786,0,0.0279383429672447,0.05634297762680097,0.039383561643835614,0,0,0,0.0225,0.027777777777777776,0,0,0,1,0,"['Gibbs sampling for the fertility IBM Model 1 is similar but simpler.', 'This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm.', 'AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.']"
1,E12-1054,N04-1038,0.03070175438596491,0.9806201550387597,False,False,0.0847953216374269,0.04285714285714286,0.04431599229287089,0.03268126976689704,0.02054794520547945,0.07894736842105263,0,0.03125,0.01,0,1,0,0,0,0,"['This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.']"
2,H05-1003,N04-1038,0.175,0.007751937984496124,True,False,0.04580896686159844,0,0.05491329479768786,0.04076373433290382,0.030251141552511414,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,0,1,0,"['BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', 'Section 4 presents experimen tal results on two corpora: the MUC4 terrorism corpus, and Reuters texts about natural disasters.']"
3,N13-1104,N04-1038,0.41603053435114506,0.46511627906976744,False,False,0.01364522417153996,0,0.012524084778420038,0.01616492913201359,0.012557077625570776,0.07894736842105263,0,0.03125,0.01,0,0,0,0,1,0,"['For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe.', 'For each case-frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus.']"
4,N13-1110,N04-1038,0.8771186440677966,0.031007751937984496,False,False,0.12475633528265107,0,0.12042389210019266,0.056108703291554415,0.021689497716894976,0.3684210526315789,0.3333333333333333,0.3125,0.3625,0.20833333333333334,0,0,0,1,0,"['The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.', 'The goal of our research was to explore the use of contextual role knowledge for coreference resolution.', 'Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.', 'A contextual role represents the role that a noun phrase plays in an event or relationship.']"
5,P05-1020,N04-1038,0.9140271493212669,0.0,False,False,0.07309941520467836,0.014285714285714285,0.07418111753371869,0.04287220335012299,0.023972602739726026,0.10526315789473684,0.3333333333333333,0.09375,0.1775,0.125,0,0,0,1,0,"['Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.']"
6,P06-1005,N04-1038,0.5166666666666667,0.007751937984496124,False,False,0.03996101364522417,0,0.05009633911368015,0.03139276092304088,0.018835616438356163,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,['BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.']
7,P06-1005,N04-1038,0.7375,0.003875968992248062,False,False,0.043859649122807015,0,0.025048169556840076,0.015344968958650578,0.0091324200913242,0.05263157894736842,0.3333333333333333,0.0625,0.1425,0.06944444444444445,0,0,0,1,0,['We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.']
8,P07-1067,N04-1038,0.2026431718061674,0.003875968992248062,True,False,0.18518518518518517,0.05714285714285714,0.11368015414258187,0.09722384912732814,0.05821917808219178,0.13157894736842105,0.3333333333333333,0.09375,0.1525,0.06944444444444445,1,0,0,1,0,"['We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.', '2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.', 'Table 1: Syntactic Seeding Heuristics BABARs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.', 'During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor.']"
9,P07-1068,N04-1038,0.052884615384615384,0.2596899224806202,True,False,0.10526315789473684,0.014285714285714285,0.10693641618497109,0.04439498652922573,0.013127853881278538,0.3684210526315789,0,0.28125,0.255,0.19444444444444445,0,0,1,0,0,"['2.2 Contextual Role Knowledge.', 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', 'In this section, we describe how contextual role knowledge is represented and learned.', 'The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'A contextual role represents the role that a noun phrase plays in an event or relationship.']"
10,P08-1090,N04-1038,0.1320754716981132,0.031007751937984496,True,False,0.17641325536062377,0.04285714285714286,0.13198458574181116,0.07637343329038303,0.04908675799086758,0.3684210526315789,0,0.28125,0.255,0.19444444444444445,0,0,0,1,0,"['The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.', 'The goal of our research was to explore the use of contextual role knowledge for coreference resolution.', 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', 'Table 1: Syntactic Seeding Heuristics BABARs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.']"
11,P09-1068,N04-1038,0.06415094339622641,0.35271317829457366,True,False,0.01364522417153996,0,0.024084778420038533,0.034438327281246336,0.021118721461187213,0.07894736842105263,0,0.03125,0.01,0,0,0,0,1,0,"['For example, co-occurring caseframes may reflect synonymy (e.g., <patient> kidnapped and <patient> abducted) or related events (e.g., <patient> kidnapped and <patient> released).', 'We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case-frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that cooccur with caseframes and using them to crosscheck anaphor/candidate compatability.']"
12,P09-1074,N04-1038,0.6819571865443425,0.9534883720930233,False,False,0.05263157894736842,0.02857142857142857,0.022157996146435453,0.022490336183671078,0.02968036529680365,0.07894736842105263,0,0.03125,0.01,0,1,0,0,0,0,"['These systems rely on a training corpus that has been manually annotated with coreference links.', 'Given a document to process, BABAR uses four modules to perform coreference resolution.']"
13,P10-1142,N04-1038,0.2737642585551331,0.5697674418604651,False,False,0.018518518518518517,0.02857142857142857,0.024084778420038533,0.03678107063371207,0.02511415525114155,0.07894736842105263,0,0.03125,0.01,0,0,0,0,1,0,"['The CFLex and CFNet knowledge sources provide positive evidence that a candidate NP and anaphor might be coreferent.', '  also used a DempsterShafer model to merge evidence from different sources for template-level coreference.']"
14,P11-1082,N04-1038,0.5633187772925764,0.031007751937984496,True,False,0.13840155945419103,0.05714285714285714,0.1541425818882466,0.08679864120885554,0.05707762557077625,0.3684210526315789,0.3333333333333333,0.3125,0.3625,0.20833333333333334,0,0,0,1,0,"['The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', 'The goal of our research was to explore the use of contextual role knowledge for coreference resolution.', 'We also performed experiments to evaluate the impact of each type of contextual role knowledge separately.', '2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations.']"
15,P13-1121,N04-1038,0.3088235294117647,0.031007751937984496,True,False,0.11695906432748537,0.02857142857142857,0.081888246628131,0.04919761040178047,0.03139269406392694,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.', 'Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.']"
16,P13-2015,N04-1038,0.26136363636363635,0.3333333333333333,True,False,0.14230019493177387,0.05714285714285714,0.11078998073217725,0.07168794658545156,0.0365296803652968,0.3684210526315789,0,0.28125,0.255,0.19444444444444445,0,0,0,1,0,"['The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.', 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', 'The focus of our work is on the use of contextual role knowledge for coreference resolution.', '2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.', 'We evaluated BABAR on two domains: terrorism and natural disasters.']"
17,W05-0612,N04-1038,0.13360323886639677,0.0,False,False,0.15692007797270954,0.02857142857142857,0.1425818882466281,0.08211315450392406,0.04452054794520548,0.4210526315789474,0.3333333333333333,0.34375,0.39749999999999996,0.2638888888888889,0,0,0,1,0,"['Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', 'This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.', 'The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.']"
18,W06-0106,N04-1038,0.9095477386934674,0.007751937984496124,True,False,0.0682261208576998,0,0.11753371868978806,0.08117605716293785,0.06164383561643835,0.13157894736842105,0.3333333333333333,0.09375,0.1525,0.06944444444444445,0,0,0,1,0,"['BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', 'Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.', '2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too.']"
19,W06-0206,N04-1038,0.05154639175257732,0.027131782945736434,False,False,0.08187134502923976,0.02857142857142857,0.04816955684007706,0.03268126976689704,0.02454337899543379,0.3157894736842105,0,0.25,0.22,0.1388888888888889,1,0,0,0,0,"['Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.']"
20,W10-3909,N04-1038,0.1116751269035533,0.031007751937984496,False,False,0.06725146198830409,0,0.07418111753371869,0.042403654679629844,0.023972602739726026,0.2894736842105263,0.3333333333333333,0.28125,0.3525,0.20833333333333334,0,0,1,1,0,"['The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.']"
21,W10-3909,N04-1038,0.18274111675126903,0.24806201550387597,True,False,0.1968810916179337,0.04285714285714286,0.1329479768786127,0.08223029167154738,0.05422374429223744,0.18421052631578946,0.3333333333333333,0.125,0.1875,0.125,1,0,0,0,0,"['Table 1: Syntactic Seeding Heuristics BABARs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.', 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', 'We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.', 'Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.']"
22,W10-3909,N04-1038,0.25380710659898476,0.3798449612403101,False,False,0.038011695906432746,0.014285714285714285,0.031791907514450865,0.022373199016047795,0.009703196347031963,0.3157894736842105,0,0.25,0.22,0.1388888888888889,0,0,0,1,1,"['During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent.', 'The focus of our work is on the use of contextual role knowledge for coreference resolution.']"
1,A97-1028,X96-1048,0.02830188679245283,0.03305785123966942,False,False,0.01949317738791423,0.014285714285714285,0.06165703275529865,0.02120182733981492,0.02682648401826484,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,1,0,0,"['EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.']"
4,J00-4003,X96-1048,0.012725344644750796,0.005509641873278237,True,False,0.009746588693957114,0,0.028901734104046242,0.01815626098160946,0.0182648401826484,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.', 'About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks.']"
5,J00-4003,X96-1048,0.07529162248144221,0.5785123966942148,True,False,0.024366471734892786,0.02857142857142857,0.050096339113680145,0.047206278552184594,0.06792237442922375,0,0,0.03125,0.0025,0.027777777777777776,0,0,0,0,1,"['The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.', 'Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels.']"
6,W97-1307,X96-1048,0.14146341463414633,0.3526170798898072,True,False,0.01949317738791423,0.02857142857142857,0.028901734104046242,0.019210495490219047,0.03881278538812785,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.']"
7,P06-1059,X96-1048,0.054945054945054944,0.1349862258953168,True,False,0.07602339181286549,0.15714285714285714,0.1425818882466281,0.09394400843387611,0.2186073059360728,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"['As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.', 'In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.', 'As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.', 'TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5.', 'Best and average error per response fill Organization object slot scores for TE task With respect to performance on ORG_DESCRIPTOR, note that there may be multiple descriptors (or none) in the text.']"
8,C04-1126,X96-1048,0.2784090909090909,0.6721763085399449,True,False,0.0029239766081871343,0.014285714285714285,0.015414258188824663,0.010776619421342386,0.02054794520547945,0,0,0.03125,0.0025,0.027777777777777776,0,0,0,1,0,"['Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator\'s templates were treated as the ""key"" and the other annotator\'s templates were treated as the ""response"".']"
9,C04-1126,X96-1048,0.19318181818181818,0.6253443526170799,True,False,0.012670565302144249,0,0.014450867052023121,0.021553238842684785,0.030821917808219176,0.23684210526315788,0,0.25,0.2125,0.16666666666666669,0,0,0,0,1,"['Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.', 'CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.']"
10,W99-0612,X96-1048,0.6650943396226415,0.14049586776859505,True,False,0.03508771929824561,0.07142857142857142,0.07803468208092484,0.05271172543047909,0.12842465753424656,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,0,1,"['It was also unexpected that one of the systems would match human performance on the task.', 'As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.', 'These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.']"
11,E99-1001,X96-1048,0.07834101382488479,0.19834710743801653,False,False,0.04775828460038986,0.05714285714285714,0.11271676300578026,0.06911092889773925,0.07819634703196346,0.23684210526315788,0,0.21875,0.21,0.1388888888888889,0,0,0,1,0,"['Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..', 'There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.', 'Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem.', 'EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.']"
12,M98-1003,X96-1048,0.034482758620689655,0.6776859504132231,True,False,0.023391812865497075,0.05714285714285714,0.07899807321772638,0.01897622115497247,0.03424657534246575,0,0,0.03125,0.0025,0.027777777777777776,0,0,1,0,0,"['The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.', 'No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.']"
