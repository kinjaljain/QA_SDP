1,C10-2101,P05-1004,0.27611940298507465,0.28936170212765955,False,False,0.047619047619047616,0.030821917808219176,0.08946322067594432,0.15479399306892572,0.16911764705882354,0,0.1111111111111111,0.16666666666666666,0.18803418803418803,0.2857142857142857,0,0,0,1,0,"['Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.', 'Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses.', 'This same technique as is used in our approach to supersense tagging.', 'Supersense Tagging of Unknown Nouns using Semantic Similarity', 'This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.']"
3,J07-4005,P05-1004,0.3262108262108262,0.28936170212765955,True,False,0.047619047619047616,0.0273972602739726,0.027833001988071572,0.08625336927223724,0.0625,0,0.3333333333333333,0.2222222222222222,0.28205128205128205,0.2857142857142857,0,0,0,1,0,"['Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.', 'Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', 'Ciaramita and Johnson   found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.']"
4,J09-3004,P05-1004,0.9933184855233853,0.39148936170212767,True,False,0,0.023972602739726026,0.03976143141153081,0.05968425105891412,0.04044117647058824,0,0,0,0.008547008547008548,0,0,0,0,1,0,"['Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.', 'Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well.']"
5,N06-1017,P05-1004,0.13756613756613756,0.625531914893617,False,False,0,0.010273972602739725,0.02783300198807157,0.038120908740854824,0.02573529411764706,0,0.2222222222222222,0.1111111111111111,0.17094017094017094,0.14285714285714285,0,0,0,1,0,"['Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super-sense for the unknown nouns.', 'The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.']"
6,N06-1017,P05-1004,1.0,0.06808510638297872,False,False,0,0.010273972602739725,0.017892644135188866,0.026184058529072008,0.011029411764705881,0,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.14285714285714285,0,0,0,1,0,"['consistency when classifying similar words into categories.', 'Widdows   uses a similar technique to insert words into the WORDNET hierarchy.']"
7,N07-1024,P05-1004,0.4129353233830846,0.00851063829787234,True,False,0.09523809523809523,0.05821917808219178,0.06560636182902584,0.10050057758952641,0.11397058823529413,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,0,1,0,"['Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', 'The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.', 'Broad semantic classification is currently used by lexicographers to organise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.']"
8,P12-2050,P05-1004,0.17647058823529413,0.00851063829787234,True,False,0.047619047619047616,0.04452054794520548,0.07157057654075545,0.13708124759337711,0.13602941176470587,0,0.3333333333333333,0.2222222222222222,0.3418803418803419,0.2857142857142857,0,0,0,1,0,"['Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', 'Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses.', 'Ciaramita and Johnson   found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.', 'To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.']"
9,S07-1032,P05-1004,0.1927710843373494,0.9744680851063829,False,False,0,0.00684931506849315,0.04970178926441351,0.06700038505968425,0.08455882352941176,0,0.1111111111111111,0.1111111111111111,0.1794871794871795,0.14285714285714285,0,0,0,1,0,"['Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson  .', 'Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses.']"
10,S10-1090,P05-1004,0.1103448275862069,0.2765957446808511,False,False,0.09523809523809523,0.04794520547945205,0.06163021868787275,0.09780515979976903,0.11029411764705883,0,0.1111111111111111,0.1111111111111111,0.19658119658119658,0.14285714285714285,0,0,0,1,0,"['Ciaramita and Johnson   implement a super-sense tagger based on the multi-class perceptron classifier  , which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.', 'Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson  .', 'Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses.']"
11,S12-1011,P05-1004,0.05042016806722689,0.16170212765957448,False,False,0,0,0.005964214711729622,0.01039661147477859,0.003676470588235294,0,0,0,0.017094017094017096,0,0,0,0,1,0,"['Ciaramita et al.', 'How are words with multiple supersenses handled?']"
12,S12-1011,P05-1004,0.42016806722689076,0.18723404255319148,False,False,0.047619047619047616,0.04452054794520548,0.08151093439363816,0.13092029264536006,0.1654411764705882,0,0,0.05555555555555555,0.08547008547008547,0.14285714285714285,0,0,0,1,0,"['Supersense tagging can provide automated or semiautomated assistance to lexicographers adding words to the WORDNET hierarchy.', 'This same technique as is used in our approach to supersense tagging.', 'Supersense Tagging of Unknown Nouns using Semantic Similarity', 'Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.', 'The most obvious solution is to sum the context vectors across the words which have each supersense.']"
13,S12-1023,P05-1004,0.924901185770751,0.2765957446808511,False,False,0.09523809523809523,0.04452054794520548,0.02186878727634195,0.04620716211012707,0.03676470588235294,0,0,0,0.017094017094017096,0,0,0,0,1,0,"['Ciaramita and Johnson   implement a super-sense tagger based on the multi-class perceptron classifier  , which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.', 'This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.']"
14,W06-1670,P05-1004,0.36015325670498083,0.28936170212765955,True,False,0.09523809523809523,0.03767123287671233,0.06361829025844928,0.13053523296110903,0.15441176470588233,0,0.3333333333333333,0.2777777777777778,0.3333333333333333,0.42857142857142855,0,0,0,1,0,"['Ciaramita and Johnson   propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.', 'Supersense Tagging of Unknown Nouns using Semantic Similarity', 'Ciaramita and Johnson   call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses.', 'Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.']"
1,A00-2034,P98-1046,0.15517241379310345,0.3389830508474576,True,False,0.09523809523809523,0.1095890410958904,0.03180914512922465,0.03273007316134,0.04044117647058823,0,0,0.1111111111111111,0.03418803418803419,0,0,0,0,1,0,"[""intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, .. ,c} such that S C S' and v E ci n ... n dm (subset criterion)."", 'Second, since the translation map-pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve  c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.']"
2,A00-2034,P98-1046,0.16379310344827586,0.02824858757062147,False,False,0.14285714285714285,0.1267123287671233,0.011928429423459242,0.021563342318059297,0.051470588235294115,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,['We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes.']
3,C00-2118,P98-1046,0.9768518518518519,0.5480225988700564,False,False,0.6190476190476191,0.44178082191780843,0.07753479125248508,0.10858683095879867,0.29779411764705865,1.0,0.2222222222222222,0.2777777777777778,0.025641025641025644,0,0,0,0,1,0,"['Even though the Levin verb classes are defined by their syntactic behavior, many reflect semantic distinctions made by WordNet, a classification hierarchy defined in terms of purely se-mantic word relations (synonyms, hypernyms, etc.).', 'Intersective Levin sets partition these classes according to more coherent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.']"
4,C00-2148,P98-1046,0.1836734693877551,0.022598870056497175,False,False,1.666666666666667,1.0136986301369864,0.11530815109343934,0.149788217173662,0.4926470588235291,0,0.6666666666666666,0.2777777777777778,0.30769230769230765,0.42857142857142855,0,0,0,1,1,"['We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.', 'Investigating regular sense extensions based on intersective Levin classes', 'We base these regular extensions on a fine-grained variation on Levin classes, inter-sective Levin classes, as a source of semantic components associated with specific adjuncts.', 'Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving  .', 'Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.']"
5,C08-1002,P98-1046,0.15916955017301038,0.022598870056497175,True,False,1.4285714285714288,0.7808219178082192,0.09741550695825046,0.15440893338467468,0.5624999999999997,0,0.2222222222222222,0.2777777777777778,0.24786324786324787,0.2857142857142857,0,0,0,0,1,"['We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.', 'We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.', '3.1 Using intersective Levin classes to.', 'Investigating regular sense extensions based on intersective Levin classes', 'Intersective Levin sets partition these classes according to more coherent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.']"
6,H01-1010,P98-1046,0.2976190476190476,0.03389830508474576,False,False,1.0952380952380951,1.0136986301369868,0.20278330019880708,0.1979206777050445,0.45588235294117624,1.0,0.7777777777777778,0.2777777777777778,0.17094017094017094,0.14285714285714285,0,1,0,0,0,"['We also have begun to ex-amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.', 'Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving  .', 'Even though the Levin verb classes are defined by their syntactic behavior, many reflect semantic distinctions made by WordNet, a classification hierarchy defined in terms of purely se-mantic word relations (synonyms, hypernyms, etc.).', ""Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes."", 'We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.']"
7,J01-3003,P98-1046,0.9593023255813954,0.022598870056497175,False,False,0.19047619047619047,0.20205479452054792,0.02385685884691849,0.03273007316134001,0.1323529411764706,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,"['We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.']"
8,J04-1003,P98-1046,0.1124031007751938,0.14689265536723164,False,False,0.9047619047619047,0.8767123287671235,0.12326043737574552,0.1328455910666154,0.3124999999999998,0,0.3333333333333333,0,0,0,0,1,0,0,0,"['Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving  .', 'Two current approaches to English verb classifications are WordNet   and Levin classes  .', 'The distribution of syntactic frames in which a verb can appear determines its class member-ship.', 'Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.', 'The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.']"
9,J05-1004,P98-1046,0.0951048951048951,0.6214689265536724,False,False,0.19047619047619047,0.25684931506849323,0.05964214711729621,0.04351174432036965,0.08455882352941177,1.0,0.5555555555555556,0.16666666666666666,0.008547008547008548,0,0,1,0,0,0,"['However, other intersective classes, such as the split/push/carry class, are no more consistent with WordNet than the original Levin classes.', 'This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.']"
10,N09-1057,P98-1046,0.14925373134328357,0.632768361581921,False,False,0.14285714285714285,0.0958904109589041,0.06560636182902582,0.021178282633808238,0.04779411764705882,1.0,0.5555555555555556,0.16666666666666666,0.008547008547008548,0,0,0,1,0,0,"['In addition, only one verb (pull) has a WordNet sense corresponding to the change of state - separation semantic component associated with the split class.', 'Where break is concerned, the only thing specified is the resulting change of state where the object becomes separated into pieces.']"
11,P03-1009,P98-1046,0.026041666666666668,0.11299435028248588,False,False,0.38095238095238093,0.3356164383561644,0.0636182902584493,0.06391990758567577,0.13970588235294118,1.0,0.5555555555555556,0.16666666666666666,0.008547008547008548,0,0,1,0,0,0,"['Two current approaches to English verb classifications are WordNet   and Levin classes  .', 'This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs.']"
12,P06-1117,P98-1046,0.3263598326359833,0.0,True,False,2.047619047619048,0.8835616438356164,0.10337972166998009,0.15748941085868318,0.5110294117647057,1.0,0.4444444444444444,0.33333333333333337,0.2393162393162393,0.2857142857142857,0,0,0,1,0,"['Investigating regular sense extensions based on intersective Levin classes', 'We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.', 'Levin verb classes are based on an underlying lat-tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.', 'We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to-gether subsets of existing classes with over-lapping members.', '3.1 Using intersective Levin classes to.']"
13,P99-1051,P98-1046,0.08695652173913043,0.6101694915254238,False,False,1.2380952380952386,0.34931506849315075,0.04373757455268389,0.056603773584905655,0.17279411764705882,1.0,0.2222222222222222,0.2222222222222222,0.059829059829059825,0.14285714285714285,1,0,0,0,0,"['Levin verb classes are based on an underlying lat-tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.', 'Investigating regular sense extensions based on intersective Levin classes']"
14,W00-0202,P98-1046,0.1836734693877551,0.864406779661017,True,False,0.09523809523809523,0.0958904109589041,0.05566600397614312,0.030419715055833654,0.04779411764705883,1.0,0.2222222222222222,0.16666666666666666,0.008547008547008548,0,0,0,1,0,0,"['As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share similar properties with the English verbs, including the causative/inchoative.']"
16,W02-1108,P98-1046,0.294478527607362,0.559322033898305,False,False,0.8095238095238095,0.8116438356164385,0.14711729622266395,0.12938005390835583,0.3014705882352941,1.0,0.7777777777777778,0.2777777777777778,0.18803418803418803,0.14285714285714285,0,1,0,0,0,"['This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs.', 'This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.', 'This distinction appears in the second-order Levin classes as membership vs. nonmember-ship in the intersective class with split.', 'We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes.', 'A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.']"
17,W03-0910,P98-1046,0.42105263157894735,0.4124293785310734,True,False,0.8095238095238095,0.6267123287671234,0.12127236580516898,0.14709279938390463,0.27941176470588225,0,0.3333333333333333,0,0.017094017094017096,0,0,0,0,1,0,"['Figure 2: Intersective class formed from Levin carry, push/pull and split verbs-verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic component of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.', 'However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components.']"
18,W04-2606,P98-1046,0.034653465346534656,0.03389830508474576,False,False,0.6666666666666665,0.551369863013699,0.07952286282306162,0.09125914516750096,0.31249999999999983,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,1,0,0,0,"['We also have begun to ex-amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.', 'We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.', 'We base these regular extensions on a fine-grained variation on Levin classes, inter-sective Levin classes, as a source of semantic components associated with specific adjuncts.']"
19,W06-2611,P98-1046,0.3562231759656652,0.0847457627118644,False,False,1.7619047619047628,0.7500000000000002,0.09145129224652085,0.12013862148633045,0.4080882352941174,1.0,0.5555555555555556,0.4444444444444444,0.3162393162393162,0.42857142857142855,0,1,0,0,0,"['We base these regular extensions on a fine-grained variation on Levin classes, inter-sective Levin classes, as a source of semantic components associated with specific adjuncts.', 'Levin verb classes are based on an underlying lat-tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.', 'We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.', 'Investigating regular sense extensions based on intersective Levin classes']"
20,W99-0503,P98-1046,0.24,0.8248587570621468,False,False,0.09523809523809523,0.2602739726027398,0.12127236580516895,0.058144012321909896,0.051470588235294115,1.0,0.5555555555555556,0.16666666666666666,0.008547008547008548,0,0,0,1,0,0,"[""Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes."", 'This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.']"
21,W99-0632,P98-1046,0.22167487684729065,0.14689265536723164,False,False,0.8571428571428572,0.784246575342466,0.09343936381709739,0.11166730843280713,0.33455882352941163,0,0.5555555555555556,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,1,0,0,0,"['Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving  .', 'The distribution of syntactic frames in which a verb can appear determines its class member-ship.', 'Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.', 'We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.']"
22,W99-0632,P98-1046,1.0,0.01694915254237288,False,False,0.8571428571428571,0.6609589041095891,0.07753479125248507,0.1347708894878707,0.3419117647058822,1.0,0.7777777777777778,0.3888888888888889,0.18803418803418803,0.14285714285714285,0,0,0,1,0,"['Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes.', 'Two current approaches to English verb classifications are WordNet   and Levin classes  .', 'We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.', 'We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes.', 'However, other intersective classes, such as the split/push/carry class, are no more consistent with WordNet than the original Levin classes.']"
1,C10-1070,C04-1089,0.3979591836734694,0.8757062146892656,True,False,0,0.02054794520547945,0.05964214711729622,0.0673854447439353,0.05147058823529412,0,0,0,0.008547008547008548,0,0,0,0,1,0,"['comm means the correct translation is a word appearing in the dictionary we used or is a stop word.', 'The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English.']"
2,C10-2164,C04-1089,0.06607929515418502,0.022598870056497175,False,False,0,0.03424657534246575,0.19483101391650093,0.17635733538698506,0.16176470588235295,0,0.3333333333333333,0.2222222222222222,0.3418803418803419,0.2857142857142857,0,0,0,1,0,"['In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.', 'In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.']"
3,D10-1042,C04-1089,0.24766355140186916,0.04519774011299435,False,False,0,0.0136986301369863,0.08747514910536777,0.022333461686561418,0.03308823529411765,0,0.1111111111111111,0.1111111111111111,0.09401709401709402,0.14285714285714285,0,0,1,0,0,"['Much research has been done on using parallel corpora to learn bilingual lexicons  .', 'For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.']"
4,D12-1003,C04-1089,0.12258064516129032,0.022598870056497175,False,False,0,0.030821917808219176,0.11332007952286283,0.143242202541394,0.13970588235294118,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,0,1,0,"['In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.', 'On the other hand, the work of   used the context of w to locate its translation in a second language.', 'When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.']"
5,D12-1003,C04-1089,0.21935483870967742,0.8926553672316384,True,False,0,0.010273972602739725,0.07157057654075547,0.06969580284944167,0.06985294117647059,0,0,0,0.09401709401709402,0,1,0,0,1,0,"['And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.']"
6,N09-1048,C04-1089,0.06224066390041494,0.096045197740113,False,False,0,0.023972602739726026,0.1391650099403578,0.1470927993839045,0.13970588235294118,0,0.3333333333333333,0.2222222222222222,0.3418803418803419,0.2857142857142857,1,0,0,1,0,"['In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.']"
7,N09-1048,C04-1089,0.2074688796680498,0.9774011299435028,True,False,0,0.0273972602739726,0.2067594433399601,0.1802079322294956,0.18749999999999992,0,0.3333333333333333,0.2222222222222222,0.35042735042735046,0.2857142857142857,1,0,0,1,0,"['In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.', 'Comparable corpora such as news documents of the same period from different news agencies are readily available.', 'And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.', 'On the other hand, using our method of combining both sources of information and setting M = , 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except ,,) have their correct English translations at rank one position.']"
8,P06-1011,C04-1089,0.8305084745762712,0.0,False,False,0,0.0410958904109589,0.21471172962226634,0.19599537928378905,0.1838235294117647,0,0.3333333333333333,0.2777777777777778,0.39316239316239315,0.42857142857142855,1,0,0,1,0,"['Mining New Word Translations from Comparable Corpora', 'In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.', 'Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.']"
9,P13-1059,C04-1089,0.9411764705882353,0.05649717514124294,False,False,0,0.03767123287671233,0.17495029821073554,0.1424720831728919,0.10661764705882354,0,0.1111111111111111,0.1111111111111111,0.1794871794871795,0.14285714285714285,0,0,0,1,0,"['Comparable corpora refer to texts that are not direct translation but are about the same topic.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'Previous research efforts on acquiring translations from comparable corpora include  .', 'While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.', 'When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.']"
10,P13-1062,C04-1089,0.6707818930041153,0.1638418079096045,False,False,0,0.0410958904109589,0.09940357852882703,0.10319599537928377,0.125,0,0,0,0.017094017094017096,0,0,0,0,1,0,"['If an English word e is the translation of a Chinese word c , they will have similar contexts.', 'The work of   noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.']"
12,P13-2036,C04-1089,0.06666666666666667,0.8983050847457628,True,False,0,0.010273972602739725,0.09940357852882703,0.09318444358875627,0.09558823529411765,0,0,0,0.09401709401709402,0,1,0,0,1,0,"['On the other hand, using our method of combining both sources of information and setting M = , 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except ,,) have their correct English translations at rank one position.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.']"
13,P13-2036,C04-1089,0.0761904761904762,0.2655367231638418,False,False,0,0.00684931506849315,0.019880715705765405,0.026569118213323063,0.007352941176470588,0,0,0,0.017094017094017096,0,0,0,0,1,0,"[' t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.']"
14,W11-1215,C04-1089,0.1630901287553648,0.9774011299435028,False,False,0,0.0410958904109589,0.21471172962226634,0.195995379283789,0.1838235294117647,0,0.3333333333333333,0.2777777777777778,0.39316239316239315,0.42857142857142855,1,0,0,1,0,"['In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'Mining New Word Translations from Comparable Corpora', 'Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.', 'In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.', 'In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.']"
16,W13-2501,C04-1089,0.115,0.11864406779661017,False,False,0,0.04452054794520548,0.22862823061630205,0.12090874085483254,0.07352941176470588,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,1,0,0,"['That is, Chinese is the source language and English is the target language.', 'Comparable corpora refer to texts that are not direct translation but are about the same topic.', 'While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.', 'As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.', 'In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.']"
17,W13-2512,C04-1089,0.2986425339366516,0.9548022598870056,False,False,0.047619047619047616,0.023972602739726026,0.029821073558648107,0.030419715055833654,0.03308823529411765,0,0.1111111111111111,0.16666666666666666,0.1111111111111111,0.14285714285714285,0,0,0,1,0,"['They attempted to improve named entity translation by combining phonetic and semantic information.', 'We translated Chinese words into English.']"
1,D13-1031,N06-2049,0.7479452054794521,0.9743589743589743,False,False,0,0,0.02982107355864811,0.023488640739314594,0.04044117647058823,0,0,0.05555555555555555,0.017094017094017096,0,0,0,0,1,1,"['Using the CRFs approaches, we prove that it outperformed the character-based method using the CRF approaches.']"
2,I08-4009,N06-2049,0.1144578313253012,0.9871794871794872,False,False,0.09523809523809523,0.0136986301369863,0.03777335984095427,0.026954177897574122,0.051470588235294115,0,0,0.05555555555555555,0.017094017094017096,0,0,0,1,0,0,['This approach is effective for performing desired segmentation based on users requirements to R-oov and R-iv.']
3,I08-4009,N06-2049,0.12048192771084337,0.36538461538461536,True,False,0,0.010273972602739725,0.14314115308151087,0.19984597612629967,0.32352941176470584,0,0,0.05555555555555555,0.06837606837606838,0.14285714285714285,0,0,0,1,0,"['In this section we introduce a confidence measure approach to combine the two results.', 'The idea of using the confidence measure has appeared in  , where it was used to recognize the OOVs.', 'In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.', 'In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.', 'We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.']"
4,I08-4015,N06-2049,0.4,0.0,False,False,0,0.02054794520547945,0.17495029821073552,0.2067770504428187,0.4705882352941174,0,0.1111111111111111,0.2777777777777778,0.17094017094017094,0.42857142857142855,0,0,0,0,1,"['Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .', 'The confidence measure comes from two sources: IOB tagging and dictionary-based word segmentation.', 'After the dictionary-based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.']"
5,I08-4030,N06-2049,0.14772727272727273,0.42948717948717946,True,False,0,0.010273972602739725,0.11332007952286281,0.14285714285714288,0.5110294117647056,0,0,0.05555555555555555,0.06837606837606838,0.14285714285714285,0,0,0,0,1,"['If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.', 'The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.', 'The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.', 'In a real application, a satisfactory tradeoff between Rivs and R-oovs could find through tuning the confidence threshold.', '2.2 Confidence-dependent word segmentation.']"
6,I08-4030,N06-2049,0.5795454545454546,0.8589743589743589,False,False,0,0.02054794520547945,0.13320079522862818,0.13323065075086643,0.2389705882352941,0,0,0.05555555555555555,0.008547008547008548,0.14285714285714285,0,0,0,1,1,"['Later, this approach was implemented by the CRF-based method  , which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem  .', 'It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.']"
7,J11-1005,N06-2049,0.24887690925426775,0.03205128205128205,False,False,0,0.00684931506849315,0.05168986083499006,0.06430496726992684,0.1323529411764706,0,0.1111111111111111,0.1111111111111111,0.09401709401709402,0.14285714285714285,0,0,0,1,0,['The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .']
8,N09-1007,N06-2049,0.67,0.01282051282051282,False,False,0,0.0136986301369863,0.10337972166998008,0.11128224874855605,0.2536764705882353,0,0.2222222222222222,0.16666666666666666,0.17094017094017094,0.2857142857142857,0,0,0,1,0,"['We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.', 'It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.']"
9,P07-1106,N06-2049,0.48947368421052634,0.967948717948718,True,False,0,0.02054794520547945,0.1630218687872763,0.19214478244127844,0.40808823529411753,0,0.3333333333333333,0.2777777777777778,0.27350427350427353,0.2857142857142857,0,0,0,1,0,"['In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.', 'The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .', 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.']"
10,P07-1106,N06-2049,0.9052631578947369,0.7115384615384616,True,False,0,0.017123287671232876,0.16103379721669978,0.16711590296495962,0.3492647058823529,0,0,0.05555555555555555,0.06837606837606838,0.14285714285714285,0,0,0,1,0,"['We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.', 'It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.']"
11,P12-1027,N06-2049,0.9215686274509803,0.00641025641025641,False,False,0,0.023972602739726026,0.19681908548707744,0.22872545244512904,0.514705882352941,0,0.3333333333333333,0.38888888888888884,0.3333333333333333,0.5714285714285714,0,0,0,1,0,"['We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', 'The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .', 'The confidence measure comes from two sources: IOB tagging and dictionary-based word segmentation.']"
12,W06-0118,N06-2049,0.14432989690721648,0.6474358974358975,True,False,0,0.0136986301369863,0.15109343936381703,0.1917597227570274,0.48529411764705876,0,0.1111111111111111,0.16666666666666666,0.1623931623931624,0.2857142857142857,0,0,0,1,0,"['The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.', 'Figure 1: Outline of word segmentation process data.', 'Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.', 'In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.', 'Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary-based approach.']"
13,W06-0118,N06-2049,0.24742268041237114,0.967948717948718,True,False,0,0.023972602739726026,0.21272365805168977,0.2525991528686948,0.5257352941176469,0,0.3333333333333333,0.33333333333333337,0.38461538461538464,0.42857142857142855,0,0,0,1,0,"['In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .', 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', 'Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', 'In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.']"
14,W08-0335,N06-2049,0.22267206477732793,0.46153846153846156,True,False,0,0.030821917808219176,0.16103379721669978,0.19214478244127842,0.3419117647058823,0,0.1111111111111111,0.16666666666666666,0.17094017094017094,0.14285714285714285,0,0,0,1,0,"['We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.', 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'We proved the new approach enhanced the word segmentation significantly.', 'The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .', 'For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.']"
15,W08-0335,N06-2049,0.6477732793522267,0.5,False,False,0,0.003424657534246575,0.021868787276341946,0.025798998844820946,0.04044117647058824,0,0,0,0.05982905982905983,0,0,0,0,1,0,"['For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.']"
16,W10-4128,N06-2049,0.0582010582010582,0.05128205128205128,True,False,0,0.010273972602739725,0.10735586481113318,0.1340007701193685,0.2463235294117647,0,0.1111111111111111,0.16666666666666666,0.1111111111111111,0.14285714285714285,0,0,0,1,0,"['In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.', 'In the results of the closed test in Bakeoff 2005  , the work of  , using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.', 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.', 'We found that so far all the existing implementations were using character-based IOB tagging.']"
17,W10-4135,N06-2049,0.10679611650485436,0.967948717948718,True,False,0,0.017123287671232876,0.15506958250497013,0.18213323065075093,0.426470588235294,0,0.3333333333333333,0.33333333333333337,0.38461538461538464,0.42857142857142855,0,0,0,1,1,"['In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.', 'Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', 'It proves the proposed word-based IOB tagging was very effective.']"
18,W10-4135,N06-2049,0.24271844660194175,0.6474358974358975,True,False,0,0.00684931506849315,0.12127236580516898,0.17019638043896812,0.41911764705882343,0,0.1111111111111111,0.1111111111111111,0.15384615384615385,0.14285714285714285,0,0,0,1,0,"['The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.', 'Table 2: Segmentation results by a pure subword-based IOB tagging.', 'If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.', 'In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.', 'subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.']"
19,W10-4135,N06-2049,0.2912621359223301,0.0,False,False,0,0.0273972602739726,0.1829025844930417,0.2125529457065846,0.46691176470588214,0,0.3333333333333333,0.33333333333333337,0.3247863247863248,0.42857142857142855,0,0,0,1,0,"['Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', 'The character-based IOB tagging approach has been widely used in Chinese word segmentation recently  .', 'While OOV recognition is very important in word segmentation, a higher IV rate is also desired.']"
20,W10-4138,N06-2049,0.40384615384615385,0.16025641025641027,False,False,0,0.017123287671232876,0.13121272365805162,0.130535232961109,0.28676470588235287,0,0.2222222222222222,0.16666666666666666,0.17094017094017094,0.2857142857142857,0,0,1,0,0,"['It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.', 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.']"
1,C02-1050,C00-2123,0.3,0.24019607843137256,False,False,0,0.017123287671232876,0.03578528827037773,0.03773584905660376,0.014705882352941176,0,0,0.05555555555555555,0.042735042735042736,0,0,0,0,1,0,"['Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.', 'In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.']"
2,C02-1050,C00-2123,0.06153846153846154,0.09803921568627451,True,False,0,0.04452054794520548,0.1332007952286282,0.11859838274932619,0.058823529411764705,0,0.3333333333333333,0.2222222222222222,0.26495726495726496,0.2857142857142857,0,0,0,1,0,"['For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.', 'The goal of machine translation is the translation of a text given in some source language into a target language.', 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an ecient search algorithm.']"
3,C02-1050,C00-2123,0.33076923076923076,0.9411764705882353,False,False,0,0,0.005964214711729622,0.015787447054293413,0.003676470588235294,0,0,0,0.08547008547008547,0,0,0,0,1,0,"['In this paper, we have presented a new, ecient DP-based search procedure for statistical machine translation.']"
4,C02-1050,C00-2123,0.6153846153846154,0.4019607843137255,True,False,0,0,0.009940357852882702,0.01039661147477859,0,0,0,0.05555555555555555,0.03418803418803419,0,0,0,0,1,0,"['The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.']"
5,C04-1091,C00-2123,0.08527131782945736,0.06862745098039216,True,False,0,0.02054794520547945,0.059642147117296214,0.07431651906045438,0.058823529411764705,0,0.1111111111111111,0.1111111111111111,0.1794871794871795,0.14285714285714285,0,0,0,1,0,"['In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.', 'The approach assumes that the word reordering is restricted to a few positions in the source sentence.']"
6,E06-1004,C00-2123,0.09170305676855896,0.5490196078431373,True,False,0,0.02054794520547945,0.10735586481113317,0.1089718906430497,0.03676470588235294,0,0.1111111111111111,0.16666666666666666,0.1282051282051282,0.14285714285714285,0,0,0,1,0,"['Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).', 'We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.', 'The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.']"
7,H01-1062,C00-2123,0.6243093922651933,0.0,False,False,0,0.0273972602739726,0.09145129224652088,0.09973045822102426,0.08088235294117647,0,0,0.1111111111111111,0.08547008547008547,0.14285714285714285,0,0,0,1,0,"['Word Re-ordering and DP-based Search in Statistical Machine Translation', 'Restrictions We compare our new approach with the word reordering used in the IBM translation approach  .']"
8,J03-1005,C00-2123,0.1539491298527443,0.9411764705882353,True,False,0,0.05136986301369863,0.11928429423459241,0.1517135155949173,0.08088235294117647,0,0.3333333333333333,0.2777777777777778,0.39316239316239315,0.42857142857142855,0,0,0,1,0,"['In this paper, we have presented a new, ecient DP-based search procedure for statistical machine translation.', 'Word Re-ordering and DP-based Search in Statistical Machine Translation', 'In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.', 'A search restriction especially useful for the translation direction from German to English is presented.', 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an ecient search algorithm.']"
9,J04-2003,C00-2123,0.078125,0.05392156862745098,True,False,0.047619047619047616,0.05821917808219178,0.18489065606361824,0.14824797843665777,0.05514705882352941,0,0.1111111111111111,0.1111111111111111,0.1794871794871795,0.14285714285714285,0,0,0,1,0,"['The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.', 'Our approach uses word-to-word dependencies between source and target words.', 'The goal of machine translation is the translation of a text given in some source language into a target language.', 'In this paper, we have presented a new, ecient DP-based search procedure for statistical machine translation.', 'Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.']"
11,J04-4002,C00-2123,0.55078125,0.8137254901960784,False,False,0,0.00684931506849315,0.02783300198807157,0.0281093569503273,0.014705882352941176,0,0.1111111111111111,0.1111111111111111,0.14529914529914528,0.2857142857142857,0,0,0,1,0,"['However there is no global pruning.', 'Our approach uses word-to-word dependencies between source and target words.']"
12,N03-1010,C00-2123,0.06611570247933884,0.004901960784313725,True,False,0.09523809523809523,0.03767123287671233,0.08548707753479125,0.10473623411628803,0.05514705882352941,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,0,1,0,"['In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).', 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an ecient search algorithm.', 'In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.']"
13,P01-1027,C00-2123,0.9407407407407408,0.9558823529411765,True,False,0.047619047619047616,0.0273972602739726,0.05964214711729621,0.03927608779360801,0.007352941176470588,0,0,0,0.13675213675213677,0.14285714285714285,0,0,0,1,0,"['Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.', 'For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.']"
14,P03-1039,C00-2123,0.6766467065868264,0.0,False,False,0,0.030821917808219176,0.12922465208747513,0.14016172506738558,0.08455882352941177,0,0.3333333333333333,0.2777777777777778,0.39316239316239315,0.42857142857142855,0,0,0,1,0,"['Word Re-ordering and DP-based Search in Statistical Machine Translation', 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an ecient search algorithm.', 'In this paper, we have presented a new, ecient DP-based search procedure for statistical machine translation.', 'The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.']"
15,P03-1039,C00-2123,0.718562874251497,0.029411764705882353,True,False,0,0.00684931506849315,0.055666003976143144,0.055448594532152465,0.011029411764705881,0,0.1111111111111111,0.1111111111111111,0.10256410256410256,0.14285714285714285,0,0,1,1,0,"['We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.', 'An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.']"
17,W01-1404,C00-2123,0.03355704697986577,0.6274509803921569,True,False,0.09523809523809523,0.0273972602739726,0.08548707753479125,0.04158644589911435,0.003676470588235294,0,0,0.05555555555555555,0.11965811965811966,0,0,0,1,0,0,"['During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.', 'Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.']"
18,W01-1407,C00-2123,0.7534246575342466,0.8333333333333334,True,False,0,0.0273972602739726,0.055666003976143144,0.08086253369272238,0.03676470588235294,0,0.1111111111111111,0.1111111111111111,0.14529914529914528,0.2857142857142857,0,0,0,1,0,"['We show translation results for three approaches: the monotone search  , the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.', 'In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.']"
19,W01-1408,C00-2123,0.2611111111111111,0.6715686274509803,False,False,0,0,0.003976143141153081,0.01386214863303812,0,0,0,0.05555555555555555,0.08547008547008547,0.14285714285714285,0,0,0,1,0,"['In this case, we have no finite-state restrictions for the search space.', 'We apply a beam search concept as in speech recognition.']"
20,W02-1020,C00-2123,0.3588235294117647,0.004901960784313725,False,False,0.09523809523809523,0.017123287671232876,0.021868787276341946,0.023873700423565652,0.003676470588235294,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,1,0,0,1,0,"['In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).']"
1,D07-1018,W03-0410,0.03765690376569038,0.9272727272727272,True,False,0.19047619047619047,0.2191780821917808,0.10337972166998008,0.10743165190604552,0.21323529411764702,0,0.3333333333333333,0.2777777777777778,0.28205128205128205,0.42857142857142855,0,0,0,1,0,"['Schulte im Walde and Brew   and Schulte im Walde  , on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.', '3.1 The Verb Classes.', 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.', 'On the other hand, in contrast to Schulte im Walde and Brew  , we demonstrated that accurate subcategorization statistics are unnecessary  .', 'In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.']"
2,D09-1138,W03-0410,0.09328358208955224,0.9163636363636364,True,False,0.23809523809523808,0.3047945205479453,0.24055666003976128,0.13515594917212173,0.35661764705882343,0,0.2222222222222222,0.2222222222222222,0.12820512820512822,0.2857142857142857,0,0,1,0,0,"['Using the same measure as ours, Stevenson and Merlo   achieved performance in clustering very close to that of their supervised classification.', 'We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson   to enable a comparison between the performance of the unsupervised and supervised methods.', 'We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  .', 'We have explored manual, unsupervised, and semisupervised methods for feature selection in a clustering approach for verb class discovery.', 'We focus here on extending the applicability of unsupervised methods, as in  , to the lexical semantic classification of verbs.']"
3,D09-1138,W03-0410,0.26119402985074625,0.7563636363636363,True,False,0.19047619047619047,0.29109589041095896,0.20079522862823054,0.13823642664613017,0.45955882352941135,0,0.3333333333333333,0.2777777777777778,0.358974358974359,0.5714285714285714,0,0,0,0,1,"['In the next two sections, we present unsupervised and minimally supervised approaches to this problem.', 'We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  .', 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.', 'Semi-supervised Verb Class Discovery Using Noisy Features', 'A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles  , selectional preferences  , and lexical semantic classification  .']"
4,D11-1095,W03-0410,0.06589147286821706,0.007272727272727273,False,False,0.047619047619047616,0.07534246575342465,0.04572564612326044,0.0346553715825953,0.10294117647058824,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,"['The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.']"
5,D11-1095,W03-0410,0.14728682170542637,0.6581818181818182,True,False,0,0.02054794520547945,0.017892644135188866,0.02695417789757412,0.022058823529411763,0,0,0.05555555555555555,0.06837606837606838,0.14285714285714285,0,0,0,1,0,"['The 13-way task includes all of our classes.', 'However, their study used a small set of five features manually devised for a set of three particular classes.']"
6,D11-1095,W03-0410,0.15503875968992248,0.4254545454545455,True,False,0.14285714285714285,0.2705479452054794,0.12127236580516898,0.09125914516750094,0.125,0,0,0,0.059829059829059825,0.2857142857142857,0,0,0,1,0,"['We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).', 'Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson  .', 'We began with this same set of 20 verbs per class for our current work.', 'To model this kind of approach, we selected a sample of five seed verbs from each class.', 'Our experimental verbs were selected as follows.']"
7,D11-1095,W03-0410,0.20930232558139536,0.08,True,False,0.42857142857142855,0.5376712328767125,0.1789264413518886,0.16519060454370432,0.3051470588235293,0,0.3333333333333333,0.2222222222222222,0.3162393162393162,0.5714285714285714,0,0,0,1,0,"['However, a general feature space means that most features will be irrelevant to any given verb discrimination task.', 'This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery.', 'We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  .', 'Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.', 'In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.']"
8,D11-1095,W03-0410,0.3178294573643411,0.96,True,False,0,0.07534246575342465,0.11928429423459241,0.07162110127069697,0.11029411764705883,0,0.1111111111111111,0.05555555555555555,0.059829059829059825,0.14285714285714285,0,0,1,0,0,"['We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.', 'The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).']"
9,D11-1095,W03-0410,0.6472868217054264,0.6836363636363636,True,False,0.047619047619047616,0.13356164383561644,0.1172962226640159,0.09202926453600309,0.1433823529411765,0,0,0.05555555555555555,0.10256410256410256,0.2857142857142857,0,0,0,1,0,"['The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).', '(See Table 3 for the number of features in the Ling and Seed sets.)', 'Semi-supervised Verb Class Discovery Using Noisy Features']"
10,D11-1095,W03-0410,0.6511627906976745,0.6836363636363636,True,False,0,0.09246575342465753,0.09145129224652088,0.07162110127069694,0.08455882352941177,0,0,0,0.059829059829059825,0.2857142857142857,0,0,0,1,0,"['The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).', 'Table 1 above shows the number of verbs in each class at the end of this process.', 'Table 2: Experimental Results.']"
11,D11-1095,W03-0410,0.6550387596899225,0.8290909090909091,True,False,0.047619047619047616,0.1678082191780822,0.1133200795228628,0.08432807085098193,0.10661764705882354,0,0,0,0.05128205128205128,0.14285714285714285,0,0,0,1,0,"['Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.']"
12,J06-2001,W03-0410,0.5872781065088757,0.5163636363636364,True,False,0.047619047619047616,0.11643835616438356,0.033797216699801194,0.031959953792837885,0.04411764705882353,0,0.1111111111111111,0.1111111111111111,0.11965811965811965,0.14285714285714285,0,0,0,1,0,"['4.2.1 Accuracy We can assign each cluster the class label of the majority of its members.', 'Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  .']"
13,J06-2001,W03-0410,0.8668639053254438,0.9490909090909091,False,False,0.19047619047619047,0.3116438356164384,0.19284294234592436,0.1532537543319216,0.19117647058823528,0,0.2222222222222222,0.16666666666666666,0.15384615384615385,0.2857142857142857,0,0,0,1,0,"['We have explored manual, unsupervised, and semisupervised methods for feature selection in a clustering approach for verb class discovery.', '  propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.', 'In this paper, we report results on several feature selection approaches to the problem: manual selection  , and a semisupervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).', 'Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard  .']"
15,P03-1009,W03-0410,0.6458333333333334,0.5236363636363637,True,False,0,0.010273972602739725,0.019880715705765408,0.03658067000385058,0.018382352941176468,0,0,0,0.07692307692307693,0.14285714285714285,0,0,0,1,0,"['Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.', 'We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.', 'We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated.']"
16,P03-1009,W03-0410,0.7447916666666666,0.20363636363636364,True,False,0.42857142857142855,0.2705479452054795,0.10536779324055665,0.07585675779745862,0.36029411764705854,0,0,0,0.008547008547008548,0.14285714285714285,0,0,0,0,1,"['For example, some classes differ in both their semantic roles and frames, while others have the same roles in different frames, or different roles in the same frames.1 Here we summarize the argument structure distinctions between the classes; Table 1 below lists the classes with their Levin class numbers.', 'We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson   to enable a comparison between the performance of the unsupervised and supervised methods.']"
17,P04-2007,W03-0410,0.0763888888888889,0.9163636363636364,False,False,0.09523809523809523,0.0821917808219178,0.10735586481113318,0.05544859453215247,0.1691176470588235,0,0,0.05555555555555555,0.025641025641025644,0.14285714285714285,0,0,0,1,0,"['Using the same measure as ours, Stevenson and Merlo   achieved performance in clustering very close to that of their supervised classification.', 'We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson   to enable a comparison between the performance of the unsupervised and supervised methods.']"
20,P07-3016,W03-0410,0.2671232876712329,0.9490909090909091,False,False,0.2857142857142857,0.6609589041095895,0.17693836978131205,0.13554100885637282,0.2058823529411764,0,0.2222222222222222,0.2222222222222222,0.11965811965811966,0.14285714285714285,0,1,0,0,0,"['We have explored manual, unsupervised, and semisupervised methods for feature selection in a clustering approach for verb class discovery.', 'However, a general feature space means that most features will be irrelevant to any given verb discrimination task.', 'In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to the curse of dimensionality?', 'More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering.', 'Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.']"
21,W06-2910,W03-0410,0.03888888888888889,0.9490909090909091,False,False,0.2857142857142857,0.5000000000000001,0.2385685884691848,0.17674239507123612,0.2536764705882352,0,0.4444444444444444,0.2777777777777778,0.26495726495726496,0.2857142857142857,1,0,0,1,0,"['We have explored manual, unsupervised, and semisupervised methods for feature selection in a clustering approach for verb class discovery.', 'However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.', 'Rather than trying to separate a set of new verbs into coherent clusters, we suggest that it may be useful to perform a nearest-neighbour type of classification using a seed set, asking for each new verb is it like these or not? In some ways our current clustering task is too easy, because all of the verbs are from one of the target classes.', 'In contrast to Merlo and Stevenson  , we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.', 'In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.']"
22,W06-2910,W03-0410,0.07222222222222222,0.12727272727272726,False,False,0.14285714285714285,0.10616438356164383,0.01789264413518887,0.021563342318059297,0.04044117647058823,0,0,0,0.008547008547008548,0,0,0,0,1,0,['Levins classes form a hierarchy of verb groupings with shared meaning and syntax.']
23,W06-2910,W03-0410,0.6555555555555556,0.5454545454545454,True,False,0,0.00684931506849315,0.011928429423459244,0.014247208317289178,0.007352941176470588,0,0,0,0.02564102564102564,0,0,0,0,1,0,"['4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.']"
24,E09-1072,W03-0410,0.4,0.06181818181818182,True,False,0.09523809523809523,0.1404109589041096,0.08946322067594432,0.06160954948016942,0.20588235294117643,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,0,0,1,"['We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification  .', 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.']"
1,C08-1107,I05-5011,0.08490566037735849,0.784037558685446,True,False,0,0.017123287671232876,0.09741550695825049,0.0797073546399692,0.07352941176470588,0,0,0.05555555555555555,0.10256410256410256,0,0,0,0,1,0,"['Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].', 'We proposed an unsupervised method to discover paraphrases from a large untagged corpus.']"
2,C10-2017,I05-5011,0.05581395348837209,0.06103286384976526,False,False,0,0.010273972602739725,0.021868787276341943,0.010396611474778591,0.025735294117647058,0,0.1111111111111111,0.1111111111111111,0.09401709401709402,0.14285714285714285,0,0,1,1,0,"['For example, in Information Retrieval (IR), we have to match a users query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the users question even if the formulation of the answer in the document is different from the question.']"
3,D12-1094,I05-5011,0.19943019943019943,0.9530516431924883,False,False,0.09523809523809523,0.02054794520547945,0.02783300198807157,0.03350019252984212,0.014705882352941176,0,0,0.16666666666666666,0.06837606837606838,0.14285714285714285,0,0,0,1,0,"['While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.', 'Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs']"
4,E09-1025,I05-5011,0.1267605633802817,0.18779342723004694,True,False,0,0,0.015904572564612324,0.006546014632268001,0,0,0,0.05555555555555555,0.017094017094017096,0,0,0,0,1,0,['The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].']
5,I08-1070,I05-5011,0.24311926605504589,0.014084507042253521,False,False,0,0,0.031809145129224656,0.028494416634578353,0,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,0,1,0,"['We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.', 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.']"
6,J10-3003,I05-5011,0.5528350515463918,0.5117370892018779,True,False,0,0.02054794520547945,0.049701789264413515,0.03311513284559106,0.03308823529411765,0,0,0.05555555555555555,0.07692307692307693,0,0,0,0,1,0,"['Among these 32 sets, we found the following pairs of sets which have two or more links.', 'As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation.']"
7,P06-2094,I05-5011,0.3282828282828283,0.9577464788732394,False,False,0,0.06164383561643835,0.23658051689860818,0.16172506738544482,0.13235294117647056,0,0.3333333333333333,0.2222222222222222,0.3418803418803419,0.2857142857142857,0,0,0,1,0,"['We proposed an unsupervised method to discover paraphrases from a large untagged corpus.', 'In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.', 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.', 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.', 'We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.']"
8,P07-1058,I05-5011,0.23671497584541062,0.9154929577464789,False,False,0,0.017123287671232876,0.06759443339960237,0.03927608779360801,0.022058823529411766,0,0,0.16666666666666666,0.03418803418803419,0,0,0,0,1,0,"['Also, the method of using keywords rules out phrases which dont contain popular words in the domain.', 'However, there are phrases which express the same meanings even though they do not share the same keyword.']"
9,P07-1058,I05-5011,0.2753623188405797,0.41784037558685444,True,False,0.09523809523809523,0.023972602739726026,0.03180914512922465,0.023873700423565652,0.011029411764705881,0,0,0.05555555555555555,0.1111111111111111,0.14285714285714285,0,0,0,1,0,"['Extract NE pair instances with contexts From the four years of newspaper corpus, we extracted 1.9 million pairs of NE instances.', 'Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs']"
10,P07-1058,I05-5011,0.32367149758454106,0.9154929577464789,False,False,0,0.003424657534246575,0.09741550695825046,0.0542934154793993,0.04044117647058824,0,0,0.16666666666666666,0.03418803418803419,0,0,0,1,0,0,"['Also, the method of using keywords rules out phrases which dont contain popular words in the domain.', 'This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited.']"
11,P09-2063,I05-5011,0.08461538461538462,0.0,False,False,0.09523809523809523,0.017123287671232876,0.017892644135188866,0.013862148633038121,0.007352941176470588,0,0,0.05555555555555555,0.05128205128205128,0.14285714285714285,1,0,0,1,0,['Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs']
12,P11-1109,I05-5011,0.8014705882352942,0.9248826291079812,True,False,0,0.00684931506849315,0.12127236580516895,0.04851752021563341,0.06985294117647059,0,0,0.16666666666666666,0.09401709401709402,0,0,0,1,0,0,"['Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.', 'As can be seen in Figure 3, the phrases in the agree set include completely different relationships, which are not paraphrases.', 'Limitations There are several limitations in the methods.', 'If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.']"
14,P12-2031,I05-5011,0.07627118644067797,0.7230046948356808,True,False,0,0.003424657534246575,0.025844930417495027,0.010011551790527531,0.003676470588235294,0,0,0.1111111111111111,0.03418803418803419,0,0,0,1,0,0,"['The work reported here is closely related to [Hasegawa et al. 04].', 'The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].']"
15,P12-2031,I05-5011,0.1694915254237288,0.2300469483568075,True,False,0.09523809523809523,0.06506849315068493,0.15904572564612318,0.08625336927223722,0.07352941176470588,0,0.1111111111111111,0.16666666666666666,0.1111111111111111,0.14285714285714285,0,0,1,0,0,"['keywords Step 3 Sets of phrases based on keywords Step 4 Links between sets of phrases All the contexts collected for a given domain are gathered in a bag and the TF/ITF scores are calculated for all the words except stopwords in the bag.', 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.']"
16,P13-1131,I05-5011,0.6175115207373272,0.22065727699530516,False,False,0,0.0136986301369863,0.049701789264413515,0.05544859453215248,0.0661764705882353,0,0,0.16666666666666666,0.03418803418803419,0,0,0,0,1,0,"['Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword.', 'It is a relatively frequent word in the domain, but it can be used in different extraction scenarios.']"
17,W12-4006,I05-5011,0.07526881720430108,0.0,False,False,0.09523809523809523,0.017123287671232876,0.03578528827037773,0.029264536003080478,0.011029411764705881,0,0,0.05555555555555555,0.1111111111111111,0.14285714285714285,0,0,0,1,0,"['Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs', ""For example, the phrase 's New York-based trust unit, is not a paraphrase of the other phrases in the unit set.""]"
18,W12-4006,I05-5011,0.22043010752688172,0.014084507042253521,False,False,0,0.0273972602739726,0.12922465208747516,0.09626492106276476,0.03676470588235294,0,0.3333333333333333,0.3333333333333333,0.4188034188034188,0.2857142857142857,0,0,0,1,0,"['We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.', 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.', 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.', 'Also, the method of using keywords rules out phrases which dont contain popular words in the domain.', 'Evaluation of links A link between two sets is considered correct if the majority of phrases in both sets have the same meaning, i.e. if the link indicates paraphrase.']"
1,A00-1020,P98-2143,0.12962962962962962,0.6624203821656051,True,False,0.047619047619047616,0.05821917808219178,0.10139165009940354,0.091259145167501,0.20220588235294107,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,0,1,"['From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.', 'Given that our knowledge-poor approach is basically an enhancement of a baseline model through a set of antecedent indicators, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.']"
2,C02-1027,P98-2143,0.0962962962962963,0.9808917197452229,False,False,0.09523809523809523,0.08561643835616438,0.23459244532803167,0.14439738159414717,0.1948529411764706,0,0.2222222222222222,0.2222222222222222,0.47008547008547,0.42857142857142855,0,0,0,1,0,"['We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'Robust pronoun resolution with limited knowledge', 'With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', 'From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.']"
3,C02-1027,P98-2143,0.48148148148148145,0.01910828025477707,False,False,0,0.0136986301369863,0.05964214711729622,0.0608394301116673,0.044117647058823525,0,0.2222222222222222,0.1111111111111111,0.24786324786324787,0.14285714285714285,0,0,0,1,0,"['This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', 'We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.']"
4,C02-1027,P98-2143,0.5259259259259259,0.10828025477707007,True,False,0,0.0136986301369863,0.10139165009940355,0.10858683095879865,0.058823529411764705,0,0,0,0.15384615384615385,0,0,0,0,1,0,"['The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the remaining candidates (see next section).', 'We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.']"
5,C02-1027,P98-2143,0.5851851851851851,0.08917197452229299,True,False,0,0.00684931506849315,0.021868787276341946,0.03619561031959953,0.03308823529411765,0,0.1111111111111111,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,"['Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).', '2.1 Antecedent indicators.']"
6,C04-1074,P98-2143,0.23809523809523808,0.8535031847133758,True,False,0,0.017123287671232876,0.07355864811133198,0.1005005775895264,0.05882352941176471,0,0,0.05555555555555555,0.17094017094017094,0.14285714285714285,0,0,0,1,0,"[""The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate."", 'The heuristics used is that in constructions of the form ""...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)"", where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun ""it"" immediately following V2 and is therefore given preference (scores 2 and 0).']"
7,C04-1075,P98-2143,0.10052910052910052,0.9808917197452229,False,False,0,0.0136986301369863,0.07554671968190851,0.08009241432422036,0.044117647058823525,0,0.3333333333333333,0.2222222222222222,0.3418803418803419,0.2857142857142857,0,0,0,1,0,"['We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', 'While various alternatives have been proposed, making use of e.g. neural networks, a situation semantics framework, or the principles of reasoning with uncertainty  , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic processing of growing language resources.']"
8,C04-1143,P98-2143,0.6733333333333333,0.7707006369426752,True,False,0,0.00684931506849315,0.03578528827037773,0.058914131690412,0.014705882352941176,0,0,0.05555555555555555,0.17094017094017094,0.14285714285714285,0,0,0,1,0,"['A case where the system failed was when the anaphor and the antecedent were in the same sentence and where preference was given to a candidate in the preceding sentence.', 'Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we con-sider it as the preferred candidate (1, 0).']"
9,D09-1101,P98-2143,0.20553359683794467,0.4840764331210191,True,False,0,0.0136986301369863,0.07952286282306162,0.1370812475933771,0.014705882352941176,0,0,0.05555555555555555,0.17094017094017094,0.14285714285714285,0,0,0,1,0,"['tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric ""it"" occurring in constructions such as ""It is important"", ""It is necessary"" is eliminated by a ""referential filter"" 5Note that this restriction may not always apply in languages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. ""government"", ""team"", ""parliament"" etc. can be referred to by ""they""; equally some plural nouns (e.g. ""data"") can be referred to by ""it"") and are exempted from the agreement test.', 'Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).']"
11,E99-1031,P98-2143,0.91,0.1592356687898089,False,False,0,0.003424657534246575,0.017892644135188863,0.054293415479399296,0.025735294117647058,0,0,0,0.06837606837606838,0,0,0,0,1,0,"['The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, ""non-prepositional"" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.', 'We should point out that the antecedent indicators are preferences and not absolute factors.']"
12,H05-1001,P98-2143,0.3575757575757576,0.445859872611465,True,False,0,0.00684931506849315,0.015904572564612324,0.010011551790527531,0,0,0,0,0.06837606837606838,0,0,0,0,1,0,['The algorithm for pronoun resolution can be described informally as follows: 1.']
13,H05-1001,P98-2143,0.3696969696969697,0.10191082802547771,True,False,0.047619047619047616,0.03767123287671233,0.22266401590457247,0.21640354254909536,0.1286764705882353,0,0.2222222222222222,0.1111111111111111,0.3162393162393162,0.14285714285714285,0,0,0,1,0,"['It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as ""antecedent indicators"").', 'We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the remaining candidates (see next section).', 'With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.']"
14,H05-1001,P98-2143,0.38181818181818183,0.10828025477707007,True,False,0,0.02054794520547945,0.4075546719681913,0.3996919522525995,0.1874999999999999,0,0.2222222222222222,0.16666666666666666,0.3418803418803419,0.2857142857142857,0,0,0,1,0,"['The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the remaining candidates (see next section).', 'Similarly to the evaluation for English, we compared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.', 'In order to evaluate the effectiveness of the approach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', 'Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).', 'Input is checked against agreement and for a number of antecedent indicators.']"
17,I05-2040,P98-2143,0.08928571428571429,0.050955414012738856,False,False,0.047619047619047616,0.0273972602739726,0.10337972166998008,0.05082787832113976,0.044117647058823525,0,0.1111111111111111,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,"['For the most part, anaphora resolution has focused on traditional linguistic methods  .', 'With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.']"
19,I08-3014,P98-2143,0.08943089430894309,0.08917197452229299,False,False,0,0.0136986301369863,0.18290258449304161,0.14670773969965348,0.09558823529411765,0,0.1111111111111111,0.16666666666666666,0.19658119658119658,0.2857142857142857,0,0,0,1,0,"['Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).', 'In order to evaluate the effectiveness of the approach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.']"
20,I08-3014,P98-2143,0.16666666666666666,0.006369426751592357,False,False,0,0.010273972602739725,0.05765407554671968,0.022718521370812473,0.011029411764705881,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,0,1,0,"['Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', 'For the most part, anaphora resolution has focused on traditional linguistic methods  .']"
21,J01-4003,P98-2143,0.013477088948787063,0.6242038216560509,True,False,0,0.00684931506849315,0.176938369781312,0.12244897959183676,0.06985294117647059,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,1,1,0,"['In order to evaluate the effectiveness of the approach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', 'The resolution of anaphors was carried out with a success rate of 95.8%.']"
22,J01-4005,P98-2143,0.7310344827586207,0.6624203821656051,False,False,0.047619047619047616,0.0547945205479452,0.09343936381709739,0.06160954948016941,0.1323529411764706,0,0.2222222222222222,0.16666666666666666,0.26495726495726496,0.2857142857142857,0,0,0,1,0,"['From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.', 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.']"
23,J01-4006,P98-2143,0.24193548387096775,0.5987261146496815,True,False,0,0,0,0.004620716211012708,0,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,0,['There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.']
24,J02-1001,P98-2143,0.020833333333333332,0.6242038216560509,False,False,0.047619047619047616,0.0273972602739726,0.2425447316103378,0.15864458991143637,0.11029411764705883,0,0,0.05555555555555555,0.17094017094017094,0.14285714285714285,0,0,1,0,0,"['In order to evaluate the effectiveness of the approach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', 'With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.']"
25,J05-3004,P98-2143,0.010043041606886656,0.050955414012738856,False,False,0,0.010273972602739725,0.03777335984095427,0.02310358105506353,0.007352941176470588,0,0.1111111111111111,0.16666666666666666,0.19658119658119658,0.2857142857142857,1,0,0,0,1,"['For the most part, anaphora resolution has focused on traditional linguistic methods  .', 'For this purpose we have drawn up a comprehensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolution has addressed the problem of ""agreement exceptions"".']"
26,N01-1008,P98-2143,0.13846153846153847,0.09554140127388536,False,False,0.047619047619047616,0.023972602739726026,0.09940357852882702,0.06969580284944168,0.0625,0,0.2222222222222222,0.1111111111111111,0.23076923076923078,0.14285714285714285,0,0,0,1,1,"['With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.']"
27,N01-1008,P98-2143,0.9692307692307692,0.9808917197452229,False,False,0.047619047619047616,0.030821917808219176,0.11530815109343931,0.07277628032345018,0.0625,0,0,0,0.15384615384615385,0,0,0,0,1,0,"['We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.']"
28,N04-1004,P98-2143,0.17374517374517376,0.09554140127388536,False,False,0.047619047619047616,0.08561643835616438,0.1272365805168986,0.09780515979976902,0.11029411764705883,0,0,0.05555555555555555,0.1794871794871795,0.14285714285714285,0,0,0,1,1,"['With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'Our preference-based approach showed clear superiority over both baseline models.', 'Usually knowledge-based approaches have difficulties in such a situation because they use preferences such as ""syntactic parallelism"" or ""semantic parallelism"".', 'Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syntactic function/semantic role of each individual word.']"
29,P00-1022,P98-2143,0.1210762331838565,0.6624203821656051,False,False,0.047619047619047616,0.05136986301369863,0.06163021868787276,0.048902579899884474,0.125,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,0,['From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.']
30,P00-1022,P98-2143,0.7982062780269058,0.9872611464968153,True,False,0,0.00684931506849315,0.039761431411530816,0.03773584905660377,0.014705882352941176,0,0,0.05555555555555555,0.18803418803418803,0.14285714285714285,0,0,0,1,1,"['Evaluation shows a success rate of 89.7% for the genre of technical manuals and at least in this genre, the approach appears to be more successful than other similar methods.', 'Our evaluation established the critical success rate as 82%.', 'The evaluation indicated 83.6% success rate.', 'The resolution of anaphors was carried out with a success rate of 95.8%.']"
31,P00-1022,P98-2143,0.9372197309417041,0.0,False,False,0,0.02054794520547945,0.14115308151093434,0.08663842895648831,0.03676470588235294,0,0.3333333333333333,0.33333333333333337,0.4957264957264957,0.5714285714285714,0,0,0,1,0,"['Robust pronoun resolution with limited knowledge', 'We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linguistic knowledge  .', 'For this purpose we have drawn up a comprehensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolution has addressed the problem of ""agreement exceptions"".', 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.']"
32,P01-1006,P98-2143,0.4482758620689655,0.9808917197452229,False,False,0.047619047619047616,0.03424657534246575,0.14711729622266392,0.08548324990373515,0.06985294117647059,0,0.2222222222222222,0.1111111111111111,0.3162393162393162,0.14285714285714285,0,0,0,1,0,"['We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.']"
33,P01-1006,P98-2143,0.5655172413793104,0.9808917197452229,True,False,0,0.0273972602739726,0.2743538767395626,0.24181748170966522,0.20588235294117638,0,0.2222222222222222,0.16666666666666666,0.41880341880341876,0.2857142857142857,0,0,0,1,0,"['We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'Given that our knowledge-poor approach is basically an enhancement of a baseline model through a set of antecedent indicators, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.', 'This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', 'In order to evaluate the effectiveness of the approach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', 'Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.']"
34,P01-1006,P98-2143,0.05517241379310345,0.01910828025477707,False,False,0,0.003424657534246575,0.021868787276341946,0.028879476318829415,0.022058823529411763,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,"['This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.']"
35,P03-1023,P98-2143,0.048327137546468404,0.1464968152866242,True,False,0.047619047619047616,0.0684931506849315,0.17495029821073552,0.1540238737004236,0.23897058823529407,0,0,0.1111111111111111,0.30769230769230765,0.2857142857142857,0,0,0,1,0,"['Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.', 'Given that our knowledge-poor approach is basically an enhancement of a baseline model through a set of antecedent indicators, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.', 'Robust pronoun resolution with limited knowledge', 'We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.']"
36,P04-1017,P98-2143,0.9455445544554455,0.9044585987261147,True,False,0,0.010273972602739725,0.07355864811133198,0.10165575664227958,0.05514705882352941,0,0,0.05555555555555555,0.1111111111111111,0.14285714285714285,0,0,0,1,0,"['We used the robust approach as a basis for developing a genre-specific reference resolution approach in Polish.', 'In terms of frequency of use (""number of nonzero applications""/""number of anaphors""), the most frequently used indicator proved to be referential distance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reiteration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and collocation (11.1%).']"
37,P04-1018,P98-2143,0.036036036036036036,0.6242038216560509,True,False,0,0.0547945205479452,0.2266401590457255,0.2291105121293802,0.09191176470588236,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,0,"['In order to evaluate the effectiveness of the approach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', 'The aggregate score for ""the drawer"" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate reference 2 = 7), whereas aggregate score for the most recent matching noun phrase (""the lit paper port LED"") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).']"
38,P05-1021,P98-2143,0.07555555555555556,0.0,False,False,0,0.003424657534246575,0.057654075546719676,0.022718521370812473,0.007352941176470588,0,0.2222222222222222,0.16666666666666666,0.2136752136752137,0.2857142857142857,0,0,1,0,0,"['Robust pronoun resolution with limited knowledge', 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.']"
39,P05-1021,P98-2143,0.4311111111111111,0.7707006369426752,False,False,0,0.003424657534246575,0.025844930417495027,0.026954177897574122,0.014705882352941176,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,0,['A case where the system failed was when the anaphor and the antecedent were in the same sentence and where preference was given to a candidate in the preceding sentence.']
40,P06-1006,P98-2143,0.05581395348837209,0.5732484076433121,True,False,0,0.0136986301369863,0.073558648111332,0.08355795148247981,0.05882352941176471,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,0,"['In fact, our evaluation shows that the results are comparable to syntax-based methods (Lappin & Leass I994).', 'Given that our approach is robust and returns antecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC\'s ""resolve all"" version by simulating it manually on the same training data used in evaluation B above.']"
41,P07-1068,P98-2143,0.02403846153846154,0.006369426751592357,False,False,0,0.00684931506849315,0.03578528827037774,0.020793222949557176,0.022058823529411763,0,0.2222222222222222,0.16666666666666666,0.26495726495726496,0.2857142857142857,0,0,0,1,0,"['Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', 'Usually knowledge-based approaches have difficulties in such a situation because they use preferences such as ""syntactic parallelism"" or ""semantic parallelism"".']"
43,P13-3012,P98-2143,0.05102040816326531,0.09554140127388536,False,False,0.047619047619047616,0.02054794520547945,0.1033797216699801,0.05082787832113977,0.04044117647058823,0,0,0.05555555555555555,0.11965811965811966,0.14285714285714285,0,0,0,1,0,"['With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'Robust pronoun resolution with limited knowledge']"
44,S10-1019,P98-2143,0.017167381974248927,0.050955414012738856,True,False,0,0.0136986301369863,0.04771371769383698,0.028879476318829415,0.01838235294117647,0,0.1111111111111111,0.16666666666666666,0.19658119658119658,0.2857142857142857,0,0,0,1,0,"['For the most part, anaphora resolution has focused on traditional linguistic methods  .', 'In fact, our evaluation shows that the results are comparable to syntax-based methods (Lappin & Leass I994).']"
45,W01-0704,P98-2143,0.6890756302521008,0.09554140127388536,False,False,0.14285714285714285,0.11301369863013698,0.1371769383697813,0.11012706969580291,0.1360294117647059,0,0,0.05555555555555555,0.17094017094017094,0.14285714285714285,0,0,0,1,1,"['With a view to avoiding complex syntactic, semantic and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realistic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.']"
47,W04-0707,P98-2143,0.8043478260869565,0.445859872611465,True,False,0,0.00684931506849315,0.041749502982107355,0.020023103581055062,0,0,0,0.05555555555555555,0.11965811965811966,0.14285714285714285,0,0,0,1,0,"['The algorithm for pronoun resolution can be described informally as follows: 1.', 'Robust pronoun resolution with limited knowledge']"
49,W04-2310,P98-2143,0.8793103448275862,0.6624203821656051,False,False,0.047619047619047616,0.0547945205479452,0.06560636182902584,0.05968425105891412,0.125,0,0.2222222222222222,0.16666666666666666,0.26495726495726496,0.2857142857142857,0,0,0,1,0,"['From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.', 'Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.']"
50,W04-2310,P98-2143,0.09482758620689655,0.006369426751592357,False,False,0,0.010273972602739725,0.08946322067594432,0.06353484790142472,0.044117647058823525,0,0.2222222222222222,0.1111111111111111,0.17094017094017094,0.14285714285714285,0,0,1,0,0,"['Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', 'We used the robust approach as a basis for developing a genre-specific reference resolution approach in Polish.']"
51,W06-2302,P98-2143,0.2513089005235602,0.5414012738853503,False,False,0.09523809523809523,0.11301369863013698,0.13717693836978126,0.1505583365421641,0.15808823529411764,0,0,0.05555555555555555,0.18803418803418803,0.14285714285714285,0,0,0,1,0,"['For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realistic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.', ""The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate."", 'We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.']"
52,W09-2411,P98-2143,0.04081632653061224,0.006369426751592357,False,False,0,0.003424657534246575,0.031809145129224656,0.012706969580284944,0.007352941176470588,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,['Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.']
53,W99-0104,P98-2143,0.22014925373134328,0.03184713375796178,False,False,0,0.00684931506849315,0.03578528827037773,0.02348864073931459,0.007352941176470588,0,0.2222222222222222,0.1111111111111111,0.1623931623931624,0.14285714285714285,0,0,0,1,0,"['Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.', 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.']"
54,W99-0104,P98-2143,0.22388059701492538,0.8471337579617835,True,False,0,0.06164383561643835,0.1133200795228628,0.1717366191759724,0.0661764705882353,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,0,"['tive evaluation of Breck Baldwin\'s CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin\'s CogNIAC   approach which features ""high precision coreference with limited knowledge and linguistics resources"".', 'The aggregate score for ""the drawer"" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate reference 2 = 7), whereas aggregate score for the most recent matching noun phrase (""the lit paper port LED"") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).']"
55,W99-0207,P98-2143,0.8421052631578947,0.6305732484076433,True,False,0,0,0.003976143141153081,0.0030804774740084712,0,0,0,0.05555555555555555,0.10256410256410256,0.14285714285714285,0,0,0,1,1,"['The success rate of the ""Baseline Subject"" was 29.2%, whereas the success rate of ""Baseline Most Recent NP"" was 62.5%.']"
56,W99-0207,P98-2143,0.8618421052631579,0.006369426751592357,False,False,0,0.0273972602739726,0.08151093439363816,0.025028879476318825,0.025735294117647058,0,0.3333333333333333,0.2222222222222222,0.2564102564102564,0.2857142857142857,0,0,1,0,0,"['Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', 'It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.']"
