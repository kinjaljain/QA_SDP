<PAPER>
  <S sid="0">Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006).</S>
    <S sid="2" ssid="2">In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG.</S>
    <S sid="3" ssid="3">We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size.</S>
    <S sid="4" ssid="4">Empirical results demonstrate the utility of our methods in predicting human reading times.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998).</S>
    <S sid="6" ssid="2">Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples.</S>
    <S sid="7" ssid="3">Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples.</S>
    <S sid="8" ssid="4">More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation.</S>
    <S sid="9" ssid="5">For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence processing (Hale, 2001; 2003; 2006).</S>
    <S sid="10" ssid="6">These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived.</S>
    <S sid="11" ssid="7">This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations.</S>
    <S sid="12" ssid="8">The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently.</S>
    <S sid="13" ssid="9">Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance.</S>
    <S sid="14" ssid="10">Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al. (2007) incremental dependency parser (Boston et al., 2008a; 2008b).</S>
    <S sid="15" ssid="11">Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise.</S>
    <S sid="16" ssid="12">There is no single measure that can account for all of the factors influencing human sentence processing performance, and some of the most recent work on using parser-derived measures for psycholinguistic modeling has looked to try to derive multiple, complementary measures.</S>
    <S sid="17" ssid="13">One of the key distinctions being looked at is syntactic versus lexical expectations (Gibson, 2006).</S>
    <S sid="18" ssid="14">For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags).</S>
    <S sid="19" ssid="15">Boston et al. (2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007).</S>
    <S sid="20" ssid="16">As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the &#8220;syntactic&#8221; expectations, such as subcategorization preferences of particular verbs, which are generally accepted to impact syntactic expectations in human sentence processing (Garnsey et al., 1997).</S>
    <S sid="21" ssid="17">Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicalized surprisal.</S>
    <S sid="22" ssid="18">Here we present a novel method for deriving separate syntactic and lexical surprisal measures from a fully lexicalized incremental parser, to allow for rich probabilistic grammars to be used to derive either measure, and demonstrate the utility of this method versus that of Demberg and Keller in empirical trials.</S>
    <S sid="23" ssid="19">The use of large-scale lexicalized grammars presents a problem for using an Earley parser to derive surprisal or for the calculation of entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set.</S>
    <S sid="24" ssid="20">With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion.</S>
    <S sid="25" ssid="21">The use of an incremental, beam-search parser provides a tractable approximation to both measures.</S>
    <S sid="26" ssid="22">Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores.</S>
    <S sid="27" ssid="23">In addition to teasing apart syntactic and lexical surprisal (defined explicitly in &#167;3), we present an approximation to the full entropy that Hale (2003; 2006) used to define the entropy reduction hypothesis.</S>
    <S sid="28" ssid="24">Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in &#167;3.3.</S>
    <S sid="29" ssid="25">We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal.</S>
    <S sid="30" ssid="26">The purpose of this paper is threefold.</S>
    <S sid="31" ssid="27">First, to present a careful and well-motivated decomposition of lexical and syntactic expectation-based measures from a given lexicalized PCFG.</S>
    <S sid="32" ssid="28">Second, to explicitly document methods for calculating these and other measures from a specific incremental parser.</S>
    <S sid="33" ssid="29">And finally, to present some empirical validation of the novel measures from real reading time trials.</S>
    <S sid="34" ssid="30">We modified the Roark (2001) parser to calculate the discussed measures1, and the empirical results in &#167;4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time.</S>
    <S sid="35" ssid="31">2 Notation and preliminaries For a given rule A &#8212;* &#945; E P, let the function RHS return the right-hand side of the rule, i.e., RHS(A &#8212;* &#945;) = &#945;.</S>
    <S sid="36" ssid="32">Without loss of generality, we will assume that for every rule A &#8212;* &#945; E P, one of two cases holds: either RHS(A &#8212;* &#945;) E T or RHS(A &#8212;* &#945;) E V *.</S>
    <S sid="37" ssid="33">That is, the right-hand side sequences consist of either (1) exactly one terminal item, or (2) zero or more non-terminals.</S>
    <S sid="38" ssid="34">Let W E Tn be a terminal string of length n, i.e., W = W1 ... Wn and JWJ = n. Let W [i, j] denote the substring beginning at word Wi and ending at word Wj of the string.</S>
    <S sid="39" ssid="35">Then W|W |is the last word in the string, and W [1, JW J] is the string as a whole.</S>
    <S sid="40" ssid="36">Adjacent strings represent concatenation, i.e., W [1, i]W [i+1, j] = W [1, j].</S>
    <S sid="41" ssid="37">Thus W [1, i]w represents the string where Wi+1 = w. We can define a &#8220;derives&#8221; relation (denoted ==&gt;'G for a given PCFG G) as follows: QA-y ==&gt;'G Q&#945;-y if and only if A &#8212;* &#945; E P. A string W E T* is in the language of a grammar G if and only if 5&#8224; +==&gt;'G W, i.e., a sequence of one or more derivation steps yields the string from the start non-terminal.</S>
    <S sid="42" ssid="38">A leftmost derivation begins with 5&#8224; and each derivation step replaces the leftmost non-terminal A in the yield with some &#945; such that A &#8212;* &#945; E P. For a leftmost derivation 5&#8224; *&#65533;G &#945;, where &#945; E (V U T)*, the sequence of derivation steps that yield &#945; can be represented as a tree, with the start symbol 5&#8224; at the root, and the &#8220;yield&#8221; sequence &#945; at the leaves of the tree.</S>
    <S sid="43" ssid="39">A complete tree has only terminal items in the yield, i.e., &#945; E T*; a partial tree has some non-terminal items in the yield.</S>
    <S sid="44" ssid="40">With a leftmost derivation, the yield &#945; = Q-y partitions into an initial sequence of terminals Q E T* followed by a sequence of non-terminals -y E V *.</S>
    <S sid="45" ssid="41">For a complete derivation, -y = E; for a partial derivation -y E V +, i.e., one or more non-terminals.</S>
    <S sid="46" ssid="42">Let T (G, W[1, i]) be the set of complete trees with W[1, i] as the yield of the tree, given PCFG G. A leftmost derivation D consists of a sequence of |D |steps.</S>
    <S sid="47" ssid="43">Let Di represent the ith step in the derivation D, and D[i, j] represent the subsequence of steps in D beginning with Di and ending with Dj.</S>
    <S sid="48" ssid="44">Note that D|D |is the last step in the derivation, and D[1, |D|] is the derivation as a whole.</S>
    <S sid="49" ssid="45">Each step Di in the derivation is a rule in G, i.e., Di E P for all i.</S>
    <S sid="50" ssid="46">The probability of the derivation and the corresponding tree is: Let D(G, W[1, i]) be the set of all possible leftmost derivations D (with respect to G) such that RHS(D|D|) = Wi.</S>
    <S sid="51" ssid="47">These are the set of partial leftmost derivations whose last step used a production with terminal Wi on the right-hand side.</S>
    <S sid="52" ssid="48">The prefix probability of W [1, i] with respect to G is From this prefix probability, we can calculate the conditional probability of each word w E T in the terminal vocabulary, given the preceding sequence W[1, i] as follows: This, in fact, is precisely the conditional probability that is used for language modeling for such applications as speech recognition and machine translation, which was the motivation for various syntactic language modeling approaches (Jelinek and Lafferty, 1991; Stolcke, 1995; Chelba and Jelinek, 1998; Roark, 2001).</S>
    <S sid="53" ssid="49">As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., &lt;/s&gt;.</S>
    <S sid="54" ssid="50">For a string W[1, i], we can calculate its prefix probability as shown above.</S>
    <S sid="55" ssid="51">To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G, W[1, i]).</S>
    <S sid="56" ssid="52">In such a way, we can calculate the conditional probability of ending the string with &lt;/s&gt; given W[1, i] as follows: In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here.</S>
    <S sid="57" ssid="53">As presented in Roark (2004), the probabilities in the PCFG are smoothed so that the parser is guaranteed not to fail due to garden pathing, despite following a beam search strategy.</S>
    <S sid="58" ssid="54">Hence there is always a nonzero prefix probability as defined in Eq.</S>
    <S sid="59" ssid="55">2.</S>
    <S sid="60" ssid="56">The parser follows a top-down leftmost derivation strategy.</S>
    <S sid="61" ssid="57">The grammar is factored so that every production has either a single terminal item on the right-hand side or is of the form A &#8212;* B A-B, where A,B E V and the factored A-B category can expand to any sequence of children categories of A that can follow B.</S>
    <S sid="62" ssid="58">This factorization of nary productions continues to nullary factored productions, i.e., the end of the original production A &#8212;* B1... B,,, is signaled with an empty production A-B1-... -B,,, &#8212;* E. The parser maintains a set of possible connected derivations, weighted via the PCFG.</S>
    <S sid="63" ssid="59">It uses a beam search, whereby the highest scoring derivations are worked on first, and derivations that fall outside of the beam are discarded.</S>
    <S sid="64" ssid="60">The reader is referred to Roark (2001; 2004) for specifics about the beam search.</S>
    <S sid="65" ssid="61">The model conditions the probability of each production on features extracted from the partial tree, including non-local node labels such as parents, grandparents and siblings from the leftcontext, as well as c-commanding lexical items.</S>
    <S sid="66" ssid="62">Hence this is a lexicalized grammar, though the incremental nature precludes a general head-first strategy, rather one that looks to the left-context for c-commanding lexical items.</S>
    <S sid="67" ssid="63">To avoid some of the early prediction of structure, the version of the Roark parser that we used performs an additional grammar transformation beyond the simple factorization already described &#8211; a selective left-corner transform of left-recursive productions (Johnson and Roark, 2000).</S>
    <S sid="68" ssid="64">In the transformed structure, slash categories are used to avoid predicting left-recursive structure until some explicit indication of modification is present, e.g., a preposition.</S>
    <S sid="69" ssid="65">The final step in parsing, following the last word in the string, is to &#8220;complete&#8221; all non-terminals in the yield of the tree.</S>
    <S sid="70" ssid="66">All of these open nonterminals are composite factored categories, such as S-NP-VP, which are &#8220;completed&#8221; by rewriting to E. The probability of these E productions is what allows for the calculation of the conditional probability of ending the string, shown in Eq.</S>
    <S sid="71" ssid="67">4.</S>
    <S sid="72" ssid="68">One final note about the size of the non-terminal set and the intractability of exact inference for such a scenario.</S>
    <S sid="73" ssid="69">The non-terminal set not only includes the original atomic non-terminals of the grammar, but also any categories created by grammar factorization (S-NP) or the left-corner transform (NP/NP).</S>
    <S sid="74" ssid="70">Additionally, however, to remain context-free, the non-terminal set must include categories that incorporate non-local features used by the statistical model into their label, including parents, grandparents and sibling categories in the left-context, as well as c-commanding lexical heads.</S>
    <S sid="75" ssid="71">These non-local features must be made local by encoding them in the non-terminal labels, leading to a very large non-terminal set and intractable exact inference.</S>
    <S sid="76" ssid="72">Heavy smoothing is required when estimating the resulting PCFG.</S>
    <S sid="77" ssid="73">The benefit of such a non-terminal set is a rich model, which enables a more peaked statistical distribution around high quality syntactic structures and thus more effective pruning of the search space.</S>
    <S sid="78" ssid="74">The fully connected left-context produced by topdown derivation strategies provides very rich features for the stochastic parsing models.</S>
    <S sid="79" ssid="75">See Roark (2001; 2004) for discussion of these issues.</S>
    <S sid="80" ssid="76">We now turn to measures that can be derived from the parser which may be of use for psycholinguistic modeling.</S>
  </SECTION>
  <SECTION title="3 Parser and grammar derived measures" number="2">
    <S sid="81" ssid="1">The surprisal at word Wi is the negative log probability of Wi given the preceding words.</S>
    <S sid="82" ssid="2">Using prefix probabilities, this can be calculated as: If we are using a beam-search parser, some of the derivations are pruned away.</S>
    <S sid="83" ssid="3">Let B(G, W[1, i]) C_ D(G, W [1, i]) be the set of derivations in the beam.</S>
    <S sid="84" ssid="4">Then the surprisal can be approximated as Any pruning in the beam search will result in a deficient probability distribution, i.e., a distribution that sums to less than 1.</S>
    <S sid="85" ssid="5">Roark&#8217;s thesis (2001) showed that the amount of probability mass lost for this particular approach is very low, hence this provides a very tight bound on the actual surprisal given the model.</S>
    <S sid="86" ssid="6">High surprisal scores result when the prefix probability at word Wi is low relative to the prefix probability at word Wi&#8722;1.</S>
    <S sid="87" ssid="7">Sometimes this is due to the identity of Wi, i.e., it is a surprising word given the context.</S>
    <S sid="88" ssid="8">Other times, it may not be the lexical identity of the word so much as the syntactic structure that must be created to integrate the word into the derivations.</S>
    <S sid="89" ssid="9">One would like to tease surprisal apart into &#8220;syntactic surprisal&#8221; versus &#8220;lexical surprisal&#8221;, which would capture this intuition of the lexical versus syntactic dimensions to the score.</S>
    <S sid="90" ssid="10">Our solution to this has the beneficial property of producing two scores whose sum equals the original surprisal score.</S>
    <S sid="91" ssid="11">The original surprisal score is calculated via sets of partial derivations at the point when each word Wi is integrated into the syntactic structure, D(G, W [1, i]).</S>
    <S sid="92" ssid="12">We then calculate the ratio from point to point in sequence.</S>
    <S sid="93" ssid="13">To tease apart the lexical and syntactic surprisal, we will consider sets of partial derivations immediately before each word Wi is integrated into the syntactic structure, i.e., D[1, |D|&#8722;1] for D E D(G,W[1,i]).</S>
    <S sid="94" ssid="14">Recall that the last derivation move for every derivation in the set is from the POS-tag to the lexical item.</S>
    <S sid="95" ssid="15">Hence the sequence of derivation moves that excludes the last one includes all structure except the word Wi.</S>
    <S sid="96" ssid="16">Then the syntactic surprisal is calculated as: and the lexical surprisal is calculated as: Note that the numerator of SynSG(WZ) is the denominator of LexSG(WZ), hence they sum to form total surprisal SG(WZ).</S>
    <S sid="97" ssid="17">As with total surprisal, these measures can be defined either for the full set D(G, W[1, i]) or for a pruned beam of derivations B(G, W[1, i]) C D(G, W[1, i]).</S>
    <S sid="98" ssid="18">Finally, we replicated the Demberg and Keller (2008) &#8220;unlexicalized&#8221; surprisal by replacing every lexical item in the training corpus with its POS-tag, and then parsing the POS-tags of the language samples rather than the words.</S>
    <S sid="99" ssid="19">This differs from our syntactic surprisal by having no lexical conditioning events for rule probabilities, and by having no ambiguity about the POS-tag of the lexical items in the string.</S>
    <S sid="100" ssid="20">We will refer to the resulting surprisal measure as &#8220;POS surprisal&#8221; to distinguish it from our syntactic surprisal measure.</S>
    <S sid="101" ssid="21">Entropy scores of the sort advocated by Hale (2003; 2006) involve calculation over the set of complete derivations consistent with the set of partial derivations.</S>
    <S sid="102" ssid="22">Hale performs this calculation efficiently via matrix inversion, which explains the use of relatively small-scale grammars with tractably sized non-terminal sets.</S>
    <S sid="103" ssid="23">Such methods are not tractable for the kinds of richly conditioned, large-scale PCFGs that we advocate using here.</S>
    <S sid="104" ssid="24">At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures.</S>
    <S sid="105" ssid="25">Let H(D) be the entropy over a set of derivations D, calculated as follows: If the set of derivations D = D(G, W[1, i]) is a set of partial derivations for string W [1, i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has already been processed.</S>
    <S sid="106" ssid="26">This can be calculated directly from the existing parser operations.</S>
    <S sid="107" ssid="27">If the set of derivations are the complete derivations consistent with the set of partial derivations &#8211; complete derivations that could occur over the set of possible continuations of the string &#8211; then this is a measure of the uncertainty about what is yet to come.</S>
    <S sid="108" ssid="28">We would like measures that can capture this distinction between (a) uncertainty of what has already been processed (&#8220;current ambiguity&#8221;) versus (b) uncertainty of what is yet to be processed (&#8220;predictive entropy&#8221;).</S>
    <S sid="109" ssid="29">In addition, as with surprisal, we would like to tease apart the syntactic uncertainty versus lexical uncertainty.</S>
    <S sid="110" ssid="30">To calculate the predictive entropy after word sequence W [1, i], we modify the parser as follows: the parser extends the set of partial derivations to include all possible next words (the entire vocabulary plus &lt;/s&gt;), and calculates the entropy over that set.</S>
    <S sid="111" ssid="31">This measure is calculated from just one additional word beyond the current word, and hence is an approximation to Hale&#8217;s conditional entropy of grammatical continuations, which is over complete derivations.</S>
    <S sid="112" ssid="32">We will denote this as H'G(W [1, i]) and calculate it as follows: This is performing a predictive step that the baseline parser does not perform, extending the parses to all possible next words.</S>
    <S sid="113" ssid="33">Unlike surprisal, entropy does not decompose straightforwardly into syntactic and lexical components that sum to the original composite measure.</S>
    <S sid="114" ssid="34">To tease apart entropy due to syntactic uncertainty versus that due to lexical uncertainty, we can define the set of derivations up to the preterminal (POS-tag) non-terminals as follows.</S>
    <S sid="115" ssid="35">Let S(D) = {D[1, |D|&#8722;1] : D E D}, i.e., the set of derivations achieved by removing the last step of all derivations in D. Then we can calculate a &#8220;syntactic&#8221; H'G as follows: Finally, &#8220;lexical&#8221; H'G is defined in terms of the conditional probabilities derived from prefix probabilities as defined in Eq.</S>
    <S sid="116" ssid="36">3.</S>
    <S sid="117" ssid="37">As a practical matter, these values are calculated within the Roark parser as follows.</S>
    <S sid="118" ssid="38">A &#8220;dummy&#8221; word is created that can be assigned every POStag, and the parser extends from the current state to this dummy word.</S>
    <S sid="119" ssid="39">(The beam threshold is greatly expanded to allow for many possible extensions.)</S>
    <S sid="120" ssid="40">Then every word in the vocabulary is substituted for the word, and the appropriate probabilities calculated over the beam.</S>
    <S sid="121" ssid="41">Finally, the actual next word is substituted, the beam threshold is reduced to the actual working threshold, and the requisite number of analyses are advanced to continue parsing the string.</S>
    <S sid="122" ssid="42">This represents a significant amount of additional work for the parser &#8211; particularly for vocabulary sizes that we currently use, on the order of tens of thousands of words.</S>
    <S sid="123" ssid="43">As with surprisal, we can calculate an &#8220;unlexicalized&#8221; version of the measure by training and parsing just to POS-tags.</S>
    <S sid="124" ssid="44">We will refer to this sort of entropy as &#8220;POS entropy&#8221;.</S>
  </SECTION>
  <SECTION title="4 Empirical validation" number="3">
    <S sid="125" ssid="1">In order to test the psycholinguistic relevance of the different measures produced by the parser, we conducted a word by word reading experiment.</S>
    <S sid="126" ssid="2">23 native speakers of English read 4 short texts (mean length: 883.5 words, 49.25 sentences).</S>
    <S sid="127" ssid="3">The texts were the written versions of narratives used in a parallel fMRI experiment making use of the same parser derived measures and whose results will be published in a different paper (Bachrach et al., 2009).</S>
    <S sid="128" ssid="4">The narratives contained a high density of syntactically complex structures (in the form of sentential embeddings, relative clauses and other non-local dependencies) but were constructed so as to appear highly natural.</S>
    <S sid="129" ssid="5">The modified version of the Roark parser, trained on the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), was used to parse the different narratives and produce the word by word measures.</S>
    <S sid="130" ssid="6">Each narrative was presented line by line (certain sentences required more than one line) on a computer screen (Dell Optiplex 755 running Windows XP Professional) using Linger 2.882.</S>
    <S sid="131" ssid="7">Each line contained 11.5 words on average.</S>
    <S sid="132" ssid="8">Each word would appear in its relative position on the screen.</S>
    <S sid="133" ssid="9">The subject would then be required to push a keyboard button to advance to the next word.</S>
    <S sid="134" ssid="10">The original word would then disappear and the following word appear in the subsequent position on the screen.</S>
    <S sid="135" ssid="11">After certain sentences a comprehension question would appear on the screen (10 per narrative).</S>
    <S sid="136" ssid="12">This was done in order to encourage subjects to pay attention and to provide data for a post-hoc evaluation of comprehension.</S>
    <S sid="137" ssid="13">After each narrative, subjects were instructed to take a short break (2 minutes on average).</S>
    <S sid="138" ssid="14">The log (base 10) of the reaction times were analyzed using a linear mixed effects regression analysis implemented in the language R (Bates et al., 2008).</S>
    <S sid="139" ssid="15">Reaction times longer than 1500 ms and shorter than 150 ms (raw) were excluded from the analysis (4.8% of total data).</S>
    <S sid="140" ssid="16">Since button press latencies inferior to 150 ms must have been planned prior to the presentation of the word, we considered that they could not reflect stimulus driven effects.</S>
    <S sid="141" ssid="17">Data from the first and last words on each line were discarded.</S>
    <S sid="142" ssid="18">The combined data from the 4 narratives was first modeled using a model which included order of word in the narrative3, word length, parserderived lexical surprisal, unigram frequency, bigram probability, syntactic surprisal, lexical entropy, syntactic entropy and mean number of parser derivation steps as numeric regressors.</S>
    <S sid="143" ssid="19">We also included the unlexicalized POS variants of syntactic surprisal and entropy, along the lines of Demberg and Keller (2008), as detailed in &#167; 3.</S>
    <S sid="144" ssid="20">Table 1 presents the correlations between these mean-centered measures.</S>
    <S sid="145" ssid="21">In addition, we modeled word class (open/closed) as a categorical factor in order to assess interaction between class and the variables of interest, since such an interaction has been observed in the case of frequency (Bradley, 1983).</S>
    <S sid="146" ssid="22">Finally, the random effect part of the model included intercepts for subjects, words and sentences.</S>
    <S sid="147" ssid="23">We report significant effects at the threshold p &lt; .05.</S>
    <S sid="148" ssid="24">Given the presence of significant interactions between lexical class (open/closed) and a number of the variables of interests, we decided to split the data set into open and closed class words and model these separately (linear mixed effects with the same numeric variables as in the full model).</S>
    <S sid="149" ssid="25">In order to evaluate the usefulness of splitting total surprisal into lexical and syntactic components we compared, using a likelihood ratio test, a model where lexical and syntactic surprisal are modeled as distinct regressors to a model where a single regressor equal to their sum (total surprisal) was included.</S>
    <S sid="150" ssid="26">If the larger model provides a significantly better fit than the smaller model, this provides evidence that distinguishing between lexical and syntactic contributions to surprisal is relevant.</S>
    <S sid="151" ssid="27">Since total entropy is not a sum of syntactic and lexical entropy, an analogous test would not be valid in that case.</S>
    <S sid="152" ssid="28">All subjects successfully answered the comprehension questions (92.8% correct responses, S.D.=5.1).</S>
    <S sid="153" ssid="29">In the full model, we observed significant main effects of word class as well as of lexical surprisal, bigram probability, unigram frequency, syntactic entropy, POS entropy and of order in the narrative.</S>
    <S sid="154" ssid="30">Syntactic surprisal, lexical entropy and number of steps had no significant effect.</S>
    <S sid="155" ssid="31">Word length also had no significant main effect but interacted significantly with word class (open/closed).</S>
    <S sid="156" ssid="32">Word class also interacted significantly with lexical surprisal, unigram frequency and syntactic surprisal.</S>
    <S sid="157" ssid="33">The presence of these interactions led us to construct models restricted to open and closed class items respectively.</S>
    <S sid="158" ssid="34">The estimated parameters are reported in Table 2.</S>
    <S sid="159" ssid="35">Reading time for open class words showed significant effects of unigram frequency, syntactic surprisal, syntactic entropy, POS entropy and order within the narrative.</S>
    <S sid="160" ssid="36">The positive effect of length approached significance.</S>
    <S sid="161" ssid="37">Reading time for closed class words exhibited significant effects of lexical surprisal, bigram probability, syntactic entropy and order in the narrative.</S>
    <S sid="162" ssid="38">Length had a non-significant negative effect, thus explaining the interaction observed in the full model.</S>
    <S sid="163" ssid="39">The models with separate lexical and syntactic surprisal performed better than models including combined surprisal.</S>
    <S sid="164" ssid="40">For open class words, the Akaike&#8217;s information criterion (AIC) was -54810 for the combined model and -54819 for the independent model (likelihood ratio test comparing the two, nested, models: x2(1)=10.7,p&lt;.001).</S>
    <S sid="165" ssid="41">For closed class items, combined model&#8217;s AIC was 61467 and full model&#8217;s AIC was -61469 (likelihood ratio test: x2(1)=3.54,p=0.06).</S>
    <S sid="166" ssid="42">Our results demonstrate the relevance of modeling psycholinguistic processes using an incremental probabilistic parser, and the utility of the novel measures presented here.</S>
    <S sid="167" ssid="43">Of particular interest are: the significant effects of our syntactic entropy measure; the independent contributions of lexical surprisal, bigram probability and unigram frequency; and the differences between the predictions of the lexicalized parsing model and the unlexicalized (POS) parsing model.</S>
    <S sid="168" ssid="44">The effect of entropy, or uncertainty regarding the upcoming input independent of the surprise of that input, has been observed in non-linguistic tasks (Hyman, 1953; Bestmann et al., 2008) but to our knowledge has not been quantified before in the context of sentence processing.</S>
    <S sid="169" ssid="45">The usefulness of computational modeling is particularly evident in the case of entropy given the absence of any subjective procedure for its evaluation4.</S>
    <S sid="170" ssid="46">The results argue in favor of a predictive parsing architecture (Van Berkum et al., 2005).</S>
    <S sid="171" ssid="47">The approach to entropy here differs from the one described in Hale (2006) in a couple of ways.</S>
    <S sid="172" ssid="48">First, as discussed above, the calculation procedure is different &#8211; we focus on extending the derivations with just one word, rather than to all possible complete derivations.</S>
    <S sid="173" ssid="49">Second, and most importantly, Hale emphasizes entropy reduction (or the gain in information, given an input, regarding the rest of the sentence) as the correlate of cognitive cost while here we are interested in the amount of entropy itself (and not the size of change).</S>
    <S sid="174" ssid="50">Interestingly, we observed only an effect of syntactic entropy, not lexical entropy.</S>
    <S sid="175" ssid="51">Recent ERP work has demonstrated that subjects do form specific lexical predictions in the context of sentence processing (Van Berkum et al., 2005; DeLong et al., 2005) and so we suspect that the absence of lexical entropy effect might be partly due to sparse data.</S>
    <S sid="176" ssid="52">Lexical surprisal and entropy were calculated using the internal state of a parser trained on the relatively small Brown corpus.</S>
    <S sid="177" ssid="53">Lexical entropy showed no significant effect while lexical surprisal affected only closed class words.</S>
    <S sid="178" ssid="54">This pattern of results might be due to the sparseness of the relevant information in such a small corpus (e.g., verb/object preferences) and the relevance of extra-textual dimensions (world knowledge, contextual information) to lexical-specific prediction.</S>
    <S sid="179" ssid="55">Closed class words are both more frequent (and hence better sampled) and are less sensitive to world knowledge, yet are often determined by the grammatical context.</S>
    <S sid="180" ssid="56">Demberg and Keller (2008) made use of the same parsing architecture used here to compute a syntactic surprisal measure, but used an unlexicalized parser (down to POS-tags rather than words) for this score.</S>
    <S sid="181" ssid="57">Their &#8220;lexicalized&#8221; surprisal is equivalent to our total surprisal (lexical surprisal + syntactic surprisal), while their POS surprisal is derived from a completely different model.</S>
    <S sid="182" ssid="58">In contrast, our approach achieves lexical and syntactic measures from the same model.</S>
    <S sid="183" ssid="59">In order to evaluate the difference between the two approaches we added unlexicalized POS surprisal calculated along the lines of that paper to our model, along with an unlexicalized POS entropy from the same model.</S>
    <S sid="184" ssid="60">We found no effect of unlexicalized POS surprisal5 and a significant (but relatively small) effect of unlexicalized POS entropy.</S>
    <S sid="185" ssid="61">While syntactic surprisal was correlated with POS surprisal (see Table 1) and syntactic entropy correlated with POS entropy, the fact that our syntactic measures still had a significant effect suggests that lexical information contributes towards the formation of syntactic expectations.</S>
    <S sid="186" ssid="62">While the effect of surprisal calculated by an incremental top down parser has been already demonstrated (Demberg and Keller, 2008), our results argue for a distinction between the effect of lexical surprisal and that of syntactic surprisal without requiring unlexicalized parsing of the sort that Demberg and Keller advocate.</S>
    <S sid="187" ssid="63">It is important to keep in mind that this distinction between types of prediction (and as a consequence, prediction error) is not equivalent to the one drawn in the traditional cognitive science modularity debate, which has focused on the source of these predictions.</S>
    <S sid="188" ssid="64">We found a positive effect of syntactic surprisal in the case of open class words.</S>
    <S sid="189" ssid="65">The absence of an effect for closed class words remains to be explained.</S>
    <S sid="190" ssid="66">We quantified word specific surprisal using 3 sources: the parser&#8217;s internal state (lexical surprisal); probability given the preceding word (negative log bigram probability); and the unigram frequency of the word in a large corpus6.</S>
    <S sid="191" ssid="67">As can be observed in Table 1, these three measures are highly correlated7.</S>
    <S sid="192" ssid="68">This is the consequence of the smoothing in the estimation procedure but also relates to a more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991).</S>
    <S sid="193" ssid="69">Despite these strong correlations, the three measures produced independent effects.</S>
    <S sid="194" ssid="70">Unigram frequency had a significant effect for open class words while bigram probability and lexical surprisal each had an effect on reading time of closed class items.</S>
    <S sid="195" ssid="71">Bigram probability has been often found to affect reading time using eye movement measures.</S>
    <S sid="196" ssid="72">This is the first study to demonstrate an additional effect of contextual surprisal given the preceding sentential context (lexical surprisal).</S>
    <S sid="197" ssid="73">Demberg and Keller found no effect for surprisal once bigram and unigram probabilities were included in the model but, importantly, they did not distinguish lexical and syntactic surprisal, rather &#8220;lexicalized&#8221; and &#8220;unlexicalized&#8221; surprisal.</S>
  </SECTION>
  <SECTION title="5 Summary" number="4">
    <S sid="198" ssid="1">We have presented novel methods for teasing apart syntactic and lexical surprisal from a fully lexicalized parser, as well as for extending the operation of a predictive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling.</S>
    <S sid="199" ssid="2">Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand.</S>
    <S sid="200" ssid="3">The empirical validation presented here demonstrated that the new measures &#8211; particularly syntactic entropy and syntactic surprisal &#8211; have high utility for modeling human reading time data.</S>
    <S sid="201" ssid="4">Our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the POS-tag based (unlexicalized) surprisal &#8211; of the sort used in Boston et al. (2008a) and Demberg and Keller (2008) &#8211; did not provide a significant effect in our trials.</S>
    <S sid="202" ssid="5">Further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model.</S>
    <S sid="203" ssid="6">This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="5">
    <S sid="204" ssid="1">Thanks to Michael Collins, John Hale and Shravan Vasishth for valuable discussions about this work.</S>
    <S sid="205" ssid="2">This research was supported in part by NSF Grant #BCS-0826654.</S>
    <S sid="206" ssid="3">Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF.</S>
  </SECTION>
</PAPER>
