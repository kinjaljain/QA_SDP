<PAPER>
	<S sid="0">Dependency Parsing with Short Dependency Relations in Unlabeled Data</S><ABSTRACT>
		<S sid="1" ssid="1">This paper presents an effective dependencyparsing approach of incorporating short de pendency information from unlabeled data.</S>
		<S sid="2" ssid="2">The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words.</S>
		<S sid="3" ssid="3">We thentrain another parser which uses the informa tion on short dependency relations extractedfrom the output of the first parser.</S>
		<S sid="4" ssid="4">Our proposed approach achieves an unlabeled at tachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">In dependency parsing, we attempt to build thedependency links between words from a sentence.</S>
			<S sid="6" ssid="6">Given sufficient labeled data, there are several supervised learning methods for training high performance dependency parsers(Nivre et al, 2007).However, current statistical dependency parsers provide worse results if the dependency length be comes longer (McDonald and Nivre, 2007).</S>
			<S sid="7" ssid="7">Here the length of a dependency from word w i and word w j is simply equal to |i ? j|.</S>
			<S sid="8" ssid="8">Figure 1 shows the F 1score1 provided by a deterministic parser rela tive to dependency length on our testing data.</S>
			<S sid="9" ssid="9">From 1precision represents the percentage of predicted arcs of length d that are correct and recall measures the percentage of gold standard arcs of length d that are correctly predicted.</S>
			<S sid="10" ssid="10">F 1 = 2?</S>
			<S sid="11" ssid="11">precision?</S>
			<S sid="12" ssid="12">recall/(precision + recall) the figure, we find that F 1score decreases when de pendency length increases as (McDonald and Nivre,2007) found.</S>
			<S sid="13" ssid="13">We also notice that the parser pro vides good results for short dependencies (94.57%for dependency length = 1 and 89.40% for depen dency length = 2).</S>
			<S sid="14" ssid="14">In this paper, short dependency refers to the dependencies whose length is 1 or 2.</S>
			<S sid="15" ssid="15">30 40 50 60 70 80 90 100 0 5 10 15 20 25 30 F1 Dependency Length baseline Figure 1: F-score relative to dependency length Labeled data is expensive, while unlabeled data can be obtained easily.</S>
			<S sid="16" ssid="16">In this paper, we present anapproach of incorporating unlabeled data for depen dency parsing.</S>
			<S sid="17" ssid="17">First, all the sentences in unlabeled data are parsed by a dependency parser, which canprovide state-of-the-art performance.</S>
			<S sid="18" ssid="18">We then ex tract information on short dependency relations from the parsed data, because the performance for shortdependencies is relatively higher than others.</S>
			<S sid="19" ssid="19">Finally, we train another parser by using the informa tion as features.The proposed method can be regarded as a semisupervised learning method.</S>
			<S sid="20" ssid="20">Currently, most semi 88 supervised methods seem to do well with artificiallyrestricted labeled data, but they are unable to outper form the best supervised baseline when more labeled data is added.</S>
			<S sid="21" ssid="21">In our experiments, we show that our approach significantly outperforms a state-of-the-art parser, which is trained on full labeled data.</S>
	</SECTION>
	<SECTION title="Motivation and previous work. " number="2">
			<S sid="22" ssid="1">The goal in dependency parsing is to tag dependency links that show the head-modifier relations between words.</S>
			<S sid="23" ssid="2">A simple example is in Figure 2, where thelink between a and bird denotes that a is the depen dent of the head bird.</S>
			<S sid="24" ssid="3">I see a beautiful bird . Figure 2: Example dependency graph.</S>
			<S sid="25" ssid="4">We define that word distance of word w i and word w j is equal to |i ? j|.</S>
			<S sid="26" ssid="5">Usually, the two words in ahead-dependent relation in one sentence can be adja cent words (word distance = 1) or neighboring words(word distance = 2) in other sentences.</S>
			<S sid="27" ssid="6">For exam ple, ?a? and ?bird?</S>
			<S sid="28" ssid="7">has head-dependent relation in the sentence at Figure 2.</S>
			<S sid="29" ssid="8">They can also be adjacent words in the sentence ?I see a bird.?.</S>
			<S sid="30" ssid="9">Suppose that our task is Chinese dependencyparsing.</S>
			<S sid="31" ssid="10">Here, the string ????JJ(Specialist level)/?</S>
			<S sid="32" ssid="11">?NN(working)/?</S>
			<S sid="33" ssid="12">?NN(discussion)?</S>
			<S sid="34" ssid="13">should be tagged as the solution (a) in Figure solution (b) in Figure 3 without any additional information.</S>
			<S sid="35" ssid="14">The point is how to assign the head for ????(Specialist-level)?.</S>
			<S sid="36" ssid="15">Is it ???(working)?</S>
			<S sid="37" ssid="16">or ???(discussion)??</S>
			<S sid="38" ssid="17">         (b) (a)Figure 3: Two solutions for ????(Specialist level)/??(working)/??(discussion)?</S>
			<S sid="39" ssid="18">As Figure 1 suggests, the current dependencyparser is good at tagging the relation between ad jacent words.</S>
			<S sid="40" ssid="19">Thus, we expect that dependencies of adjacent words can provide useful information for parsing words, whose word distances are longer.When we search the string ????(Specialistlevel)/??(discussion)?</S>
			<S sid="41" ssid="20">at google.com, many rele vant documents can be retrieved.</S>
			<S sid="42" ssid="21">If we have a good parser, we may assign the relations between the two words in the retrieved documents as Figure 4 shows.</S>
			<S sid="43" ssid="22">We can find that ???(discussion)?</S>
			<S sid="44" ssid="23">is the head of ????(Specialist-level)?</S>
			<S sid="45" ssid="24">in many cases.</S>
			<S sid="46" ssid="25">1)?525 26 / //,//?</S>
			<S sid="47" ssid="26">2)? ,/ //!#$2004%218?</S>
			<S sid="48" ssid="27">3)?()*+,-.()*/01234/ //5?</S>
			<S sid="49" ssid="28">n)?6789:/ //;=?@A? .)?</S>
			<S sid="50" ssid="29">Figure 4: Parsing ????(Specialist-level)/?</S>
			<S sid="51" ssid="30">?(discussion)?</S>
			<S sid="52" ssid="31">in unlabeled data Now, consider what a learning model could do to assign the appropriate relation between ???</S>
			<S sid="53" ssid="32">?(Specialist-level)?</S>
			<S sid="54" ssid="33">and ???(discussion)?</S>
			<S sid="55" ssid="34">in the string ????(Specialist-level)/??(working)/?</S>
			<S sid="56" ssid="35">?(discussion)?.</S>
			<S sid="57" ssid="36">In this case, we provide additional information to ???(discussion)?</S>
			<S sid="58" ssid="37">as the possible head of ????(Specialist-level)?</S>
			<S sid="59" ssid="38">in the unlabeled data.</S>
			<S sid="60" ssid="39">In this way, the learning model may use this information to make correct decision.Till now, we demonstrate how to use the depen dency relation between adjacent words in unlabeled data to help tag the relation between two words whose word distance is 2.</S>
			<S sid="61" ssid="40">In the similar way, we can also assign the relation between two words whose word distance is longer by using the information.</S>
			<S sid="62" ssid="41">Based on the above observations, we propose anapproach of exploiting the information from a large scale unlabeled data for dependency parsing.</S>
			<S sid="63" ssid="42">We use a parser to parse the sentences in unlabeled data.</S>
			<S sid="64" ssid="43">Then another parser makes use of the information on short dependency relations in the newly parsed data to improve performance.</S>
			<S sid="65" ssid="44">Our study is relative to incorporating unlabeled 89 data into a model for parsing.</S>
			<S sid="66" ssid="45">There are several other studies relevant to ours as described below.A simple method is self-training in which the ex isting model first labels unlabeled data and then the newly labeled data is then treated as hand annotateddata for training a new model.</S>
			<S sid="67" ssid="46">But it seems that self training is not so effective.</S>
			<S sid="68" ssid="47">(Steedman et al, 2003) reports minor improvement by using self-trainingfor syntactic parsing on small labeled data.</S>
			<S sid="69" ssid="48">The rea son may be that errors in the original model would be amplified in the new model.</S>
			<S sid="70" ssid="49">(McClosky et al, 2006) presents a successful instance of parsing withself-training by using a re-ranker.</S>
			<S sid="71" ssid="50">As Figure 1 suggests, the dependency parser performs bad for pars ing the words with long distances.</S>
			<S sid="72" ssid="51">In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser.</S>
			<S sid="73" ssid="52">(Smith and Eisner, 2006) presents an approach toimprove the accuracy of a dependency grammar in duction models by EM from unlabeled data.</S>
			<S sid="74" ssid="53">Theyobtain consistent improvements by penalizing de pendencies between two words that are farther apart in the string.The study most relevant to ours is done by (Kawahara and Kurohashi, 2006).</S>
			<S sid="75" ssid="54">They present an in tegrated probabilistic model for Japanese parsing.</S>
			<S sid="76" ssid="55">They also use partial information after current parser parses the sentences.</S>
			<S sid="77" ssid="56">Our work differs in that we consider general dependency relations while theyonly consider case frames.</S>
			<S sid="78" ssid="57">And we represent additional information as the features for learning mod els while they use the case frames as one component for a probabilistic model.</S>
	</SECTION>
	<SECTION title="Our Approach. " number="3">
			<S sid="79" ssid="1">In this section, we describe our approach of exploit ing reliable features from unlabeled data, which is parsed by a basic parser.</S>
			<S sid="80" ssid="2">We then train another parser based on new feature space.</S>
			<S sid="81" ssid="3">3.1 Training a basic parser.</S>
			<S sid="82" ssid="4">In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003).</S>
			<S sid="83" ssid="5">This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al, 2006) and CoNLL2007(Hall et al, 2007).</S>
			<S sid="84" ssid="6">In fact, our approachcan also be applied to other parsers, such as (Ya mada and Matsumoto, 2003)?s parser, (McDonald et al., 2006)?s parser, and so on.</S>
			<S sid="85" ssid="7">3.1.1 The parserThe parser predicts unlabeled directed dependen cies between words in sentences.</S>
			<S sid="86" ssid="8">The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens.</S>
			<S sid="87" ssid="9">The behaviors of the parser are defined by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): ? Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack.</S>
			<S sid="88" ssid="10">Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stack.</S>
			<S sid="89" ssid="11">Reduce(RE): Pop the stack.</S>
			<S sid="90" ssid="12">Shift(SH): Push NEXT onto the stack.</S>
			<S sid="91" ssid="13">The first two actions mean that there is a dependency relation between TOP and NEXT.More information about the parser can be avail able in the paper(Nivre, 2003).</S>
			<S sid="92" ssid="14">The parser uses aclassifier to produce a sequence of actions for a sen tence.</S>
			<S sid="93" ssid="15">In our experiments, we use the SVM model as the classifier.</S>
			<S sid="94" ssid="16">More specifically, our parser uses LIBSVM(Chang and Lin, 2001) with a polynomial kernel (degree = 3) and the built-in one-versus-all strategy for multi-class classification.</S>
			<S sid="95" ssid="17">3.1.2 Basic features We represent basic features extracted from the fields of data representation, including word and part-of-speech(POS) tags.</S>
			<S sid="96" ssid="18">The basic features used in our parser are listed as follows: ? The features based on words: the words of TOP and NEXT, the word of the head of TOP, the words of leftmost and rightmost dependent of TOP, and the word of the token immediately after NEXT in original input string.</S>
			<S sid="97" ssid="19">The features based on POS: the POS of TOP and NEXT, the POS of the token immediately below TOP, the POS of leftmost and rightmost dependent of TOP, the POS of next three tokensafter NEXT, and the POS of the token immedi ately before NEXT in original input string.</S>
			<S sid="98" ssid="20">90With these basic features, we can train a state-ofthe-art supervised parser on labeled data.</S>
			<S sid="99" ssid="21">In the fol lowing content, we call this parser Basic Parser.</S>
			<S sid="100" ssid="22">3.2 Unlabeled data preprocessing and parsing.</S>
			<S sid="101" ssid="23">The input of our approach is unlabeled data, whichcan be obtained easily.</S>
			<S sid="102" ssid="24">For the Basic Parser, the corpus should have part-of-speech (POS) tags.</S>
			<S sid="103" ssid="25">There fore, we should assign the POS tags using a POS tagger.</S>
			<S sid="104" ssid="26">For Chinese sentences, we should segmentthe sentences into words before POS tagging.</S>
			<S sid="105" ssid="27">Af ter data preprocessing, we have the word-segmented sentences with POS tags.</S>
			<S sid="106" ssid="28">We then use the Basic Parser to parse all sentences in unlabeled data.</S>
			<S sid="107" ssid="29">3.3 Using short dependency relations as.</S>
			<S sid="108" ssid="30">features The Basic Parser can provide complete dependency parsing trees for all sentences in unlabeled data.</S>
			<S sid="109" ssid="31">AsFigure 1 shows, short dependencies are more reli able.</S>
			<S sid="110" ssid="32">To offer reliable information for the model, wepropose the features based on short dependency re lations from the newly parsed data.</S>
			<S sid="111" ssid="33">3.3.1 Collecting reliable information In a parsed sentence, if the dependency length of two words is 1 or 2, we add this word pair into a list named DepList and count its frequency.We consider the direction and length of the de pendency.</S>
			<S sid="112" ssid="34">D1 refers to the pairs with dependency length 1, D2 refers to the pairs with dependency length 2, R refers to right arc, and L refers to left arc. For example, ????(specialist-level)?</S>
			<S sid="113" ssid="35">and ???(discussion)?</S>
			<S sid="114" ssid="36">are adjacent words in a sentence ???(We)/??(held)/???(specialist-level)/?</S>
			<S sid="115" ssid="37">?(discussion)/b? and have a left dependency arc assigned by the Basic Parser.</S>
			<S sid="116" ssid="38">We add a word pair ????(specialist-level)-??(discussion)?</S>
			<S sid="117" ssid="39">with ?D1-L?</S>
			<S sid="118" ssid="40">and its frequency into the DepList.</S>
			<S sid="119" ssid="41">According to frequency, we then group word pairs into different buckets, with a bucket ONE for frequency 1, a single bucket LOW for 2-7, a single bucket MID for 8-14, and a single bucketHIGH for 15+.</S>
			<S sid="120" ssid="42">We choose these threshold val ues via testing on development data.</S>
			<S sid="121" ssid="43">For example,the frequency of the pair ????(specialist-level) ??(discussion)?</S>
			<S sid="122" ssid="44">with ?D1-L?</S>
			<S sid="123" ssid="45">is 20.</S>
			<S sid="124" ssid="46">Then it is grouped into the bucket ?D1-L-HIGH?.</S>
			<S sid="125" ssid="47">Here, we do not use the frequencies as the weight of the features.</S>
			<S sid="126" ssid="48">We derive the weights of the features by the SVM model from training data rather than approximating the weights from unlabeled data.</S>
			<S sid="127" ssid="49">3.3.2 New features Based on the DepList, we represent new features for training or parsing current two words: TOP and NEXT.</S>
			<S sid="128" ssid="50">We consider word pairs from the context around TOP and NEXT, and get the buckets of the pairs in the DepList.</S>
			<S sid="129" ssid="51">First, we represent the features based on D1.</S>
			<S sid="130" ssid="52">We name these features D1 features.</S>
			<S sid="131" ssid="53">The D1 featuresare listed according to different word distances be tween TOP and NEXT as follows: 1.</S>
			<S sid="132" ssid="54">Word distance is 1: (TN0) the bucket of the.</S>
			<S sid="133" ssid="55">word pair of TOP and NEXT, and (TN1) the bucket of the word pair of TOP and next token after NEXT.</S>
			<S sid="134" ssid="56">2.</S>
			<S sid="135" ssid="57">Word distance is 2 or 3+: (TN0) the bucket of.</S>
			<S sid="136" ssid="58">the word pair of TOP and NEXT, (TN1) the bucket of the word pair of TOP and next token after NEXT, and (TN 1) the bucket of the word pair of TOP and the token immediately before NEXT.</S>
			<S sid="137" ssid="59">In item 2), all features are in turn combined with two sets of distances: a set for distance 2 and a single set for distances 3+.</S>
			<S sid="138" ssid="60">Thus, we have 8 types of D1 features, including 2 types in item1) and 6 types in item 2).</S>
			<S sid="139" ssid="61">The feature is format ted as ?Position:WordDistance:PairBucket?.</S>
			<S sid="140" ssid="62">Forexample, we have the string ????(specialist level)/w 1 /w 2 /w 3 /?</S>
			<S sid="141" ssid="63">?(discussion)?, and ??</S>
			<S sid="142" ssid="64">?(specialist-level)?</S>
			<S sid="143" ssid="65">is TOP and ???(discussion)?</S>
			<S sid="144" ssid="66">is NEXT.</S>
			<S sid="145" ssid="67">Thus we can have the feature ?TN0:3+:D1-L-HIGH?</S>
			<S sid="146" ssid="68">for TOP and NEXT, because the word distance is 4(3+) and ???</S>
			<S sid="147" ssid="69">?(specialist-level)-??(discussion)?</S>
			<S sid="148" ssid="70">belongs to the bucket ?D1-L-HIGH?.</S>
			<S sid="149" ssid="71">Here, if a string belongs to two buckets, we use the most frequent bucket.</S>
			<S sid="150" ssid="72">Then, we represent the features based on D2.</S>
			<S sid="151" ssid="73">We name these features D2 features.</S>
			<S sid="152" ssid="74">The D2 features are listed as follows: 1.</S>
			<S sid="153" ssid="75">Word distance is 1: (TN1) the bucket of the.</S>
			<S sid="154" ssid="76">word pair of TOP and next token after NEXT.</S>
			<S sid="155" ssid="77">91 2.</S>
			<S sid="156" ssid="78">Word distance is 2: (TN0) the bucket of the.</S>
			<S sid="157" ssid="79">word pair of TOP and NEXT, and (TN1) the bucket of the word pair of TOP and next token after NEXT.</S>
	</SECTION>
	<SECTION title="Experiments. " number="4">
			<S sid="158" ssid="1">For labeled data, we used the Chinese Treebank (CTB) version 4.02 in our experiments.</S>
			<S sid="159" ssid="2">We used the same rules for conversion and created the same data split as (Wang et al, 2007): files 1-270 and 400-931 as training, 271-300 as testing and files 301-325 asdevelopment.</S>
			<S sid="160" ssid="3">We used the gold standard segmenta tion and POS tags in the CTB.</S>
			<S sid="161" ssid="4">For unlabeled data, we used the PFR corpus 3.</S>
			<S sid="162" ssid="5">It includes the documents from People?s Daily at 1998 (12 months).</S>
			<S sid="163" ssid="6">There are about 290 thousand sentences and 15 million words in the PFR corpus.To simplify, we used its segmentation.</S>
			<S sid="164" ssid="7">And we discarded the POS tags because PFR and CTB used dif ferent POS sets.</S>
			<S sid="165" ssid="8">We used the package TNT (Brants,2000), a very efficient statistical part-of-speech tag ger, to train a POS tagger4 on training data of the CTB.We measured the quality of the parser by the un labeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD.</S>
			<S sid="166" ssid="9">We reported two typesof scores: ?UAS without p?</S>
			<S sid="167" ssid="10">is the UAS score with out all punctuation tokens and ?UAS with p?</S>
			<S sid="168" ssid="11">is the one with all punctuation tokens.</S>
			<S sid="169" ssid="12">4.1 Experimental results.</S>
			<S sid="170" ssid="13">In the experiments, we trained the parsers on train ing data and tuned the parameters on development data.</S>
			<S sid="171" ssid="14">In the following sessions, ?baseline?</S>
			<S sid="172" ssid="15">refers to Basic Parser (the model with basic features), and ?OURS?</S>
			<S sid="173" ssid="16">refers to our proposed parser (the model with all features).</S>
			<S sid="174" ssid="17">4.1.1 Our approachTable 1 shows the results of the parser with differ ent feature sets, where ?+D1?</S>
			<S sid="175" ssid="18">refers to the parser 2More detailed information can be found at http://www.cis.upenn.edu/?chinese/.</S>
			<S sid="176" ssid="19">3More detailed information can be found at http://www.icl.pku.edu.</S>
			<S sid="177" ssid="20">4To know whether our POS tagger is good, we also tested the TNT package on the standard training and testing sets forfull parsing (Wang et al, 2006).</S>
			<S sid="178" ssid="21">The TNT-based tagger pro vided 91.52% accuracy, the comparative result with (Wang et al., 2006).</S>
			<S sid="179" ssid="22">with basic features and D1 features, and ?+D2?</S>
			<S sid="180" ssid="23">refers to the parser with all features(basic features, D1 features, and D2 features).</S>
			<S sid="181" ssid="24">From the table, wefound a large improvement (1.12% for UAS with out p and 1.23% for UAS with p) from adding D1features.</S>
			<S sid="182" ssid="25">And D2 features provided minor improve ment, 0.12% for UAS without p and 0.14% for UASwith p. This may be due to the information from de pendency length 2 containing more noise.</S>
			<S sid="183" ssid="26">Totally, we achieved 1.24% improvement for UAS with p and 1.37% for UAS without p. The improvement is significant in one-tail paired t-test (p  10?5).</S>
			<S sid="184" ssid="27">Table 1: The results with different feature sets UAS without p UAS with p baseline 85.28 83.79 +D1 86.40 85.02 +D2(OURS) 86.52 85.16We also attempted to discover the effect of different numbers of unlabeled sentences to use.</S>
			<S sid="185" ssid="28">Ta ble 2 shows the results with different numbers ofsentences.</S>
			<S sid="186" ssid="29">Here, we randomly chose different per centages of sentences from unlabeled data.</S>
			<S sid="187" ssid="30">When we used 1% sentences of unlabeled data, the parser achieved a large improvement.</S>
			<S sid="188" ssid="31">As we added more sentences, the parser obtained more benefit.Table 2: The results with different numbers of unla beled sentences Sentences UAS without p UAS with p 0%(baseline) 85.28 83.79 1% 85.68 84.40 2% 85.69 84.51 5% 85.78 84.59 10% 85.97 84.62 20% 86.25 84.86 50% 86.34 84.92 100%(OURS) 86.52 85.16 4.1.2 Comparison of other systems Finally, we compare our parser to the state of the art.</S>
			<S sid="189" ssid="32">We used the same testing data as (Wang et al, 2005) did, selecting the sentences length up to 40.</S>
			<S sid="190" ssid="33">Table 3 shows the results achieved by our method and other researchers (UAS with p), where Wang05 refers to (Wang et al, 2005), Wang07 refers 92 to (Wang et al, 2007), and McDonaldPereira065 refers to (McDonald and Pereira, 2006).</S>
			<S sid="191" ssid="34">From the table, we found that our parser performed best.</S>
			<S sid="192" ssid="35">Table 3: The results on the sentences length up to 40 UAS with p Wang05 79.9 McDonaldPereira06 82.5 Wang07 86.6 baseline 87.1 OURS 88.4</S>
	</SECTION>
	<SECTION title="Analysis. " number="5">
			<S sid="193" ssid="1">5.1 Improvement relative to dependency length.</S>
			<S sid="194" ssid="2">We now look at the improvement relative to depen dency length as Figure 5 shows.</S>
			<S sid="195" ssid="3">From the figure, we found that our method provided better performancewhen dependency lengths are less than 13.</S>
			<S sid="196" ssid="4">Espe cially, we had improvements 2.35% for dependency length 4, 3.13% for length 5, 2.56% for length 6, and 4.90% for length 7.</S>
			<S sid="197" ssid="5">For longer ones, the parser can not provide stable improvement.</S>
			<S sid="198" ssid="6">The reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronounsmodifying their direct neighbors, while longer de pendencies typically represent modifiers of the root or the main verb in a sentence(McDonald and Nivre,2007).</S>
			<S sid="199" ssid="7">We did not provide new features for modi fiers of the root.</S>
			<S sid="200" ssid="8">30 40 50 60 70 80 90 100 0 5 10 15 20 F1 Dependency Length baseline OURS Figure 5: Improvement relative to dependency length 5(Wang, 2007) reported this result.</S>
			<S sid="201" ssid="9">JJ NN NN JJ NN NN JJ NN NN NN NN NN NN NN NN NN NN NN AD VV VV AD VV VV AD VV VV JJ NN CC NN JJ NN CC NN JJ NN CC NN Figure 6: Ambiguities 5.2 Cases study in neighborhood.</S>
			<S sid="202" ssid="10">In Chinese dependency parsing, there are many am biguities in neighborhood, such as ?JJ NN NN?, ?AD VV VV?, ?NN NN NN?, ?JJ NN CC NN?.</S>
			<S sid="203" ssid="11">They have possible parsing trees as Figure 6 shows.</S>
			<S sid="204" ssid="12">For these ambiguities, our approach can provideadditional information for the parser.</S>
			<S sid="205" ssid="13">For ex ample, we have the following case in the data set: ???JJ(friendly)/ ??NN(corporation)/ ??NN(relationship)/?.</S>
			<S sid="206" ssid="14">We can provide additional in formation about the relations of ???JJ(friendly)/ ??NN(corporation)?</S>
			<S sid="207" ssid="15">and ???JJ(friendly)/ ? ?NN(relationship)/?</S>
			<S sid="208" ssid="16">in unlabeled data to help the parser make the correct decision.Our approach can also work for the longer con structions, such as ?JJ NN NN NN?</S>
			<S sid="209" ssid="17">and ?NN NN NN NN?</S>
			<S sid="210" ssid="18">in the similar way.</S>
			<S sid="211" ssid="19">For the construction ?JJ NN1 CC NN2?, we now do not define special features to solvethe ambiguity.</S>
			<S sid="212" ssid="20">However, based on the cur rent DepList, we can also provide additional information about the relations of JJ/NN1 and JJ/NN2.</S>
			<S sid="213" ssid="21">For example, for the string ???</S>
			<S sid="214" ssid="22">?JJ(further)/ ??NN(improvement)/ ?CC(and)/ ??NN(development)/?, the parser often assigns ??</S>
			<S sid="215" ssid="23">?(improvement)?</S>
			<S sid="216" ssid="24">as the head of ??</S>
			<S sid="217" ssid="25">?(further)?</S>
			<S sid="218" ssid="26">instead of ???(development)?.</S>
			<S sid="219" ssid="27">There is an entry ????(further)-??(development)?</S>
			<S sid="220" ssid="28">in the DepList.</S>
			<S sid="221" ssid="29">Here, we need a coordination identifier to identify these constructions.</S>
			<S sid="222" ssid="30">After that, we can provide the information for the model.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="223" ssid="1">This paper presents an effective approach to improvedependency parsing by using unlabeled data.</S>
			<S sid="224" ssid="2">We ex tract the information on short dependency relations 93in an automatically generated corpus parsed by a basic parser.</S>
			<S sid="225" ssid="3">We then train a new parser with the information.</S>
			<S sid="226" ssid="4">The new parser achieves an absolute im provement of 1.24% over the state-of-the-art parser on Chinese Treebank (from 85.28% to 86.52%).</S>
			<S sid="227" ssid="5">There are many ways in which this research should be continued.</S>
			<S sid="228" ssid="6">First, feature representationneeds to be improved.</S>
			<S sid="229" ssid="7">Here, we use a simple fea ture representation on short dependency relations.We may use a combined representation to use the in formation from long dependency relations even they are not so reliable.</S>
			<S sid="230" ssid="8">Second, we can try to select more accurately parsed sentences.</S>
			<S sid="231" ssid="9">Then we may collect more reliable information than the current one.</S>
	</SECTION>
</PAPER>
