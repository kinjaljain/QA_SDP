<PAPER>
  <S sid="0">Dependency Trees and the Strong Generative Capacity of CCG</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG.</S>
    <S sid="2" ssid="2">Unlike earlier proposals, our dependency structures are always tree-shaped.</S>
    <S sid="3" ssid="3">We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees &#8211; but the mechanisms they use to bring the words in these trees into a linear order are incomparable.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism.</S>
    <S sid="5" ssid="2">Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004).</S>
    <S sid="6" ssid="3">However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms.</S>
    <S sid="7" ssid="4">In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG.</S>
    <S sid="8" ssid="5">This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another.</S>
    <S sid="9" ssid="6">In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG.</S>
    <S sid="10" ssid="7">We then explore the strong generative capacity of PF-CCG in terms of dependency trees.</S>
    <S sid="11" ssid="8">In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)).</S>
    <S sid="12" ssid="9">We show that if we only look at valencies and ignore word order, then the dependency trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS.</S>
    <S sid="13" ssid="10">To our knowledge, this is the first time that the regularity of CCG&#8217;s derivational structures has been exposed.</S>
    <S sid="14" ssid="11">However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot.</S>
    <S sid="15" ssid="12">The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees.</S>
    <S sid="16" ssid="13">The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002).</S>
    <S sid="17" ssid="14">Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled.</S>
    <S sid="18" ssid="15">However, we show that our dependency trees are still informative enough to reconstruct the semantic representations.</S>
    <S sid="19" ssid="16">The paper is structured as follows.</S>
    <S sid="20" ssid="17">In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribution to earlier research.</S>
    <S sid="21" ssid="18">In Section 3, we then show how to read off a dependency tree from a CCG derivation.</S>
    <S sid="22" ssid="19">Finally, we explore the strong generative capacity of CCG in Section 4 and conclude with ideas for future work.</S>
  </SECTION>
  <SECTION title="2 Combinatory Categorial Grammars" number="2">
    <S sid="23" ssid="1">We start by introducing the Combinatory Categorial Grammar (CCG) formalism.</S>
    <S sid="24" ssid="2">Then we introduce the fragment of CCG that we consider in this paper, and discuss some related work.</S>
    <S sid="25" ssid="3">Combinatory Categorial Grammar (Steedman, 2001) is a grammar formalism that assigns categories to substrings of an input sentence.</S>
    <S sid="26" ssid="4">There are atomic categories such as s and np; and if A and B are categories, then A\B and A/B are functional categories representing a constituent that will have category A once it is combined with another constituent of type B to the left or right, respectively.</S>
    <S sid="27" ssid="5">Each word is assigned a category by the lexicon; adjacent substrings can then be combined by combinatory rules.</S>
    <S sid="28" ssid="6">As an example, Steedman and Baldridge&#8217;s (2009) analysis of Shieber&#8217;s (1985) Swiss German subordinate clause (das) mer em Hans es huus h&#228;lfed aastriiche (&#8216;(that) we help Hans paint the house&#8217;) is shown in Figure 1.</S>
    <S sid="29" ssid="7">Intuitively, the arguments of a functional category can be thought of as the syntactic valencies of the lexicon entry, or as arguments of a function that maps categories to categories.</S>
    <S sid="30" ssid="8">The core combinatory mechanism underlying CCG is the composition and application of these functions.</S>
    <S sid="31" ssid="9">In their most general forms, the combinatory rules of (forward and backward) application and composition can be written as in Figure 2.</S>
    <S sid="32" ssid="10">The symbol &#65533; stands for an arbitrary (forward or backward) slash; it is understood that the slash before each BZ above the line is the same as below.</S>
    <S sid="33" ssid="11">The rules derive statements about triples w &#65533;- A : f, expressing that the substring w can be assigned the category A and the semantic representation f; an entire string counts as grammatical if it can be assigned the start category s. In parallel to the combination of substrings by the combinatory rules, their semantic representations are combined by functional composition.</S>
    <S sid="34" ssid="12">We have presented the composition rules of CCG in their most general form.</S>
    <S sid="35" ssid="13">In the literature, the special cases for n = 0 are called forward and backward application; the cases for n &gt; 0 where the slash before Bn is the same as the slash before B are called composition of degree n; and the cases where n &gt; 0 and the slashes have different directions are called crossed composition of degree n. For instance, the F application that combines h&#228;lfed and aastriche in Figure 1 is a forward crossed composition of degree 1.</S>
    <S sid="36" ssid="14">In addition to the composition rules introduced above, CCG also allows rules of substitution and type-raising.</S>
    <S sid="37" ssid="15">Substitution is used to handle syntactic phenomena such as parasitic gaps; type-raising allows a constituent to serve syntactically as a functor, while being used semantically as an argument.</S>
    <S sid="38" ssid="16">Furthermore, it is possible in CCG to restrict the instances of the rule schemata in Figure 2&#8212;for instance, to say that the application rule may only be used for the case A = s. We call a CCG grammar pure if it does not use substitution, type-raising, or restricted rule schemata.</S>
    <S sid="39" ssid="17">Finally, the argument categories of a CCG category may themselves be functional categories; for instance, the category of a VP modifier like passionately is (s\np)\(s\np).</S>
    <S sid="40" ssid="18">We call a category that is either atomic or only has atomic arguments a first-order category, and call a CCG grammar first-order if all categories that its lexicon assigns to words are first-order.</S>
    <S sid="41" ssid="19">In this paper, we only consider CCG grammars that are pure and first-order.</S>
    <S sid="42" ssid="20">This fragment, which we call PF-CCG, is less expressive than full CCG, but it significantly simplifies the definitions in Section 3.</S>
    <S sid="43" ssid="21">At the same time, many real-world CCG grammars do not use the substitution rule, and typeraising can be compiled into the grammar in the sense that for any CCG grammar, there is an equivalent CCG grammar that does not use type-raising and assigns the same semantic representations to each string.</S>
    <S sid="44" ssid="22">On the other hand, the restriction to first-order grammars is indeed a limitation in practice.</S>
    <S sid="45" ssid="23">We take the work reported here as a first step towards a full dependency-tree analysis of CCG, and discuss ideas for generalization in the conclusion.</S>
  </SECTION>
  <SECTION title="2.3 Related work" number="3">
    <S sid="46" ssid="1">The main objective of this paper is the definition of a novel way in which dependency trees can be extracted from CCG derivations.</S>
    <S sid="47" ssid="2">This is similar to Clark et al. (2002), who aim at capturing &#8216;deep&#8217; dependencies, and encode these into annotated lexical categories.</S>
    <S sid="48" ssid="3">For instance, they write (npi\npi)/(s\npi) for subject relative pronouns to express that the relative pronoun, the trace of the relative clause, and the modified noun phrase are all semantically the same.</S>
    <S sid="49" ssid="4">This means that the relative pronoun has multiple parents; in general, their dependency structures are not necessarily trees.</S>
    <S sid="50" ssid="5">By contrast, we aim to extract only dependency trees, and achieve this by recording only the fillers of syntactic valencies, rather than the semantic dependencies: the relative pronoun gets two dependents and one parent (the verb whose argument the modified np is), just as the category specifies.</S>
    <S sid="51" ssid="6">So Clark et al.&#8217;s and our dependency approach represent two alternatives of dealing with the tradeoff between simple and expressive dependency structures.</S>
    <S sid="52" ssid="7">Our paper differs from the well-known results of Vijay-Shanker and Weir (1994) in that they establish the weak equivalence of different grammar formalisms, while we focus on comparing the derivational structures.</S>
    <S sid="53" ssid="8">Hockenmaier and Young (2008) present linguistic motivations for comparing the strong generative capacities of CCG and TAG, and the beginnings of a formal comparison between CCG and spinal TAG in terms of Linear Indexed Grammars.</S>
  </SECTION>
  <SECTION title="3 Induction of dependency trees" number="4">
    <S sid="54" ssid="1">We now explain how to extract a dependency tree from a PF-CCG derivation.</S>
    <S sid="55" ssid="2">The basic idea is to associate, with every step of the derivation, a corresponding operation on dependency trees, in much the same way as derivation steps can be associated with operations on semantic representations.</S>
    <S sid="56" ssid="3">When talking about a dependency tree, it is usually convenient to specify its tree structure and the linear order of its nodes separately.</S>
    <S sid="57" ssid="4">The tree structure encodes the valency structure of the sentence (immediate dominance), whereas the linear precedence of the words is captured by the linear order.</S>
    <S sid="58" ssid="5">For the purposes of this paper, we represent a dependency tree as a pair d = (t, s), where t is a ground term over some suitable alphabet, and s is a linearization of the nodes (term addresses) of t, where by a linearization of a set S we mean a list of elements of S in which each element occurs exactly once (see also Kuhlmann and M&#246;hl (2007)).</S>
    <S sid="59" ssid="6">As examples, consider (f(a, b), [1, &#949;, 2]) and (f(g(a)), [1 &#183; 1, &#949;,1]) .</S>
    <S sid="60" ssid="7">These expressions represent the dependency trees d1 = and d2 = . a f b a f g Notice that it is because of the separate specification of the tree and the order that dependency trees can become non-projective; d2 is an example.</S>
    <S sid="61" ssid="8">A partial dependency tree is a pair (t, s) where t is a term that may contain variables, and s is a linearization of those nodes of t that are not labelled with variables.</S>
    <S sid="62" ssid="9">We restrict ourselves to terms in which each variable appears exactly once, and will also prefix partial dependency trees with &#955;-binders to order the variables.</S>
    <S sid="63" ssid="10">Let t be a term, and let x be a variable in t. The result of the substitution of the term t' into t for x is denoted by t[ x := t' ].</S>
    <S sid="64" ssid="11">We extend this operation to dependency trees as follows.</S>
    <S sid="65" ssid="12">Given a list of addresses s, let xs be the list of addresses obtained from s by prefixing every address with the address of the (unique) node that is labelled with x in t. Then the operations of forward and backward concatenation are defined as (t, s)[ x := (t', s') ]F = (t[ x := t' ], s &#183; xs') , (t, s)[ x := (t', s') ]B = (t[ x := t' ], xs' &#183; s) .</S>
    <S sid="66" ssid="13">The concatenation operations combine two given dependency trees (t, s) and (t', s') into a new tree by substituting t' into t for some variable x of t, and adding the (appropriately prefixed) list s' of nodes of t' either before or after the list s of nodes of t. Using these two operations, the dependency trees d1 and d2 from above can be written as follows.</S>
    <S sid="67" ssid="14">Let da = (a, [&#949;]) and db = (b, [&#949;]).</S>
    <S sid="68" ssid="15">Here is an alternative graphical notation for the composition of d2: In this notation, nodes that are not marked with variables are positioned (indicated by the dotted projection lines), while the (dashed) variable nodes dangle unpositioned.</S>
    <S sid="69" ssid="16">To encode CCG derivations as dependency trees, we annotate each composition rule of PF-CCG with instructions for combining the partial dependency trees for the substrings into a partial dependency tree for the larger string.</S>
    <S sid="70" ssid="17">Essentially, we now combine partial dependency trees using forward and backward concatenation rather than combining semantic representations by functional composition and application.</S>
    <S sid="71" ssid="18">From now on, we assume that the node labels in the dependency trees are CCG lexicon entries, and represent these by just the word in them.</S>
    <S sid="72" ssid="19">The modified rules are shown in Figure 3.</S>
    <S sid="73" ssid="20">They derive statements about triples w ` A : p, where w is a substring, A is a category, and p is a lambda expression over a partial dependency tree.</S>
    <S sid="74" ssid="21">Each variable of p corresponds to an argument category in A, and vice versa.</S>
    <S sid="75" ssid="22">Rule L covers the base case: the dependency tree for a lexical entry e is a tree with one node for the item itself, labelled with e, and one node for each of its syntactic arguments, labelled with a variable.</S>
    <S sid="76" ssid="23">Rule F captures forward composition: given two dependency trees d and d', the new dependency tree is obtained by forward concatenation, binding the outermost variable in d. Rule B is the rule for backward composition.</S>
    <S sid="77" ssid="24">The result of translating a complete PF-CCG derivation &#948; in this way is always a dependency tree without variables; we call it d(&#948;).</S>
    <S sid="78" ssid="25">As an example, Figure 4 shows the construction for the derivation in Figure 1.</S>
    <S sid="79" ssid="26">The induced dependency tree looks like this: mer em Hans es huus h&#228;lfed aastriche For instance, the partial dependency tree for the lexicon entry of aastriiche contains two nodes: the root (with address &#949;) is labelled with the lexicon entry, and its child (address 1) is labelled with the variable x.</S>
    <S sid="80" ssid="27">This tree is inserted into the tree from h&#228;lfed by forward concatenation.</S>
    <S sid="81" ssid="28">The variable w is passed on into the new dependency tree, and later filled by backward concatenation to huus.</S>
    <S sid="82" ssid="29">Passing the argument slot of aastriiche to h&#228;lfed to be filled on its left creates a non-projectivity; it corresponds to a crossed composition in CCG terms.</S>
    <S sid="83" ssid="30">Notice that the categories derived in Figure 1 mirror the functional structure of the partial dependency trees at each step of the derivation.</S>
    <S sid="84" ssid="31">The mapping from derivations to dependency trees loses some information: different derivations may induce the same dependency tree.</S>
    <S sid="85" ssid="32">This is illustrated by Figure 5, which provides two possible derivations for the phrase big white rabbit, both of which induce the same dependency tree.</S>
    <S sid="86" ssid="33">Especially in light of the fact that our dependency trees will typically contain fewer dependencies than the DAGs derived by Clark et al. (2002), one could ask whether dependency trees are an appropriate way of representing the structure of a CCG derivation.</S>
    <S sid="87" ssid="34">However, at the end of the day, the most important information that can be extracted from a CCG derivation is the semantic representation it computes; and it is possible to reconstruct the semantic representation of a derivation S from d(S) alone.</S>
    <S sid="88" ssid="35">If we forget the word order information in the dependency trees, the rules F and B in Figure 3 are merely ,7-expanded versions of the semantic construction rules in Figure 2.</S>
    <S sid="89" ssid="36">This means that d(S) records everything we need to know about constructing the semantic representation: We can traverse it bottomup and apply the lexical semantic representation of each node to those of its subterms.</S>
    <S sid="90" ssid="37">So while the dependency trees obliterate some information in the CCG derivations (particularly its associative structure), they are indeed appropriate representations because they record all syntactic valencies and encode enough information to recompute the semantics.</S>
  </SECTION>
  <SECTION title="4 Strong generative capacity" number="5">
    <S sid="91" ssid="1">Now that we know how to see PF-CCG derivations as dependency trees, we can ask what sets of such trees can be generated by PF-CCG grammars.</S>
    <S sid="92" ssid="2">This is the question about the strong generative capacity of PF-CCG, measured in terms of dependency trees (Miller, 2000).</S>
    <S sid="93" ssid="3">In this section, we give a partial answer to this question: We show that the sets of PF-CCG-induced valency trees (dependency trees without their linear order) form regular tree languages, but that the sets of dependency trees themselves are irregular.</S>
    <S sid="94" ssid="4">This is in contrast to other prominent mildly context-sensitive grammar formalisms such as Tree Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear ContextFree Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)), in which both languages are regular.</S>
    <S sid="95" ssid="5">Formally, we define the language of all dependency trees generated by a PF-CCG grammar G as the set Furthermore, we define the set of valency trees to be the set of just the term parts of each d(S): By our previous assumption, the node labels of a valency tree are CCG lexicon entries.</S>
    <S sid="96" ssid="6">We will now show that the valency tree languages of PF-CCG grammars are regular tree languages (G&#233;cseg and Steinby, 1997).</S>
    <S sid="97" ssid="7">Regular tree languages are sets of trees that can be generated by regular tree grammars.</S>
    <S sid="98" ssid="8">Formally, a regular tree grammar (RTG) is a construct T = (N, E, 5, P), where N is an alphabet of non-terminal symbols, E is an alphabet of ranked term constructors called terminal symbols, 5 &#8712; N is a distinguished start symbol, and P is a finite set of production rules of the form A &#8594; 'y, where A &#8712; N and -y is a term over E and N, where the nonterminals can be used np as constants.</S>
    <S sid="99" ssid="9">The grammar F generates trees from the start symbol by successively expanding occurrences of nonterminals using production rules.</S>
    <S sid="100" ssid="10">For instance, the grammar that contains the productions 5 &#8212;* f(A, A), A &#8212;* g(A), and A &#8212;* a generates the tree language { f(gm(a), gn(a))  |m, n &gt; 0 }.</S>
    <S sid="101" ssid="11">We now construct an RTG F(G) that generates the set of valency trees of a PF-CCG G. For the terminal alphabet, we choose the lexicon entries: If e = (a, A  |Bi ...  |Bn, f) is a lexicon entry of G, we take e as an n-ary term constructor.</S>
    <S sid="102" ssid="12">We also take the atomic categories of G as our nonterminal symbols; the start category s of G counts as the start symbol.</S>
    <S sid="103" ssid="13">Finally, we encode each lexicon entry as a production rule: The lexicon entry e above encodes to the rule A &#8212;* e(Bn, ... , Bi).</S>
    <S sid="104" ssid="14">Let us look at our running example to see how this works.</S>
    <S sid="105" ssid="15">Representing the lexicon entries as just the words for brevity, we can write the valency tree corresponding to the CCG derivation in Figure 4 as to = h&#228;lfed(aastriiche(huus), Hans, mer); here h&#228;lfed is a ternary constructor, aastriiche is unary, and all others are constants.</S>
    <S sid="106" ssid="16">Taking the lexical categories into account, we obtain the RTG with This grammar indeed generates to, and all other valency trees induced by the sample grammar.</S>
    <S sid="107" ssid="17">More generally, LV (G) C_ L(F(G)) because the construction rules in Figure 3 ensure that if a node v becomes the i-th child of a node u in the term, then the result category of v&#8217;s lexicon entry equals the i-th argument category of u&#8217;s lexicon entry.</S>
    <S sid="108" ssid="18">This guarantees that the i-th nonterminal child introduced by the production for u can be expanded by the production for v. The converse inclusion can be shown by reconstructing, for each valency tree t, a CCG derivation S that induces t. This construction can be done by arranging the nodes in t into an order that allows us to combine every parent in t with its children using only forward and backward application.</S>
    <S sid="109" ssid="19">The CCG derivation we obtain for the example is shown in Figure 6; it is a derivation for the sentence das mer em Hans h&#228;lfed es huus aastriiche, using the same lexicon entries.</S>
    <S sid="110" ssid="20">Together, this shows that By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987).</S>
    <S sid="111" ssid="21">To our knowledge, this is the first time the regular structure of CCG derivations has been exposed.</S>
    <S sid="112" ssid="22">It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al., 1987).</S>
    <S sid="113" ssid="23">Consider for instance the CCG grammar from Vijay-Shanker and Weir&#8217;s (1994) Example 2.4, which generates the string language anbncndn; Figure 7 shows the derivation of aabbccdd.</S>
    <S sid="114" ssid="24">If we follow this derivation bottom-up, starting at the first c, the intermediate categories collect an increasingly long tail of\a arguments; for longer words from the language, this tail becomes as long as the number of cs in the string.</S>
    <S sid="115" ssid="25">The infinite set of categories this produces translates into the need for an infinite nonterminal alphabet in an RTG, which is of course not allowed.</S>
    <S sid="116" ssid="26">If we now compare PF-CCG to its most prominent mildly context-sensitive cousin, TAG, the regularity result above paints a suggestive picture: A PF-CCG valency tree assigns a lexicon entry to each word and says which other lexicon entry fills each syntactic valency.</S>
    <S sid="117" ssid="27">In this respect, it is the analogue of a TAG derivation tree (in which the lexicon entries are elementary trees), and we just saw that PF-CCG and TAG generate the same tree languages.</S>
    <S sid="118" ssid="28">On the other hand, CCG and TAG are weakly equivalent (Vijay-Shanker and Weir, 1994), i.e. they generate the same linear word orders.</S>
    <S sid="119" ssid="29">So one could expect that CCG and TAG also induce the same dependency trees.</S>
    <S sid="120" ssid="30">Interestingly, this is not the case.</S>
    <S sid="121" ssid="31">We know from the literature that those dependency trees that can be constructed from TAG derivation trees are exactly those that are well-nested and have a block-degree of at most 2 (Kuhlmann and M&#246;hl, 2007).</S>
    <S sid="122" ssid="32">The block-degree of a node u in a dependency tree is the number of &#8216;blocks&#8217; into which the subtree below u is separated by intervening nodes that are not below u, and the block-degree of a dependency tree is the maximum block-degree of its nodes.</S>
    <S sid="123" ssid="33">So for instance, the dependency tree on the right-hand side of Figure 8 has block-degree two.</S>
    <S sid="124" ssid="34">It is also well-nested, and can therefore be induced by TAG derivations.</S>
    <S sid="125" ssid="35">Things are different for the dependency trees that can be induced by PF-CCG.</S>
    <S sid="126" ssid="36">Consider the left-hand dependency tree in Figure 8, which is induced by a PF-CCG derivation built from words with the lexical categories a/a, b\a, b\b, and a.</S>
    <S sid="127" ssid="37">While this dependency tree is well-nested, it has blockdegree three: The subtree below the leftmost node consists of three parts.</S>
    <S sid="128" ssid="38">More generally, we can insert more words with the categories a/a and b\b in the middle of the sentence to obtain dependency trees with arbitrarily high block-degrees from this grammar.</S>
    <S sid="129" ssid="39">This means that unlike for TAGinduced dependency trees, there is no upper bound on the block-degree of dependency trees induced by PF-CCG&#8212;as a consequence, there are CCG dependency trees that cannot be induced by TAG.</S>
    <S sid="130" ssid="40">On the other hand, there are also dependency trees that can be induced by TAG, but not by PFCCG.</S>
    <S sid="131" ssid="41">The tree on the right-hand side of Figure 8 is an example.</S>
    <S sid="132" ssid="42">We have already argued that this tree can be induced by a TAG.</S>
    <S sid="133" ssid="43">However, it contains no two adjacent nodes that are connected by an edge; and every nontrivial PF-CCG derivation must combine two adjacent words at least at one point during the derivation.</S>
    <S sid="134" ssid="44">Therefore, the tree cannot be induced by a PF-CCG grammar.</S>
    <S sid="135" ssid="45">Furthermore, it is known that all dependency languages that can be generated by TAG or even, more generally, by LCRFS, are regular in the sense of Kuhlmann and M&#246;hl (2007).</S>
    <S sid="136" ssid="46">One crucial property of regular dependency languages is that they have a bounded block-degree; but as we have seen, there are PF-CCG dependency languages with unbounded block-degree.</S>
    <S sid="137" ssid="47">Therefore there are PF-CCG dependency languages that are not regular.</S>
    <S sid="138" ssid="48">Hence: Theorem 2 The sets of dependency trees generated by PF-CCG and TAG are incomparable.</S>
    <S sid="139" ssid="49">&#10065; We believe that these results will generalize to full CCG.</S>
    <S sid="140" ssid="50">While we have not yet worked out the induction of dependency trees from full CCG, the basic rule that CCG combines adjacent substrings should still hold; therefore, every CCG-induced dependency tree will contain at least one edge between adjacent nodes.</S>
    <S sid="141" ssid="51">We are thus left with a very surprising result: TAG and CCG both generate the same string languages and the same sets of valency trees, but they use incomparable mechanisms for linearizing valency trees into sentences.</S>
    <S sid="142" ssid="52">As a final aside, we note that the construction for extracting purely applicative derivations from the terms described by the RTG has interesting consequences for the weak generative capacity of PFCCG.</S>
    <S sid="143" ssid="53">In particular, it has the corollary that for any PF-CCG derivation 6 over a string w, there is a permutation of w that can be accepted by a PF-CCG derivation that uses only application&#8212;that is, every string language L that can be generated by a PFCCG grammar has a context-free sublanguage L' such that all words in L are permutations of words in L'.</S>
    <S sid="144" ssid="54">This means that many string languages that we commonly associate with CCG cannot be generated by PF-CCG.</S>
    <S sid="145" ssid="55">One such language is anbncndn.</S>
    <S sid="146" ssid="56">This language is not itself context-free, and therefore any PF-CCG grammar whose language contains it also contains permutations in which the order of the symbols is mixed up.</S>
    <S sid="147" ssid="57">The culprit for this among the restrictions that distinguish PF-CCG from full CCG seems to be that PF-CCG grammars must allow all instances of the application rules.</S>
    <S sid="148" ssid="58">This would mean that the ability of CCG to generate noncontext-free languages (also linguistically relevant ones) hinges crucially on its ability to restrict the allowable instances of rule schemata, for instance, using slash types (Baldridge and Kruijff, 2003).</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="6">
    <S sid="149" ssid="1">In this paper, we have shown how to read derivations of PF-CCG as dependency trees.</S>
    <S sid="150" ssid="2">Unlike previous proposals, our view on CCG dependencies is in line with the mainstream dependency parsing literature, which assumes tree-shaped dependency structures; while our dependency trees are less informative than the CCG derivations themselves, they contain sufficient information to reconstruct the semantic representation.</S>
    <S sid="151" ssid="3">We used our new dependency view to compare the strong generative capacity of PF-CCG with other mildly contextsensitive grammar formalisms.</S>
    <S sid="152" ssid="4">It turns out that the valency trees generated by a PF-CCG grammar form regular tree languages, as in TAG and LCFRS; however, unlike these formalisms, the sets of dependency trees including word order are not regular, and in particular can be more non-projective than the other formalisms permit.</S>
    <S sid="153" ssid="5">Finally, we found new formal evidence for the importance of restricting rule schemata for describing non-context-free languages in CCG.</S>
    <S sid="154" ssid="6">All these results were technically restricted to the fragment of PF-CCG, and one focus of future work will be to extend them to as large a fragment of CCG as possible.</S>
    <S sid="155" ssid="7">In particular, we plan to extend the lambda notation used in Figure 3 to cover typeraising and higher-order categories.</S>
    <S sid="156" ssid="8">We would then be set to compare the behavior of wide-coverage statistical parsers for CCG with statistical dependency parsers.</S>
    <S sid="157" ssid="9">We anticipate that our results about the strong generative capacity of PF-CCG will be useful to transfer algorithms and linguistic insights between formalisms.</S>
    <S sid="158" ssid="10">For instance, the CRISP generation algorithm (Koller and Stone, 2007), while specified for TAG, could be generalized to arbitrary grammar formalisms that use regular tree languages&#8212; given our results, to CCG in particular.</S>
    <S sid="159" ssid="11">On the other hand, we find it striking that CCG and TAG generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree.</S>
    <S sid="160" ssid="12">Indeed, the exact characterization of the class of CCG-inducable dependency languages is an open issue.</S>
    <S sid="161" ssid="13">This also has consequences for parsing complexity: We can understand why TAG and LCFRS can be parsed in polynomial time from the bounded block-degree of their dependency trees (Kuhlmann and M&#246;hl, 2007), but CCG can be parsed in polynomial time (Vijay-Shanker and Weir, 1990) without being restricted in this way.</S>
    <S sid="162" ssid="14">This constitutes a most interesting avenue of future research that is opened up by our results.</S>
    <S sid="163" ssid="15">Acknowledgments.</S>
    <S sid="164" ssid="16">We thank Mark Steedman, Jason Baldridge, and Julia Hockenmaier for valuable discussions about CCG, and the reviewers for their comments.</S>
    <S sid="165" ssid="17">The work of Alexander Koller was funded by a DFG Research Fellowship and the Cluster of Excellence &#8220;Multimodal Computing and Interaction&#8221;.</S>
    <S sid="166" ssid="18">The work of Marco Kuhlmann was funded by the Swedish Research Council.</S>
  </SECTION>
</PAPER>
