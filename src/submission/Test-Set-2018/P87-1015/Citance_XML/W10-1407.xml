<PAPER>
  <S sid="0">Direct Parsing of Discontinuous Constituents in German</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Discontinuities occur especially frequently in languages with a relatively free word order, such as German.</S>
    <S sid="2" ssid="2">Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser.</S>
    <S sid="3" ssid="3">In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks.</S>
    <S sid="4" ssid="4">In both treebanks, discontinuities are annotated with crossing branches.</S>
    <S sid="5" ssid="5">Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Languages with a rather free word order, like German, display discontinuous constituents particularly frequently.</S>
    <S sid="7" ssid="2">In (1), the discontinuity is caused by an extraposed relative clause.</S>
    <S sid="8" ssid="3">In most constituency treebanks, sentence annotation is restricted to having the shape of trees without crossing branches, and the non-local dependencies induced by the discontinuities are modeled by an additional mechanism.</S>
    <S sid="9" ssid="4">In the Penn Treebank (PTB) (Marcus et al., 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges.</S>
    <S sid="10" ssid="5">In the German T&#168;uBa-D/Z (Telljohann et al., 2006), additional edges are established by a combination of topological field annotation and special edge labels.</S>
    <S sid="11" ssid="6">As an example, Fig.</S>
    <S sid="12" ssid="7">1 shows a tree from T&#168;uBa-D/Z with the annotation of (1).</S>
    <S sid="13" ssid="8">Note here the edge label ONMOD on the relative clause which indicates that the subject of the sentence (alle Attribute) is modified.</S>
    <S sid="14" ssid="9">Another language with a rather free word order is Bulgarian.</S>
    <S sid="15" ssid="10">In (2), the discontinuity is caused by topicalization.</S>
    <S sid="16" ssid="11">&#8216;As for pens, I only buy expensive ones.&#8217; However, in a few other treebanks, such as the German NeGra and TIGER treebanks (Skut et al., 1997; Brants et al., 2002), crossing branches are allowed.</S>
    <S sid="17" ssid="12">This way, all dependents of a long-distance dependency can be grouped under a single node.</S>
    <S sid="18" ssid="13">Since in general, the annotation mechanisms for non-local dependencies lie beyond the expressivity of Context-Free Grammar, non-local information is inaccessible for PCFG parsing and therefore generally discarded.</S>
    <S sid="19" ssid="14">In NeGra/TIGER annotation, e.g., tree transformation algorithms are applied before parsing in order to resolve the crossing branches.</S>
    <S sid="20" ssid="15">See, e.g., K&#168;ubler et al. (2008) and Boyd (2007) for details.</S>
    <S sid="21" ssid="16">If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG.</S>
    <S sid="22" ssid="17">In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser.</S>
    <S sid="23" ssid="18">Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010).</S>
    <S sid="24" ssid="19">LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals.</S>
    <S sid="25" ssid="20">We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect.</S>
    <S sid="26" ssid="21">2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003).</S>
    <S sid="27" ssid="22">Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source.</S>
    <S sid="28" ssid="23">In order to judge parser output quality, we use four different evaluation types.</S>
    <S sid="29" ssid="24">We use an EVALB-style measure, adapted for LCFRS, in order to compare our parser to previous work on parsing German treebanks.</S>
    <S sid="30" ssid="25">In order to address the known shortcomings of EVALB, we perform an additional evaluation using the tree distance metric of Zhang and Shasha (1989), which works independently of the fact if there are crossing branches in the trees or not, and a dependency evaluation (Lin, 1995), which has also be applied before in the context of parsing German (K&#168;ubler et al., 2008).</S>
    <S sid="31" ssid="26">Last, we evaluate certain difficult phenomena by hand on TePaCoC (K&#168;ubler et al., 2008), a set of sentences hand-picked from TIGER.</S>
    <S sid="32" ssid="27">The evaluations show that with a PLCFRS parser, competitive results can be achieved.</S>
    <S sid="33" ssid="28">The remainder of the article is structured as follows.</S>
    <S sid="34" ssid="29">In Sect.</S>
    <S sid="35" ssid="30">2, we present the formalism, the parser, and how we obtain our grammars.</S>
    <S sid="36" ssid="31">In Sect.</S>
    <S sid="37" ssid="32">3, we discuss the evaluation methods we employ.</S>
    <S sid="38" ssid="33">Sect.</S>
    <S sid="39" ssid="34">4 contains our experimental results.</S>
    <S sid="40" ssid="35">Sect.</S>
    <S sid="41" ssid="36">5 is dedicated to related work.</S>
    <S sid="42" ssid="37">Sect.</S>
    <S sid="43" ssid="38">6 contains the conclusion and presents some possible future work.</S>
  </SECTION>
  <SECTION title="2 A Parser for PLCFRS" number="2">
    <S sid="44" ssid="1">LCFRS are an extension of CFG where the nonterminals can span not only single strings but, instead, tuples of strings.</S>
    <S sid="45" ssid="2">We will notate LCFRS with the syntax of simple Range Concatenation Grammars (SRCG) (Boullier, 1998), a formalism that is equivalent to LCFRS.</S>
    <S sid="46" ssid="3">A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, 5) where form &gt; 0 where A, A1, ... , Am E N, X(i) j E V for 1 &lt; i &lt; m,1 &lt; j &lt; dim(Ai) and &#945;i E (T U V)* for 1 &lt; i &lt; dim(A).</S>
    <S sid="47" ssid="4">For all r E P, it holds that every variable X occurring in r occurs exactly once in the left-hand side (LHS) and exactly once in the right-hand side (RHS).</S>
    <S sid="48" ssid="5">The fan-out of an LCFRS G is the maximal fanout of all non-terminals in G. Furthermore, the RHS length of a rewriting rules r E P is called the rank of r and the maximal rank of all rules in P is called the rank of G. An LCFRS is called ordered if for every r E P and every RHS non-terminal A in r and each pair X1, X2 of arguments of A in the RHS of r, X1 precedes X2 in the RHS iff X1 precedes X2 in the LHS.</S>
    <S sid="49" ssid="6">Borrowed from SRCG, we specify the language of an LCFRS based on the notion of ranges.</S>
    <S sid="50" ssid="7">For some input word w = w1 &#183; &#183; &#183; wn, a range is a pair (i, j) of integers with 0 &lt; i &lt; n denoting the substring wi+1 &#183; &#183; &#183; wj.</S>
    <S sid="51" ssid="8">Note that a range denotes E iff i = j.</S>
    <S sid="52" ssid="9">Only consecutive ranges can be concatenated into new ranges.</S>
    <S sid="53" ssid="10">We can replace the variables and terminals of the rewriting rules with ranges.</S>
    <S sid="54" ssid="11">E.g., A((g, h)) &#8212;* B((g + 1, h &#8212; 1)) is a replacement of the clause A(aX1b) &#8212;* B(X1) if the input word w is such that wg+1 = a and wh = b.</S>
    <S sid="55" ssid="12">A rewriting rule in which all elements of all arguments have been consistently replaced by ranges is called an instantiated rule.</S>
    <S sid="56" ssid="13">A derivation is built by successively rewriting the LHSs of instantiated rules with its RHSs.</S>
    <S sid="57" ssid="14">The language L(G) of some LCFRS G consists of all words w = w1 &#183; &#183; &#183; wn for which it holds that there is a rule with the start symbol on the LHS which can be instantiated to (0, n) and rewritten to E. A probabilistic LCFRS (PLCFRS) is a tuple (N, T, V, P, S, p) such that (N, T, V, P, S) is a LCFRS and p : P &#8212;* [0..1] a function such that for all A E N: &#931;A(9)11$EPp(A(x) &#8212;* -6) = 1.</S>
    <S sid="58" ssid="15">There are possibly other ways to extend LCFRS with probabilities.</S>
    <S sid="59" ssid="16">This definition is supported by the fact that probabilistic MCFGs1 have been defined in the same way (Kato et al., 2006).</S>
    <S sid="60" ssid="17">Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 where p : A( pA) &#8212;* B( pB)C( &#65533;&#961;C) is an instantiated rule.</S>
    <S sid="61" ssid="18">Goal: [S, ((0, n))] We use the parser of Kallmeyer and Maier (2010).</S>
    <S sid="62" ssid="19">It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003).</S>
    <S sid="63" ssid="20">While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice.</S>
    <S sid="64" ssid="21">It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are E. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G&#180;omez-Rodriguez et al., 2009) and E-components on LHS of rules can be removed (Boullier, 1998).</S>
    <S sid="65" ssid="22">We make the assumption that POS tagging is done before parsing.</S>
    <S sid="66" ssid="23">The POS tags are special non-terminals of fan-out 1.</S>
    <S sid="67" ssid="24">Consequently, the rules are either of the form A(a) &#8212;* E where A is a POS tag and a E T or of the form A(a) &#8212;* B(x) or A(a) &#8212;* B(x)C(y) where &#945;&#65533; E (V+)di-(A), i.e., only the rules for POS tags contain terminals in their LHSs.</S>
    <S sid="68" ssid="25">The parser items have the form [A, pl, with A E N and &#961;&#65533; a vector of ranges characterizing all components of the span of A.</S>
    <S sid="69" ssid="26">We specify the set of weighted parse items via the deduction rules in Fig.</S>
    <S sid="70" ssid="27">3.</S>
    <S sid="71" ssid="28">Parsing time can be reduced by reordering the agenda during parsing such that those items are processed first which lead to a complete parse more quickly than others (Klein and Manning, 2003a).</S>
    <S sid="72" ssid="29">The parser uses for this purpose an admissible, but not monotonic estimate called LR estimate.</S>
    <S sid="73" ssid="30">It gives (relative to a sentence length) an estimate of the outside probability of some non-terminal A with a span of a certain length (the sum of the lengths of all the components of the span), a certain number of terminals to the left of the first and to the right of the last component and a certain number of terminals gaps in between the components of the A span, i.e., filling the gaps.</S>
    <S sid="74" ssid="31">A discussion of other estimates is presented at length in Kallmeyer and Maier (2010).</S>
    <S sid="75" ssid="32">We use the algorithm from Maier and S&#248;gaard (2008) to extract LCFRS rules from our data sets.</S>
    <S sid="76" ssid="33">For all nonterminals A0 with the children A1 &#183; &#183; &#183; Am (i.e., for all non-terminals which are not preterminals), we create a clause 00 &#8212;* 01 &#183; &#183; &#183; 0m with 0Z, 0 &lt; i &lt; m, labeled AZ.</S>
    <S sid="77" ssid="34">The arguments of each 0Z, 1 &lt; i &lt; m, are single variables, one for each of the continuous yield part dominated by the node AZ.</S>
    <S sid="78" ssid="35">The arguments of 00 are concatenations of these variables that describe how the discontinuous parts of the yield of A0 are obtained from the yields of its daughters.</S>
    <S sid="79" ssid="36">For all preterminals A dominating some terminal a, we extract a production A(a) &#8212;* &#949;.</S>
    <S sid="80" ssid="37">Since by definition, a label is associated with a certain fan-out, we distinguish the labels by corresponding subscripts.</S>
    <S sid="81" ssid="38">Note that this extraction algorithm yields only ordered LCFRS.</S>
    <S sid="82" ssid="39">Furthermore, note that for trees without crossing branches, this algorithm yields a PLCFRS with fan-out 1, i.e., a PCFG.</S>
    <S sid="83" ssid="40">As mentioned before, the advantage of using LCFRS is that grammar extraction is straightforward and that no separate assumptions must be made.</S>
    <S sid="84" ssid="41">Note that unlike, e.g., Range Concatenation Grammar (RCG) (Boullier, 1998), LCFRS cannot model re-entrancies, i.e., nodes with more than one incoming edge.</S>
    <S sid="85" ssid="42">While those do not occur in NeGrastyle annotation, some of the annotation in the PTB, e.g., the annotation for right node raising, can be interpreted as re-entrancies.</S>
    <S sid="86" ssid="43">This topic is left for future work.</S>
    <S sid="87" ssid="44">See Maier and Lichte (2009) for further details, especially on how treebank properties relate to properties of extracted grammars.</S>
    <S sid="88" ssid="45">Before parsing, we binarize our grammar.</S>
    <S sid="89" ssid="46">We first mark the head daughters of all non-terminal nodes using Collins-style head rules based on the NeGra rules of the Stanford Parser (Klein and Manning, 2003b) and the reorder the RHSs of all LCFRS rules such that sequence of elements to the right of the head daughter is reversed and moved to the beginning of the RHS.</S>
    <S sid="90" ssid="47">From this point, the binarization works like the transformation into Chomsky Normal Form for CFGs.</S>
    <S sid="91" ssid="48">For each rule with an RHS of length &gt; 3, we introduce a new non-terminal which covers the RHS without the first element and continue successively from left to right.</S>
    <S sid="92" ssid="49">The rightmost new rule, which covers the head daughter, is binarized to unary.</S>
    <S sid="93" ssid="50">We markovize the grammar as in the CFG case.</S>
    <S sid="94" ssid="51">To the new symbols introduced during the binarization, a variable number of symbols from the vertical and horizontal context of the original rule is added.</S>
    <S sid="95" ssid="52">Following the literature, we call the respective quantities v and h. As an example, Fig.</S>
    <S sid="96" ssid="53">4 shows the output for the production for the VP in the left tree in Fig.</S>
    <S sid="97" ssid="54">2.</S>
    <S sid="98" ssid="55">After extraction and head marking: The probabilities are then computed based on the number of occurrences of rules in the transformed treebank, using a Maximum Likelihood estimator.</S>
  </SECTION>
  <SECTION title="3 Evaluation methods" number="3">
    <S sid="99" ssid="1">We assess the quality of our parser output using different methods.</S>
    <S sid="100" ssid="2">The first is an EVALB-style metric (henceforth EVALB), i.e., we compare phrase boundaries.</S>
    <S sid="101" ssid="3">In spite of its shortcomings (Rehbein and van Genabith, 2007), it allows us to compare to previous work on parsing NeGra.</S>
    <S sid="102" ssid="4">In the context of LCFRS, we compare sets of tuples of the form [A,(i1&#65533; , i1&#65533;), ... , (i&#65533; &#65533; , i&#65533; &#65533;)&#65533;, where A is a non-terminal in some derivation tree with dim(A) = k and each (im&#65533;, im&#65533;), 1 &lt; m &lt; k, is a tuple of indices denotling a continuous sequence of terminals dominated by A.</S>
    <S sid="103" ssid="5">One set is obtained from the parser output, and one from the corresponding treebank trees.</S>
    <S sid="104" ssid="6">Using these tuple sets, we compute labeled and unlabeled recall (LR/UR), precision (LP/UP), and the F1 measure (LF1/UF1) in the usual way.</S>
    <S sid="105" ssid="7">Note that if k = 1, our metric is identical to its PCFG version.</S>
    <S sid="106" ssid="8">EVALB does not necessarily reflect parser output quality (Rehbein and van Genabith, 2007; Emms, 2008; K&#168;ubler et al., 2008).</S>
    <S sid="107" ssid="9">One of its major problems is that attachment errors are penalized too hard.</S>
    <S sid="108" ssid="10">As the second evaluation method, we therefore choose the tree-distance measure (henceforth TDIST) (Zhang and Shasha, 1989), which levitates this problem.</S>
    <S sid="109" ssid="11">It has been proposed for parser evaluation by Emms (2008).</S>
    <S sid="110" ssid="12">TDIST is an ideal candidate for evaluation of the output of a PLCFRS, since it the fact if trees have crossing branches or not is not relevant to it.</S>
    <S sid="111" ssid="13">Two trees &#964;k and &#964;A are compared on the basis of T-mappings from &#964;k to &#964;A.</S>
    <S sid="112" ssid="14">A T-mapping is a partial mapping Q of nodes of &#964;k to nodes of &#964;A where all node mappings preserve left-to-right order and ancestry.</S>
    <S sid="113" ssid="15">Within the mappings, node insertion, node deletion, and label swap operations are identified, represented resp. by the sets I, D and S. Furthermore, we consider the set M representing the matched (i.e., unchanged) nodes.</S>
    <S sid="114" ssid="16">The cost of a T-mapping is the total number of operations, i.e.</S>
    <S sid="115" ssid="17">|I|+|D|+|S|.</S>
    <S sid="116" ssid="18">The tree distance between two trees &#964;K and &#964;A is the cost of the cheapest T-mapping.</S>
    <S sid="117" ssid="19">Fig.</S>
    <S sid="118" ssid="20">5, borrowed from Emms, shows an example for a T-mapping.</S>
    <S sid="119" ssid="21">Inserted nodes are prefixed with &gt;, deleted nodes are suffixed with &lt;, and nodes with swapped labels are linked with arrows.</S>
    <S sid="120" ssid="22">Since in total, four operations are involved, to this T-mapping, a cost of 4 is assigned.</S>
    <S sid="121" ssid="23">For more details, especially on algorithms which compute TDIST, refer to Bille (2005).</S>
    <S sid="122" ssid="24">In order to convert the tree distance measure into a similarity measure like EVALB, we use the macro-averaged Dice and Jaccard normalizations as defined by Emms.</S>
    <S sid="123" ssid="25">Let &#964;K and &#964;A be two trees with |&#964;K |and |&#964;A |nodes, respectively.</S>
    <S sid="124" ssid="26">For a T-mapping Q from &#964;K to &#964;A with the sets D, I, S and M, we compute them as follows. where, in order to achieve macro-averaging, we sum the numerators and denominators over all tree pairs before dividing.</S>
    <S sid="125" ssid="27">See Emms (2008) for further details.</S>
    <S sid="126" ssid="28">The third method is dependency evaluation (henceforth DEP), as described by Lin (1995).</S>
    <S sid="127" ssid="29">It consists of comparing dependency graphs extracted from the gold data and from the parser output.</S>
    <S sid="128" ssid="30">The dependency extraction algorithm as given by Lin does also not rely on trees to be free of crossing branches.</S>
    <S sid="129" ssid="31">It only relies on a method to identify the head of each phrase.</S>
    <S sid="130" ssid="32">We use our own implementation of the algorithm which is described in Sect.</S>
    <S sid="131" ssid="33">4 of Lin (1995), combined with the head finding algorithm of the parser.</S>
    <S sid="132" ssid="34">Dependency evaluation abstracts away from another bias of EVALB.</S>
    <S sid="133" ssid="35">Concretely, it does not prefer trees with a high node/token ratio, since two dependency graphs to be compared necessarily have the same number of (terminal) nodes.</S>
    <S sid="134" ssid="36">In the context of parsing German, this evaluation has been employed previously by K&#168;ubler et al. (2008).</S>
    <S sid="135" ssid="37">Last, we evaluate on TePaCoC (Testing Parser Performance on Complex Grammatical Constructions), a set of particularly difficult sentences hand-picked from TIGER (K&#168;ubler et al., 2008).</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="136" ssid="1">Our data sources are the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks.</S>
    <S sid="137" ssid="2">In a preprocessing step, following common practice, we attach all punctuation to nodes within the tree, since it is not included in the NeGra annotation.</S>
    <S sid="138" ssid="3">In a first pass, using heuristics, we attach all nodes to the in each case highest available phrasal node such that ideally, we do not introduce new crossing branches.</S>
    <S sid="139" ssid="4">In a second pass, parentheses and quotation marks are preferably attached to the same node.</S>
    <S sid="140" ssid="5">Grammatical function labels are discarded.</S>
    <S sid="141" ssid="6">After this preprocessing step, we create a separate version of the data set, in which we resolve the crossing branches in the trees, using the common approach of re-attaching nodes to higher constituents.</S>
    <S sid="142" ssid="7">We use the first 90% of our data sets for training and the remaining 10% for testing.</S>
    <S sid="143" ssid="8">Due to memory limitations, we restrict ourselves to sentences of a maximal length of 30 words.</S>
    <S sid="144" ssid="9">Our TIGER data sets (TIGER and T-CF) have 31,568 sentences of an average length of 14.81, splitted into 31,568 sentences for training and 3,508 sentences for testing.</S>
    <S sid="145" ssid="10">Our NeGra data sets (NeGra and N-CF) have 18,335 sentences, splitted into 16,501 sentences for training and 1,834 sentences for testing.</S>
    <S sid="146" ssid="11">We parse the data sets described above with activated LR estimate.</S>
    <S sid="147" ssid="12">For all our experiments, we use the markovization settings v = 2 and h = 1, which have proven to be successful in previous work on parsing NeGra (Rafferty and Manning, 2008).</S>
    <S sid="148" ssid="13">We provide the parser with the gold tagging.</S>
    <S sid="149" ssid="14">Fig.</S>
    <S sid="150" ssid="15">6 shows the average parsing times for all data sets on an AMD Opteron node with 8GB of RAM (pure Java implementation), Tab.</S>
    <S sid="151" ssid="16">1 shows the percentage of parsed sentences.</S>
    <S sid="152" ssid="17">Tab.</S>
    <S sid="153" ssid="18">2 shows the evaluation of the parser output using EVALB, as described in the previous section.</S>
    <S sid="154" ssid="19">We report labeled and unlabeled precision, recall and F1 measure.</S>
    <S sid="155" ssid="20">Not surprisingly, reconstructing discontinuities is hard.</S>
    <S sid="156" ssid="21">Therefore, when parsing without crossing branches, the results are slightly better.</S>
    <S sid="157" ssid="22">In order to see the influence of discontinuous structures during parsing on the underlying phrase structure, we resolve the crossing branches in the parser output of NeGra and TIGER and compare it to the respective gold test data of N-CF and T-CF.</S>
    <S sid="158" ssid="23">Tab.</S>
    <S sid="159" ssid="24">3 shows the results.</S>
    <S sid="160" ssid="25">The results deteriorate slightly in comparison with N-CF and T-CF, however, they are slightly higher than for than for NeGra and TIGER.</S>
    <S sid="161" ssid="26">This is due to the fact that during the transformation, some errors in the LCFRS parses get &#8220;corrected&#8221;: Wrongly attached phrasal nodes are re-attached to unique higher positions in the trees.</S>
    <S sid="162" ssid="27">In order to give a point of comparison with previous work on parsing TIGER and NeGra, in Tab.</S>
    <S sid="163" ssid="28">4, we report some of the results from the literature.</S>
    <S sid="164" ssid="29">All of them were obtained using PCFG parsers: K&#168;ubler (2005) (Tab.</S>
    <S sid="165" ssid="30">1, plain PCFG for NeGra), K&#168;ubler et al. (2008) (Tab.</S>
    <S sid="166" ssid="31">3, plain PCFG and Stanford parser with markovization v = 2 and h = 1 for TIGER), and Petrov and Klein (2007) (Tab.</S>
    <S sid="167" ssid="32">1, Berkeley parser, latent variables).</S>
    <S sid="168" ssid="33">We include the results for N-CF and T-CF.</S>
    <S sid="169" ssid="34">Our results are slightly better than for the plain PCFG models.</S>
    <S sid="170" ssid="35">We would expect the result for TCF to be closer to the corresponding result for the Stanford parser, since we are using a comparable model.</S>
    <S sid="171" ssid="36">This difference is mostly likely due to losses induced by the LR estimate.</S>
    <S sid="172" ssid="37">All items to which the estimate assigns an outside log probability estimate of &#8722;oc get blocked and are not put on the agenda.</S>
    <S sid="173" ssid="38">This blocking has an extremely beneficial effect on parser speed.</S>
    <S sid="174" ssid="39">However, it is paid by a worse recall, as experiments with smaller data sets have shown.</S>
    <S sid="175" ssid="40">A complete discussion of the effects of estimates, as well as a discussion of other possible optimizations, is presented in Kallmeyer and Maier (2010).</S>
    <S sid="176" ssid="41">Recall finally that LCFRS parses are more informative than PCFG parses &#8211; a lower score for LCFRS EVALB than for PCFG EVALB does not necessarily mean that the PCFG parse is &#8220;better&#8221;.</S>
    <S sid="177" ssid="42">Tab.</S>
    <S sid="178" ssid="43">5 shows the results of evaluating with TDIST, excluding unparsed sentences.</S>
    <S sid="179" ssid="44">We report the dice and jaccard normalizations, as well as a summary of the distribution of the tree distances between gold trees and trees from the parser output (see Sect.</S>
    <S sid="180" ssid="45">3).</S>
    <S sid="181" ssid="46">Again, we can observe that parsing LCFRS is harder than parsing PCFG.</S>
    <S sid="182" ssid="47">As for EVALB, the results for TIGER are slightly higher than the ones for NeGra.</S>
    <S sid="183" ssid="48">The distribution of the tree distances shows that about a third of all sentences receive a completely correct parse.</S>
    <S sid="184" ssid="49">More than a half, resp. a third of all parser output trees require &#8804; 3 operations to be mapped to the corresponding gold tree, and a only a small percentage requires &#8805; 10 operations.</S>
    <S sid="185" ssid="50">To our knowledge, TDIST has not been used to evaluate parser output for NeGra and TIGER.</S>
    <S sid="186" ssid="51">However, Emms (2008) reports results for the PTB using different parsers.</S>
    <S sid="187" ssid="52">Collins&#8217; Model 1 (Collins, 1999), e.g., lies at 93.62 (Dice) and 87.87 (Jaccard).</S>
    <S sid="188" ssid="53">For the Berkeley Parser (Petrov and Klein, 2007), 94.72 and 89.87 is reported.</S>
    <S sid="189" ssid="54">We see that our results lie in them same range.</S>
    <S sid="190" ssid="55">However, Jaccard scores are lower since this normalization punishes a higher number of edit operations more severely than Dice.</S>
    <S sid="191" ssid="56">In order to meaningfully interpret which treebank properties are responsible for the fact that between the gold trees and the trees from the parser, the German data requires more tree edit operations than the English data, a TDIST evaluation of the output of an off-the-shelf PCFG parser would be necessary.</S>
    <S sid="192" ssid="57">This is left for future work.</S>
    <S sid="193" ssid="58">For the dependency evaluation, we extract dependency graphs from both the gold data and the test data and compare the unlabeled accuracy.</S>
    <S sid="194" ssid="59">Tab.</S>
    <S sid="195" ssid="60">6 shows the results.</S>
    <S sid="196" ssid="61">We report unlabeled attachment score (UAS).</S>
    <S sid="197" ssid="62">The dependency results are consistent with the previous results in as much as the scores for PCFG parsing are again higher.</S>
    <S sid="198" ssid="63">The dependency results reported in K&#168;ubler et al. (2008) however are much higher (85.6 UAS for the markovized Stanford parser).</S>
    <S sid="199" ssid="64">While a part of the losses can again be attributed to the LR estimate, another reason lies undoubtedly in the different dependency conversion method which we employ, and in further treebank transformations which K&#168;ubler et al. perform.</S>
    <S sid="200" ssid="65">In order to get a more fine grained result, in future work, we will consider graph modifications as proposed by Lin (1995) as well as including annotation-specific information from NeGra/TIGER in our conversion procedure.</S>
    <S sid="201" ssid="66">The TePaCoC data set (K&#168;ubler et al., 2008) provides 100 hand-picked sentences from TIGER which contain constructions that are especially difficult to NeGra TIGER parse.</S>
    <S sid="202" ssid="67">Out of these 100 sentences, we only consider 69.</S>
    <S sid="203" ssid="68">The remaining 31 sentences are either longer than 30 words or not included in the TIGER 2003 release (K&#168;ubler et al. use the 2005 release).</S>
    <S sid="204" ssid="69">The data is partitioned in groups of sentences with extraposed relative clauses (ERC), forward conjunction reduction (FCR), noun PP attachment (PPN), verb PP attachment (PPV), subject gap with finite/fronted verbs (SGF) and coordination of unlike constituents (CUC).</S>
    <S sid="205" ssid="70">Tab.</S>
    <S sid="206" ssid="71">7 shows the EVALB results for the (discontinuous) TePaCoC.</S>
    <S sid="207" ssid="72">We parse these sentences using the same training set as before with all TePaCoC sentences removed.</S>
    <S sid="208" ssid="73">While we cannot compare our results directly with the PCFG results (using grammatical function labels) of K&#168;ubler et al., their results nevertheless give an orientation.</S>
    <S sid="209" ssid="74">We take a closer look at all sentence groups.</S>
    <S sid="210" ssid="75">Our result for ERC is more than 15 points worse than the result of K&#168;ubler et al. The relative clause itself is mostly recognized as a sentence (though not explicitly marked as a relative clause, since we do not consider grammatical functions).</S>
    <S sid="211" ssid="76">However, it is almost consistently attached too high (on the VP or on clause level).</S>
    <S sid="212" ssid="77">While this is correct for K&#168;ubler et al., with crossing branches, it treated as an error and punished especially hard by EVALB.</S>
    <S sid="213" ssid="78">FCR is parsed mostly well and with comparable results to K&#168;ubler et al. There are too few sentences to make a strong claim about PP attachment.</S>
    <S sid="214" ssid="79">However, in both PPN and PPV flat phrases seem to be preferred, which has as a consequence that in PPN, PPs are attached too high and in PPV too low.</S>
    <S sid="215" ssid="80">Our output confirms the claim of K&#168;ubler et al.&#8217;s that unlike coordinations is the most difficult of all TePaCoC phenomena.</S>
    <S sid="216" ssid="81">The conjuncts themselves are correctly identified in most cases, however then coordinated at the wrong level.</S>
    <S sid="217" ssid="82">SGF is parsed best.</S>
    <S sid="218" ssid="83">K&#168;ubler et al. report for this group only 78.6 labeled F1 for the Stanford Parser.</S>
    <S sid="219" ssid="84">Our overall results are slightly worse than the results of K&#168;ubler et al., but show less variance.</S>
    <S sid="220" ssid="85">To sum up, not surprisingly, getting the right attachment positions seems to be hard for LCFRS, too.</S>
    <S sid="221" ssid="86">Additionally, with crossing branches, the output is rated worse, since some attachments are not present anymore without crossing branches.</S>
    <S sid="222" ssid="87">Since especially for the relative clauses, attachment positions are in fact a matter of discussion from a syntactic point of view, we will consider in future studies to selectively resolve some of the crossing branches, e.g., by attaching relative clauses to higher positions.</S>
  </SECTION>
  <SECTION title="5 Related Work" number="5">
    <S sid="223" ssid="1">The use of formalisms with a high expressivity has been explored before (Plaehn, 2004; Levy, 2005).</S>
    <S sid="224" ssid="2">To our knowledge, Plaehn is the only one to report evaluation results.</S>
    <S sid="225" ssid="3">He uses the formalism of Discontinuous Phrase Structure Grammar (DPSG).</S>
    <S sid="226" ssid="4">Limiting the sentence length to 15, he obtains 73.16 labeled F1 on NeGra.</S>
    <S sid="227" ssid="5">Evaluating all sentences of our NeGra data with a length of up to 15 words results, however, in 81.27 labeled F1.</S>
    <S sid="228" ssid="6">For a comparison between DPSG and LCFRS, refer to Maier and S&#248;gaard (2008).</S>
  </SECTION>
  <SECTION title="6 Conclusion and Future Work" number="6">
    <S sid="229" ssid="1">We have investigated the possibility of using Probabilistic Linear Context-Free Rewriting Systems for direct parsing of discontinuous constituents.</S>
    <S sid="230" ssid="2">Consequently, we have applied a PLCFRS parser on the German NeGra and TIGER treebanks.</S>
    <S sid="231" ssid="3">Our evaluation, which used different metrics, showed that a PLCFRS parser can achieve competitive results.</S>
    <S sid="232" ssid="4">In future work, all of the presented evaluation methods will be investigated to greater detail.</S>
    <S sid="233" ssid="5">In order to do this, we will parse our data sets with current state-of-the-art systems.</S>
    <S sid="234" ssid="6">Especially a more elaborate dependency conversion should enable a more informative comparison between the output of PCFG parsers and the output of the PLCFRS parser.</S>
    <S sid="235" ssid="7">Last, since an algorithm is available which extracts LCFRSs from dependency structures (Kuhlmann and Satta, 2009), the parser is instantly ready for parsing them.</S>
    <S sid="236" ssid="8">We are currently performing the corresponding experiments.</S>
  </SECTION>
</PAPER>
