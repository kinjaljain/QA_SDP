<PAPER>
	<S sid="0">Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging</S><ABSTRACT>
		<S sid="1" ssid="1">In this paper, we describe a new rerank ing strategy named word lattice reranking,for the task of joint Chinese word segmen tation and part-of-speech (POS) tagging.</S>
		<S sid="2" ssid="2">As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking.</S>
		<S sid="3" ssid="3">With aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local fea tures that can?t be easily incorporated intothe perceptron baseline.</S>
		<S sid="4" ssid="4">Experimental results show that, this strategy achieves im provement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">Recent work for Chinese word segmentation and POS tagging pays much attention to discriminative methods, such as Maximum Entropy Model (ME)(Ratnaparkhi and Adwait, 1996), Conditional Random Fields (CRFs) (Lafferty et al, 2001), perceptron training algorithm (Collins, 2002), etc. Com pared to generative ones such as Hidden Markov Model (HMM) (Rabiner, 1989; Fine et al, 1998),discriminative models have the advantage of flexi bility in representing features, and usually obtains almost perfect accuracy in two tasks.Originated by Xue and Shen (2003), the typ ical approach of discriminative models conducts c?</S>
			<S sid="6" ssid="6">2008.</S>
			<S sid="7" ssid="7">Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium.segmentation in a classification style, by assign ing each character a positional tag indicating its relative position in the word.</S>
			<S sid="8" ssid="8">If we extend thesepositional tags to include POS information, seg mentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004).</S>
			<S sid="9" ssid="9">In the rest of the paper, we call this operation mode Joint S&amp;T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&amp;T can achieve higher accuracy not only on segmentation but also on POS tagging.</S>
			<S sid="10" ssid="10">Besides the usual local features such as the character-based ones (Xue and Shen, 2003; Ng and Low, 2004), many non-local features related to POSs or words can also be employed to improveperformance.</S>
			<S sid="11" ssid="11">However, as such features are gener ated dynamically during the decoding procedure,incorporating these features directly into the classifier results in problems.</S>
			<S sid="12" ssid="12">First, the classifier?s fea ture space will grow much rapidly, which is apt to overfit on training corpus.</S>
			<S sid="13" ssid="13">Second, the variance of non-local features caused by the model evolutionduring the training procedure will hurt the parameter tuning.</S>
			<S sid="14" ssid="14">Last but not the lest, since the current predication relies on the results of prior predi cations, exact inference by dynamic programming can?t be obtained, and then we have to maintain a n-best candidate list at each considering position,which also evokes the potential risk of depress ing the parameter tuning procedure.</S>
			<S sid="15" ssid="15">As a result,many theoretically useful features such as higherorder word- or POS- grams can not be utilized ef ficiently.</S>
			<S sid="16" ssid="16">A widely used approach of using non-local features is the well-known reranking technique, which has been proved effective in many NLP tasks, for instance, syntactic parsing and machine 385 v0 v 1 v 2 v 3 v 4 v 5 v 6 v 7 C 1 :e C 2 :?</S>
			<S sid="17" ssid="17">C 3 :U C 4 :/ C 5 :?</S>
			<S sid="18" ssid="18">C 6 :?</S>
			<S sid="19" ssid="19">C 7 :Y NN VV NN M NN NN NN NN VV NN NNFigure 1: Pruned word lattice as directed graph.</S>
			<S sid="20" ssid="20">The character sequence we choose is ?e-?-U-/ ?-?-Y?.</S>
			<S sid="21" ssid="21">For clarity, we represent each subsequence-POS pair as a single edge, while ignore the corresponding scores of the edges.translation (Collins, 2000; Huang, 2008), etc. Especially, Huang (2008) reranked the packed for est, which contains exponentially many parses.</S>
			<S sid="22" ssid="22">Inspired by his work, we propose word lattice reranking, a strategy that reranks the pruned word lattice outputted by a baseline classifier, rather than only a n-best list.</S>
			<S sid="23" ssid="23">Word lattice, a directed graph as shown in Figure 1, is a packed structure that can represent many possibilities of segmentation andPOS tagging.</S>
			<S sid="24" ssid="24">Our experiments on the Penn Chi nese Treebank 5.0 show that, reranking on wordlattice gains obvious improvement over the base line classifier and the reranking on n-best list.</S>
			<S sid="25" ssid="25">Compared against the baseline, we obtain an error reduction of 11.9% on segmentation, and 16.3% on Joint S&amp;T.</S>
	</SECTION>
	<SECTION title="Word Lattice. " number="2">
			<S sid="26" ssid="1">Formally, a word lattice L is a directed graph ?V,E?, where V is the node set, and E is the edge set.</S>
			<S sid="27" ssid="2">Suppose the word lattice is for sentence C 1:n = C 1 ..C n , node v i?</S>
			<S sid="28" ssid="3">V (i = 1..n ? 1) de notes the position between C i and C i+1 , while v 0 before C 1 is the source node, and v n after C n is the sink node.</S>
			<S sid="29" ssid="4">An edge e ? E departs from v b and arrives at v e(0 ? b &lt; e ? n), it covers a subse quence of C 1:n , which is recognized as a possible word.</S>
			<S sid="30" ssid="5">Considering Joint S&amp;T, we label each edge a POS tag to represent a word-POS pair.</S>
			<S sid="31" ssid="6">A seriesof adjoining edges forms a path, and a path con necting the source node and the sink node is calleddiameter, which indicates a specific pattern of seg mentation and POS tagging.</S>
			<S sid="32" ssid="7">For a diameter d, |d| denotes the length of d, which is the count of edges contained in this diameter.</S>
			<S sid="33" ssid="8">In Figure 1, the path p ? = v?</S>
			<S sid="34" ssid="9">0 v 3 ? v? 3 v 5 ? v? 5 v 7 is a diameter, and |p ? | is 3.</S>
			<S sid="35" ssid="10">2.1 Oracle Diameter in Lattice.</S>
			<S sid="36" ssid="11">Given a sentence s, its reference r and prunedword lattice L generated by the baseline classi fier, the oracle diameter d?</S>
			<S sid="37" ssid="12">of L is define as the diameter most similar to r. With F-measure as thescoring function, we can identify d?</S>
			<S sid="38" ssid="13">using the al gorithm depicted in Algorithm 1, which is adaptedto lexical analysis from the forest oracle computa tion of Huang (2008).Before describe this algorithm in detail, we de pict the key point for finding the oracle diameter.</S>
			<S sid="39" ssid="14">Given the system?s output y and the reference y?, using |y| and |y?| to denote word counts of them respectively, and |y ? y?| to denote matched word count of |y| and |y?|, F-measure can be computed by: F (y, y ? ) = 2PR P + R = 2|y ? y ? | |y| + |y ? | (1) Here, P = |y?y ? | |y| is precision, and R = |y?y ? | |y ? |is recall.</S>
			<S sid="40" ssid="15">Notice that F (y, y?)</S>
			<S sid="41" ssid="16">isn?t a linear func tion, we need access the largest |y ? y?| for each possible |y| in order to determine the diameter with maximum F , or another word, we should know the maximum matched word count for each possible diameter length.</S>
			<S sid="42" ssid="17">The algorithm shown in Algorithm 1 works in a dynamic programming manner.</S>
			<S sid="43" ssid="18">A table node T [i, j] is defined for sequence span [i, j], and it has a structure S to remember the best |y i:j ? y ? i:j | for each |y i:j |, as well as the back pointer for this best choice.</S>
			<S sid="44" ssid="19">The for-loop in line 2 ? 14 processes for each node T [i, j] in a shorter-span-first order.</S>
			<S sid="45" ssid="20">Line 3?</S>
			<S sid="46" ssid="21">7 initialize T [i, j] according to the reference r and the word lattice?s edge set L ?E. If there exists an edge e in L ?E covering the span [i, j], then we 386 Algorithm 1 Oracle Diameter, U la Huang (2008, Sec.</S>
			<S sid="47" ssid="22">4.1).</S>
			<S sid="48" ssid="23">1: Input: sentence s, reference r and lattice L 2: for [i, j] ? [1, |s|] in topological order do 3: if ?e ? L ? E s.t. e spans from i to j then 4: if e ? label exists in r then 5: T [i, j] ? S[1]?</S>
			<S sid="49" ssid="24">1 6: else 7: T [i, j] ? S[1]?</S>
			<S sid="50" ssid="25">0 8: for k s.t. T [i, k ? 1] and T [k, j] defined do 9: for p s.t. T [i, k ? 1] ? S[p] defined do 10: for q s.t. T [k, j] ? S[q] defined do 11: n?</S>
			<S sid="51" ssid="26">T [i, k ? 1] ? S[p] + T [k, j] ? S[q] 12: if n &gt; T [i, j] ? S[p + q] then 13: T [i, j] ? S[p + q]?</S>
			<S sid="52" ssid="27">n 14: T [i, j] ? S[p + q] ? bp?</S>
			<S sid="53" ssid="28">?k, p, q?</S>
			<S sid="54" ssid="29">15: t?</S>
			<S sid="55" ssid="30">argmax t 2?T [1,|s|]?S[t] t+|r| 16: d?</S>
			<S sid="56" ssid="31">Tr(T [1, |s|] ? S[t].bp) 17: Output: oracle diameter: d?define T [i, j], otherwise we leave this node unde fined.</S>
			<S sid="57" ssid="32">In the first situation, we initialize this node?s S structure according to whether the word-POS pair of e is in the reference (line 4?7).</S>
			<S sid="58" ssid="33">Line 8?14 update T [i, j]?s S structure using the S structures from all possible child-node pair, T [i, k ? 1] andT [k, j].</S>
			<S sid="59" ssid="34">Especially, line 9?</S>
			<S sid="60" ssid="35">10 enumerate all combinations of p and q, where p and q each repre sent a kind of diameter length in T [i, k ? 1] and T [k, j].</S>
			<S sid="61" ssid="36">Line 12 ? 14 refreshes the structure S of node T [i, j] when necessary, and meanwhile, a back pointer ?k, p, q?</S>
			<S sid="62" ssid="37">is also recorded.</S>
			<S sid="63" ssid="38">Whenthe dynamic programming procedure ends, we se lect the diameter length t of the top node T [1, |s|], which maximizes the F-measure formula in line15, then we use function Tr to find the oracle di ameter d?</S>
			<S sid="64" ssid="39">by tracing the back pointer bp.</S>
			<S sid="65" ssid="40">2.2 Generation of the Word Lattice.</S>
			<S sid="66" ssid="41">We can generate the pruned word lattice using the baseline classifier, with a slight modification.</S>
			<S sid="67" ssid="42">The classifier conducts decoding by considering eachcharacter in a left-to-right fashion.</S>
			<S sid="68" ssid="43">At each considering position i, the classifier enumerates all can didate results for subsequence C 1:i , by attaching each current candidate word-POS pair p to the tail of each candidate result at p?s prior position, as the endmost of the new generated candidate.</S>
			<S sid="69" ssid="44">We give each p a score, which is the highest, among all C 1:i ?s candidates that have p as their endmost.Then we select N word-POS pairs with the high est scores, and insert them to the lattice?s edge set.</S>
			<S sid="70" ssid="45">This approach of selecting edges implies that, for the lattice?s node set, we generate a node v i at each position i. Because N is the limitation on the count Algorithm 2 Lattice generation algorithm.</S>
			<S sid="71" ssid="46">1: Input: character sequence C 1:n 2: E ? ?</S>
			<S sid="72" ssid="47">3: for i?</S>
			<S sid="73" ssid="48">1 ..</S>
			<S sid="74" ssid="49">n do 4: cands?</S>
			<S sid="75" ssid="50">5: for l?</S>
			<S sid="76" ssid="51">1 ..</S>
			<S sid="77" ssid="52">min(i, K) do 6: w ? C i?l+1:i 7: for t ? POS do 8: p?</S>
			<S sid="78" ssid="53">?w, t?</S>
			<S sid="79" ssid="54">9: p ? score?</S>
			<S sid="80" ssid="55">Eval(p) 10: s?</S>
			<S sid="81" ssid="56">p ? score + Best[i? l] 11: Best[i]?</S>
			<S sid="82" ssid="57">max(s,Best[i]) 12: insert ?s, p?</S>
			<S sid="83" ssid="58">into cands 13: sort cands according to s 14: E ? E ? cands[1..N ] ? p 15: Output: edge set of lattice: E of edges that point to the node at position i, we callthis pruning strategy in-degree pruning.</S>
			<S sid="84" ssid="59">The gen eration algorithm is shown in Algorithm 2.</S>
			<S sid="85" ssid="60">Line 3 ? 14 consider each character C iin se quence, cands is used to keep the edges closing at position i. Line 5 enumerates the candidate words ending with C i and no longer than K, where K is 20 in our experiments.</S>
			<S sid="86" ssid="61">Line 5 enumerates all POS tags for the current candidate word w, where POS denotes the POS tag set.</S>
			<S sid="87" ssid="62">Function Eval in line 9 returns the score for word-POS pair p from the baseline classifier.</S>
			<S sid="88" ssid="63">The array Best preserve the score for sequence C 1:i?s best labelling results.</S>
			<S sid="89" ssid="64">After all possible word-POS pairs (or edges) consid ered, line 13?</S>
			<S sid="90" ssid="65">14 select the N edges we want, and add them to edge set E. Though this pruning strategy seems relative rough ? simple pruning for edge set while no pruning for node set, we still achieve a promisingimprovement by reranking on such lattices.</S>
			<S sid="91" ssid="66">We be lieve more elaborate pruning strategy will results in more valuable pruned lattice.</S>
	</SECTION>
	<SECTION title="Reranking. " number="3">
			<S sid="92" ssid="1">A unified framework can be applied to describingreranking for both n-best list and pruned word lattices (Collins, 2000; Huang, 2008).</S>
			<S sid="93" ssid="2">Given the candidate set cand(s) for sentence s, the reranker se lects the best item y?</S>
			<S sid="94" ssid="3">from cand(s): y?</S>
			<S sid="95" ssid="4">= argmax y?cand(s) w ? f(y) (2) For reranking n-best list, cand(s) is simply the set of n best results from the baseline classifier.</S>
			<S sid="96" ssid="5">While for reranking word lattice, cand(s) is the set of all diameters that are impliedly built in the lattice.w ? f(y) is the dot product between a feature vec tor f and a weight vector w, its value is used to 387 Algorithm 3 Perceptron training for reranking 1: Input: Training examples{cand(s i ), y ? i } N i=1 2: w?</S>
			<S sid="97" ssid="6">0 3: for t?</S>
			<S sid="98" ssid="7">1 ..</S>
			<S sid="99" ssid="8">T do 4: for i?</S>
			<S sid="100" ssid="9">1 ..</S>
			<S sid="101" ssid="10">N do 5: y?</S>
			<S sid="102" ssid="11">argmax y?cand(s i ) w ? f(y) 6: if y?</S>
			<S sid="103" ssid="12">6= y?</S>
			<S sid="104" ssid="13">i then 7: w?</S>
			<S sid="105" ssid="14">w + f(y? i )?</S>
			<S sid="106" ssid="15">f(y?) 8: Output: Parameters: w Non-local Template Comment W 0 T 0 current word-POS pair W ?1 word 1-gram before W 0 T 0 T ?1 POS 1-gram before W 0 T 0 T ?2 T ?1 POS 2-gram before W 0 T 0 T ?3 T ?2 T ?1 POS 3-gram before W 0 T 0 Table 1: Non-local feature templates used for rerankingrerank cand(s).</S>
			<S sid="107" ssid="16">Following usual practice in pars ing, the first feature f 1 (y) is specified as the score outputted by the baseline classifier, and its value is a real number.</S>
			<S sid="108" ssid="17">The other features are non-local ones such as word- and POS- n-grams extractedfrom candidates in n-best list (for n-best rerank ing) or diameters (for word lattice reranking), and they are 0 ? 1 valued.</S>
			<S sid="109" ssid="18">3.1 Training of the Reranker.</S>
			<S sid="110" ssid="19">We adopt the perceptron algorithm (Collins, 2002) to train the reranker.</S>
			<S sid="111" ssid="20">as shown in Algorithm 3.</S>
			<S sid="112" ssid="21">Weuse a simple refinement strategy of ?averaged pa rameters?</S>
			<S sid="113" ssid="22">of Collins (2002) to alleviate overfittingon the training corpus and obtain more stable per formance.</S>
			<S sid="114" ssid="23">For every training example {cand(s i ), y ? i }, y ? i denotes the best candidate in cand(s i).</S>
			<S sid="115" ssid="24">For n best reranking, the best candidate is easy to find, whereas for word lattice reranking, we should usethe algorithm in Algorithm 1 to determine the or acle diameter, which represents the best candidate result.</S>
			<S sid="116" ssid="25">3.2 Non-local Feature Templates.</S>
			<S sid="117" ssid="26">The non-local feature templates we use to train thereranker are listed in Table 1.</S>
			<S sid="118" ssid="27">Notice that all fea tures generated from these templates don?t contain ?future?</S>
			<S sid="119" ssid="28">words or POS tags, it means that we only use current or history word- or POS- n-grams to evaluate the current considering word-POS pair.</S>
			<S sid="120" ssid="29">Although it is possible to use ?future?</S>
			<S sid="121" ssid="30">information in n-best list reranking, it?s not the same when wererank the pruned word lattice.</S>
			<S sid="122" ssid="31">As we have to tra verse the lattice topologically, we face difficulty in Algorithm 4 Cube pruning for non-local features.</S>
			<S sid="123" ssid="32">1: function CUBE(L) 2: for v ? L ? V in topological order do 3: NBEST(v) 4: return D v sink [1] 5: procedure NBEST(v) 6: heap?</S>
			<S sid="124" ssid="33">7: for v?</S>
			<S sid="125" ssid="34">topologically before v do 8: ??</S>
			<S sid="126" ssid="35">all edges from v?</S>
			<S sid="127" ssid="36">to v 9: p?</S>
			<S sid="128" ssid="37">?D v ? ,??</S>
			<S sid="129" ssid="38">10: ?p,1??score?</S>
			<S sid="130" ssid="39">Eval(p,1) 11: PUSH(?p,1?, heap) 12: HEAPIFY(heap) 13: buf ? ?</S>
			<S sid="131" ssid="40">14: while |heap| &gt; 0 and |buf | &lt; N do 15: item?</S>
			<S sid="132" ssid="41">POP-MAX(heap) 16: append item to buf 17: PUSHSUCC(item, heap) 18: sort buf to D v 19: procedure PUSHSUCC(?p, j?, heap) 20: p is ?vec 1 ,vec 2 ? 21: for i?</S>
			<S sid="133" ssid="42">1..2 do 22: j?</S>
			<S sid="134" ssid="43">j+ bi 23: if |vec i | ? j ? i then 24: ?p, j???score?</S>
			<S sid="135" ssid="44">Eval(p, j?)</S>
			<S sid="136" ssid="45">25: PUSH(?p, j??, heap)utilizing the information ahead of the current con sidering node.</S>
			<S sid="137" ssid="46">3.3 Reranking by Cube Pruning.</S>
			<S sid="138" ssid="47">Because of the non-local features such as wordand POS- n-grams, the reranking procedure is similar to machine translation decoding with inter grated language models, and should maintain alist of N best candidates at each node of the lat tice.</S>
			<S sid="139" ssid="48">To speed up the procedure of obtaining the N best candidates, following Huang (2008, Sec.3.3), we adapt the cube pruning method from machine translation (Chiang, 2007; Huang and Chiang 2007) which is based on efficient k-best pars ing algorithms (Huang and Chiang, 2005).</S>
			<S sid="140" ssid="49">As shown in Algorithm 4, cube pruning workstopologically in the pruned word lattice, and main tains a list of N best derivations at each node.</S>
			<S sid="141" ssid="50">When deducing a new derivation by attaching a current word-POS pair to the tail of a antecedent derivation, a function Eval is used to compute the new derivation?s score (line 10 and 24).</S>
			<S sid="142" ssid="51">We use a max-heap heap to hold the candidates for the next-best derivation.</S>
			<S sid="143" ssid="52">Line 7 ? 11 initialize heap to the set of top derivations along each deducing source, the vector pair ?D v head,??.Here, ? de notes the vector of current word-POS pairs, while D v head denotes the vector of N best derivations at ??s antecedent node.</S>
			<S sid="144" ssid="53">Then at each iteration, 388 Non-lexical-target Instances C n (n = ?2..2) C ?2 =e, C ?1 =?, C 0 =U, C 1 =/, C 2 =?</S>
			<S sid="145" ssid="54">C n C n+1 (n = ?2..1) C ?2 C ?1 =e?, C ?1 C 0 =?U, C 0 C 1 =U/, C 1 C 2 =/?</S>
			<S sid="146" ssid="55">C ?1 C 1 C ?1 C 1 =?/ Lexical-target Instances C 0 C n (n = ?2..2) C 0 C ?2 =Ue, C 0 C ?1 =U?, C 0 C 0 =UU, C 0 C 1 =U/, C 0 C 2 =U? C 0 C n C n+1 (n = ?2..1) C 0 C ?2 C ?1 =Ue?, C 0 C ?1 C 0 =U?U, C 0 C 0 C 1 =UU/, C 0 C 1 C 2 =U/?</S>
			<S sid="147" ssid="56">C 0 C ?1 C 1 C 0 C ?1 C 1 = U?/ Table 2: Feature templates and instances.</S>
			<S sid="148" ssid="57">Suppose we consider the third character ?U? in the sequence ?e?U/??.</S>
			<S sid="149" ssid="58">we pop the best derivation from heap (line 15), and push its successors into heap (line 17), until we get N derivations or heap is empty.</S>
			<S sid="150" ssid="59">In line 22 of function PUSHSUCC, j is a vector composed of two index numbers, indicating the two candidates?</S>
			<S sid="151" ssid="60">indexes in the two vectors of the deducing source p, where the two candidates are selected to deduce a new derivation.</S>
			<S sid="152" ssid="61">j? is a increment vector, whoseith dimension is 1, while others are 0.</S>
			<S sid="153" ssid="62">As non local features (word- and POS- n-grams) are used by function Eval to compute derivation?s score, the derivations extracted from heap may be out of order.</S>
			<S sid="154" ssid="63">So we use a buffer buf to keep extracted derivations (line 16), then sort buf and put its first N items to D v (line 18).</S>
	</SECTION>
	<SECTION title="Baseline Perceptron Classifier. " number="4">
			<S sid="155" ssid="1">4.1 Joint S&amp;T as Classification.</S>
			<S sid="156" ssid="2">Following Jiang et al (2008), we describe segmen tation and Joint S&amp;T as below: For a given Chinese sentence appearing as a character sequence: C 1:n = C 1 C 2 ..</S>
			<S sid="157" ssid="3">C n the goal of segmentation is splitting the sequence into several subsequences: C 1:e 1 C e 1 +1:e 2 ..</S>
			<S sid="158" ssid="4">C e m?1 +1:e m While in Joint S&amp;T, each of these subsequences is labelled a POS tag: C 1:e 1 /t 1 C e 1 +1:e 2 /t 2 ..</S>
			<S sid="159" ssid="5">C e m?1 +1:e m /t m Where C i (i = 1..n) denotes a character, C l:r (l ? r) denotes the subsequence ranging from C l to C r , and t i (i = 1..m,m ? n) denotes the POS tag of C e i?1 +1:e i .If we label each character a positional tag indicating its relative position in an expected subsequence, we can obtain the segmentation result ac cordingly.</S>
			<S sid="160" ssid="6">As described in Ng and Low (2004) andJiang et al (2008), we use s indicating a singlecharacter word, while b, m and e indicating the be gin, middle and end of a word respectively.</S>
			<S sid="161" ssid="7">With these positional tags, the segmentation transforms to a classification problem.</S>
			<S sid="162" ssid="8">For Joint S&amp;T, we expand positional tags by attaching POS to their tails as postfix.</S>
			<S sid="163" ssid="9">As each tag now contains both positional- and POS- information, Joint S&amp;T canalso be resolved in a classification style frame work.</S>
			<S sid="164" ssid="10">It means that, a subsequence is a word withPOS t, only if the positional part of the tag se quence conforms to s or bm?e pattern, and each element in the POS part equals to t. For example, a tag sequence b NN m NN e NN represents a three-character word with POS tag NN . 4.2 Feature Templates.</S>
			<S sid="165" ssid="11">The features we use to build the classifier are gen erated from the templates of Ng and Low (2004).</S>
			<S sid="166" ssid="12">For convenience of comparing with other, theydidn?t adopt the ones containing external knowl edge, such as punctuation information.</S>
			<S sid="167" ssid="13">All theirtemplates are shown in Table 2.</S>
			<S sid="168" ssid="14">C denotes a character, while its subscript indicates its position rela tive to the current considering character(it has the subscript 0).</S>
			<S sid="169" ssid="15">The table?s upper column lists the templates that immediately from Ng and Low (2004).</S>
			<S sid="170" ssid="16">they named these templates non-lexical-target becausepredications derived from them can predicate with out considering the current character C 0 . Tem-.</S>
			<S sid="171" ssid="17">plates called lexical-target in the column below areintroduced by Jiang et al (2008).</S>
			<S sid="172" ssid="18">They are gener ated by adding an additional field C 0to each nonlexical-target template, so they can carry out pred ication not only according to the context, but also according to the current character itself.</S>
			<S sid="173" ssid="19">Notice that features derived from the templates in Table 2 are all local features, which means allfeatures are determined only by the training instances, and they can be generated before the train ing procedure.</S>
			<S sid="174" ssid="20">389 Algorithm 5 Perceptron training algorithm.</S>
			<S sid="175" ssid="21">1: Input: Training examples (x i , y i ) 2: ??</S>
			<S sid="176" ssid="22">0 3: for t?</S>
			<S sid="177" ssid="23">1 ..</S>
			<S sid="178" ssid="24">T do 4: for i?</S>
			<S sid="179" ssid="25">1 ..</S>
			<S sid="180" ssid="26">N do 5: z i ? argmax z?GEN(x i ) ?(x i , z) ? ?</S>
			<S sid="181" ssid="27">6: if z i 6= y i then 7: ??</S>
			<S sid="182" ssid="28">+?(x i , y i )??(x i , z i ) 8: Output: Parameters: ? 4.3 Training of the Classifier.</S>
			<S sid="183" ssid="29">Collins (2002)?s perceptron training algorithmwere adopted again, to learn a discriminative clas sifier, mapping from inputs x ? X to outputs y ? Y . Here x is a character sequence, and y isthe sequence of classification result of each character in x. For segmentation, the classification re sult is a positional tag, while for Joint S&amp;T, it is an extended tag with POS information.</S>
			<S sid="184" ssid="30">X denotes the set of character sequence, while Y denotes the corresponding set of tag sequence.</S>
			<S sid="185" ssid="31">According to Collins (2002), the function GEN(x) generates all candidate tag sequences for the character sequence x , the representation ? maps each training example (x, y) ? X ? Y to a feature vector ?(x, y) ? Rd, and the parameter vector ? ?</S>
			<S sid="186" ssid="32">Rd is the weight vector corresponding to the expected perceptron model?s feature space.</S>
			<S sid="187" ssid="33">For a given input character sequence x, the mission of the classifier is to find the tag sequence F (x) satisfying: F (x) = argmax y?GEN(x) ?(x, y) ? ?</S>
			<S sid="188" ssid="34">(3) The inner product ?(x, y) ? ?</S>
			<S sid="189" ssid="35">is the score of the result y given x, it represents how much plausibly we can label character sequence x as tag sequence y. The training algorithm is depicted in Algorithm to alleviate overfitting.</S>
	</SECTION>
	<SECTION title="Experiments. " number="5">
			<S sid="190" ssid="1">Our experiments are conducted on the Penn Chi nese Treebank 5.0 (CTB 5.0).</S>
			<S sid="191" ssid="2">Following usual practice of Chinese parsing, we choose chapters1?260 (18074 sentences) as the training set, chap ters 301?</S>
			<S sid="192" ssid="3">325 (350 sentences) as the development set, and chapters 271 ? 300 (348 sentences) as the final test set.</S>
			<S sid="193" ssid="4">We report the performance ofthe baseline classifier, and then compare the per formance of the word lattice reranking against the 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0 1 2 3 4 5 6 7 8 9 10F me as ur e number of iterations Perceptron Learning Curves Segmentation Joint ST Figure 2: Baseline averaged perceptron learning curves for segmentation and Joint S&amp;T. n-best reranking, based on this baseline classifier.For each experiment, we give accuracies on segmentation and Joint S&amp;T. Analogous to the situa tion in parsing, the accuracy of Joint S&amp;T means that, a word-POS is recognized only if both the positional- and POS- tags are correctly labelled for each character in the word?s span.</S>
			<S sid="194" ssid="5">5.1 Baseline Perceptron Classifier.</S>
			<S sid="195" ssid="6">The perceptron classifier are trained on the train ing set using features generated from the templates in Table 2, and the development set is used to determine the best parameter vector.</S>
			<S sid="196" ssid="7">Figure 2 shows the learning curves for segmentation and Joint S&amp;T on the development set.</S>
			<S sid="197" ssid="8">We choose the averaged parameter vector after 7 iterations forthe final test, this parameter vector achieves an F measure of 0.973 on segmentation, and 0.925 on Joint S&amp;T. Although the accuracy on segmentation is quite high, it is obviously lower on Joint S&amp;T. Experiments of Ng and Low (2004) on CTB 3.0 also shown the similar trend, where they obtained F-measure 0.952 on segmentation, and 0.919 on Joint S&amp;T. 5.2 Preparation for Reranking.</S>
			<S sid="198" ssid="9">For n-best reranking, we can easily generate n bestresults for every training instance, by a modification for the baseline classifier to hold n best can didates at each considering point.</S>
			<S sid="199" ssid="10">For word lattice reranking, we use the algorithm in Algorithm 2 to generate the pruned word lattice.</S>
			<S sid="200" ssid="11">Given a training instance s i , its n best result list or pruned word lattice is used as a reranking instance cand(s i ),the best candidate result (of the n best list) or or acle diameter (of the pruned word lattice) is the reranking target y?</S>
			<S sid="201" ssid="12">i . We find the best result of the.</S>
			<S sid="202" ssid="13">n best results simply by computing each result?s 390F-measure, and we determine the oracle diame ter of the pruned word lattice using the algorithm depicted in Algorithm 1.</S>
			<S sid="203" ssid="14">All pairs of cand(s i ) and y?</S>
			<S sid="204" ssid="15">i deduced from the baseline model?s training instances comprise the training set for reranking.</S>
			<S sid="205" ssid="16">The development set and test set for reranking areobtained in the same way.</S>
			<S sid="206" ssid="17">For the reranking train ing set {cand(s i ), y ? i } N i=1 , {y ? i } N i=1 is called oracle set, and the F-measure of {y? i } N i=1against the ref erence set is called oracle F-measure.</S>
			<S sid="207" ssid="18">We use theoracle F-measure indicating the utmost improve ment that an reranking algorithm can achieve.</S>
			<S sid="208" ssid="19">5.3 Results and Analysis.</S>
			<S sid="209" ssid="20">The flows of the n-best list reranking and the pruned word lattice reranking are similar to the training procedure for the baseline classifier.</S>
			<S sid="210" ssid="21">Thetraining set for reranking is used to tune the param eter vector of the reranker, while the development set for reranking is used to determine the optimalnumber of iterations for the reranker?s training pro cedure.We compare the performance of the word lattice reranking against the n-best list reranking.</S>
			<S sid="211" ssid="22">Table 3 shows the experimental results.</S>
			<S sid="212" ssid="23">The upper four rows are the experimental results for n best list reranking, while the four rows below arefor word lattice reranking.</S>
			<S sid="213" ssid="24">In n-best list rerank ing, with list size 20, the oracle F-measure on Joint S&amp;T is 0.9455, and the reranked F-measure is 0.9280.</S>
			<S sid="214" ssid="25">When list size grows up to 50, the oracle F-measure on Joint S&amp;T jumps to 0.9552, whilethe reranked F-measure becomes 0.9302.</S>
			<S sid="215" ssid="26">However, when n grows to 100, it brings tiny improvement over the situation of n = 50.</S>
			<S sid="216" ssid="27">In word lat tice reranking, there is a trend similar to that inn-best reranking, the performance difference between in degree = 2 and in degree = 5 is ob vious, whereas the setting in degree = 10 doesnot obtain a notable improvement over the perfor mance of in degree = 5.</S>
			<S sid="217" ssid="28">We also notice that even with a relative small in degree limitation, such asin degree = 5, the oracle F-measures for seg mentation and Joint S&amp;T both reach a quite highlevel.</S>
			<S sid="218" ssid="29">This indicates the pruned word lattice con tains much more possibilities of segmentation and tagging, compared to n-best list.With the setting in degree = 5, the oracle F measure on Joint S&amp;T reaches 0.9774, and the reranked F-measure climbs to 0.9336.</S>
			<S sid="219" ssid="30">It achieves an error reduction of 16.3% on Joint S&amp;T, and an error reduction of 11.9% on segmentation, over the n-best Ora Seg Tst Seg Ora S&amp;T Tst S&amp;T 20 0.9827 0.9749 0.9455 0.9280 50 0.9903 0.9754 0.9552 0.9302 100 0.9907 0.9755 0.9558 0.9305 Degree Ora Seg Rnk Seg Ora S&amp;T Rnk S&amp;T 2 0.9898 0.9753 0.9549 0.9296 5 0.9927 0.9774 0.9768 0.9336 10 0.9934 0.9774 0.9779 0.9337 Table 3: Performance of n-best list reranking andword lattice reranking.</S>
			<S sid="220" ssid="31">n-best: the size of the nbest list for n-best list reranking; Degree: the in de gree limitation for word lattice reranking; Ora Seg: oracle F-measure on segmentation of n-best lists or word lattices; Ora S&amp;T: oracle F-measure on JointS&amp;T of n-best lists or word lattices; Rnk Seg: F measure on segmentation of reranked result; Rnk S&amp;T: F-measure on Joint S&amp;T of reranked result baseline classifier.</S>
			<S sid="221" ssid="32">While for n-best reranking with setting n = 50, the Joint S&amp;T?s error reduction is 6.9% , and the segmentation?s error reduction is 8.9%.</S>
			<S sid="222" ssid="33">We can see that reranking on pruned word lattice is a practical method for segmentation andPOS tagging.</S>
			<S sid="223" ssid="34">Even with a much small data rep resentation, it obtains obvious advantage over the n-best list reranking.</S>
			<S sid="224" ssid="35">Comparing between the baseline and the tworeranking techniques, We find the non-local infor mation such as word- or POS- grams do improve accuracy of segmentation and POS tagging, and we also find the reranking technique is effective to utilize these kinds of information.</S>
			<S sid="225" ssid="36">As even a small scale n-best list or pruned word lattice can achievea rather high oracle F-measure, reranking tech nique, especially the word lattice reranking would be a promising refining strategy for segmentation and POS tagging.</S>
			<S sid="226" ssid="37">This is based on this viewpoint: On the one hand, compared with the initial input character sequence, the pruned word lattice has aquite smaller search space while with a high ora cle F-measure, which enables us to conduct more precise reranking over this search space to find the best result.</S>
			<S sid="227" ssid="38">On the other hand, as the structure of the search space is approximately outlined by the topological directed architecture of pruned wordlattice, we have a much wider choice for feature selection, which means that we would be able to utilize not only features topologically before the cur rent considering position, just like those depictedin Table 2 in section 4, but also information topo logically after it, for example the next word W 1 or the next POS tag T 1 . We believe the pruned word.</S>
			<S sid="228" ssid="39">391lattice reranking technique will obtain higher im provement, if we develop more precise reranking algorithm and more appropriate features.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="229" ssid="1">This paper describes a reranking strategy calledword lattice reranking.</S>
			<S sid="230" ssid="2">As a derivation of the forest reranking of Huang (2008), it performs rerank ing on pruned word lattice, instead of on n-best list.</S>
			<S sid="231" ssid="3">Using word- and POS- gram information, this reranking technique achieves an error reduction of 16.3% on Joint S&amp;T, and 11.9% on segmentation, over the baseline classifier, and it also outperformsreranking on n-best list.</S>
			<S sid="232" ssid="4">It confirms that word lattice reranking can effectively use non-local infor mation to select the best candidate result, from a relative small representation structure while with aquite high oracle F-measure.</S>
			<S sid="233" ssid="5">However, our rerank ing implementation is relative coarse, and it must have many chances for improvement.</S>
			<S sid="234" ssid="6">In futurework, we will develop more precise pruning al gorithm for word lattice generation, to further cutdown the search space while maintaining the ora cle F-measure.</S>
			<S sid="235" ssid="7">We will also investigate the featureselection strategy under the word lattice architec ture, for effective use of non-local information.</S>
			<S sid="236" ssid="8">AcknowledgementThis work was supported by National Natural Sci ence Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No. 2006AA010108.</S>
			<S sid="237" ssid="9">We show our special thanks to Liang Huang for his valuable suggestions.</S>
	</SECTION>
</PAPER>
