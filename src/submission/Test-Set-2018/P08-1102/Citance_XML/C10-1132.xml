<PAPER>
	<S sid="0">A Character-Based Joint Model for Chinese Word Segmentation</S><ABSTRACT>
		<S sid="1" ssid="1">The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discrimi native and generative models can be adopted in that framework.</S>
		<S sid="2" ssid="2">However, generative and discriminative charac ter-based approaches are significantly different and complement each other.</S>
		<S sid="3" ssid="3">A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches.</S>
		<S sid="4" ssid="4">Experiments on the Sec ond SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one.</S>
		<S sid="5" ssid="5">In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best F score in four out of five corpora.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">Chinese word segmentation (CWS) plays an important role in most Chinese NLP applica tions such as machine translation, information retrieval and question answering.</S>
			<S sid="7" ssid="7">Many statis tical methods for CWS have been proposed in the last two decades, which can be classified as either word-based or character-based.</S>
			<S sid="8" ssid="8">The word-based approach regards the word as the basic unit, and the desired segmentation result is the best word sequence found by the search process.</S>
			<S sid="9" ssid="9">On the other hand, the character-based approach treats the word segmentation task as a character tagging problem.</S>
			<S sid="10" ssid="10">The final segmentation result is thus indirectly generated ac cording to the tag assigned to each associated character.</S>
			<S sid="11" ssid="11">Since the vocabulary size of possible character-tag-pairs is limited, the character based models can tolerate out-of-vocabulary (OOV) words and have become the dominant technique for CWS in recent years.</S>
			<S sid="12" ssid="12">On the other hand, statistical approaches can also be classified as either adopting a genera tive model or adopting a discriminative model.</S>
			<S sid="13" ssid="13">The generative model learns the joint probabil ity of the given input and its associated label sequence, while the discriminative model learns the posterior probability directly.</S>
			<S sid="14" ssid="14">Generative models often do not perform well because they make strong independence assump tions between features and labels.</S>
			<S sid="15" ssid="15">However, (Toutanova, 2006) shows that generative models can also achieve very similar or better performance than the corresponding discrimina tive models if they have a structure that avoids unrealistic independence assumptions.</S>
			<S sid="16" ssid="16">In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al, 2003), which is a well known approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach?</S>
			<S sid="17" ssid="17">3) The character-based discriminative model (Xue, 2003; Peng et al, 2004; Tseng et al, 2005; Jiang et al, 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works; 1173 4) The character-based generative model (Wang et al, 2009), which adopts a character tag-pair-based n-gram model and achieves comparable results with the popular character based discriminative model.</S>
			<S sid="18" ssid="18">In general, character-based models are much more robust on OOV words than word-based approaches do, as the vocabulary size of char acters is a closed set (versus the open set of that of words).</S>
			<S sid="19" ssid="19">Furthermore, among those character-based approaches, the generative model and the discriminative one complement each other in handling in-vocabulary (IV) words and OOV words.</S>
			<S sid="20" ssid="20">Therefore, a character based joint model is proposed to combine them.</S>
			<S sid="21" ssid="21">This proposed joint approach has achieved good balance between IV word recognition and OOV word identification.</S>
			<S sid="22" ssid="22">The experiments of closed tests on the second SIGHAN Bakeoff (Emerson, 2005) show that the joint model significantly outperforms the baseline models of both generative and discriminative ap proaches.</S>
			<S sid="23" ssid="23">Moreover, statistical significance tests also show that the joint model is significantly better than all those state-of-the-art sys tems reported in the literature and achieves the best F-score in four of the five corpora tested.</S>
	</SECTION>
	<SECTION title="Character-Based Models for CWS. " number="2">
			<S sid="24" ssid="1">The goal of CWS is to find the corresponding word sequence for a given character sequence.</S>
			<S sid="25" ssid="2">Character-based model is to find out the corre sponding tags for given character sequence.</S>
			<S sid="26" ssid="3">2.1 Character-Based Discriminative Model.</S>
			<S sid="27" ssid="4">The character-based discriminative model (Xue, 2003) treats segmentation as a tagging problem, which assigns a corresponding tag to each character.</S>
			<S sid="28" ssid="5">The model is formulated as: 1 1 1 1 1 2 1 1 ( ) ( , ) ( n n n n k n k k k k k P t c P t t c P t c?</S>
			<S sid="29" ssid="6">2 )k + (1) Where tk is a member of {Begin, Middle, End, Single} (abbreviated as B, M, E and S from now on) to indicate the corresponding position of character ck in its associated word.</S>
			<S sid="30" ssid="7">For ex ample, the word ????</S>
			<S sid="31" ssid="8">(Beijing City)?</S>
			<S sid="32" ssid="9">will be assigned with the corresponding tags as: ??</S>
			<S sid="33" ssid="10">/B (North) ?/M (Capital) ?/E (City)?.</S>
			<S sid="34" ssid="11">Since this tagging approach treats characters as basic units, the vocabulary size of those possible character-tag-pairs is limited.</S>
			<S sid="35" ssid="12">There fore, this method is robust to OOV words and could possess a high recall of OOV words (ROOV).</S>
			<S sid="36" ssid="13">Although the dependency between adjacent tags/labels can be addressed, the de pendency between adjacent characters within a word cannot be directly modeled under this framework.</S>
			<S sid="37" ssid="14">Lower recall of IV words (RIV) is thus usually accompanied (Wang et al, 2009).</S>
			<S sid="38" ssid="15">In this work, the character-based discriminative model is implemented by adopting the fea ture templates given by (Ng and Low, 2004), but excluding those ones that are forbidden by the closed test regulation of SIGHAN (e.g., Pu(C0): whether C0 is a punctuation).</S>
			<S sid="39" ssid="16">Those feature templates adopted are listed below: 1 1 1 ( ) ( 2, 1,0,1, 2); ( ) ( 2, 1,0,1); ( ) n n n a C n b C C n c C C + ? = ? ?</S>
			<S sid="40" ssid="17">For example, when we consider the third character ???</S>
			<S sid="41" ssid="18">in the sequence ???????, template (a) results in the features as following: C-2=?, C-1=?, C0=?, C1=?, C2=?, and tem plate (b) generates the features as: C-2C-1=??, C-1C0=??, C0C1=??, C1C2=??, and tem plate (c) gives the feature C-1C1=??.</S>
			<S sid="42" ssid="19">2.2 Character-Based Generative Model.</S>
			<S sid="43" ssid="20">To incorporate the dependency between adja cent characters in the character-based approach, (Wang et al, 2009) proposes a character-based generative model.</S>
			<S sid="44" ssid="21">In this approach, word wi is first replaced with its corresponding sequence of [character, tag] (denoted as [c, t]), where tag is the same as that adopted in the above char acter-based discriminative model.</S>
			<S sid="45" ssid="22">With this representation, this model can be expressed as: 1 1 1 1 1 1 1 1 ( ) ([ , ] ) ( [ , ] ) ([ , ] ) ( ) m n n n n n n n P w c P c t c P c c t P c t P c ? = ?</S>
			<S sid="46" ssid="23">(2) Since 1 1( [ , ] ) 1 n nP c c t ? and is the same for various candidates, only should be considered.</S>
			<S sid="47" ssid="24">It can be further simplified with Markov Chain assumption as: 1( ) nP c ([ ,P c 1] ) nt 11 1 ([ , ] ) ([ , ] [ , ] ).</S>
			<S sid="48" ssid="25">n n i i k i P c t P c t c t ??</S>
			<S sid="49" ssid="26">i (3) Compared with the character-based dis criminative model, this generative model keeps the capability to handle OOV words because it also regards the character as basic unit.</S>
			<S sid="50" ssid="27">In ad dition, the dependency between adjacent 1174 ? Gold and Discriminative Tag: M Generative Trigram Tag: E Tag probability: B/0.0333 E/0.2236 M/0.7401 S/0.0030 Feature Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1 B -1.4375 0.1572 0.0800 0.2282 0.7709 0.2741 0.0000 0.0000 -0.6718 0.0000 E 1.3558 0.1910 0.7229 -1.2696 -0.5970 0.0049 0.0921 0.0000 0.8049 0.0000 M 1.1071 -0.5527 -0.3174 2.9422 0.4636 -0.1708 0.0000 0.0000 -0.9700 0.0000 S -1.0254 0.2046 -0.4856 -1.9008 -0.6375 0.0000 0.0000 0.0000 0.8368 0.0000 ? Gold and Discriminative Tag: E Generative Trigram Tag: S Tag probability: B/0.0009 E/0.8138 M/0.0012 S/0.1841 Feature Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1 B 0.3586 0.4175 0.0000 -0.7207 0.4626 0.0085 0.0000 0.0000 0.0000 0.0000 E 0.3666 0.0687 4.5381 2.8300 -0.0846 0.0000 0.0000 -1.0279 0.6127 0.0000 M -0.5657 -0.4330 1.8847 0.0000 -0.0918 0.0000 0.0000 0.0000 0.0000 0.0000 S -0.1595 -0.0532 2.7360 1.8223 -0.2862 -0.0024 0.0000 1.0494 0.7113 0.0000 Table 1: The corresponding lambda weight of features for ?????</S>
			<S sid="51" ssid="28">in the sentence ?[?]</S>
			<S sid="52" ssid="29">In the Feature column and Tag row, the value is the corresponding lambda weight for the feature and tag under ME framework.</S>
			<S sid="53" ssid="30">The meanings of those features are explained in Section 2.1.</S>
			<S sid="54" ssid="31">characters is now directly modeled.</S>
			<S sid="55" ssid="32">This will give sharper preference when the history of assignment is given.</S>
			<S sid="56" ssid="33">Therefore, this approach not only holds robust IV performance but also achieves comparable results with the discrimi native model.</S>
			<S sid="57" ssid="34">However, the OOV performance of this approach is still lower than that of the discriminative model (see in Table 5), which would be discussed in the next section.</S>
	</SECTION>
	<SECTION title="Problems with the Character-Based. " number="3">
			<S sid="58" ssid="1">Generative Model The character-based generative model can handle the dependency between adjacent char acters and thus performs well on IV words.</S>
			<S sid="59" ssid="2">However, this generative trigram model is de rived under the second order Markov Chain assumption.</S>
			<S sid="60" ssid="3">Future character context (i.e., C1 and C2) is thus not utilized in the model when the tag of the current character (i.e., t0) is de termined.</S>
			<S sid="61" ssid="4">Nevertheless, the future context would help to select the correct tag when the associated trigram has not been observed in the training-set, which is just the case for those OOV words.</S>
			<S sid="62" ssid="5">In contrast, the discriminative one could get help from the future context in this case.</S>
			<S sid="63" ssid="6">The example given in the next para graph clearly shows the above situation.</S>
			<S sid="64" ssid="7">At the sentence ??(that) ?(place) ?(of) ? ??(street sleeper) ?(only) ?(have) ?(some) ?(person) (There are only some street sleepers in that place)?</S>
			<S sid="65" ssid="8">in the CITYU corpus, ??/B? /M?/E(street sleeper)?</S>
			<S sid="66" ssid="9">is observed to be an OOV word, while ??</S>
			<S sid="67" ssid="10">/B? /E(sleep on the street)?</S>
			<S sid="68" ssid="11">is an IV word, where the associated tag of each character is given after the slash sym bol.</S>
			<S sid="69" ssid="12">The character-based generative model wrongly splits ?????</S>
			<S sid="70" ssid="13">into two words ??/B ?/E? and ??/S (person)?, as the associated trigram for ?????</S>
			<S sid="71" ssid="14">is not seen in the training set.</S>
			<S sid="72" ssid="15">However, the discriminative model gives the correct result for ??/M? and the dominant features come from its future context ???</S>
			<S sid="73" ssid="16">and ???.</S>
			<S sid="74" ssid="17">Similarly, the future context ???</S>
			<S sid="75" ssid="18">helps to give the correct tag to ??/E?.</S>
			<S sid="76" ssid="19">Table 1 gives the corresponding lambda feature weights (un der the Maximum Entropy (ME) (Ratnaparkhi, 1998) framework) for ?????</S>
			<S sid="77" ssid="20">in the dis criminative model.</S>
			<S sid="78" ssid="21">It shows that in the column of ?C1?</S>
			<S sid="79" ssid="22">below ???, the lambda value associ ated with the correct tag ?M? is 2.9422, which is the highest value in that column and is far greater than that of the wrong tag ?E?</S>
			<S sid="80" ssid="23">(i.e., 1.2696) assigned by the generative model.</S>
			<S sid="81" ssid="24">Which indicates that the future feature ?C1?</S>
			<S sid="82" ssid="25">is the most useful feature for tagging ???.</S>
			<S sid="83" ssid="26">The above example shows the character based generative model fails to handle some OOV words such as ?????</S>
			<S sid="84" ssid="27">because this ap proach cannot utilize future context when it is indeed required.</S>
			<S sid="85" ssid="28">However, the future context for the generative model scanning from left to right is just its past context when it scans from right to left.</S>
			<S sid="86" ssid="29">It is thus expected that this kind of 1175 errors will be fixed if we let the model scans from both directions, and then combine their results.</S>
			<S sid="87" ssid="30">Unfortunately, it is observed that these two scanning modes share over 90% of their errors.</S>
			<S sid="88" ssid="31">For example, in CITYU corpus, the left-to-right scan generates 1,958 wrong words and the right-to-left scan results 1,947 ones, while 1,795 of them are the same.</S>
			<S sid="89" ssid="32">Similar be havior can also be observed on other corpora.</S>
			<S sid="90" ssid="33">To find out what are the problems, 10 errors that are similar to ?????</S>
			<S sid="91" ssid="34">are selected to ex amine.</S>
			<S sid="92" ssid="35">Among those errors, only one of them is fixed, and ?????</S>
			<S sid="93" ssid="36">still cannot be correctly segmented.</S>
			<S sid="94" ssid="37">Having analyzed the scores of the model scanning from both directions, we found that the original scores (from left-to-right scan) at the stages ???</S>
			<S sid="95" ssid="38">and ???</S>
			<S sid="96" ssid="39">indeed get better if the model scans from right-to-left.</S>
			<S sid="97" ssid="40">However, the score at the stage ???</S>
			<S sid="98" ssid="41">deteriorates because the useful feature ???</S>
			<S sid="99" ssid="42">(a past non-adjacent character for ???</S>
			<S sid="100" ssid="43">when scans form right-toleft) still cannot be utilized when the past context ????</S>
			<S sid="101" ssid="44">as a whole is unseen, when the re lated probabilities are estimated via modified Kneser-Ney smoothing (Chen and Goodman, 1998) technique.</S>
			<S sid="102" ssid="45">Two scanning modes seem not complementing each other, which is out of our original expectation.</S>
			<S sid="103" ssid="46">However, we found that the character-based generative model and the discrimina tive one complement each other much more than the two scanning modes do.</S>
			<S sid="104" ssid="47">It is observed that these two approaches share less than 50% of their errors.</S>
			<S sid="105" ssid="48">For example, in CITYU corpus, the generative approach generates 1,958 wrong words and the discriminative one results 2,338 ones, while only 835 of them are the same.</S>
			<S sid="106" ssid="49">The statistics of the remaining errors resulted from the generative model and the dis criminative model is shown in Table 2.</S>
			<S sid="107" ssid="50">As shown in the table, it can be seen that the gen erative model and the discriminative model complement each other on handling IV words and OOV words (In the ?IV Errors?</S>
			<S sid="108" ssid="51">column, the number of ?G+D-?</S>
			<S sid="109" ssid="52">is much more than the ?G-D+?, while the behavior is reversed in the ?OOV Errors?</S>
			<S sid="110" ssid="53">column).</S>
	</SECTION>
	<SECTION title="Proposed Joint Model. " number="4">
			<S sid="111" ssid="1">Since the performance of both IV words and OOV words are important for real applications, IV Errors OOV Errors G+D- G-D+ G-D- G+D- G-D+ G-D 12,027 4,723 7,481 2,384 6,139 3,975Table 2: Statistics for remaining errors of the char acter-based generative model and the discriminative one on the second SIGHAN Bakeoff (?G+D-?</S>
			<S sid="112" ssid="2">in the ?IV Errors?</S>
			<S sid="113" ssid="3">column means that the generative model segments the IV words correctly but the dis criminative one gives wrong results.</S>
			<S sid="114" ssid="4">The meanings of other abbreviations are similar with this one.).</S>
			<S sid="115" ssid="5">we need to combine the strength from both models.</S>
			<S sid="116" ssid="6">Among various combining methods, log-linear interpolation combination is a sim ple but effective one (Bishop, 2006).</S>
			<S sid="117" ssid="7">Therefore, the following character-based joint model is proposed, and a parameter ? is used to weight the generative model in a cross-validation set.</S>
			<S sid="118" ssid="8">1 2 2 2 ( ) log( ([ , ] [ , ] )) (1 ) log( ( )) k k k k k k Score t P c t c t P t c ? ?</S>
			<S sid="119" ssid="9">k (4) Where tk indicates the corresponding position of character ck, and (0.0 1.0)?</S>
			<S sid="120" ssid="10">is the weight for the generative model.</S>
			<S sid="121" ssid="11">Score(tk) will be used during searching the best sequence.</S>
			<S sid="122" ssid="12">It can be seen that these two models are inte grated naturally as both are character-based.</S>
			<S sid="123" ssid="13">Generally speaking, if the ?G(or D)+?</S>
			<S sid="124" ssid="14">has a strong preference on the desired candidate, but the ?D(or G)-?</S>
			<S sid="125" ssid="15">has a weak preference on its top-1 incorrect candidate, then this combining method would correct most ?G+D- (also G D+)?</S>
			<S sid="126" ssid="16">errors.</S>
			<S sid="127" ssid="17">On the other hand, the advantage of combining two models would vanish if the ?G(or D)+?</S>
			<S sid="128" ssid="18">has a weak preference while the ?D(or G)-?</S>
			<S sid="129" ssid="19">has a strong preference over their top-1 candidates.</S>
			<S sid="130" ssid="20">In our observation, these two models meet this requirement quite well.</S>
	</SECTION>
	<SECTION title="Weigh Various Features Differently. " number="5">
			<S sid="131" ssid="1">For a given observation, intuitively each feature should be trained only once under the ME framework and its associated weight will be automatically learned from the training cor pus.</S>
			<S sid="132" ssid="2">However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some fea tures (e.g., C0) are unnoticeably trained several times in their model (which are implicitly gen erated from different feature templates used in the paper).</S>
			<S sid="133" ssid="3">For example, the feature C0 actually 1176 Corpus Abbrev.</S>
			<S sid="134" ssid="4">Encoding Training Size(Words/Type) Test Size (Words/Type) OOV Rate Academia Sinica (Taipei) AS Unicode/Big5 5.45M/141K 122K/19K 0.046 City University of Hong Kong CITYU Unicode/Big5 1.46M/69K 41K/9K 0.074 Microsoft Research (Beijing) MSR Unicode/CP936 2.37M/88K 107K/13K 0.026 PKU(ucvt.)</S>
			<S sid="135" ssid="5">Unicode/CP936 1.1M/55K 104K/13K 0.058 Peking University PKU(cvt.)</S>
			<S sid="136" ssid="6">Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0]).</S>
			<S sid="137" ssid="7">The meanings of features are illustrated in Section 2.1.</S>
			<S sid="138" ssid="8">Those re petitive features also include [C-1C0] and [C0C1], which implicitly appear thrice.</S>
			<S sid="139" ssid="9">And it is surprising to discover that its better performance is mainly due to this implicit feature repe tition but the authors do not point out this fact.</S>
			<S sid="140" ssid="10">As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be ?n?, at least in principle.</S>
			<S sid="141" ssid="11">Inspired by the above discovery, accordingly, we convert all the binaryvalue features into their corresponding realvalued features.</S>
			<S sid="142" ssid="12">After having transformed binary features into their corresponding real valued ones, the original discriminative model is re-trained under the ME framework.</S>
			<S sid="143" ssid="13">This new implementation, which would be named as the character-based discriminativeplus model, just weights various features differently before conducting ME training.</S>
			<S sid="144" ssid="14">Afterwards, it is further combined with the generative trigram model, and is called the charac ter-based joint-plus model.</S>
	</SECTION>
	<SECTION title="Experiments. " number="6">
			<S sid="145" ssid="1">The corpora provided by the second SIGHAN Bakeoff (Emerson, 2005) were used in our ex periments.</S>
			<S sid="146" ssid="2">The statistics of those corpora are shown in Table 3.</S>
			<S sid="147" ssid="3">Note that the PKU corpus is a little different from others.</S>
			<S sid="148" ssid="4">In the training set, Arabic num bers and English characters are in full-width form occupying two bytes.</S>
			<S sid="149" ssid="5">However, in the testing set, these characters are in half-width form occupying only one byte.</S>
			<S sid="150" ssid="6">Most researchers in the SIGHAN Bakeoff competition per formed a conversion before segmentation (Xiong et al, 2009).</S>
			<S sid="151" ssid="7">In this work, we conduct the tests on both unconverted (ucvt.)</S>
			<S sid="152" ssid="8">case and converted (cvt.)</S>
			<S sid="153" ssid="9">case.</S>
			<S sid="154" ssid="10">After the conversion, the OOV rate of converted corpus is obviously lower than that of unconverted corpus.</S>
			<S sid="155" ssid="11">To fairly compare the proposed approach with previous works, we only conduct closed tests1.</S>
			<S sid="156" ssid="12">The metrics Precision (P), Recall (R), F-score (F) (F=2PR/(P+R)), Recall of OOV (ROOV) and Recall of IV (RIV) are used to evaluate the results.</S>
			<S sid="157" ssid="13">6.1 Character-Based Generative Model.</S>
			<S sid="158" ssid="14">and Discriminative Model As shown in (Wang et al, 2009), the character based generative trigram model significantly exceeds its related bigram model and performs the same as its 4-gram model.</S>
			<S sid="159" ssid="15">Therefore, SRI Language Modeling Toolkit2 (Stolcke, 2002) is used to train the trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998).</S>
			<S sid="160" ssid="16">Afterwards, a beam search de coder is applied to find out the best sequence.</S>
			<S sid="161" ssid="17">For the character-based discriminative model, the ME Package3 given by Zhang Le is used to conduct the experiments.</S>
			<S sid="162" ssid="18">Training was done with Gaussian prior 1.0 and 300, 150 it erations for AS and other corpora respectively.</S>
			<S sid="163" ssid="19">Ta ble 5 gives the segmentation results of both the character-based generative model and the discriminative model.</S>
			<S sid="164" ssid="20">From the results, it can be seen that the generative model achieves comparable results with the discriminative one and they outperform each other on different corpus.</S>
			<S sid="165" ssid="21">However, the generative model ex ceeds the discriminative one on RIV (0.973 vs. 0.956) but loses on ROOV (0.511 vs. 0.680).</S>
			<S sid="166" ssid="22">It illustrates that they complement each other.</S>
			<S sid="167" ssid="23">1 According to the second Sighan Bakeoff regulation, the.</S>
			<S sid="168" ssid="24">closed test could only use the training data directly provided.</S>
			<S sid="169" ssid="25">Any other data or information is forbidden, includ ing the knowledge of characters set, punctuation set, etc. 2 http://www.speech.sri.com/projects/srilm/ 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 1177 Joint model performance on Development sets 0.9300 0.9400 0.9500 0.9600 0.9700 0.9800 0.9900 0.0 0 0.1 0 0.2 0 0.3 0 0.4 0 0.5 0 0.6 0 0.7 0 0.8 0 0.9 0 1.0 0 alphaF sc or e AS CITYU MSR PKUFigure 1: Development sets performance of Charac ter-based joint model.</S>
			<S sid="170" ssid="26">Corpus Set Words OOV Num OOV Rate Development 17,243 445 0.026 AS Testing 122,610 5,308/5,311 0.043/0.043 Development 17,324 355 0.020 MSR Testing 106,873 2,829/2,833 0.026/0.027 Development 12,075 537 0.044 CITYU Testing 40,936 3,028/3,034 0.074/0.074 Development 13,576 532 0.039 Testing (ucvt.)</S>
			<S sid="171" ssid="27">104,372 6,006/6,054 0.058/0.058PKU Testing (cvt.)</S>
			<S sid="172" ssid="28">104,372 3,611/3,661 0.035/0.035 Table 4: Corpus statistics for Development sets and Testing sets.</S>
			<S sid="173" ssid="29">A ?/?</S>
			<S sid="174" ssid="30">separates the OOV number (or OOV rate) with respect to the original training sets and the new training sets.</S>
			<S sid="175" ssid="31">6.2 Character-Based Joint Model.</S>
			<S sid="176" ssid="32">For the character-based joint model, a devel opment set is required to obtain the weight ? for its associated generative model.</S>
			<S sid="177" ssid="33">A small portion of each original training corpus is thus extracted as the development set and the remaining data is regarded as the new trainingset, which is used to train two new parameter sets for both generative and discriminative models associated.</S>
			<S sid="178" ssid="34">The last 2,000, 600, 400, and 300 sentences for AS, MSR, CITYU, and PKU are extracted from the original training corpora as their cor responding development sets.</S>
			<S sid="179" ssid="35">The statistics for new data sets are shown in Table 4.</S>
			<S sid="180" ssid="36">It can be seen that the variation of the OOV rate could be hardly noticed.</S>
			<S sid="181" ssid="37">The F-scores of the joint model, versus different ? , evaluated on four development sets are shown in Figure 1.</S>
			<S sid="182" ssid="38">It can be seen that the curves are not sharp but flat near the top, which indicates that the character based joint model is not sensitive to the ? value selected.</S>
			<S sid="183" ssid="39">From those curves, the best suitable ? for AS, CITYU, MSR and PKU are found to be 0.30, 0.60, 0.60 and 0.60, respec Corpus Model R P F ROOV RIV tively.</S>
			<S sid="184" ssid="40">Those alpha values will then be adopted to conduct the experiments on the testing sets.</S>
			<S sid="185" ssid="41">G 0.958 0.938 0.948 0.518 0.978 D 0 0.946 0 0.967.955 .951 0.707 D-Plus 0.960 0.948 0.954 0.680 0.973 J 0.962 0.950 0.956 0.679 0.975 AS J-Plus 0.963 0.949 0.956 0.652 0.977 G 0.951 0.937 0.944 0.609 0.978 D 0.941 0.944 0.942 0.708 0.959 D-Plus 0.951 0.952 0.952 0.720 0.970 J 0.957 0.951 0.954 0.691 0.979 CITYU J-Plus 0.959 0.952 0.956 0.700 0.980 G 0.974 0.967 0.970 0.561 0.985 D 0.957 0.962 0.960 0.719 0.964 D-Plus 0.965 0.967 0.966 0.675 0.973 J 0.974 0.971 0.972 0.659 0.983 MSR J-Plus 0.975 0.970 0.972 0.632 0.984 G 0.929 0.933 0.931 0.435 0.959 D 0.922 0.941 0.932 0.620 0.941 D-Plus 0.934 0.949 0.941 0.649 0.951 J 0.935 0.946 0.941 0.561 0.958 PKU (ucvt.)</S>
			<S sid="186" ssid="42">J-Plus 0.937 0.947 0.942 0.556 0.960 G 0.952 0.951 0.952 0.503 0.968 D 0.940 0.951 0.946 0.685 0.949 D-Plus 0.949 0.958 0.953 0.674 0.958 J 0.954 0.958 0.956 0.616 0.966 PKU (cvt.)</S>
			<S sid="187" ssid="43">J-Plus 0.955 0.958 0.957 0.610 0.967 G 0.953 0.946 0.950 0.511 0.973 D 0.944 0.950 0.947 0.680 0.956 D-Plus 0.952 0.955 0.953 0.676 0.965 J 0.957 0.955 0.956 0.633 0.971 Overall J-Plus 0.958 0.955 0.957 0.621 0.973 Table 5: ent e based m n t G ificantly outperforms both the character ba Segm odels oation r sults of various character he second SI HAN Bakeoff, the generative trigram model (G), the discriminative model (D), the discriminative-plus model (D-Plus), the joint model (J) and the joint-plus model (J-Plus).</S>
			<S sid="188" ssid="44">As shown in Table 5, the joint model sig n sed generative model and the discriminative one in F-score on all the testing corpora.</S>
			<S sid="189" ssid="45">Com pared with the generative approach, the joint model increases the overall ROOV from 0.510 to 0.633, with the cost of slightly degrading the overall RIV from 0.973 to 0.971.</S>
			<S sid="190" ssid="46">This shows that the joint model holds the advantage of the generative model on IV words.</S>
			<S sid="191" ssid="47">Compared with the discriminative model, the proposed joint model improves the overall RIV from 0.956 to 0.971, with the cost of degrading the overall ROOV from 0.680 to 0.633.</S>
			<S sid="192" ssid="48">It clearly shows that the joint model achieves a good balance be tween IV words and OOV words and achieves the best F-scores obtained so far (21% relative error reduction over the discriminative model and 14% over the generative model).</S>
			<S sid="193" ssid="49">1178 6.3 Weigh Various Features Differently.</S>
			<S sid="194" ssid="50">Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0 anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model.</S>
			<S sid="195" ssid="51">Although it seems reasonable to weight those closely relevant features more (C0 should be the most relevant fea ture for assigning tag t0), both implementations seem to be equal if their corresponding lambda-values are also updated accordingly.</S>
			<S sid="196" ssid="52">However, Table 5 shows that this new discriminative-plus implementation (D-Plus) sig nificantly outperforms the original one (overall F-score is raised from 0.947 to 0.953) when both of them adopt real-valued features.</S>
			<S sid="197" ssid="53">It is not clear how this change makes the difference.</S>
			<S sid="198" ssid="54">Similar improvements can be observed with two other ME packages.</S>
			<S sid="199" ssid="55">One anonymous re viewer pointed out that the duplicated features should not make difference if there is no regularization.</S>
			<S sid="200" ssid="56">However, we found that the dupli cated features would improve the performance whether we give Gaussian penalty or not.</S>
			<S sid="201" ssid="57">Afterwards, this new implementation and the generative trigram model are further com bined (named as the joint-plus model).</S>
			<S sid="202" ssid="58">Table 5 shows that this joint-plus model also achieves better results compared with the discrimina tive-plus model, which illustrates that our joint approach is an effective and robust method for CWS.</S>
			<S sid="203" ssid="59">However, compared with the original joint model, the new joint-plus approach does not show much improvement, regardless of the significant improvement made by the discrimi native-plus model, as the additional benefit generated by the discriminative-plus model has already covered by the generative approach (Among the 6,965 error words corrected by the discriminative-plus model, 6,292 (90%) of them are covered by the generative model).</S>
	</SECTION>
	<SECTION title="Statistical Significance Tests. " number="7">
			<S sid="204" ssid="1">joint (joint-plus) model outperforms baselines mentioned above, we want to know if the difference is statistically significant enough to make such a claim.</S>
			<S sid="205" ssid="2">Since there is only one testing set for each training corpus, the bootstrapping technique (Zhang et al, 2004) is adopted to conduct the tests: Giving an Models A B AS CITYU MSR PKU (ucvt.)</S>
			<S sid="206" ssid="3">PKU (cvt.)</S>
			<S sid="207" ssid="4">G D &lt; ~ &gt; ~ &gt; D-Plus G &gt; &gt; &lt; &gt; &gt; D-Plus D &gt; &gt; &gt; &gt; &gt; J G &gt; &gt; &gt; &gt; &gt; J D &gt; &gt; &gt; &gt; &gt; J-Plus G &gt; &gt; &gt; &gt; &gt; J-Plus D-Plus &gt; &gt; &gt; ~ &gt; J-Plus J ~ &gt; ~ &gt; &gt; Table 6 atistic sign anc est F- e v er-b d m ls.</S>
			<S sid="208" ssid="5">f T0) will be generated by repeatedly re-sampling data eas the dishe confithe pro poe ng d. tegory includes (Asahara et al, 2005) (denoted as : St al ific e t of scor among arious charact ase ode testing-set T0, additional M-1 new testing-sets T0,?,TM-1 (each with the same size o from T0.</S>
			<S sid="209" ssid="6">Then, we will have a total of M testing-sets (M=2000 in our experiments).</S>
			<S sid="210" ssid="7">7.1 Comparisons with Baselines.</S>
			<S sid="211" ssid="8">We then follow (Zhang et al, 2004) to m ure the 95% confidence interval for crepancy between two models.</S>
			<S sid="212" ssid="9">If t dence interval does not include the origin point, we then claim that system A is significantly different from system B. Table 6 gives the re sults of significant tests among various models mentioned above.</S>
			<S sid="213" ssid="10">In this table, ?&gt;?</S>
			<S sid="214" ssid="11">means that system A is significantly better than B, where as ?&lt;?</S>
			<S sid="215" ssid="12">denotes that system A is significantly worse than B, and ?~?</S>
			<S sid="216" ssid="13">indicates that these two systems are not significantly different.</S>
			<S sid="217" ssid="14">As shown in Table 6, the proposed joint model is significantly better than the two base line models on all corpora.</S>
			<S sid="218" ssid="15">Similarly, sed joint-plus model also significantly outperforms the generative model and the dis criminative-plus model on all corpora except on the PKU(ucvt.).</S>
			<S sid="219" ssid="16">The comparison shows that the proposed joint (also joint-plus) model in deed exceeds each of its component models.</S>
			<S sid="220" ssid="17">7.2 Comparisons with Previous Works.</S>
			<S sid="221" ssid="18">The above comparison mainly shows the sup riority of the proposed joint model amo those approaches that have been implemente However, it would be interesting to know if the joint (and joint-plus) model also outperforms those previous state-of-the-art systems.</S>
			<S sid="222" ssid="19">The systems that performed best for at least one corpus in the second SIGHAN Bakeoff are first selected for comparison.</S>
			<S sid="223" ssid="20">This ca 1179A-sets.</S>
			<S sid="224" ssid="21">In st th sahara05) and (Tseng et al, 2005) 4 (Tseng05).</S>
			<S sid="225" ssid="22">(Asahara et al, 2005) achieves the best result in the AS corpus, and (Tseng et al, 2005) performs best in the remaining three corpora.</S>
			<S sid="226" ssid="23">Besides, those systems that are re ported to exceed the above two systems are also selected.</S>
			<S sid="227" ssid="24">This category includes (Zhang et al., 2006) (Zhang06), (Zhang and Clark, 2007) (Z&amp;C07) and (Jiang et al, 2008) (Jiang08).</S>
			<S sid="228" ssid="25">They are briefly summarized as follows.</S>
			<S sid="229" ssid="26">(Zhang et al, 2006) is based on sub-word tag ging and uses a confidence measure method to combine the sub-word CRF (Lafferty et al, 2001) and rule-based models.</S>
			<S sid="230" ssid="27">(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features.</S>
			<S sid="231" ssid="28">Last, (Jiang et al, 2008)5 adds repeated features implicitly based on (Ng and Low, 2004).</S>
			<S sid="232" ssid="29">All of the above models, except (Zhang and Clark, 2007), adopt the char acter-based discriminative approach.</S>
			<S sid="233" ssid="30">All the results of the systems mentioned above are shown in Table 7.</S>
			<S sid="234" ssid="31">Since the systems are not re-implemented, we cannot generate paired samples from those M testingead, we calculate the 95% confidence interval of the joint (also joint-plus) model.</S>
			<S sid="235" ssid="32">After wards, those systems can be compared with our proposed models.</S>
			<S sid="236" ssid="33">If the F-score of system B does not fall within the 95% confidence in terval of system A (joint or joint-plus), then they are statistically significantly different.</S>
			<S sid="237" ssid="34">Table 8 gives the results of significant tests for those systems mentioned in this section.</S>
			<S sid="238" ssid="35">It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU(ucvt.).</S>
			<S sid="239" ssid="36">In that special case, (Zhang and Clark, 2007) 4 We are not sure whether (Asahara et al, 2005) and.</S>
			<S sid="240" ssid="37">(Tseng et al, 2005) performed a conversion before seg mentation in PKU corpus.</S>
			<S sid="241" ssid="38">In this paper, we followed previous works, which cited and compared with them.</S>
			<S sid="242" ssid="39">5 The data for (Jiang et al, 2008) given at Table 7 are.</S>
			<S sid="243" ssid="40">different from what were reported at their paper.</S>
			<S sid="244" ssid="41">In the communication with the authors, it is found that the script for evaluating performance, provided by the SIGHAN Bakeoff, does not work correctly in their platform.</S>
			<S sid="245" ssid="42">After the problem is fixed, the re-evaluated real performances reported here deteriorate from their original version.</S>
			<S sid="246" ssid="43">Please see the announcement in Jiang?s homepage (http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_corre ction.pdf).</S>
			<S sid="247" ssid="44">Corpus Participants AS CITYU MSR PKU (ucvt.)</S>
			<S sid="248" ssid="45">PKU (cvt.)</S>
			<S sid="249" ssid="46">Asahara05 0.952 0.941 0.958 N/A 0.941 Tseng05 0.947 0.943 0.964 N/A 0.950 Zhang06 0.951 0.951 0.971 N/A 0.951 Z&amp;C07 0.946 0.951 0.972 0.945 N/A Jiang08 0.953 0.948 0.966 0.937 N/A Our Joint 0.956 0.954 0.972 0.941 0.956 Our Joint-Plus 0.956 0.956 0.972 0.942 0.957 Table 7: Compari r p u the-art sy sons of F-sco e with revio s state-of- stems.</S>
			<S sid="250" ssid="47">Systems A B AS CITYU MSR (ucvt.)</S>
			<S sid="251" ssid="48">PKU (cvt.)</S>
			<S sid="252" ssid="49">PKU Asahara05 &gt; &gt; &gt; N/A &gt; Tseng05 &gt; &gt; &gt; N/A &gt; Zhang06 &gt; ~ ~ N/A &gt; Z&amp;C07 &gt; &gt; ~ &lt; N/A J Jiang08 &gt; &gt; &gt; &gt; N/A Asahara05 &gt; &gt; &gt; N/A &gt; Tseng05 &gt; &gt; &gt; N/A &gt; Zhang06 &gt; &gt; ~ N/A &gt; Z&amp;C07 &gt; &gt; ~ &lt; N/A J-Plus Jiang08 ~ &gt; &gt; &gt; N/A Table al s ific e te of r f-the syst s. outpe he jo -plu model by .3% and 0.5%, re ne, e two models complement dling IV words and OOV e nomenon.</S>
			<S sid="253" ssid="50">8: Statistic ign anc st F-score fo previous state-o -art em rforms t int s 0 on F- score (0.4% for the joint model).</S>
			<S sid="254" ssid="51">However, our joint-plus model exceeds it more over AS and CITYU corpora by 1.0% spectively (1.0% and 0.3% for the joint model).</S>
			<S sid="255" ssid="52">Thus, it is fair to say that both our joint model and joint-plus model are superior to the state of-the-art systems reported in the literature.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="8">
			<S sid="256" ssid="1">From the error analysis of the character-based generative model and the discriminative o we found that thes each other on hanwords.</S>
			<S sid="257" ssid="2">To take advantage of these two ap proaches, a joint model is thus proposed to combine them.</S>
			<S sid="258" ssid="3">Experiments on the Second SIGHAN Bakeoff show that the joint model achieves 21% error reduction over the dis criminative model (14% over the generative model).</S>
			<S sid="259" ssid="4">Moreover, closed tests on the second SIGHAN Bakeoff corpora show that this joint model significantly outperforms all the state of-the-art systems reported in the literature.</S>
			<S sid="260" ssid="5">Last, it is found that weighting various features differently would give better result.</S>
			<S sid="261" ssid="6">How ever, further study is required to find out the true reason for this strange but interesting ph 1180 A Generic-Beam-Search code and o Ms. Nanyan Kuo for eric-Beam-Search code.</S>
			<S sid="262" ssid="7">m optimum entation.</S>
			<S sid="263" ssid="8">In Proceedings of GHAN Workshop on Chinese Lan St Th Jia W Jo Hw Fu ction using conditional random fields.</S>
			<S sid="264" ssid="9">Ad MNLP, pages Hu ld Word Segmenter Ku d Keh-Yih Su, 2009.</S>
			<S sid="265" ssid="10">Yi scriminative Ni ssing, 8 (1).</S>
			<S sid="266" ssid="11">pages Hu Second Ru2006.</S>
			<S sid="267" ssid="12">Subword-based Tagging for Con Yi scores: How much im Yu f ACL, pages 840-847, cknowledgement The authors extend sincere thanks to Wenbing Jiang for his helps with our experiments.</S>
			<S sid="268" ssid="13">Also, we thank Behavior Design Corporation for using their show special thanks t her helps with the Gen The research work has been partially funded by the Natural Science Foundation of China under Grant No. 60975053, 90820303 and 60736014, the National Key Technology R&amp;D Program under Grant No. 2006BAH03B02, and also the Hi-Tech Research and Develop ent Program (?863?</S>
			<S sid="269" ssid="14">Program) of China under Grant No. 2006AA010108-4 as well.</S>
	</SECTION>
</PAPER>
