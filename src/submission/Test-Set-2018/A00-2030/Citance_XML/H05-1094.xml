<PAPER>
	<S sid="0">Composition Of Conditional Random Fields For Transfer Learning</S><ABSTRACT>
		<S sid="1" ssid="1">Many learning tasks have subtasks for which much training data exists.</S>
		<S sid="2" ssid="2">Therefore, we wantto transfer learning from the old, general purpose subtask to a more specific new task, for which there is often less data.</S>
		<S sid="3" ssid="3">While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old.</S>
		<S sid="4" ssid="4">Specifically, we perform joint decoding ofseparately-trained sequence models, preserv ing uncertainty between the tasks and allowinginformation from the new task to affect predic tions on the old task.</S>
		<S sid="5" ssid="5">On two standard text data sets, we show that joint decoding outperforms cascaded decoding.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">Many tasks in natural language processing are solved bychaining errorful subtasks.</S>
			<S sid="7" ssid="7">Within information extrac tion, for example, part-of-speech tagging and shallow parsing are often performed before the main extraction task.</S>
			<S sid="8" ssid="8">Commonly these subtasks have their own standard sets of labeled training data: for example, many large data sets exist for learning to extract person names from newswire text; whereas the available training data for new applications, such as extracting appointment information from email, tends to be much smaller.</S>
			<S sid="9" ssid="9">Thus, we need to transfer regularities learned from a well-studied subtask, such as finding person names in newswire text, to a new, related task, such as finding names of speakers in email seminar announcements.In previous NLP systems, transfer is often accom plished by training a model for the subtask, and using itsprediction as a feature for the new task.</S>
			<S sid="10" ssid="10">For example, recent CoNLL shared tasks (Tjong Kim Sang &amp; De Meulder, 2003; Carreras &amp; Marquez, 2004), which are standard data sets for such common NLP tasks as clause identification and named-entity recognition, include predic tions from a part-of-phrase tagger and a shallow parser asfeatures.</S>
			<S sid="11" ssid="11">But including only the single most likely subtask prediction fails to exploit useful dependencies be tween the tasks.</S>
			<S sid="12" ssid="12">First, if the subtask prediction is wrong,the model for the new task may not be able to recover.</S>
			<S sid="13" ssid="13">Of ten, errors propagate upward through the chain of tasks, causing errors in the final output.</S>
			<S sid="14" ssid="14">This problem can beameliorated by preserving uncertainty in the subtask pre dictions, because even if the best subtask prediction iswrong, the distribution over predictions can still be some what accurate.</S>
			<S sid="15" ssid="15">Second, information from the main task can inform thesubtask.</S>
			<S sid="16" ssid="16">This is especially important for learning transfer, because the new domain often has different charac teristics than the old domain, which is often a standardbenchmark data set.</S>
			<S sid="17" ssid="17">For example, named-entity recog nizers are usually trained on newswire text, which is more structured and grammatical than email, so we expect anoff-the-shelf named-entity recognizer to perform some what worse on email.</S>
			<S sid="18" ssid="18">An email task, however, often has domain-specific features, such as PREVIOUS WORD IS Speaker:), which were unavailable or uninformative tothe subtask on the old training set, but are very informa tive to the subtask in the new domain.</S>
			<S sid="19" ssid="19">While previous work in transfer learning has considered how the old task can help the new task, in this paper we show how the new task can help itself by improving predictions on the old.In this paper we address the issue of transfer by train ing a cascade of models independently on the various training sets, but at test time combining them into a singlemodel in which decoding is performed jointly.</S>
			<S sid="20" ssid="20">For the in dividual models, we use linear-chain conditional random fields (CRFs), because the great freedom that they allowin feature engineering facilitates the learning of richer in teractions between the subtasks.</S>
			<S sid="21" ssid="21">We train a linear chain CRF on each subtask, using the prediction of the previous subtask as a feature.</S>
			<S sid="22" ssid="22">At test time, we combine the learned weights from the original CRFs into a single grid-shaped factorial CRF, which makes predictions for all the tasks 748at once.</S>
			<S sid="23" ssid="23">Viterbi decoding in this combined model im plicitly considers all possible predictions for the subtask when making decisions in the main task.</S>
			<S sid="24" ssid="24">We evaluate joint decoding for learning transfer on a standard email data set and a standard entity recognition task.</S>
			<S sid="25" ssid="25">On the email data set, we show a significant gain in performance, including new state-of-the-art results.</S>
			<S sid="26" ssid="26">Of particular interest for transfer learning, we also show thatusing joint decoding, we achieve equivalent results to cas caded decoding with 25% less training data.</S>
	</SECTION>
	<SECTION title="Linear-chain CRFs. " number="2">
			<S sid="27" ssid="1">Conditional random fields (CRFs) (Lafferty et al, 2001) are undirected graphical models that are conditionallytrained.</S>
			<S sid="28" ssid="2">In this section, we describe CRFs for the linearchain case.</S>
			<S sid="29" ssid="3">Linear-chain CRFs can be roughly under stood as conditionally-trained finite state machines.</S>
			<S sid="30" ssid="4">Alinear-chain CRF defines a distribution over state se quences s = {s1, s2, . . .</S>
			<S sid="31" ssid="5">, sT } given an input sequence x = {x1, x2, . . .</S>
			<S sid="32" ssid="6">, xT } by making a first-order Markov assumption on states.</S>
			<S sid="33" ssid="7">These Markov assumptions imply that the distribution over sequences factorizes in terms of pairwise functions ?t(st?1, st,x) as: p(s|x) = ? t ?t(st?1, st,x) Z(x) , (1) The partition function Z(x) is defined to ensure that the distribution is normalized: Z(x) = ? s? ? t ?t(s ? t?1, s ? t,x).</S>
			<S sid="34" ssid="8">(2) The potential functions ?t(st?1, st,x) can be interpreted as the cost of making a transition from state st?1 to state st at time t, similar to a transition probability in an HMM.Computing the partition function Z(x) requires sum ming over all of the exponentially many possible statesequences s?.</S>
			<S sid="35" ssid="9">By exploiting Markov assumptions, how ever, Z(x) (as well as the node marginals p(st|x) and the Viterbi labeling) can be calculated efficiently by variants of the standard dynamic programming algorithms used for HMMs.</S>
			<S sid="36" ssid="10">We assume the potentials factorize according to a set of features {fk}, which are given and fixed, so that ?(st?1, st,x) = exp ( ? k ?kfk(st?1, st,x, t) ) .</S>
			<S sid="37" ssid="11">(3) The model parameters are a set of real weights?</S>
			<S sid="38" ssid="12">= {?k}, one for each feature.</S>
			<S sid="39" ssid="13">Feature functions can be arbitrary.</S>
			<S sid="40" ssid="14">For example, one feature function could be a binary test fk(st?1, st,x, t)that has value 1 if and only if st?1 has the label SPEAKERNAME, st has the label OTHER, and the word xt be gins with a capital letter.</S>
			<S sid="41" ssid="15">The chief practical advantageof conditional models, in fact, is that we can include arbitrary highly-dependent features without needing to es timate their distribution, as would be required to learn a generative model.</S>
			<S sid="42" ssid="16">Given fully-labeled training instances {(sj ,xj)}Mj=1,CRF training is usually performed by maximizing the pe nalized log likelihood `(?)</S>
			<S sid="43" ssid="17">= ? j ? t ? k ?kfk(sj,t?1, sj,t,x, t) ? ?</S>
			<S sid="44" ssid="18">j logZ(xj)?</S>
			<S sid="45" ssid="19">k ?2k 2?2 (4) where the final term is a zero-mean Gaussian prior placedon parameters to avoid overfitting.</S>
			<S sid="46" ssid="20">Although this maximization cannot be done in closed form, it can be optimized numerically.</S>
			<S sid="47" ssid="21">Particularly effective are gradientbased methods that use approximate second-order infor mation, such as conjugate gradient and limited-memory BFGS (Byrd et al, 1994).</S>
			<S sid="48" ssid="22">For more information on current training methods for CRFs, see Sha and Pereira (2003).</S>
	</SECTION>
	<SECTION title="Dynamic CRFs. " number="3">
			<S sid="49" ssid="1">Dynamic conditional random fields (Sutton et al, 2004) extend linear-chain CRFs in the same way that dynamic Bayes nets (Dean &amp; Kanazawa, 1989) extend HMMs.</S>
			<S sid="50" ssid="2">Rather than having a single monolithic state variable,DCRFs factorize the state at each time step by an undi rected model.</S>
			<S sid="51" ssid="3">Formally, DCRFs are the class of conditionally-trained undirected models that repeat structure and parametersover a sequence.</S>
			<S sid="52" ssid="4">If we denote by ?c(yc,t,xt) the repe tition of clique c at time step t, then a DCRF defines the probability of a label sequence s given the input x as: p(s|x) = ? t ?c(yc,t,xt) Z(x) , (5) where as before, the clique templates are parameterized in terms of input features as ?c(yc,t,xt) = exp { ? k ?kfk(yc,t,xt) } .</S>
			<S sid="53" ssid="5">(6) Exact inference in DCRFs can be performed by forward-backward in the cross product state space, if the cross-product space is not so large as to be infeasible.</S>
			<S sid="54" ssid="6">Otherwise, approximate methods must be used; in our experience, loopy belief propagation is often effective in grid-shaped DCRFs.</S>
			<S sid="55" ssid="7">Even if inference is performed monolithically, however, a factorized state representationis still useful because it requires much fewer parameters than a fully-parameterized linear chain in the cross product state space.</S>
			<S sid="56" ssid="8">749 Sutton et al (2004) introduced the factorial CRF (FCRF), in which the factorized state structure is a grid (Figure 1).</S>
			<S sid="57" ssid="9">FCRFs were originally applied to jointly performing interdependent language processing tasks, inparticular part-of-speech tagging and noun-phrase chunk ing.</S>
			<S sid="58" ssid="10">The previous work on FCRFs used joint training, which requires a single training set that is jointly labeled for all tasks in the cascade.</S>
			<S sid="59" ssid="11">For many tasks such datais not readily available, for example, labeling syntac tic parse trees for every new Web extraction task would be prohibitively expensive.</S>
			<S sid="60" ssid="12">In this paper, we train the subtasks separately, which allows us the freedom to use large, standard data sets for well-studied subtasks such as named-entity recognition.</S>
	</SECTION>
	<SECTION title="Alternatives for Learning Transfer. " number="4">
			<S sid="61" ssid="1">In this section, we enumerate several classes of methods for learning transfer, based on the amount and type of interaction they allow between the tasks.</S>
			<S sid="62" ssid="2">The principal differences between methods are whether the individual tasks are performed separately in a cascade or jointly; whether a single prediction from the lower task is used, or several; and what kind of confidence information is shared between the subtasks.</S>
			<S sid="63" ssid="3">The main types of transfer learning methods are: 1.</S>
			<S sid="64" ssid="4">Cascaded training and testing.</S>
			<S sid="65" ssid="5">This is the traditional.</S>
			<S sid="66" ssid="6">approach in NLP, in which the single best prediction from the old task is used in the new task at training and test time.</S>
			<S sid="67" ssid="7">In this paper, we show that allowing richer interactions between the subtasks can benefit performance.</S>
			<S sid="68" ssid="8">2.</S>
			<S sid="69" ssid="9">Joint training and testing.</S>
			<S sid="70" ssid="10">In this family of ap-.</S>
			<S sid="71" ssid="11">proaches, a single model is trained to perform all the subtasks at once.</S>
			<S sid="72" ssid="12">For example, in Caruana?s workon multitask learning (Caruana, 1997), a neural network is trained to jointly performmultiple classification tasks, with hidden nodes that form a shared representation among the tasks.</S>
			<S sid="73" ssid="13">Jointly trained meth ods allow potentially the richest interaction between tasks, but can be expensive in both computation time required for training and in human effort required to label the joint training data.</S>
			<S sid="74" ssid="14">Exact inference in a jointly-trained model, suchas forward-backward in an FCRF, implicitly considers all possible subtask predictions with confidence given by the model?s probability of the pre diction.</S>
			<S sid="75" ssid="15">However, for computational efficiency, we can use inference methods such as particle filtering and sparse message-passing (Pal et al, 2005), which communicate only a limited number of predictions between sections of the model.</S>
			<S sid="76" ssid="16">Main Task Subtask A Subtask B Input Figure 1: Graphical model for the jointly-decoded CRF.</S>
			<S sid="77" ssid="17">All of the pairwise cliques also have links to the observed input, although we omit these edges in the diagram for clarity.</S>
			<S sid="78" ssid="18">3.</S>
			<S sid="79" ssid="19">Joint testing with cascaded training.</S>
			<S sid="80" ssid="20">Although a. joint model over all the subtasks can have better per formance, it is often much more expensive to train.</S>
			<S sid="81" ssid="21">One approach for reducing training time is cascadedtraining, which provides both computational efficiency and the ability to reuse large, standard train ing sets for the subtasks.</S>
			<S sid="82" ssid="22">At test time, though, theseparately-trained models are combined into a single model, so that joint decoding can propagate in formation between the tasks.Even with cascaded training, it is possible to pre serve some uncertainty in the subtask?s predictions.</S>
			<S sid="83" ssid="23">Instead of using only a single subtask predictionfor training the main task, the subtask can pass up wards a lattice of likely predictions, each of which is weighted by the model?s confidence.</S>
			<S sid="84" ssid="24">This has the advantage of making the training procedure more similar to the joint testing procedure, in which all possible subtask predictions are considered.</S>
			<S sid="85" ssid="25">In the next two sections, we describe and evaluate joint testing with cascaded training for transfer learning in linear-chain CRFs.</S>
			<S sid="86" ssid="26">At training time, only the bestsubtask prediction is used, without any confidence information.</S>
			<S sid="87" ssid="27">Even though this is perhaps the simplest joint testing/cascaded-training method, we show that it still leads to a significant gain in accuracy.</S>
	</SECTION>
	<SECTION title="Composition of CRFs. " number="5">
			<S sid="88" ssid="1">In this section we briefly describe how we combineindividually-trained linear-chain CRFs using composition.</S>
			<S sid="89" ssid="2">For a series of N cascaded tasks, we train indi vidual CRFs separately on each task, using the prediction of the previous CRF as a feature.</S>
			<S sid="90" ssid="3">We index the CRFs by i, so that the state of CRF i at time t is denoted sit.</S>
			<S sid="91" ssid="4">Thus, the feature functions for CRF i are of the form f ik(s i t?1, s i t, s i?1 t ,x, t)?that is, they depend not only on the observed input x and the transition (sit?1 ? s i t) but 750 wt = w wt matches [A-Z][a-z]+ wt matches [A-Z][A-Z]+ wt matches [A-Z] wt matches [A-Z]+ wt matches [A-Z]+[a-z]+[A-Z]+[a-z] wt appears in list of first names, last names, honorifics, etc. wt appears to be part of a time followed by a dash wt appears to be part of a time preceded by a dash wt appears to be part of a date Tt = T qk(x, t + ?) for all k and ? ?</S>
			<S sid="92" ssid="5">[?4, 4] Table 1: Input features qk(x, t) for the seminars data.</S>
			<S sid="93" ssid="6">In the above wt is the word at position t, Tt is the POS tag at position t, w ranges over all words in the training data, and T ranges over all Penn Treebank part-of-speech tags.</S>
			<S sid="94" ssid="7">The ?appears to be?</S>
			<S sid="95" ssid="8">features are based on hand-designed regular expressions that can span several tokens.</S>
			<S sid="96" ssid="9">also on the state si?1t of the previous transducer.</S>
			<S sid="97" ssid="10">We also add all conjunctions of the input features and the previous transducer?s state, for example, a feature thatis 1 if the current state is SPEAKERNAME, the previous transducer predicted PERSONNAME, and the previ ous word is Host:.</S>
			<S sid="98" ssid="11">To perform joint decoding at test time, we form thecomposition of the individual CRFs, viewed as finite state transducers.</S>
			<S sid="99" ssid="12">That is, we define a new linear-chain CRF whose state space is the cross product of the states of the individual CRFs, and whose transition costs are the sum of the transition costs of the individual CRFs.</S>
			<S sid="100" ssid="13">Formally, let S1, S2, . . .</S>
			<S sid="101" ssid="14">SN be the state sets and ?1,?2, . . .?N the weights of the individual CRFs.</S>
			<S sid="102" ssid="15">Then the state set of the combined CRF is S = S1?S2?</S>
			<S sid="103" ssid="16">SN . We will denote weight k in an individual CRF i by ?ik and a single feature by f i k(s i t?1, s i t, s i?1 t ,x, t).</S>
			<S sid="104" ssid="17">Then for s ? S, the combined model is given by: p(s|x) = ? t exp {?N i=1 ? k ? i kf i k(s i t?1, s i t, s i?1 t ,x, t) } Z(x) .</S>
			<S sid="105" ssid="18">(7)The graphical model for the combined model is the fac torial CRF in Figure 1.</S>
	</SECTION>
	<SECTION title="Experiments. " number="6">
			<S sid="106" ssid="1">6.1 Email Seminar Announcements.</S>
			<S sid="107" ssid="2">We evaluate joint decoding on a collection of 485 e-mailmessages announcing seminars at Carnegie Mellon Uni versity, gathered by Freitag (1998).</S>
			<S sid="108" ssid="3">The messages are annotated with the seminar?s starting time, ending time,location, and speaker.</S>
			<S sid="109" ssid="4">This data set has been the subject of much previous work using a wide variety of learn ing methods.</S>
			<S sid="110" ssid="5">Despite all this work, however, the best 50 100 150 200 250 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Number of training instances F1 JointCascaded Figure 2: Learning curves for the seminars data set on the speaker field, averaged over 10-fold cross validation.</S>
			<S sid="111" ssid="6">Joint training performs equivalently to cascaded decoding with 25% more data.</S>
			<S sid="112" ssid="7">reported systems have precision and recall on speaker names of only about 70%?too low to use in a practical system.</S>
			<S sid="113" ssid="8">This task is so challenging because the messagesare written by many different people, who each have dif ferent ways of presenting the announcement information.Because the task includes finding locations and person names, the output of a named-entity tagger is a useful feature.</S>
			<S sid="114" ssid="9">It is not a perfectly indicative feature, how ever, because many other kinds of person names appear in seminar announcements?for example, names of faculty hosts, departmental secretaries, and sponsors of lecture series.</S>
			<S sid="115" ssid="10">For example, the token Host: indicates strongly both that what follows is a person name, but that person is not the seminars?</S>
			<S sid="116" ssid="11">speaker.Even so, named-entity predictions do improve per formance on this task.</S>
			<S sid="117" ssid="12">We use the predictions from a CRF named-entity tagger that we trained on the standard CoNLL 2003 English data set.</S>
			<S sid="118" ssid="13">The CoNLL 2003 data set consists of newswire articles from Reuters labeled as either people, locations, organizations, or miscellaneousentities.</S>
			<S sid="119" ssid="14">It is much larger than the seminar announce ments data set.</S>
			<S sid="120" ssid="15">While the named-entity data contains 203,621 tokens for training, the seminar announcementsdata set contains only slightly over 60,000 training to kens.Previous work on the seminars data has used a one field-per-document evaluation.</S>
			<S sid="121" ssid="16">That is, for each field, the CRF selects a single field value from its Viterbi path, and this extraction is counted as correct if it exactly matchesany of the true field mentions in the document.</S>
			<S sid="122" ssid="17">We com pute precision and recall following this convention, and report their harmonic mean F1.</S>
			<S sid="123" ssid="18">As in the previous work, 751 System stime etime location speaker overall WHISK (Soderland, 1999) 92.6 86.1 66.6 18.3 65.9 SRV (Freitag, 1998) 98.5 77.9 72.7 56.3 76.4 HMM (Frietag &amp; McCallum, 1999) 98.5 62.1 78.6 76.6 78.9 RAPIER (Califf &amp; Mooney, 1999) 95.9 94.6 73.4 53.1 79.3 SNOW-IE (Roth &amp; Wen-tau Yih, 2001) 99.6 96.3 75.2 73.8 86.2 (LP)2 (Ciravegna, 2001) 99.0 95.5 75.0 77.6 86.8 CRF (no transfer) This paper 99.1 97.3 81.0 73.7 87.8 CRF (cascaded) This paper 99.2 96.0 84.3 74.2 88.4 CRF (joint) This paper 99.1 96.0 85.3 76.3 89.2 Table 2: Comparison of F1 performance on the seminars data.</S>
			<S sid="124" ssid="19">Joint decoding performs significantly better than cascaded decoding.</S>
			<S sid="125" ssid="20">The overall column is the mean of the other four.</S>
			<S sid="126" ssid="21">(This table was adapted from Peshkin and Pfeffer (2003).)</S>
			<S sid="127" ssid="22">we use 10-fold cross validation with a 50/50 training/test split.</S>
			<S sid="128" ssid="23">We use a spherical Gaussian prior on parameters with variance ?2 = 0.5.</S>
			<S sid="129" ssid="24">We evaluate whether joint decoding with cascadedtraining performs better than cascaded training and de coding.</S>
			<S sid="130" ssid="25">Table 2 compares cascaded and joint decoding for CRFs with other previous results from the literature.1The features we use are listed in Table 1.</S>
			<S sid="131" ssid="26">Although previ ous work has used very different feature sets, we include a no-transfer CRF baseline to assess the impact of transfer from the CoNLL data set.</S>
			<S sid="132" ssid="27">All the CRF runs used exactly the same features.</S>
			<S sid="133" ssid="28">On the most challenging fields, location and speaker, cascaded transfer is more accurate than no transfer at all,and joint decoding is more accurate than cascaded decod ing.</S>
			<S sid="134" ssid="29">In particular, for speaker, we see an error reductionof 8% by using joint decoding over cascaded.</S>
			<S sid="135" ssid="30">The difference in F1 between cascaded and joint decoding is statis tically significant for speaker (paired t-test; p = 0.017) but only marginally significant for location (p = 0.067).Our results are competitive with previous work; for ex ample, on location, the CRF is more accurate than any of the existing systems.</S>
			<S sid="136" ssid="31">Examining the trained models, we can observe both errors made by the general-purpose named entity tagger,and how they can be corrected by considering the sem inars labels.</S>
			<S sid="137" ssid="32">In newswire text, long runs of capitalized words are rare, often indicating the name of an entity.</S>
			<S sid="138" ssid="33">Inemail announcements, runs of capitalized words are com mon in formatted text blocks like: Location: Baker Hall Host: Michael Erdmann In this type of situation, the named entity tagger oftenmistakes Host: for the name of an entity, especially be cause the word precedingHost is also capitalized.</S>
			<S sid="139" ssid="34">On one of the cross-validated testing sets, of 80 occurrences of1We omit one relevant paper (Peshkin &amp; Pfeffer, 2003) be cause its evaluation method differs from all the other previous work.</S>
			<S sid="140" ssid="35">wt = w wt matches [A-Z][a-z]+ wt matches [A-Z][A-Z]+ wt matches [A-Z] wt matches [A-Z]+ wt matches [A-Z]+[a-z]+[A-Z]+[a-z] wt is punctuation wt appears in list of first names, last names, honorifics, etc. qk(x, t + ?) for all k and ? ?</S>
			<S sid="141" ssid="36">[?2, 2] Conjunction qk(x, t) and qk?(x, t) for all features k, k ? Conjunction qk(x, t) and qk?(x, t + 1) for all features k, k ? Table 3: Input features qk(x, t) for the ACE named-entity data.</S>
			<S sid="142" ssid="37">In the above wt is the word at position t, and w ranges over all words in the training data.</S>
			<S sid="143" ssid="38">the wordHost:, the named-entity tagger labels 52 as some kind of entity.</S>
			<S sid="144" ssid="39">When joint decoding is used, however, only 20 occurrences are labeled as entities.</S>
			<S sid="145" ssid="40">Recall thatthe joint model uses exactly the same weights as the cas caded model; the only difference is that the joint model takes into account information about the seminar labels when choosing named-entity labels.</S>
			<S sid="146" ssid="41">This is an example of how domain-specific information from the main taskcan improve performance on a more standard, general purpose subtask.</S>
			<S sid="147" ssid="42">Figure 2 shows the difference in performance between joint and cascaded decoding as a function of training set size.</S>
			<S sid="148" ssid="43">Cascaded decoding with the full training set of 242 emails performs equivalently to joint decoding on only 181 training instances, a 25% reduction in the training set.</S>
			<S sid="149" ssid="44">In summary, even with a simple cascaded trainingmethod on a well-studied data set, joint decoding per forms better for transfer than cascaded decoding.</S>
			<S sid="150" ssid="45">6.2 Entity Recognition.</S>
			<S sid="151" ssid="46">In this section we give results on joint decoding for transfer between two newswire data sets with similar but over lapping label sets.</S>
			<S sid="152" ssid="47">The Automatic Content Extraction (ACE) data set is another standard entity recognition data 752 Transfer Type none cascaded joint Person name 81.0 86.9 87.3 Person nominal 34.9 36.1 42.4 Organization name 53.9 62.6 61.1 Organization nominal 33.7 35.3 40.8 GPE name 78.5 84.0 84.0 GPE nominal 51.2 54.1 59.2 Table 4: Comparison of F1 performance between joint and cascaded training on the ACE entity recognition task.</S>
			<S sid="153" ssid="48">GPE means geopolitical entities, such as countries.</S>
			<S sid="154" ssid="49">Joint decoding helps most on the harder nominal (common noun) references.</S>
			<S sid="155" ssid="50">These results were obtained using a small subset of the training set.</S>
			<S sid="156" ssid="51">set, containing 422 stories from newspaper, newswire,and broadcast news.</S>
			<S sid="157" ssid="52">Unlike the CoNLL entity recog nition data set, in which only proper names of entities are annotated, the ACE data includes annotation both fornamed entities like United States, and also nominal men tions of entities like the nation.</S>
			<S sid="158" ssid="53">Thus, although the input text has similar distribution in the CoNLL NER and ACE data set, the label distributions are very different.Current state-of-the-art systems for the ACE task (Flo rian et al, 2004) use the predictions of other named-entityrecognizers as features, that is, they use cascaded transfer.</S>
			<S sid="159" ssid="54">In this experiment, we test whether the transfer be tween these datasets can be further improved using joint decoding.</S>
			<S sid="160" ssid="55">We train a CRF entity recognizer on the ACEdataset, with the output of a named-entity entity recog nizer trained on the CoNLL 2003 English data set.</S>
			<S sid="161" ssid="56">The CoNLL recognizer is the same CRF as was used in the previous experiment.</S>
			<S sid="162" ssid="57">In these results, we use a subset of 10% of the ACE training data.</S>
			<S sid="163" ssid="58">Table 3 lists the featureswe use.</S>
			<S sid="164" ssid="59">Table 4 compares the results on some represen tative entity types.</S>
			<S sid="165" ssid="60">Again, cascaded decoding for transfer is better than no transfer at al, and joint decoding is better than cascaded decoding.</S>
			<S sid="166" ssid="61">Interestingly, joint decoding has most impact on the harder nominal references, showing marked improvement over the cascaded approach.</S>
	</SECTION>
	<SECTION title="Related Work. " number="7">
			<S sid="167" ssid="1">Researchers have begun to accumulate experimental evidence that joint training and decoding yields better performance than the cascaded approach.</S>
			<S sid="168" ssid="2">As mentioned ear lier, the original work on dynamic CRFs (Sutton et al, 2004) demonstrated improvement due to joint training in the domains of part-of-speech tagging and noun-phrase chunking.</S>
			<S sid="169" ssid="3">Also, Carreras and Marquez (Carreras &amp; Ma`rquez, 2004) have obtained increased performance in clause finding by training a cascade of perceptrons to minimize a single global error function.</S>
			<S sid="170" ssid="4">Finally, Miller et al.</S>
			<S sid="171" ssid="5">(Miller et al, 2000) have combined entity recognition,parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved per formance on all the subtasks.Part of the contribution of the current work is to sug gest that joint decoding can be effective even when jointtraining is not possible because jointly-labeled data is unavailable.</S>
			<S sid="172" ssid="6">For example, Miller et al report that they orig inally attempted to annotate newswire articles for all of parsing, relations, and named entities, but they stoppedbecause the annotation was simply too expensive.</S>
			<S sid="173" ssid="7">In stead they hand-labeled relations only, assigning parse trees to the training set using a standard statistical parser,which is potentially less flexible than the cascaded train ing, because the model for main task is trained explicitly to match the noisy subtask predictions, rather than being free to correct them.In the speech community, it is common to com pose separately trained weighted finite-state transducers(Mohri et al, 2002) for joint decoding.</S>
			<S sid="174" ssid="8">Our method extends this work to conditional models.</S>
			<S sid="175" ssid="9">Ordinarily, higherlevel transducers depend only on the output of the previous transducer: a transducer for the lexicon, for exam ple, consumes only phonemes, not the original speechsignal.</S>
			<S sid="176" ssid="10">In text, however, such an approach is not sensi ble, because there is simply not enough information in the named-entity labels, for example, to do extraction if the original words are discarded.</S>
			<S sid="177" ssid="11">In a conditional model, weights in higher-level transducers are free to depend onarbitrary features of the original input without any addi tional complexity in the finite-state structure.Finally, stacked sequential learning (Cohen &amp; Car valho, 2005) is another potential method for combining the results of the subtask transducers.</S>
			<S sid="178" ssid="12">In this general meta-learning method for sequential classification, first a base classifier predicts the label at each time step, and then a higher-level classifier makes the final prediction, including as features a window of predictions from thebase classifier.</S>
			<S sid="179" ssid="13">For transfer learning, this would correspond to having an independent base model for each subtask (e.g., independent CRFs for named-entity and sem inars), and then having a higher-level CRF that includes as a feature the predictions from the base models.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="8">
			<S sid="180" ssid="1">In this paper we have shown that joint decoding improves transfer between interdependent NLP tasks, even when the old task is named-entity recognition, for which highly accurate systems exist.</S>
			<S sid="181" ssid="2">The rich features afforded by aconditional model allow the new task to influence the pre 753 dictions of the old task, an effect that is only possible with joint decoding.</S>
			<S sid="182" ssid="3">It is now common for researchers to publicly release trained models for standard tasks such as part-of-speechtagging, named-entity recognition, and parsing.</S>
			<S sid="183" ssid="4">This paper has implications for how such standard tools are pack aged.</S>
			<S sid="184" ssid="5">Our results suggest that off-the-shelf NLP tools will need not only to provide a single-best prediction, but also to be engineered so that they can easily communicate distributions over predictions to models for higher-level tasks.</S>
			<S sid="185" ssid="6">AcknowledgmentsThis work was supported in part by the Center for Intelligent In formation Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grants #IIS-0326249 and #IIS-0427594, and in part by the Defense Advanced Research Projects Agency (DARPA),through the Department of the Interior, NBC, Acquisition Ser vices Division, under contract number NBCHD030010.</S>
			<S sid="186" ssid="7">Anyopinions, findings and conclusions or recommendations ex pressed in this material are the author(s) and do not necessarily reflect those of the sponsor.</S>
	</SECTION>
</PAPER>
