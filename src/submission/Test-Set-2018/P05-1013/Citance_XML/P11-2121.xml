<PAPER>
  <S sid="0">Getting the Most out of Transition-based Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper suggests two ways of improving transition-based, non-projective dependency parsing.</S>
    <S sid="2" ssid="2">First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed.</S>
    <S sid="3" ssid="3">Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features.</S>
    <S sid="4" ssid="4">The new addition to the algorithm shows a clear advantage in parsing speed.</S>
    <S sid="5" ssid="5">The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010).</S>
    <S sid="7" ssid="2">There are two main dependency parsing approaches (Nivre and McDonald, 2008).</S>
    <S sid="8" ssid="3">One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003).</S>
    <S sid="9" ssid="4">The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005).</S>
    <S sid="10" ssid="5">Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 approach (Cer et al., 2010).</S>
    <S sid="11" ssid="6">The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2) for non-projective parsing (Nivre, 2008).</S>
    <S sid="12" ssid="7">The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing.</S>
    <S sid="13" ssid="8">Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009).</S>
    <S sid="14" ssid="9">This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed.</S>
    <S sid="15" ssid="10">One other advantage of the transition-based approach is that it can use parse history as features to make the next prediction.</S>
    <S sid="16" ssid="11">This parse information helps to improve parsing accuracy without hurting parsing complexity (Nivre, 2006).</S>
    <S sid="17" ssid="12">Most current transition-based approaches use gold-standard parses as features during training; however, this is not necessarily what parsers encounter during decoding.</S>
    <S sid="18" ssid="13">Thus, it is desirable to minimize the gap between gold-standard and automatic parses for the best results.</S>
    <S sid="19" ssid="14">This paper improves the engineering of different aspects of transition-based, non-projective dependency parsing.</S>
    <S sid="20" ssid="15">To reduce the search space, we add a transition to an existing non-projective parsing algorithm.</S>
    <S sid="21" ssid="16">To narrow down the discrepancies between gold-standard and automatic parses, we present a bootstrapping technique.</S>
    <S sid="22" ssid="17">The new addition to the algorithm shows a clear advantage in parsing speed.</S>
    <S sid="23" ssid="18">The bootstrapping technique gives a significant improvement to parsing accuracy.</S>
  </SECTION>
  <SECTION title="2 Reducing search space" number="2">
    <S sid="24" ssid="1">Our algorithm is based on Choi-Nicolov&#8217;s approach to Nivre&#8217;s list-based algorithm (Nivre, 2008).</S>
    <S sid="25" ssid="2">The main difference between these two approaches is in their implementation of the SHIFT transition.</S>
    <S sid="26" ssid="3">ChoiNicolov&#8217;s approach divides the SHIFT transition into two, deterministic and non-deterministic SHIFT&#8217;s, and trains the non-deterministic SHIFT with a classifier so it can be predicted during decoding.</S>
    <S sid="27" ssid="4">Choi and Nicolov (2009) showed that this implementation reduces the parsing complexity from O(n2) to linear time in practice (a worst-case complexity is O(n2)).</S>
    <S sid="28" ssid="5">We suggest another transition-based parsing approach that reduces the search space even more.</S>
    <S sid="29" ssid="6">The idea is to merge transitions in Choi-Nicolov&#8217;s non-projective algorithm with transitions in Nivre&#8217;s projective algorithm (Nivre, 2003).</S>
    <S sid="30" ssid="7">Nivre&#8217;s projective algorithm has a worst-case complexity of O(n), which is faster than any non-projective parsing algorithm.</S>
    <S sid="31" ssid="8">Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases.</S>
    <S sid="32" ssid="9">Ideally, it is better to perform projective parsing for most cases and perform non-projective parsing only when it is needed.</S>
    <S sid="33" ssid="10">In this algorithm, we add another transition to Choi-Nicolov&#8217;s approach, LEFT-POP, similar to the LEFT-ARC transition in Nivre&#8217;s projective algorithm.</S>
    <S sid="34" ssid="11">By adding this transition, an oracle can now choose either projective or non-projective parsing depending on parsing states.1 Note that Nivre (2009) has a similar idea of performing projective and non-projective parsing selectively.</S>
    <S sid="35" ssid="12">That algorithm uses a SWAP transition to reorder tokens related to non-projective dependencies, and runs in linear time in practice (a worst-case complexity is still O(n2)).</S>
    <S sid="36" ssid="13">Our algorithm is distinguished in that it does not require such reordering.</S>
    <S sid="37" ssid="14">Table 1 shows transitions used in our algorithm.</S>
    <S sid="38" ssid="15">All parsing states are represented as tuples (&#955;1, &#955;2, &#946;, E), where &#955;1, &#955;2, and &#946; are lists of word tokens.</S>
    <S sid="39" ssid="16">E is a set of labeled edges representing previously identified dependencies.</S>
    <S sid="40" ssid="17">L is a dependency label and i, j, k represent indices of their corresponding word tokens.</S>
    <S sid="41" ssid="18">The initial state is ([0], [ ], [1,...,n], &#8709;).</S>
    <S sid="42" ssid="19">The 0 identifier corresponds to an initial token, w0, introduced as the root of the sentence.</S>
    <S sid="43" ssid="20">The final state is (&#955;1, &#955;2, [ ], E), i.e., the algorithm terminates when all tokens in &#946; are consumed.</S>
    <S sid="44" ssid="21">The algorithm uses five kinds of transitions.</S>
    <S sid="45" ssid="22">All transitions are performed by comparing the last token in &#955;1, wi, and the first token in &#946;, wj.</S>
    <S sid="46" ssid="23">Both LEFT-POPL and LEFT-ARCL are performed when wj is the head of wi with a dependency relation L. The difference is that LEFT-POP removes wi from &#955;1 after the transition, assuming that the token is no longer needed in later parsing states, whereas LEFTARC keeps the token so it can be the head of some token wj&lt;k&lt;n in &#946;.</S>
    <S sid="47" ssid="24">This wi &#8594; wk relation causes a non-projective dependency.</S>
    <S sid="48" ssid="25">RIGHT-ARCL is performed when wi is the head of wj with a dependency relation L. SHIFT is performed when &#955;1 is empty (DT) or there is no token in &#955;1 that is either the head or a dependent of wj (NT).</S>
    <S sid="49" ssid="26">NO-ARC is there to move tokens around so each token in &#946; can be compared to all (or some) tokens prior to it.</S>
    <S sid="50" ssid="27">During training, the algorithm checks for the preconditions of all transitions and generates training instances with corresponding labels.</S>
    <S sid="51" ssid="28">During decoding, the oracle decides which transition to perform based on the parsing states.</S>
    <S sid="52" ssid="29">With the addition of LEFT-POP, the oracle can choose either projective or non-projective parsing by selecting LEFT-POP or LEFT-ARC, respectively.</S>
    <S sid="53" ssid="30">Our experiments show that this additional transition improves both parsing accuracy and speed.</S>
    <S sid="54" ssid="31">The advantage derives from improving the efficiency of the choice mechanism; it is now simply a transition choice and requires no additional processing.</S>
  </SECTION>
  <SECTION title="3 Bootstrapping automatic parses" number="3">
    <S sid="55" ssid="1">Transition-based parsing has the advantage of using parse history as features to make the next prediction.</S>
    <S sid="56" ssid="2">In our algorithm, when wi and wj are compared, subtree and head information of these tokens is partially provided by previous parsing states.</S>
    <S sid="57" ssid="3">Graphbased parsing can also take advantage of using parse information.</S>
    <S sid="58" ssid="4">This is done by performing &#8216;higherorder parsing&#8217;, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006).</S>
    <S sid="59" ssid="5">The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding.</S>
    <S sid="60" ssid="6">This can confuse a statistical model trained only on the gold-standard trees.</S>
    <S sid="61" ssid="7">To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses.</S>
    <S sid="62" ssid="8">First, we train a statistical model using goldstandard trees.</S>
    <S sid="63" ssid="9">Then, we parse the training data using the statistical model.</S>
    <S sid="64" ssid="10">During parsing, we extract features for each parsing state, consisting of automatic parse information, and generate a training instance by joining the features with the goldstandard label.</S>
    <S sid="65" ssid="11">The gold-standard label is achieved by comparing the dependency relation between wz and wj in the gold-standard tree.</S>
    <S sid="66" ssid="12">When the parsing is done, we train a different model using the training instances induced by the previous model.</S>
    <S sid="67" ssid="13">We repeat the procedure until a stopping criteria is met.</S>
    <S sid="68" ssid="14">The stopping criteria is determined by performing cross-validation.</S>
    <S sid="69" ssid="15">For each stage, we perform crossvalidation to check if the average parsing accuracy on the current cross-validation set is higher than the one from the previous stage.</S>
    <S sid="70" ssid="16">We stop the procedure when the parsing accuracy on cross-validation sets starts decreasing.</S>
    <S sid="71" ssid="17">Our experiments show that this simple bootstrapping technique gives a significant improvement to parsing accuracy.</S>
  </SECTION>
  <SECTION title="4 Related work" number="4">
    <S sid="72" ssid="1">Daum&#180;e et al. (2009) presented an algorithm, called SEARN, for integrating search and learning to solve complex structured prediction problems.</S>
    <S sid="73" ssid="2">Our bootstrapping technique can be viewed as a simplified version of SEARN.</S>
    <S sid="74" ssid="3">During training, SEARN iteratively creates a set of new cost-sensitive examples using a known policy.</S>
    <S sid="75" ssid="4">In our case, the new examples are instances containing automatic parses induced by the previous model.</S>
    <S sid="76" ssid="5">Our technique is simplified because the new examples are not cost-sensitive.</S>
    <S sid="77" ssid="6">Furthermore, SEARN interpolates the current policy with the previous policy whereas we do not perform such interpolation.</S>
    <S sid="78" ssid="7">During decoding, SEARN generates a sequence of decisions and makes a final prediction.</S>
    <S sid="79" ssid="8">In our case, the decisions are predicted dependency relations and the final prediction is a dependency tree.</S>
    <S sid="80" ssid="9">SEARN has been successfully adapted to several NLP tasks such as named entity recognition, syntactic chunking, and POS tagging.</S>
    <S sid="81" ssid="10">To the best of our knowledge, this is the first time that this idea has been applied to transition-based parsing and shown promising results.</S>
    <S sid="82" ssid="11">Zhang and Clark (2008) suggested a transitionbased projective parsing algorithm that keeps B different sequences of parsing states and chooses the one with the best score.</S>
    <S sid="83" ssid="12">They use beam search and show a worst-case parsing complexity of O(n) given a fixed beam size.</S>
    <S sid="84" ssid="13">Similarly to ours, their learning mechanism using the structured perceptron algorithm involves training on automatically derived parsing states that closely resemble potential states encountered during decoding.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="85" ssid="1">All models are trained and tested on English and Czech data using automatic lemmas, POS tags, and feats, as distributed by the CoNLL&#8217;09 shared task (Haji&#711;c et al., 2009).</S>
    <S sid="86" ssid="2">We use Liblinear L2-L1 SVM for learning (L2 regularization, L1 loss; Hsieh et al. (2008)).</S>
    <S sid="87" ssid="3">For our experiments, we use the following learning parameters: c = 0.1 (cost), e = 0.1 (termination criterion), B = 0 (bias).</S>
    <S sid="88" ssid="4">First, we evaluate the impact of the LEFT-POP transition we add to Choi-Nicolov&#8217;s approach.</S>
    <S sid="89" ssid="5">To make a fair comparison, we implemented both approaches and built models using the exact same feature set.</S>
    <S sid="90" ssid="6">The &#8216;CN&#8217; and &#8216;Our&#8217; rows in Table 3 show accuracies achieved by Choi-Nicolov&#8217;s and our approaches, respectively.</S>
    <S sid="91" ssid="7">Our approach shows higher accuracies for all categories.</S>
    <S sid="92" ssid="8">Next, we evaluate the impact of our bootstrapping technique.</S>
    <S sid="93" ssid="9">The &#8216;Our+&#8217; row shows accuracies achieved by our algorithm using the bootstrapping technique.</S>
    <S sid="94" ssid="10">The improvement from &#8216;Our&#8217; to &#8216;Our+&#8217; is statistically significant for all categories (McNemar, p &lt; .0001).</S>
    <S sid="95" ssid="11">The improvment is even more significant in a language like Czech for which parsers generally perform more poorly.</S>
    <S sid="96" ssid="12">Finally, we compare our work against other state-ofthe-art systems.</S>
    <S sid="97" ssid="13">For the CoNLL&#8217;09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (&#8216;Merlo&#8217;), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (&#8216;Bohnet&#8217;).</S>
    <S sid="98" ssid="14">Our approach shows quite comparable results with these systems.3 Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4).</S>
    <S sid="99" ssid="15">&#8216;Nivre&#8217; is Nivre&#8217;s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org).</S>
    <S sid="100" ssid="16">The other approaches are implemented in our open source project, called ClearParser (code. google.com/p/clearparser).</S>
    <S sid="101" ssid="17">Note that features used in MaltParser have not been optimized for these evaluation sets.</S>
    <S sid="102" ssid="18">All experiments are tested on an Intel Xeon 2.57GHz machine.</S>
    <S sid="103" ssid="19">For generalization, we run five trials for each parser, cut off the top and bottom speeds, and average the middle three.</S>
    <S sid="104" ssid="20">The loading times for machine learning models are excluded because they are independent from the parsing algorithms.</S>
    <S sid="105" ssid="21">The average parsing speeds are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre, CN, and Our+, respectively.</S>
    <S sid="106" ssid="22">Our approach shows linear growth all along, even for the sentence groups where some approaches start showing curves.</S>
    <S sid="107" ssid="23">We also measured average parsing speeds for &#8216;Our&#8217;, which showed a very similar growth to &#8216;Our+&#8217;.</S>
    <S sid="108" ssid="24">The average parsing speed of &#8216;Our&#8217; was 2.20 ms; it performed slightly faster than &#8216;Our+&#8217; because it skipped more nodes by performing more non-deterministic SHIFT&#8217;s, which may or may not have been correct decisions for the corresponding parsing states.</S>
    <S sid="109" ssid="25">It is worth mentioning that the curve shown by &#8216;Nivre&#8217; might be caused by implementation details regarding feature extraction, which we included as part of parsing.</S>
    <S sid="110" ssid="26">To abstract away from these implementation details and focus purely on the algorithms, we would need to compare the actual number of transitions performed by each parser, which will be explored in future work.</S>
  </SECTION>
  <SECTION title="6 Conclusion and future work" number="6">
    <S sid="111" ssid="1">We present two ways of improving transition-based, non-projective dependency parsing.</S>
    <S sid="112" ssid="2">The additional transition gives improvements to both parsing speed and accuracy, showing a linear time parsing speed with respect to sentence length.</S>
    <S sid="113" ssid="3">The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the-art performance with respect to other parsing approaches.</S>
    <S sid="114" ssid="4">In the future, we will test the robustness of these approaches in more languages.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="115" ssid="1">We gratefully acknowledge the support of the National Science Foundation Grants CISE-IIS-RI-0910992, Richer Representations for Machine Translation, a subcontract from the Mayo Clinic and Harvard Children&#8217;s Hospital based on a grant from the ONC, 90TR0002/01, Strategic Health Advanced Research Project Area 4: Natural Language Processing, and a grant from the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.</S>
    <S sid="116" ssid="2">HR0011-06-C-0022, subcontract from BBN, Inc. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</S>
  </SECTION>
</PAPER>
