<PAPER>
  <S sid="0">Corrective Modeling For Non-Projective Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers.</S>
    <S sid="2" ssid="2">The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees.</S>
    <S sid="3" ssid="3">Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser.</S>
    <S sid="4" ssid="4">Our model, based on a MaxEnt classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Statistical parsing models have been shown to be successful in recovering labeled constituencies (Collins, 2003; Charniak and Johnson, 2005; Roark and Collins, 2004) and have also been shown to be adequate in recovering dependency relationships (Collins et al., 1999; Levy and Manning, 2004; Dubey and Keller, 2003).</S>
    <S sid="6" ssid="2">The most successful models are based on lexicalized probabilistic context free grammars (PCFGs) induced from constituencybased treebanks.</S>
    <S sid="7" ssid="3">The linear-precedence constraint of these grammars restricts the types of dependency structures that can be encoded in such trees.1 A shortcoming of the constituency-based paradigm for parsing is that it is inherently incapable of representing non-projective dependencies trees (we define non-projectivity in the following section).</S>
    <S sid="8" ssid="4">This is particularly problematic when parsing free wordorder languages, such as Czech, due to the frequency of sentences with non-projective constructions.</S>
    <S sid="9" ssid="5">In this work, we explore a corrective model which recovers non-projective dependency structures by training a classifier to select correct dependency pairs from a set of candidates based on parses generated by a constituency-based parser.</S>
    <S sid="10" ssid="6">We chose to use this model due to the observations that the dependency errors made by the parsers are generally local errors.</S>
    <S sid="11" ssid="7">For the nodes with incorrect dependency links in the parser output, the correct governor of a node is often found within a local context of the proposed governor.</S>
    <S sid="12" ssid="8">By considering alternative dependencies based on local deviations of the parser output we constrain the set of candidate governors for each node during the corrective procedure.</S>
    <S sid="13" ssid="9">We examine two state-of-the-art constituencybased parsers in this work: the Collins Czech parser (1999) and a version of the Charniak parser (2001) that was modified to parse Czech.</S>
    <S sid="14" ssid="10">Alternative efforts to recover dependency structure from English are based on reconstructing the movement traces encoded in constituency trees (Collins, 2003; Levy and Manning, 2004; Johnson, 2002; Dubey and Keller, 2003).</S>
    <S sid="15" ssid="11">In fact, the features we use in the current model are similar to those proposed by Levy and Manning (2004).</S>
    <S sid="16" ssid="12">However, the approach we propose discards the constituency structure prior to the modeling phase; we model corrective transformations of dependency trees.</S>
    <S sid="17" ssid="13">The technique proposed in this paper is similar to that of recent parser reranking approaches (Collins, 2000; Charniak and Johnson, 2005); however, while reranking approaches allow a parser to generate a likely candidate set according to a generative model, we consider a set of candidates based on local perturbations of the single most likely tree generated.</S>
    <S sid="18" ssid="14">The primary reason for such an approach is that we allow dependency structures which would never be hypothesized by the parser.</S>
    <S sid="19" ssid="15">Specifically, we allow for non-projective dependencies.</S>
    <S sid="20" ssid="16">The corrective algorithm proposed in this paper shares the motivation of the transformation-based learning work (Brill, 1995).</S>
    <S sid="21" ssid="17">We do consider local transformations of the dependency trees; however, the technique presented here is based on a generative model that maximizes the likelihood of good dependents.</S>
    <S sid="22" ssid="18">We consider a finite set of local perturbations of the tree and use a fixed model to select the best tree by independently choosing optimal dependency links.</S>
    <S sid="23" ssid="19">In the remainder of the paper we provide a definition of a dependency tree and the motivation for using such trees as well as a description of the particular dataset that we use in our experiments, the Prague Dependency Treebank (PDT).</S>
    <S sid="24" ssid="20">In Section 3 we describe the techniques used to adapt constituencybased parsers to train from and generate dependency trees.</S>
    <S sid="25" ssid="21">Section 4 describes corrective modeling as used in this work and Section 4.2 describes the particular features with which we have experimented.</S>
    <S sid="26" ssid="22">Section 5 presents the results of a set of experiments we performed on data from the PDT.</S>
  </SECTION>
  <SECTION title="2 Syntactic Dependency Trees and the Prague Dependency Treebank" number="2">
    <S sid="27" ssid="1">A dependency tree is a set of nodes Q = {w0, w1, ... , wk} where w0 is the imaginary root node2 and a set of dependency links G = {g1, ... , gk} where gi is an index into Q representing the governor of wi.</S>
    <S sid="28" ssid="2">In other words g3 = 1 indicates that the governor of w3 is w1.</S>
    <S sid="29" ssid="3">Finally, every node has exactly one governor except for w0, which has no governor (the tree constraints).3 The index of the nodes represents the surface order of the nodes in the sequence (i.e., wi precedes wj in the sentence if i &lt; j).</S>
    <S sid="30" ssid="4">A tree is projective if for every three nodes: wa, wb, and w, where a &lt; b &lt; c; if wa is governed by w, then wb is transitively governed by w, or if w, is governed by wa then wb is transitively governed by wa.4 Figure 1 shows examples of projective and non-projective trees.</S>
    <S sid="31" ssid="5">The rightmost tree, which is non-projective, contains a subtree consisting of wa and w, but not wb; however, wb occurs between wa and w, in the linear ordering of the nodes.</S>
    <S sid="32" ssid="6">Projectivity in a dependency tree is akin to the continuity constraint in a constituency tree; such a constraint is implicitly imposed by trees generated from context free grammars (CFGs).</S>
    <S sid="33" ssid="7">Strict word-order languages, such as English, exhibit non-projective dependency structures in a relatively constrained set of syntactic configurations (e.g., right-node raising).</S>
    <S sid="34" ssid="8">Traditionally, these movements are encoded in syntactic analyses as traces.</S>
    <S sid="35" ssid="9">In languages with free word-order, such as Czech, constituency-based representations are overly constrained (Sgall et al., 1986).</S>
    <S sid="36" ssid="10">Syntactic dependency trees encode syntactic subordination relationships allowing the structure to be non-specific about the underlying deep representation.</S>
    <S sid="37" ssid="11">The relationship between a node and its subordinates expresses a sense of syntactic (functional) entailment.</S>
    <S sid="38" ssid="12">In this work we explore the dependency structures encoded in the Prague Dependency Treebank (Haji&#711;c, 1998; B&#168;ohmov&#180;a et al., 2002).</S>
    <S sid="39" ssid="13">The PDT 1.0 analytical layer is a set of Czech syntactic dependency trees; the nodes of which contain the word forms, morphological features, and syntactic annotations.</S>
    <S sid="40" ssid="14">These trees were annotated by hand and are intended as an intermediate stage in the annotation of the Tectogrammatical Representation (TR), a deep-syntactic or syntacto-semantic theory of language (Sgall et al., 1986).</S>
    <S sid="41" ssid="15">All current automatic techniques for generating TR structures are based on syntactic dependency parsing.</S>
    <S sid="42" ssid="16">When evaluating the correctness of dependency trees, we only consider the structural relationships between the words of the sentence (unlabeled dependencies).</S>
    <S sid="43" ssid="17">However, the model we propose contains features that are considered part of the dependency rather than the nodes in isolation (e.g., agreement features).</S>
    <S sid="44" ssid="18">We do not propose a model for correctly labeling dependency structures in this work.</S>
  </SECTION>
  <SECTION title="3 Constituency Parsing for Dependency Trees" number="3">
    <S sid="45" ssid="1">A pragmatic justification for using constituencybased parsers in order to predict dependency structures is that currently the best Czech dependencytree parser is a constituency-based parser (Collins et al., 1999; Zeman, 2004).</S>
    <S sid="46" ssid="2">In fact both Charniak&#8217;s and Collins&#8217; generative probabilistic models contain lexical dependency features.5 From a generative modeling perspective, we use the constraints imposed by constituents (i.e., projectivity) to enable the encapsulation of syntactic substructures.</S>
    <S sid="47" ssid="3">This directly leads to efficient parsing algorithms such as the CKY algorithm and related agenda-based parsing algorithms (Manning and Sch&#168;utze, 1999).</S>
    <S sid="48" ssid="4">Additionally, this allows for the efficient computation of the scores for the dynamic-programming state variables (i.e., the inside and outside probabilities) that are used in efficient statistical parsers.</S>
    <S sid="49" ssid="5">The computational complexity advantages of dynamic programming techniques along with efficient search techniques (Caraballo and Charniak, 1998; Klein and Manning, 2003) allow for richer predictive models which include local contextual information.</S>
    <S sid="50" ssid="6">In an attempt to extend a constituency-based parsing model to train on dependency trees, Collins transforms the PDT dependency trees into constituency trees (Collins et al., 1999).</S>
    <S sid="51" ssid="7">In order to accomplish this task, he first normalizes the trees to remove non-projectivities.</S>
    <S sid="52" ssid="8">Then, he creates artificial constituents based on the parts-of-speech of the words associated with each dependency node.</S>
    <S sid="53" ssid="9">The mapping from dependency tree to constituency tree is not one-to-one.</S>
    <S sid="54" ssid="10">Collins describes a heuristic for choosing trees that work well with his parsing model.</S>
    <S sid="55" ssid="11">We consider two approaches to creating projective trees from dependency trees exhibiting nonprojectivities.</S>
    <S sid="56" ssid="12">The first is based on word-reordering and is the model that was used with the Collins parser.</S>
    <S sid="57" ssid="13">This algorithm identifies non-projective structures and deterministically reorders the words of the sentence to create projective trees.</S>
    <S sid="58" ssid="14">An alternative method, used by Charniak in the adaptation of his parser for Czech6 and used by Nivre and Nilsson (2005), alters the dependency links by raising the governor to a higher node in the tree whenever 5Bilexical dependencies are components of both the Collins and Charniak parsers and effectively model the types of syntactic subordination that we wish to extract in a dependency tree.</S>
    <S sid="59" ssid="15">(Bilexical models were also proposed by Eisner (Eisner, 1996)).</S>
    <S sid="60" ssid="16">In the absence of lexicalization, both parsers have dependency features that are encoded as head-constituent to sibling features.</S>
    <S sid="61" ssid="17">6This information was provided by Eugene Charniak in a personal communication. a non-projectivity is observed.</S>
    <S sid="62" ssid="18">The trees are then transformed into Penn Treebank style constituencies using the technique described in (Collins et al., 1999).</S>
    <S sid="63" ssid="19">Both of these techniques have advantages and disadvantages which we briefly outline here: Reordering The dependency structure is preserved, but the training procedure will learn statistics for structures over word-strings that may not be part of the language.</S>
    <S sid="64" ssid="20">The parser, however, may be capable of constructing parses for any string of words if a smoothed grammar is being used.</S>
    <S sid="65" ssid="21">Governor&#8211;Raising The dependency structure is corrupted leading the parser to incorporate arbitrary dependency statistics into the model.</S>
    <S sid="66" ssid="22">However, the parser is trained on true sentences, the words of which are in the correct linear order.</S>
    <S sid="67" ssid="23">We expect the parser to predict similar incorrect dependencies when sentences similar to the training data are observed.</S>
    <S sid="68" ssid="24">Although the results presented in (Collins et al., 1999) used the reordering technique, we have experimented with his parser using the governor&#8211;raising technique and observe an increase in dependency accuracy.</S>
    <S sid="69" ssid="25">For the remainder of the paper, we assume the governor&#8211;raising technique.</S>
    <S sid="70" ssid="26">The process of generating dependency trees from parsed constituency trees is relatively straightforward.</S>
    <S sid="71" ssid="27">Both the Collins and Charniak parsers provide head-word annotation on each constituent.</S>
    <S sid="72" ssid="28">This is precisely the information that we encode in an unlabeled dependency tree, so the dependency structure can simply be extracted from the parsed constituency trees.</S>
    <S sid="73" ssid="29">Furthermore, the constituency labels can be used to identify the dependency labels; however, we do not attempt to identify correct dependency labels in this work.</S>
    <S sid="74" ssid="30">We now discuss a quantitative measure for the types of dependency errors made by constituency-based parsing techniques.</S>
    <S sid="75" ssid="31">For node wi and the correct governor wgz the distance between the two nodes in the hypothesized dependency tree is: Ancestor, sibling, cousin, and descendant have the standard interpretation in the context of a tree.</S>
    <S sid="76" ssid="32">The dependency distance d(wi7 wgz) is the minimum number of dependency links traversed on the undirected path from wi to wgz in the hypothesized dependency tree.</S>
    <S sid="77" ssid="33">The definition of the dist function makes a distinction between paths through the parent of wi (positive values) and paths through children of wi (negative values).</S>
    <S sid="78" ssid="34">We found that a vast majority of the correct governors were actually hypothesized as siblings or grandparents (a dist values of 2) &#8211; an extreme local error.</S>
    <S sid="79" ssid="35">Figure 2 shows a histogram of the fraction of nodes whose correct governor was within a particular dist in the hypothesized tree.</S>
    <S sid="80" ssid="36">A dist of 1 indicates the correct governor was selected by the parser; in these graphs, the density at dist = 1 (on the x axis) shows the baseline dependency accuracy of each parser.</S>
    <S sid="81" ssid="37">Note that if we repaired only the nodes that are within a dist of 2 (grandparents and siblings), we can recover more than 50% of the incorrect dependency links (a raw accuracy improvement of up to 9%).</S>
    <S sid="82" ssid="38">We believe this distribution to be indirectly caused by the governor raising projectivization routine.</S>
    <S sid="83" ssid="39">In the cases where non-projective structures can be repaired by raising the node&#8217;s governor to its parent, the correct governor becomes a sibling of the node.</S>
  </SECTION>
  <SECTION title="4 Corrective Modeling" number="4">
    <S sid="84" ssid="1">The error analysis of the previous section suggests that by looking only at a local neighborhood of the proposed governor in the hypothesized trees, we can correct many of the incorrect dependencies.</S>
    <S sid="85" ssid="2">This fact motivates the corrective modeling procedure employed here.</S>
    <S sid="86" ssid="3">Table 1 presents the pseudo-code for the corrective procedure.</S>
    <S sid="87" ssid="4">The set gh contains the indices of governors as predicted by the parser.</S>
    <S sid="88" ssid="5">The set of governors predicted by the corrective procedure is denoted as g .</S>
    <S sid="89" ssid="6">The procedure independently corrects each node of the parsed trees meaning that there is potential for inconsistent governor relationships to exist in the proposed set; specifically, the resulting dependency graph may have cycles.</S>
    <S sid="90" ssid="7">We employ a greedy search to remove cycles when they are present in the output graph.</S>
    <S sid="91" ssid="8">The final line of the algorithm picks the governor in which we are most confident.</S>
    <S sid="92" ssid="9">We use the correctgovernor classification likelihood, P (g&#8727;i = j|wi, N(wghi )), as a measure of the confidence that wc is the correct governor of wi where the parser had proposed wghi as the governor.</S>
    <S sid="93" ssid="10">In effect, we create a decision list using the most likely decision if we can (i.e., there are no cycles).</S>
    <S sid="94" ssid="11">If the dependency graph resulting from the most likely decisions does not result in a tree, we use the decision lists to greedily select the tree for which the product of the independent decisions is maximal.</S>
    <S sid="95" ssid="12">Training the corrective model requires pairs of dependency trees; each pair contains a manuallyannotated tree (i.e., the gold standard tree) and a tree generated by the parser.</S>
    <S sid="96" ssid="13">This data is trivially transformed into per-node samples.</S>
    <S sid="97" ssid="14">For each node wi in the tree, there are |N(wgh i ) |samples; one for each governor candidate in the local neighborhood.</S>
    <S sid="98" ssid="15">One advantage to the type of corrective algorithm presented here is that it is completely disconnected from the parser used to generate the tree hypotheses.</S>
    <S sid="99" ssid="16">This means that the original parser need not be statistical or even constituency based.</S>
    <S sid="100" ssid="17">What is critical for this technique to work is that the distribution of dependency errors be relatively local as is the case with the errors made by the Charniak and Collins parsers.</S>
    <S sid="101" ssid="18">This can be determined via data analysis using the dist metric.</S>
    <S sid="102" ssid="19">Determining the size of the local neighborhood is data dependent.</S>
    <S sid="103" ssid="20">If subordinate nodes are considered as candidate governors, then a more robust cycle removal technique is be required.</S>
    <S sid="104" ssid="21">We have chosen a MaxEnt model to estimate the governor distributions, P(gz = j|wi,N(wghi )).</S>
    <S sid="105" ssid="22">In the next section we outline the feature set with which we have experimented, noting that the features are selected based on linguistic intuition (specifically for Czech).</S>
    <S sid="106" ssid="23">We choose not to factor the feature vector as it is not clear what constitutes a reasonable factorization of these features.</S>
    <S sid="107" ssid="24">For this reason we use the MaxEnt estimator which provides us with the flexibility to incorporate interdependent features independently while still optimizing for likelihood.</S>
    <S sid="108" ssid="25">The maximum entropy principle states that we wish to find an estimate of p(y|x) E C that maximizes the entropy over a sample set X for some set of observations Y , where x E X is an observation and y E Y is a outcome label assigned to that observation, The set C is the candidate set of distributions from which we wish to select p(y|x).</S>
    <S sid="109" ssid="26">We define this set as the p(y|x) that meets a feature-based expectation constraint.</S>
    <S sid="110" ssid="27">Specifically, we want the expected count of a feature, f(x, y), to be equivalent under the distribution p(y|x) and under the observed distribution &#732;p(y|x). capture correlations between observations and outcomes.</S>
    <S sid="111" ssid="28">In the following section, we describe a set of features with which we have experimented to determine when a word is likely to be the correct governor of another word.</S>
    <S sid="112" ssid="29">We incorporate the expected feature-count constraints into the maximum entropy objective using Lagrange multipliers (additionally, constraints are added to ensure the distributions p(y|x) are consistent probability distributions): Holding the &#945;i&#8217;s constant, we compute the unconstrained maximum of the above Lagrangian form: giving us the log-linear form of the distributions p(y|x) in C (Z is a normalization constant).</S>
    <S sid="113" ssid="30">Finally, we compute the &#945;i&#8217;s that maximize the objective function: A number of algorithms have been proposed to efficiently compute the optimization described in this derivation.</S>
    <S sid="114" ssid="31">For a more detailed introduction to maximum entropy estimation see (Berger et al., 1996).</S>
    <S sid="115" ssid="32">Given the above formulation of the MaxEnt estimation procedure, we define features over pairs of observations and outcomes.</S>
    <S sid="116" ssid="33">In our case, the observations are simply wi, wc, and N(wghi ) and the outcome is a binary variable indicating whether c = gz (i.e., wc is the correct governor).</S>
    <S sid="117" ssid="34">In order to limit the dimensionality of the feature space, we consider feature functions over the outcome, the current node wi, the candidate governor node wc and the node proposed as the governor by the parser wghi .</S>
    <S sid="118" ssid="35">Table 2 describes the general classes of features used.</S>
    <S sid="119" ssid="36">We write Fi to indicate the form of the current child node, Fc for the form of the candidate, and Fg as the form of the governor proposed by the parser.</S>
    <S sid="120" ssid="37">A combined feature is denoted as LiTc and indicates we observed a particular lemma for the current node with a particular tag of the candidate.</S>
    <S sid="121" ssid="38">Feature Type Id Description Form F the fully inflected word form as it appears in the data Lemma L the morphologically reduced lemma MTag T a subset of the morphological tag as described in (Collins et al., 1999) POS P major part-of-speech tag (first field of the morphological tag) ParserGov G true if candidate was proposed as governor by parser ChildCount C the number of children Agreement A(x, y) check for case/number agreement between word x and y In all models, we include features containing the form, the lemma, the morphological tag, and the ParserGov feature.</S>
    <S sid="122" ssid="39">We have experimented with different sets of feature combinations.</S>
    <S sid="123" ssid="40">Each combination set is intended to capture some intuitive linguistic correlation.</S>
    <S sid="124" ssid="41">For example, the feature component LiTc will fire if a particular child&#8217;s lemma Li is observed with a particular candidate&#8217;s morphological tag Tc.</S>
    <S sid="125" ssid="42">This feature is intended to capture phenomena surrounding particles; for example, in Czech, the governor of the reflexive particle se will likely be a verb.</S>
    <S sid="126" ssid="43">Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005).</S>
    <S sid="127" ssid="44">This allows for a deterministic procedure that undoes the projectivization in the generated parse trees, creating non-projective structures.</S>
    <S sid="128" ssid="45">This technique could be incorporated into a statistical parsing framework, however we believe the sparsity of such nonprojective configurations may be problematic when using smoothed backed-off grammars.</S>
    <S sid="129" ssid="46">We suspect that the deterministic procedure employed by Nivre and Nilsson enables their parser to greedily consider non-projective constructions when possible.</S>
    <S sid="130" ssid="47">This may also explain the relatively low overall performance of their parser.</S>
    <S sid="131" ssid="48">A primary difference between the Nivre and Nilsson approach and what we propose in this paper is that of determining the projectivization procedure.</S>
    <S sid="132" ssid="49">While we exploit particular side-effects of the projectivization procedure, we do not assume any particular algorithm.</S>
    <S sid="133" ssid="50">Additionally, we consider transformations for all dependency errors where their technique explicitly addresses non-projectivity errors.</S>
    <S sid="134" ssid="51">We mentioned above that our approach appears to be similar to that of reranking for statistical parsing (Collins, 2000; Charniak and Johnson, 2005).</S>
    <S sid="135" ssid="52">While it is true that we are improving upon the output of the automatic parser, we are not considering multiple alternate parses.</S>
    <S sid="136" ssid="53">Instead, we consider a complete set of alternate trees that are minimal perturbations of the best tree generated by the parser.</S>
    <S sid="137" ssid="54">In the context of dependency parsing, we do this in order to generate structures that constituency-based parsers are incapable of generating (i.e., non-projectivities).</S>
    <S sid="138" ssid="55">Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model).</S>
  </SECTION>
  <SECTION title="5 Empirical Results" number="5">
    <S sid="139" ssid="1">In this section we report results from experiments on the PDT Czech dataset.</S>
    <S sid="140" ssid="2">Approximately 1.9% of the words&#8217; dependencies are non-projective in version 1.0 of this corpus and these occur in 23.2% of the sentences (Haji&#711;cov&#180;a et al., 2004).</S>
    <S sid="141" ssid="3">We used the standard training, development, and evaluation datasets defined in the PDT documentation for all experiments.7 We use Zhang Lee&#8217;s implementation of the 7We have used PDT 1.0 (2002) data for the Charniak experiments and PDT 2.0 (2005) data for the Collins experiments.</S>
    <S sid="142" ssid="4">We use the most recent version of each parser; however we do not have a training program for the Charniak parser and have used the pretrained parser provided by Charniak; this was trained on the training section of the PDT 1.0.</S>
    <S sid="143" ssid="5">We train our model on the MaxEnt estimator using the L-BFGS optimization algorithms and Gaussian smoothing.8 Table 4 presents results on development data for the correction model with different feature sets.</S>
    <S sid="144" ssid="6">The features of the Simple model are the form (F), lemma (L), and morphological tag (M) for the each node, the parser-proposed governor node, and the candidate node; this model also contains the ParserGov feature.</S>
    <S sid="145" ssid="7">In the table&#8217;s following rows, we show the results for the simple model augmented with feature sets of the categories described in Table 2.</S>
    <S sid="146" ssid="8">Table 3 provides a short description of each of the models.</S>
    <S sid="147" ssid="9">As we believe the Simple model provides the minimum information needed to perform this task, Collins trees via a 20-fold Jackknife training procedure. we experimented with the feature-classes as additions to it.</S>
    <S sid="148" ssid="10">The final row of Table 4 contains results for the model which includes all features from all other models.</S>
    <S sid="149" ssid="11">We define NonP Accuracy as the accuracy for the nodes which were non-projective in the original trees.</S>
    <S sid="150" ssid="12">Although both the Charniak and the Collins parser can never produce non-projective trees, the baseline NonP accuracy is greater than zero.</S>
    <S sid="151" ssid="13">This is due to the parser making mistakes in the tree such that the originally non-projective node&#8217;s dependency is projective.</S>
    <S sid="152" ssid="14">Alternatively, we report the Non-Projective Precision and Recall for our experiment suite in Table 5.</S>
    <S sid="153" ssid="15">Here the numerator of the precision is the number of nodes that are non-projective in the correct tree and end up in a non-projective configuration; however, this new configuration may be based on incorrect dependencies.</S>
    <S sid="154" ssid="16">Recall is the obvious counterpart to precision.</S>
    <S sid="155" ssid="17">These values correspond to the NonP for Collins&#8217; and Charniak&#8217;s trees with and without the corrective model Finally, Table 6 shows the results of the full model run on the evaluation data for the Collins and Charniak parse trees.</S>
    <S sid="156" ssid="18">It appears that the Charniak parser fares better on the evaluation data than does the Collins parser.</S>
    <S sid="157" ssid="19">However, the corrective model is still successful at recovering non-projective structures.</S>
    <S sid="158" ssid="20">Overall, we see a significant improvement in the dependency accuracy.</S>
    <S sid="159" ssid="21">We have performed a review of the errors that the corrective process makes and observed that the model does a poor job dealing with punctuation.</S>
    <S sid="160" ssid="22">This is shown in Table 7 along with other types of nodes on which we performed well and poorly, respectively.</S>
    <S sid="161" ssid="23">Collins (1999) explicitly added features to his parser to improve punctuation dependency parsing accuracy.</S>
    <S sid="162" ssid="24">The PARSEVAL evaluation metmade by our model on trees from the Charniak parser. root is the artificial root node of the PDT tree.</S>
    <S sid="163" ssid="25">For each node position (child, proposed parent, and correct parent), the top five words are reported (based on absolute count of occurrences).</S>
    <S sid="164" ssid="26">The particle &#8216;se&#8217; occurs frequently explaining why it occurs in the top five good and top five bad repairs. ric for constituency-based parsing explicitly ignores punctuation in determining the correct boundaries of constituents (Harrison et al., 1991) and so should the dependency evaluation.</S>
    <S sid="165" ssid="27">However, the reported results include punctuation for comparative purposes.</S>
    <S sid="166" ssid="28">Finally, we show in Table 8 a coarse analysis of the corrective performance of our model.</S>
    <S sid="167" ssid="29">We are repairing more dependencies than we are corrupting.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="168" ssid="1">We have presented a Maximum Entropy-based corrective model for dependency parsing.</S>
    <S sid="169" ssid="2">The goal is to recover non-projective dependency structures that are lost when using state-of-the-art constituencybased parsers; we show that our technique recovers over 50% of these dependencies.</S>
    <S sid="170" ssid="3">Our algorithm provides a simple framework for corrective modeling of dependency trees, making no prior assumptions about the trees.</S>
    <S sid="171" ssid="4">However, in the current model, we focus on trees with local errors.</S>
    <S sid="172" ssid="5">Overall, our technique improves dependency parsing and provides the necessary mechanism to recover non-projective structures.</S>
  </SECTION>
</PAPER>
