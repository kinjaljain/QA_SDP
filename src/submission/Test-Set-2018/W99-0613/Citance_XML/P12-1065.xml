<PAPER>
  <S sid="0">Bootstrapping via Graph Propagation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them.</S>
    <S sid="2" ssid="2">This paper introduces a novel variant of the Yarowsky algorithm based on this view.</S>
    <S sid="3" ssid="3">It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function.</S>
    <S sid="4" ssid="4">The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">In this paper, we are concerned with a case of semisupervised learning that is close to unsupervised learning, in that the labelled and unlabelled data points are from the same domain and only a small set of seed rules is used to derive the labelled points.</S>
    <S sid="6" ssid="2">We refer to this setting as bootstrapping.</S>
    <S sid="7" ssid="3">In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain.</S>
    <S sid="8" ssid="4">The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998).</S>
    <S sid="9" ssid="5">In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995).</S>
    <S sid="10" ssid="6">Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself.</S>
    <S sid="11" ssid="7">We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010).</S>
    <S sid="12" ssid="8">It is theoretically *This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant.</S>
    <S sid="13" ssid="9">We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments.</S>
    <S sid="14" ssid="10">We particularly thank Michael Collins, Jason Eisner, and Damianos Karakos for the data we used in our experiments. x denotes an example f, g denote features i, k denote labels X set of training examples Fx set of features for example x Y current labelling of X Yx current label for example x well-understood as minimizing an objective function at each iteration, and it obtains state of the art performance on several different NLP data sets.</S>
    <S sid="15" ssid="11">To our knowledge, this is the first theoretically motivated self-training bootstrapping algorithm which performs as well as the Yarowsky algorithm.</S>
  </SECTION>
  <SECTION title="2 Bootstrapping" number="2">
    <S sid="16" ssid="1">Abney (2004) defines useful notation for semisupervised learning, shown in table 1.</S>
    <S sid="17" ssid="2">Note that A, V , etc. are relative to the current labelling Y .</S>
    <S sid="18" ssid="3">We additionally define F to be the set of all features, and use U to denote the uniform distribution.</S>
    <S sid="19" ssid="4">In the bootstrapping setting the learner is given an initial partial labelling Y (&#176;) where only a few examples are labelled (i.e.</S>
    <S sid="20" ssid="5">Y (&#176;) x = L for most x).</S>
    <S sid="21" ssid="6">Abney (2004) defines three probability distributions in his analysis of bootstrapping: Bfj is the parameter for feature f with label j, taken to be normalized so that Bf is a distribution over labels. ox is the labelling distribution representing the current Y ; it is a point distribution for labelled examples and uniform for unlabelled examples.</S>
    <S sid="22" ssid="7">7rx is the prediction distribution over labels for example x.</S>
    <S sid="23" ssid="8">The approach of Haghighi and Klein (2006b) and Haghighi and Klein (2006a) also uses a small set of seed rules but uses them to inject features into a joint model p(x, j) which they train using expectationmaximization for Markov random fields.</S>
    <S sid="24" ssid="9">We focus on discriminative training which does not require complex partition functions for normalization.</S>
    <S sid="25" ssid="10">Blum and Chawla (2001) introduce an early use of transductive learning using graph propagation.</S>
    <S sid="26" ssid="11">X. Zhu and Z. Ghahramani and J. Lafferty (2003)&#8217;s method of graph propagation is predominantly transductive, and the non-transductive version is closely related to Abney (2004) c.f.</S>
    <S sid="27" ssid="12">Haffari and Sarkar (2007).1</S>
  </SECTION>
  <SECTION title="3 Existing algorithms" number="3">
    <S sid="28" ssid="1">A decision list (DL) is a (ordered) list of featurelabel pairs (rules) which is produced by assigning a score to each rule and sorting on this score.</S>
    <S sid="29" ssid="2">It chooses a label for an example from the first rule whose feature is a feature of the example.</S>
    <S sid="30" ssid="3">For a DL the prediction distribution is defined by 7rx(j) &#8733; maxfEFx Bfj.</S>
    <S sid="31" ssid="4">The basic Yarowsky algorithm is shown in algorithm 1.</S>
    <S sid="32" ssid="5">Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores where c is a smoothing constant.</S>
    <S sid="33" ssid="6">When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g.</S>
    <S sid="34" ssid="7">(Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach.</S>
    <S sid="35" ssid="8">We focus on the formal analysis of the Yarowsky algorithm by Abney (2004).</S>
    <S sid="36" ssid="9">2It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations.</S>
    <S sid="37" ssid="10">The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm.</S>
    <S sid="38" ssid="11">Collins and Singer (1999) also introduce a variant algorithm Yarowsky-cautious.</S>
    <S sid="39" ssid="12">Here the DL training step keeps only the top n rules (f, j) over the threshold for each label j, ordered by |Af|.</S>
    <S sid="40" ssid="13">Additionally the threshold ( is checked against |Afj|/|Af |instead of the smoothed score. n begins at no and is incremented by An at each iteration.</S>
    <S sid="41" ssid="14">We add the seed DL to the new DL after applying the cautious pruning.</S>
    <S sid="42" ssid="15">Cautiousness limits not only the size of the DL but also the number of labelled examples, prioritizing decisions which are believed to be of high accuracy.</S>
    <S sid="43" ssid="16">At the final iteration Yarowsky-cautious uses the current labelling to train a DL without a threshold or cautiousness, and this DL is used for testing.</S>
    <S sid="44" ssid="17">We call this the retraining step.4 Collins and Singer (1999) also introduce the cotraining algorithm DL-CoTrain.</S>
    <S sid="45" ssid="18">This algorithm alternates between two DLs using disjoint views of the features in the data.</S>
    <S sid="46" ssid="19">At each step it trains a DL and then produces a new labelling for the other DL.</S>
    <S sid="47" ssid="20">Each DL uses thresholding and cautiousness as we describe for Yarowsky-cautious.</S>
    <S sid="48" ssid="21">At the end the DLs are combined, the result is used to label the data, and a retraining step is done from this single labelling.</S>
    <S sid="49" ssid="22">One of the variant algorithms of Abney (2004) is Y-1/DL-1-VS (referred to by Haffari and Sarkar (2007) as simply DL-1).</S>
    <S sid="50" ssid="23">Besides various changes in the specifics of how the labelling is produced, this algorithm has two differences versus Yarowsky.</S>
    <S sid="51" ssid="24">Firstly, the smoothing constant c in (1) is replaced by 1/|Vf|.</S>
    <S sid="52" ssid="25">Secondly, 7r is redefined as 7rx(j) _ B , which we refer to as the sum definition of 7r.</S>
    <S sid="53" ssid="26">This definition does not match a literal DL but is easier to analyze.</S>
    <S sid="54" ssid="27">We are not concerned here with the details of Y-1/DL-1-VS, but we note that Haffari and Sarkar (2007) provide an objective function for this algorithm using a generalized definition of crossentropy in terms of Bregman distance, which motivates our objective in section 4.</S>
    <S sid="55" ssid="28">The Bregman distance between two discrete probability distributions p and q is defined as BV,(p, q) = Ei [0(pi) &#8722; 0(qi) &#8722; 0'(qi)(pi &#8722; qi)].</S>
    <S sid="56" ssid="29">As a specific case we have Bt2(p, q) = Ei(pi &#8722; qi)2 = ||p &#8722; q||2.</S>
    <S sid="57" ssid="30">Then Bregman distance-based entropy is Ht2(p) = &#8722; Ei p2i, KL-Divergence is Bt2, and cross-entropy follows the standard definition in terms of Ht2 and Bt2.</S>
    <S sid="58" ssid="31">The objective minimized by Y-1/DL-1-VS is: As a baseline for the sum definition of 7r, we introduce the Yarowsky-sum algorithm.</S>
    <S sid="59" ssid="32">It is the same as Yarowsky except that we use the sum definition when labelling: for example x we choose the label j with the highest (sum) 7rx(j), but set Yx = 1 if the sum is zero.</S>
    <S sid="60" ssid="33">Note that this is a linear model similar to a conditional random field (CRF) (Lafferty et al., 2001) for unstructured multiclass problems.</S>
    <S sid="61" ssid="34">Haffari and Sarkar (2007) suggest a bipartite graph framework for semi-supervised learning based on their analysis of Y-1/DL-1-VS and objective (2).</S>
    <S sid="62" ssid="35">The graph has vertices X U F and edges {(x, f) : x E X, f E Fx}, as in the graph shown in figure 1(a).</S>
    <S sid="63" ssid="36">Each vertex represents a distribution over labels, and in this view Yarowsky can be seen as alternately updating the example distributions based on the feature distributions and visa versa.</S>
    <S sid="64" ssid="37">Based on this they give algorithm 2, which we call HS-bipartite.</S>
    <S sid="65" ssid="38">It is parametrized by two functions which are called features-to-example and examples-to-feature here.</S>
    <S sid="66" ssid="39">Each can be one of two choices: average(S) is the normalized average of the distributions of S, while majority(S) is a uniform distribution if all labels are supported by equal numbers of distributions of S, and otherwise a point distribution with mass on the best supported label.</S>
    <S sid="67" ssid="40">The average-majority form is similar to Y-1/DL-1-VS, and the majority-majority form minimizes a different objective similar to (2).</S>
    <S sid="68" ssid="41">In our implementation we label training data (for the convergence check) with the 0 distributions from the graph.</S>
    <S sid="69" ssid="42">We label test data by constructing new Ox = examples-to-feature(Fx) for the unseen x. ramanya et al. (2010) Subramanya et al.</S>
    <S sid="70" ssid="43">(2010) give a semi-supervised algorithm for part of speech tagging.</S>
    <S sid="71" ssid="44">Unlike the algorithms described above, it is for domain adaptation with large amounts of labelled data rather than bootstrapping with a small number of seeds.</S>
    <S sid="72" ssid="45">This algorithm is structurally similar to Yarowsky in that it begins from an initial partial labelling and repeatedly trains a classifier on the labelling and then relabels the data.</S>
    <S sid="73" ssid="46">It uses a CRF (Lafferty et al., 2001) as the underlying supervised learner.</S>
    <S sid="74" ssid="47">It differs significantly from Yarowsky in two other ways: First, instead of only training a CRF it also uses a step of graph propagation between distributions over the n-grams in the data.</S>
    <S sid="75" ssid="48">Second, it does the propagation on distributions over n-gram types rather than over n-gram tokens (instances in the data).</S>
    <S sid="76" ssid="49">They argue that using propagation over types allows the algorithm to enforce constraints and find similarities that self-training cannot.</S>
    <S sid="77" ssid="50">We are not concerned here with the details of this algorithm, but it motivates our work firstly in providing the graph propagation which we will describe in more detail in section 4, and secondly in providing an algorithmic structure that we use for our algorithm in section 5.</S>
    <S sid="78" ssid="51">We implemented the EM algorithm of Collins and Singer (1999) as a baseline for the other algorithms.</S>
    <S sid="79" ssid="52">They do not specify tuning details, but to get comparable accuracy we found it was necessary to do smoothing and to include weights A1 and A2 on the expected counts of seed-labelled and initially unlabelled examples respectively (Nigam et al., 2000).</S>
  </SECTION>
  <SECTION title="4 Graph propagation" number="4">
    <S sid="80" ssid="1">The graph propagation of Subramanya et al. (2010) is a method for smoothing distributions attached to vertices of a graph.</S>
    <S sid="81" ssid="2">Here we present it with an alternate notation using Bregman distances as described in section 3.4.5 The objective is where V is a set of vertices, N(v) is the neighbourhood of vertex v, and qv is an initial distribution for each vertex v to be smoothed.</S>
    <S sid="82" ssid="3">They give an iterative update to minimize (3).</S>
    <S sid="83" ssid="4">Note that (3) is independent of their specific graph structure, distributions, and semi-supervised learning algorithm.</S>
    <S sid="84" ssid="5">We propose four methods for using this propagation with Yarowsky.</S>
    <S sid="85" ssid="6">These methods all use constant edge weights (wuv = 1).</S>
    <S sid="86" ssid="7">The distributions and graph structures are shown in table 2.</S>
    <S sid="87" ssid="8">Figure 1 shows example graphs for 0-B and B-only.</S>
    <S sid="88" ssid="9">7r-B and BT-only are similar, and are described below.</S>
    <S sid="89" ssid="10">The graph structure of 0-B is the bipartite graph of Haffari and Sarkar (2007).</S>
    <S sid="90" ssid="11">In fact, 0-B the propagation objective (3) and Haffari and Sarkar (2007)&#8217;s Y-1/DL-1-VS objective (2) are identical up to constant coefficients and an extra constant term.6 0-B 5We omit the option to hold some of the distributions at fixed values, which would add an extra term to the objective.</S>
    <S sid="91" ssid="12">6The differences are specifically: First, (3) adds the constant coefficients &#181; and v. Second, (3) sums over each edge twice (once in each direction), while (2) sums over each only once.</S>
    <S sid="92" ssid="13">Since wuv = wvu and Bt2(qu, qv) = Bt2(qv, qu), this can be folded into the constant &#181;.</S>
    <S sid="93" ssid="14">Third, after expanding (2) there is a term |Fx |inside the sum for Ht2(0x) which is not present in (3).</S>
    <S sid="94" ssid="15">This does not effect the direction of minimization.</S>
    <S sid="95" ssid="16">Fourth, Bt2(qu, U) in (3) expands to Ht2(qu) plus a constant, adding an extra constant term to the total. therefore gives us a direct way to optimize (2).</S>
    <S sid="96" ssid="17">The other three methods do not correspond to the objective of Haffari and Sarkar (2007).</S>
    <S sid="97" ssid="18">The 7r-B method is like 0-B except for using 7r as the distribution for example vertices.</S>
    <S sid="98" ssid="19">The bipartite graph of the first two methods differs from the structure used by Subramanya et al. (2010) in that it does propagation between two different kinds of distributions instead of only one kind.</S>
    <S sid="99" ssid="20">We also adopt a more comparable approach with a graph over only features.</S>
    <S sid="100" ssid="21">Here we define adjacency by co-occurrence in the same example.</S>
    <S sid="101" ssid="22">The B-only method uses this graph and B as the distribution.</S>
    <S sid="102" ssid="23">Finally, we noted in section 3.7 that the algorithm of Subramanya et al. (2010) does one additional step in converting from token level distributions to type level distributions.</S>
    <S sid="103" ssid="24">The BT-only method therefore uses the feature-only graph but for the distribution uses a type level version of B defined by</S>
  </SECTION>
  <SECTION title="5 Novel Yarowsky-prop algorithm" number="5">
    <S sid="104" ssid="1">We call our graph propagation based algorithm Yarowsky-prop.</S>
    <S sid="105" ssid="2">It is shown with BT-only propagation in algorithm 3.</S>
    <S sid="106" ssid="3">It is based on the Yarowsky algorithm, with the following changes: an added step to calculate BT (line 4), an added step to calculate BP (line 5), the use of BP rather than the DL to update the labelling (line 6), and the use of the sum definition of 7r.</S>
    <S sid="107" ssid="4">Line 7 does DL training as we describe in sections 3.1 and 3.2.</S>
    <S sid="108" ssid="5">Propagation is done with the iterative update of Subramanya et al. (2010).</S>
    <S sid="109" ssid="6">This algorithm is adapted to the other propagation methods described in section 4 by changing the type of propagation on line 5.</S>
    <S sid="110" ssid="7">In B-only, propagation is done on B, using the graph of figure 1(b).</S>
    <S sid="111" ssid="8">In 0-B and 7r-B propagation is done on the respective bipartite graph (figure 1(a) or the equivalent with 7r).</S>
    <S sid="112" ssid="9">Line 4 is skipped for these methods, and 0 is as defined in section 2.</S>
    <S sid="113" ssid="10">For the bipartite graph methods 0-B and 7r-B only the propagated B values on the feature nodes are used for BP (the distributions on the example nodes are ignored after the propagation itself).</S>
    <S sid="114" ssid="11">The algorithm uses Bfj values rather than an explicit DL for labelling.</S>
    <S sid="115" ssid="12">The (pre-normalized) score for any (f, j) not in the DL is taken to be zero.</S>
    <S sid="116" ssid="13">Besides using the sum definition of 7r when calculating BT, we also use a sum in labelling.</S>
    <S sid="117" ssid="14">When labelling an example x (at line 6 and also on testing data) we the sum is zero.</S>
    <S sid="118" ssid="15">Ignoring uniform BPf values is intended to provide an equivalent to the DL behaviour of using evidence only from rules that are in the list.</S>
    <S sid="119" ssid="16">We include the cautiousness of Yarowskycautious (section 3.2) in the DL training on line 7.</S>
    <S sid="120" ssid="17">At the labelling step on line 6 we label only examples which the pre-propagated B would also assign a label (using the same rules described above for BP).</S>
    <S sid="121" ssid="18">This choice is intended to provide an equivalent to the Yarowsky-cautious behaviour of limiting the number of labelled examples; most BPf are non-uniform, so without it most examples become labelled early.</S>
    <S sid="122" ssid="19">We observe further similarity between the Yarowsky algorithm and the general approach of Subramanya et al. (2010) by comparing algorithm 3 here with their algorithm 1.</S>
    <S sid="123" ssid="20">The comments in algorithm 3 give the corresponding parts of their algorithm.</S>
    <S sid="124" ssid="21">Note that each line has a similar purpose.</S>
  </SECTION>
  <SECTION title="6 Evaluation" number="6">
    <S sid="125" ssid="1">For evaluation we use the tasks of Collins and Singer (1999) and Eisner and Karakos (2005), with data kindly provided by the respective authors.</S>
    <S sid="126" ssid="2">The task of Collins and Singer (1999) is named entity classification on data from New York Times text.7 The data set was pre-processed by a statistical parser (Collins, 1997) and all noun phrases that are potential named entities were extracted from the parse tree.</S>
    <S sid="127" ssid="3">Each noun phrase is to be labelled as a person, organization, or location.</S>
    <S sid="128" ssid="4">The parse tree provides the surrounding context as context features such as the words in prepositional phrase and relative clause modifiers, etc., and the actual words in the noun phrase provide the spelling features.</S>
    <S sid="129" ssid="5">The test data additionally contains some noise examples which are not in the three named entity categories.</S>
    <S sid="130" ssid="6">We use the seed rules the authors provide, which are the first seven items in figure 2.</S>
    <S sid="131" ssid="7">For DL-CoTrain, we use their two views: one view is the spelling features, and the other is the context features.</S>
    <S sid="132" ssid="8">Figure 2 shows a DL from Yarowsky training on this task.</S>
    <S sid="133" ssid="9">The tasks of Eisner and Karakos (2005) are word sense disambiguation on several English words which have two senses corresponding to two different words in French.</S>
    <S sid="134" ssid="10">Data was extracted from the Canadian Hansards, using the English side to produce training and test data and the French side to produce the gold labelling.</S>
    <S sid="135" ssid="11">Features are the original and lemmatized words immediately adja7We removed weekday and month examples from the test set as they describe.</S>
    <S sid="136" ssid="12">They note 88962 examples in their training set, but the file has 89305.</S>
    <S sid="137" ssid="13">We did not find any filtering criteria that produced the expected size, and therefore used all examples. cent to the word to be disambiguated, and original and lemmatized context words in the same sentence.</S>
    <S sid="138" ssid="14">Their seeds are pairs of adjacent word features, with one feature for each label (sense).</S>
    <S sid="139" ssid="15">We use the &#8216;drug&#8217;, &#8216;land&#8217;, and &#8216;sentence&#8217; tasks, and the seed rules from their best seed selection: &#8216;alcohol&#8217;/&#8216;medical&#8217;, &#8216;acres&#8217;/&#8216;court&#8217;, and &#8216;reads&#8217;/&#8216;served&#8217; respectively (they do not provide seeds for their other three tasks).</S>
    <S sid="140" ssid="16">For DL-CoTrain we use adjacent words for one view and context words for the other.</S>
    <S sid="141" ssid="17">Where applicable we use smoothing c = 0.1, a threshold ( = 0.95, and cautiousness parameters n0 = An = 5 as in Collins and Singer (1999) and propagation parameters &#181; = 0.6, v = 0.01 as in Subramanya et al. (2010).</S>
    <S sid="142" ssid="18">Initial experiments with different propagation parameters suggested that as long as v was set at this value changing &#181; had relatively little effect on the accuracy.</S>
    <S sid="143" ssid="19">We did not find any propagation parameter settings that outperformed this choice.</S>
    <S sid="144" ssid="20">For the Yarowsky-prop algorithms we perform a single iteration of the propagation update for each iteration of the algorithm.</S>
    <S sid="145" ssid="21">For EM we use weights A1 = 0.98, and A2 = 0.02 (see section 3.8), which were found in initial experiments to be the best values, and results are averaged over 10 random initializations.</S>
    <S sid="146" ssid="22">The named entity test set contains some examples that are neither person, organization, nor location.</S>
    <S sid="147" ssid="23">Collins and Singer (1999) define noise accuracy as accuracy that includes such instances, and clean accuracy as accuracy calculated across only the examples which are one of the known labels.</S>
    <S sid="148" ssid="24">We report only clean accuracy in this paper; noise accuracy tracks clean accuracy but is a little lower.</S>
    <S sid="149" ssid="25">There is no difference on the word sense data sets.</S>
    <S sid="150" ssid="26">We also report (clean) non-seeded accuracy, which we define to be clean accuracy over only examples which are not assigned a label by the seed rules.</S>
    <S sid="151" ssid="27">This is intended to evaluate what the algorithm has learned, rather than what it can achieve by using the input information directly (Daume, 2011).</S>
    <S sid="152" ssid="28">We test Yarowsky, Yarowsky-cautious, Yarowsky-sum, DL-CoTrain, HS-bipartite in all four forms, and Yarowsky-prop cautious and non-cautious and in all four forms.</S>
    <S sid="153" ssid="29">For each algorithm except EM we perform a final retraining step as described for Yarowsky-cautious (section 3.2).</S>
    <S sid="154" ssid="30">Our programs and experiment scripts have been made available.8 Table 3 shows the final test set accuracies for the all the algorithms.</S>
    <S sid="155" ssid="31">The seed DL accuracy is also included for reference.</S>
    <S sid="156" ssid="32">The best performing form of our novel algorithm is Yarowsky-prop-cautious 0-only.</S>
    <S sid="157" ssid="33">It numerically outperforms DL-CoTrain on the named entity task, is not (statistically) significantly worse on the drug and land tasks, and is significantly better on the sentence task.</S>
    <S sid="158" ssid="34">It also numerically outperforms Yarowsky-cautious on the named entity task and is significantly better on the drug task.</S>
    <S sid="159" ssid="35">Is significantly worse on the land task, where most algorithms converge at labelling all examples with the first sense.</S>
    <S sid="160" ssid="36">It is significantly worse on the sentence task, although it is the second best performing algorithm and several percent above DL-CoTrain on that task.</S>
    <S sid="161" ssid="37">Figure 3 shows (all) three examples from the named entity test set where Yarowsky-prop-cautious 0-only is correct but none of the other Yarowsky variants are.</S>
    <S sid="162" ssid="38">Note that it succeeds despite misleading features; &#8220;maker&#8221; and &#8220;company&#8221; might be taken to indicate a company and &#8220;president-of&#8221; an organization, but all three examples are locations.</S>
    <S sid="163" ssid="39">Yarowsky-prop-cautious 0-0 and 7r-0 also perform respectably, although not as well.</S>
    <S sid="164" ssid="40">Yarowskyprop-cautious 0T-only and the non-cautious versions are significantly worse.</S>
    <S sid="165" ssid="41">Although 0T-only was intended to incorporate Subramanya et al. (2010)&#8217;s idea of type level distributions, it in fact performs worse than 0-only.</S>
    <S sid="166" ssid="42">We believe that Collins and Singer (1999)&#8217;s definition (1) of 0 incorporates sufficient type level information that the creation of a separate distribution is unnecessary in this case.</S>
    <S sid="167" ssid="43">Figure 4 shows the test set non-seeded accuracies as a function of the iteration for many of the algorithms on the named entity task.</S>
    <S sid="168" ssid="44">The Yarowsky-prop non-cautious algorithms quickly converge to the final accuracy and are not shown.</S>
    <S sid="169" ssid="45">While the other algorithms (figure 4(a)) make a large accuracy improvement in the final retraining step, the Yarowskyprop (figure 4(b)) algorithms reach comparable accuracies earlier and gain much less from retraining.</S>
    <S sid="170" ssid="46">We did not implement Collins and Singer (1999)&#8217;s CoBoost; however, in their results it performs comparably to DL-CoTrain and Yarowsky-cautious.</S>
    <S sid="171" ssid="47">As with DL-CoTrain, CoBoost requires two views.</S>
    <S sid="172" ssid="48">Cautiousness appears to be important in the performance of the algorithms we tested.</S>
    <S sid="173" ssid="49">In table 3, only the cautious algorithms are able to reach the 90% accuracy range.</S>
    <S sid="174" ssid="50">To evaluate the effects of cautiousness we examine the Yarowsky-prop O-only algorithm on the named entity task in more detail.</S>
    <S sid="175" ssid="51">This algorithm has two classifiers which are trained in conjunction: the DL and the propagated OP.</S>
    <S sid="176" ssid="52">Figure 5 shows the training set coverage (of the labelling on line 6 of algorithm 3) and the test set accuracy of both classifiers, for the cautious and non-cautious versions.</S>
    <S sid="177" ssid="53">The non-cautious version immediately learns a DL over all feature-label pairs, and therefore has full coverage after the first iteration.</S>
    <S sid="178" ssid="54">The DL and OP converge to similar accuracies within a few more iterations, and the retraining step increases accuracy by less than one percent.</S>
    <S sid="179" ssid="55">On the other hand, the cautious version gradually increases the coverage over the iterations.</S>
    <S sid="180" ssid="56">The DL accuracy follows the coverage closely (similar to the behaviour of Yarowskycautious, not shown here), while the propagated classifier accuracy jumps quickly to near 90% and then increases only gradually.</S>
    <S sid="181" ssid="57">Although the DL prior to retraining achieves a roughly similar accuracy in both versions, only the cautious version is able to reach the 90% accuracy range in the propagated classifier and retraining.</S>
    <S sid="182" ssid="58">Presumably the non-cautious version makes an early mistake, reaching a local minimum which it cannot escape.</S>
    <S sid="183" ssid="59">The cautious version avoids this by making only safe rule selection and labelling choices.</S>
    <S sid="184" ssid="60">Figure 5(b) also helps to clarify the difference in retraining that we noted in section 6.3.</S>
    <S sid="185" ssid="61">Like the non-propagated DL algorithms, the DL component of Yarowsky-prop has much lower accuracy than the propagated classifier prior to the retraining step.</S>
    <S sid="186" ssid="62">But after retraining, the DL and OP reach very similar accuracies.</S>
    <S sid="187" ssid="63">The propagation method 0-0 was motivated by optimizing the equivalent objectives (2) and (3) at each iteration.</S>
    <S sid="188" ssid="64">Figure 6 shows the graph propagation objective (3) along with accuracy for Yarowsky-prop 0-0 without cautiousness.</S>
    <S sid="189" ssid="65">The objective value decreases as expected, and converges along with accuracy.</S>
    <S sid="190" ssid="66">Conversely, the cautious version (not shown here) does not clearly minimize the objective, since cautiousness limits the effect of the propagation.</S>
  </SECTION>
  <SECTION title="7 Conclusions" number="7">
    <S sid="191" ssid="1">N Our novel algorithm achieves accuracy comparable to Yarowsky-cautious, but is better theoretically motivated by combining ideas from Haffari and Sarkar (2007) and Subramanya et al. (2010).</S>
    <S sid="192" ssid="2">It also achieves accuracy comparable to DL-CoTrain, but does not require the features to be split into two independent views.</S>
    <S sid="193" ssid="3">As future work, we would like to apply our algorithm to a structured task such as part of speech tagging.</S>
    <S sid="194" ssid="4">We also believe that our method for adapting Collins and Singer (1999)&#8217;s cautiousness to Yarowsky-prop can be applied to similar algorithms with other underlying classifiers, even to structured output models such as conditional random fields.</S>
  </SECTION>
</PAPER>
